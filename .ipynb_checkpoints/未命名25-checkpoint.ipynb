{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e16bba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import Trainer\n",
    "from network import NFM\n",
    "import torch.utils.data as Data\n",
    "from Utils.criteo_loader import getTestData, getTrainData\n",
    "\n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':9,\n",
    "    'linear_hidden1':1000,\n",
    "    #'linear_hidden':100,#线性模型输出层（隐层个数）\n",
    "    'embed_input_dim':1001,#embed输入维度\n",
    "    'embed_dim': 100, # 用于控制稀疏特征经过Embedding层后的稠密特征大小，embed输出维度\n",
    "    #'dnn_hidden_units': [100,11],#MLP隐层和输出层\n",
    "    'linear1_drop':0.5,\n",
    "    'dnn_hidden_units':[100,9],#MLP隐层\n",
    "    'dnn_layer_units':[100,9],\n",
    "    'num_sparse_features_cols':10477,#the number of the gene columns\n",
    "    'num_dense_features': 0,#dense features number\n",
    "    'bi_dropout': 0.3,#Bi-Interaction 的dropout\n",
    "    'num_epoch': 500,#训练epoch次数\n",
    "    'batch_size': 24,#batch_size\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    'epoch':1000,\n",
    "    \n",
    "    #'train_file': '../Data/criteo/processed_data/train_set.csv',\n",
    "    #'fea_file': '../Data/criteo/processed_data/fea_col.npy',\n",
    "    #'validate_file': '../Data/criteo/processed_data/val_set.csv',\n",
    "    #'test_file': '../Data/criteo/processed_data/test_set.csv',\n",
    "    #'model_name': '../TrainedModels/NFM.model'\n",
    "    #'train_file':'data/xiaoqiu_gene_5000/train/final_5000_encode_100x.csv',\n",
    "    'train_data':'dataset/qiuguan/encode/encode_1000/train/train_encode_data_1000_new.csv',\n",
    "    'train_label':'dataset/qiuguan/non_code/train/train_label.csv',\n",
    "    #'test_data':'dataset/qiuguan/non_code/test/test_encode_data.csv',\n",
    "    'test_data':'dataset/qiuguan/encode/encode_1000/test/test_encode_data_1000_new.csv',\n",
    "    'test_label':'dataset/qiuguan/non_code/test/test_labels.csv'\n",
    "    #'title':'dataset/xiaoguan/RF/RF_for_train/train_class_9/test/test_data.csv',\n",
    "    \n",
    "    #'all':''\n",
    "    #'title':'data/xiaoqiu_gene_5000/train/gene_5000_gene_name.csv',\n",
    "    #'all':'data/xiaoqiu_gene_5000/train/gene_5000_label_name.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25de7fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#准备训练集\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "#from utils import \n",
    "#准备训练集\n",
    "#from new_dataset_processed import FMData\n",
    "from dataset_process import FMData\n",
    "def prepare_dataset(m_data,m_label,batch_size,n_class):\n",
    "    m_dataset=FMData(m_data,m_label,n_class)\n",
    "    m_dataloader=data.DataLoader(m_dataset, drop_last=True,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "    \n",
    "    return m_dataset,m_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e3d09ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import sys \n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "import torchmetrics\n",
    "def   train_data(model,train_loader,test_loader,batch_size,model_path):\n",
    "    #train_accuracy=torchmetrics.Accuracy()\n",
    "    #test_accuracy=torchmetrics.Accuracy()\n",
    "    BATCH_SIZE=batch_size\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    total = 0\n",
    "    \n",
    "    #loss_func = torch.nn.BCELoss()\n",
    "    loss_func=torch.nn.CrossEntropyLoss()\n",
    "    #loss_func=nn.MultiLabelSoftMarginLoss()\n",
    "    #loss_func=torch.nn.LogSoftmax()\n",
    "    num=2000\n",
    "    #model=nn.Softmax(nn.Linear(10149,16)).to(device)\n",
    "    # 从DataLoader中获取小批量的id以及数据\n",
    "    \n",
    "    batch_size=0\n",
    "    for epoch_id in range(1000):\n",
    "        correct=0\n",
    "        total=0\n",
    "        total_test_acc=0\n",
    "        total_train_accuracy=0\n",
    "        for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "            x = Variable(x)\n",
    "            labels = Variable(labels)\n",
    "            \n",
    "            \n",
    "            #x = torch.tensor(x, dtype=torch.float)\n",
    "            #x=x.clone().detach().requires_grad_(True)\n",
    "            x=torch.tensor(x,dtype=torch.float)\n",
    "            labels=torch.tensor(labels,dtype=torch.float)\n",
    "            x, labels = x.cuda(), labels.cuda()\n",
    "            labels_int=labels=torch.max(labels,1)[1]\n",
    "            #labels_int.cuda()\n",
    "            #print(\"labels:\",labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_predict = model(x)\n",
    "            #print(\"y_predict:\",y_predict.dtype)\n",
    "            #print(\"y_predict:\",y_predict)\n",
    "            #loss = loss_func(y_predict.view(-1), labels)\n",
    "            loss = loss_func(y_predict, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss = loss.item()\n",
    "            #loss, predicted = self._train_single_batch(x, labels)\n",
    "\n",
    "            total += loss\n",
    "            \n",
    "            \n",
    "            #predicted = torch.max(y_predict.data,1)\n",
    "            #print(\"predicted:\",predicted)\n",
    "            #predicted = torch.max(y_predict.data,1)[1]\n",
    "            #predicted=predicted.detach().cpu().numpy()\n",
    "            \n",
    "            #labels=torch.max(labels,1)\n",
    "            #print(\"labels:\",labels)\n",
    "            #labels=labels[1]\n",
    "            #y_predict=y_predict.cuda()\n",
    "            #labels=torch.max(labels,1)[1].cuda()\n",
    "            #labels=labels.detach().cpu().numpy()\n",
    "            #correct += (predicted == labels).sum()\n",
    "            #print(\"correct:\",correct)\n",
    "            #correct=correct[0]\n",
    "            #print(\"new_correct:\",float(correct))\n",
    "            #correct=float(correct)   \n",
    "            #if batch_idx % 10 == 0:\n",
    "            #print(\"batch_idx:\",batch_idx)\n",
    "            #print(correct/(BATCH_SIZE*(batch_idx+1)))\n",
    "            batch_train_acc=torchmetrics.functional.accuracy(y_predict,labels_int)\n",
    "            #print('batch_train_acc:',batch_train_acc)\n",
    "            total_train_accuracy+=batch_train_acc\n",
    "        #total_train_accuracy=torchmetrics.functional.compute_details()\n",
    "        total_train_accuracy/=(batch_idx+1)\n",
    "        print('total_train_accuracy:',total_train_accuracy)\n",
    "        #print('total_train_accuracy:',total_train_accuracy)\n",
    "        for i , (inputs , targets) in enumerate(test_loader):   \n",
    "            print(\"test\")\n",
    "            # evaluate the model on the test set   \n",
    "            #print(\\ inputs:\\  inputs)   \n",
    "            #print(\\ targets:\\  targets)   \n",
    "            inputs = Variable(inputs)   \n",
    "            targets = Variable(targets)     \n",
    "            #x = torch.tensor(x  dtype=torch.float)   \n",
    "            #x=x.clone().detach().requires_grad_(True)   \n",
    "            inputs=torch.tensor(inputs ,dtype=torch.float)   \n",
    "            targets=torch.tensor(targets ,dtype=torch.float)   \n",
    "            inputs , targets = inputs.cuda(),  targets.cuda()   \n",
    "            yhat = model(inputs)  \n",
    "            \n",
    "            #yhat = torch.max(yhat.data,1)[1]\n",
    "            #yhat=yhat.detach().cpu().numpy()\n",
    "            #print(\"predicted:\",predicted)\n",
    "            #predicted = torch.max(y_predict.data,1)[1]\n",
    "             #predicted = torch.max(y_predict.data,1)[1]\n",
    "            \n",
    "            \n",
    "            targets=torch.max(targets,1)[1]\n",
    "            #print(\"labels:\",labels)\n",
    "            #labels=labels[1]\n",
    "            #targets=targets.detach().cpu().numpy()\n",
    "            \n",
    "            \n",
    "            \n",
    "            batch_test_acc=torchmetrics.functional.accuracy(yhat,targets)\n",
    "            #print(\"batch_test_acc:\",batch_test_acc)\n",
    "            total_test_acc+=batch_test_acc\n",
    "            #total_test_accuracy=torchmetrics.functional.compute_details()\n",
    "            batch_size=i\n",
    "        print('total_test_accuracy:',total_test_acc/(batch_size+1))\n",
    "        \n",
    "                    \n",
    "                    \n",
    "            \n",
    "            #model.evaluate()\n",
    "            #model.eval()\n",
    "            #train_result = evaluate.metrics(model, train_loader)\n",
    "            #valid_result = evaluate.metrics(model, valid_loader)\n",
    "            #est_result = evaluate.metrics(model, test_loader)\n",
    "            #acturals,predictions,acc_test=evaluate_model(test_loader,model)\n",
    "            #print(\"acc_test:  %d  \" %(acc_test))\n",
    "            #print(\"Train_RMSE: {:.3f}, Valid_RMSE: {:.3f}, Test_RMSE: {:.3f}\".format(\n",
    "            #train_result, valid_result, test_result))\n",
    "            # print('[Training Epoch: {}] Batch: {}, Loss: {}'.format(epoch_id, batch_id, loss))\n",
    "        print(\"Training Epoch: %d, total loss: %f\" % (epoch_id, total))\n",
    "        #print(\"auc:\",roc_auc_score)\n",
    "    #功能：保存训练完的网络的各层参数（即weights和bias)\n",
    "    #path='dataset/xiaoguan/RF/RF_for_train/train_class_9/model/gene_4000_NFM.pkl'\n",
    "        if epoch_id %100==0:\n",
    "            num=num+1\n",
    "            path=os.path.join(model_path,'NMF'+str(num)+'.pkl')\n",
    "            torch.save(model.state_dict(),path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e90666ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from basemodel import BaseModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BiInteractionPooling(nn.Module):\n",
    "    \"\"\"Bi-Interaction Layer used in Neural FM,compress the\n",
    "      pairwise element-wise product of features into one single vector.\n",
    "      Input shape\n",
    "        - A 3D tensor with shape:``(batch_size,field_size,embedding_size)``.\n",
    "      Output shape\n",
    "    http://127.0.0.1:3000/notebooks/NFM-pyorch-master/NFM-pyorch-master/%E6%9C%AA%E5%91%BD%E5%90%8D5.ipynb?kernel_name=python3#    - 3D tensor with shape: ``(batch_size,1,embedding_size)``.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(BiInteractionPooling, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        concated_embeds_value = inputs\n",
    "        square_of_sum = torch.pow(\n",
    "            torch.sum(concated_embeds_value, dim=1, keepdim=True), 2)\n",
    "        sum_of_square = torch.sum(\n",
    "            concated_embeds_value * concated_embeds_value, dim=1, keepdim=True)\n",
    "        cross_term = 0.5 * (square_of_sum - sum_of_square)\n",
    "        return cross_term\n",
    "    \n",
    "class DNN_module(BaseModel):\n",
    "    def __init__(self,config):\n",
    "        super(DNN_module,self).__init__(config)\n",
    "        self.__config=config\n",
    "        \n",
    "        \n",
    "        self.linear1=nn.Linear(config['num_sparse_features_cols'],config['linear_hidden1'])\n",
    "        self.bn1=nn.BatchNorm1d(config['linear_hidden1'])\n",
    "        self.drop1=nn.Dropout(0.5)\n",
    "        self.relu1=nn.ReLU()\n",
    "        \n",
    "        self.linear2=nn.Linear(config['linear_hidden1'],config['dnn_hidden_units'][0])\n",
    "        self.bn2=nn.BatchNorm1d(config['dnn_hidden_units'][0])\n",
    "        self.drop2=nn.Dropout(0.5)\n",
    "        self.relu2=nn.ReLU()\n",
    "        \n",
    "        self.linear3=nn.Linear(config['dnn_hidden_units'][0]+config['embed_dim'],config['dnn_hidden_units'][1])\n",
    "        self.bn3=nn.BatchNorm1d(config['dnn_hidden_units'][1])\n",
    "        self.drop3=nn.Dropout(0.3)\n",
    "        self.relu3=nn.ReLU()\n",
    "        \n",
    "    def forward(self,input):\n",
    "        x1,x2=input\n",
    "        #print('x1,x2',x1.dtype,x2.dtype)\n",
    "        y1=self.relu1(self.drop1(self.bn1(self.linear1(x1))))\n",
    "        y2=self.relu2(self.drop2(self.bn2(self.linear2(y1))))\n",
    "        \n",
    "        x3=torch.cat((y2,x2),dim=1)\n",
    "        y3=self.relu3(self.drop3(self.bn3(self.linear3(x3))))\n",
    "        y=F.softmax(y3,dim=1)\n",
    "        #print('y:',y.dtype)\n",
    "        return y#非常重要，切勿忘记写\n",
    "dnn_module=nn.Sequential(DNN_module(nfm_config))\n",
    "dnn_module.cuda()\n",
    "\n",
    "class NFM(BaseModel):\n",
    "    def __init__(self, config, dense_features_cols=[]):#=[]为新增\n",
    "    #def __init__(self, config, dense_features_cols, sparse_features_cols):\n",
    "        super(NFM, self).__init__(config)\n",
    "        # 稠密和稀疏特征的数量\n",
    "        #self.num_dense_feature = dense_features_cols.__len__()\n",
    "        self.num_dense_feature = 0#修改\n",
    "        self.num_sparse_feature = config['num_sparse_features_cols']\n",
    "        #self.num_sparse_feature = 0##修改\n",
    "        self.__config=config\n",
    "        \n",
    "        \n",
    "        self.BN_num=nn.BatchNorm1d(self.num_sparse_feature)\n",
    "        \n",
    "        \n",
    "        self.embedding_layers=nn.Embedding(config['embed_input_dim'],config['embed_dim'])\n",
    "        \n",
    "        # B-Interaction 层\n",
    "        self.bi_pooling = BiInteractionPooling()\n",
    "        self.bi_dropout = config['bi_dropout']\n",
    "        if self.bi_dropout > 0:\n",
    "            self.dropout = nn.Dropout(self.bi_dropout)\n",
    "            \n",
    "            \n",
    "        self.BN_bi = nn.BatchNorm1d(config['embed_dim'])\n",
    "        \n",
    "        \n",
    "        #self.dnn_softmax=nn.Softmax(dim=1) # 按列SoftMax,列和为1  #注意nn.softmax的定义和调用\n",
    "        #self.dnn_softmax_=F.softmax(dim=1)\n",
    "        #self.dnn_hidden_units=config['dnn_hidden_units']\n",
    "    def forward(self, x):\n",
    "        # 先区分出稀疏特征和稠密特征，这里是按照列来划分的，即所有的行都要进行筛选\n",
    "        dense_input, sparse_inputs = x[:, :self.num_dense_feature], x[:, self.num_dense_feature:]\n",
    "        sparse_inputs = sparse_inputs.long()\n",
    "        #print(x.dtype)\n",
    "        # 求出线性部分\n",
    "        #x=F.relu(self.drop(self.BN_linear1(self.linear_model1(self.BN_num(x))))\n",
    "        #x=F.relu(self.drop(self.BN_linear1(self.linear_model1(x))))\n",
    "        x=self.BN_num(x)\n",
    "        #x=self.linear_model1(x)\n",
    "        \n",
    "        \n",
    "        #print(\"linear_output:\",linear_output)\n",
    "        #linear_output=linear_output.view(-1,self.__config['linear_hidden1'])\n",
    "        #linear_output=self.drop(linear_output)\n",
    "        #linear_output=self.BN_linear(linear_output)\n",
    "        # 求出稀疏特征的embedding向量\n",
    "        sparse_embeds = [self.embedding_layers(sparse_inputs[:, i]) for i in range(sparse_inputs.shape[1])]\n",
    "        sparse_embeds = torch.cat(sparse_embeds, axis=-1)\n",
    "\n",
    "        # 送入B-Interaction层\n",
    "        fm_input = sparse_embeds.view(-1, self.num_sparse_feature, self.__config['embed_dim'])#整理成n行m列\n",
    "        # print(fm_input)\n",
    "        # print(fm_input.shape)\n",
    "\n",
    "        bi_out = self.bi_pooling(fm_input)\n",
    "        if self.bi_dropout:\n",
    "            bi_out = self.dropout(bi_out)\n",
    "\n",
    "        bi_out = bi_out.view(-1, self.__config['embed_dim'])\n",
    "        \n",
    "        bi_out=self.BN_bi(bi_out)\n",
    "        #print(x.dtype)\n",
    "        #print(bi_out.dtype)\n",
    "        \n",
    "        input=x,bi_out#不能是list，必须是tensor\n",
    "        \n",
    "        \n",
    "        y_pred=dnn_module(input)\n",
    "        #y_pred=self.dnn_softmax(dnn_output)#增加\n",
    "        #y_pred=F.softmax(dnn_output,dim=0)\n",
    "        # Final\n",
    "        #output = linear_output + y_pred#修改\n",
    "        #y_pred = self.dnn_softmax(output,dim=0)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f3fdea9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMData\n",
      "FMData\n",
      "NFM: NFM(\n",
      "  (BN_num): BatchNorm1d(10477, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (embedding_layers): Embedding(1001, 100)\n",
      "  (bi_pooling): BiInteractionPooling()\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (BN_bi): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_train_accuracy: tensor(0.1394, device='cuda:0')\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_test_accuracy: tensor(0.1667, device='cuda:0')\n",
      "Training Epoch: 0, total loss: 57.208950\n",
      "total_train_accuracy: tensor(0.1587, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.1667, device='cuda:0')\n",
      "Training Epoch: 1, total loss: 57.148886\n",
      "total_train_accuracy: tensor(0.1795, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.1250, device='cuda:0')\n",
      "Training Epoch: 2, total loss: 57.010650\n",
      "total_train_accuracy: tensor(0.2035, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.0833, device='cuda:0')\n",
      "Training Epoch: 3, total loss: 56.999363\n",
      "total_train_accuracy: tensor(0.2003, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.1667, device='cuda:0')\n",
      "Training Epoch: 4, total loss: 56.900461\n",
      "total_train_accuracy: tensor(0.2276, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.0417, device='cuda:0')\n",
      "Training Epoch: 5, total loss: 56.885148\n",
      "total_train_accuracy: tensor(0.2548, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.1250, device='cuda:0')\n",
      "Training Epoch: 6, total loss: 56.753983\n",
      "total_train_accuracy: tensor(0.2484, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.1667, device='cuda:0')\n",
      "Training Epoch: 7, total loss: 56.792011\n",
      "total_train_accuracy: tensor(0.2500, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0., device='cuda:0')\n",
      "Training Epoch: 8, total loss: 56.832945\n",
      "total_train_accuracy: tensor(0.2965, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.1667, device='cuda:0')\n",
      "Training Epoch: 9, total loss: 56.555102\n",
      "total_train_accuracy: tensor(0.3061, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.0833, device='cuda:0')\n",
      "Training Epoch: 10, total loss: 56.590074\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-d0c6dfab9467>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NFM:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnfm_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'dataset/qiuguan/model/NFM_sequential_encode_1000/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-06705915c390>\u001b[0m in \u001b[0;36mtrain_data\u001b[0;34m(model, train_loader, test_loader, batch_size, model_path)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0;31m#print(\"y_predict:\",y_predict.dtype)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;31m#print(\"y_predict:\",y_predict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-1d5920250fa7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m#linear_output=self.BN_linear(linear_output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# 求出稀疏特征的embedding向量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0msparse_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0msparse_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-1d5920250fa7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m#linear_output=self.BN_linear(linear_output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# 求出稀疏特征的embedding向量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0msparse_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0msparse_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset,train_loader=prepare_dataset(nfm_config['train_data'],nfm_config['train_label'],nfm_config['batch_size'],nfm_config['n_class'])\n",
    "test_dataset,test_loader=prepare_dataset(nfm_config['test_data'],nfm_config['test_label'],nfm_config['batch_size'],nfm_config['n_class'])\n",
    "#from MLP import MLP\n",
    "#from nfm_network_adjust import NFM\n",
    "#model=MLP(4224,1000,100,9)\n",
    "model=NFM(nfm_config)\n",
    "model.cuda()\n",
    "print(\"NFM:\",model)\n",
    "train_data(model,train_loader,test_loader,nfm_config['batch_size'],'dataset/qiuguan/model/NFM_sequential_encode_1000/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b8f341b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4\n"
     ]
    }
   ],
   "source": [
    "x1=[3,4]\n",
    "x2=[3,2]\n",
    "a,b=x1\n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39e9b449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4\n"
     ]
    }
   ],
   "source": [
    "l=[]\n",
    "l.append(3)\n",
    "l.append(4)\n",
    "c,d=l\n",
    "print(c,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72923911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2]) tensor([3, 4])\n"
     ]
    }
   ],
   "source": [
    "x1=torch.tensor([1,2])\n",
    "x2=torch.tensor([3,4])\n",
    "input=x1,x2\n",
    "x3,x4=input\n",
    "print(x3,x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb641f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
