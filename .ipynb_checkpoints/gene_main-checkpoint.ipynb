{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "432e5d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import Trainer\n",
    "from network import NFM\n",
    "import torch.utils.data as Data\n",
    "from Utils.criteo_loader import getTestData, getTrainData\n",
    "\n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':8,\n",
    "    'embed_dim': 100, # 用于控制稀疏特征经过Embedding层后的稠密特征大小\n",
    "    'dnn_hidden_units': [50,8],\n",
    "    'num_sparse_features_cols':10149,#the number of the gene columns\n",
    "   # 'num_dense_features': 13,\n",
    "    'bi_dropout': 0.8,\n",
    "    'num_epoch': 500,\n",
    "    'batch_size': 128,\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    #'train_file': '../Data/criteo/processed_data/train_set.csv',\n",
    "    #'fea_file': '../Data/criteo/processed_data/fea_col.npy',\n",
    "    #'validate_file': '../Data/criteo/processed_data/val_set.csv',\n",
    "    #'test_file': '../Data/criteo/processed_data/test_set.csv',\n",
    "    #'model_name': '../TrainedModels/NFM.model'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c8a83b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "#from utils import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9566c6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, classes, label_smoothing=0.2):\n",
    "    n = len(labels)\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        print(\"row:\",row,\"label:\",label)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b5c9e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class=8\n",
    "class FMData(data.Dataset):\n",
    "    \"\"\" Construct the FM pytorch dataset. \"\"\"\n",
    "    #def __init__(self, file,label_file, feature_map,n_class=16):\n",
    "    def __init__(self, file,label_file, n_class=8):\n",
    "        super(FMData, self).__init__()\n",
    "        self.label = []\n",
    "        self.features = []\n",
    "        #self.feature_values = []\n",
    "        \n",
    "        features=[]\n",
    "        #feature_map.keys()\n",
    "        #self.features=np.array(feature_map)\n",
    "        #feature_map\n",
    "        #file=\"data/frappe/c_df_v_fff2000.csv\"\n",
    "        #num = len(features)\n",
    "        fd=pd.read_csv(file,sep=',')\n",
    "        #nrow=fd.shape[0]\n",
    "        #ncol=fd.shape[1]\n",
    "        n_fd=np.array(fd)\n",
    "        #print(n_fd)\n",
    "        n_fd=n_fd[:,2:]\n",
    "        \n",
    "        \"\"\"\n",
    "        for i, item in enumerate(n_fd):\n",
    "            u=[feature_map[x] for x in item]\n",
    "            features.append(u)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.features=np.array(n_fd)\n",
    "        #self.features=features.tolist()\n",
    "        \n",
    "        \n",
    "        nrow,ncol=n_fd.shape\n",
    "        #ncol=10150\n",
    "        #feature_v=[]\n",
    "        \"\"\"\n",
    "            feature_v=[1 for i in range(ncol)]\n",
    "            #print(feature_v)\n",
    "            #feature_values=[feature_v for j in range(nrow)]\n",
    "\n",
    "            for item in range(nrow):\n",
    "            feature_values.append(feature_v)\n",
    "        \"\"\"\n",
    "\n",
    "        #feature_v=[[1 for i in range(ncol)] for i in range(nrow)]\n",
    "        #self.feature_values=np.array(feature_v)\n",
    "        #print(feature_value)\n",
    "        #self.feature_values=feature_value.tolist()\n",
    "        #feature_map,lenth=map_features()\n",
    "        #raw = [item for item in  enumerate(n_fd)]\n",
    "        #print(raw)\n",
    "        #raw=raw.tolist()\n",
    "        \n",
    "        #label_file=[]\n",
    "        label_fd=pd.read_csv(label_file,sep=',')\n",
    "        #print(features)\n",
    "        #print(label_fd)\n",
    "        label=np.array(label_fd)\n",
    "        #label=label[:,1:]\n",
    "        label=one_hot(label,n_class)\n",
    "        self.label=label\n",
    "        print(\"label:\",label)\n",
    "        print(\"features:\",self.features)\n",
    "        #print(label)\n",
    "        # convert labels\n",
    "        \"\"\"if config.loss_type == 'square_loss':\n",
    "            self.label.append(np.float32(items[0]))\n",
    "        else: # log_loss\n",
    "            label = 1 if float(items[0]) > 0 else 0\n",
    "            self.label.append(label)\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        assert all(len(item) == len(self.features[0]\n",
    "            ) for item in self.features), 'features are of different length'\n",
    "        \"\"\"\n",
    "        #print(len(self.features))\n",
    "        #print(len(self.feature_values))\n",
    "        #print(len(self.label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.label[idx]\n",
    "        features = self.features[idx]\n",
    "        #feature_values = self.feature_values[idx]\n",
    "        #return features, feature_values, label\n",
    "        return features,  label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "386b0597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row: 0 label: [0]\n",
      "row: 1 label: [0]\n",
      "row: 2 label: [0]\n",
      "row: 3 label: [0]\n",
      "row: 4 label: [0]\n",
      "row: 5 label: [0]\n",
      "row: 6 label: [0]\n",
      "row: 7 label: [0]\n",
      "row: 8 label: [0]\n",
      "row: 9 label: [0]\n",
      "row: 10 label: [0]\n",
      "row: 11 label: [0]\n",
      "row: 12 label: [0]\n",
      "row: 13 label: [0]\n",
      "row: 14 label: [0]\n",
      "row: 15 label: [0]\n",
      "row: 16 label: [0]\n",
      "row: 17 label: [0]\n",
      "row: 18 label: [0]\n",
      "row: 19 label: [0]\n",
      "row: 20 label: [0]\n",
      "row: 21 label: [0]\n",
      "row: 22 label: [0]\n",
      "row: 23 label: [0]\n",
      "row: 24 label: [0]\n",
      "row: 25 label: [0]\n",
      "row: 26 label: [0]\n",
      "row: 27 label: [0]\n",
      "row: 28 label: [0]\n",
      "row: 29 label: [0]\n",
      "row: 30 label: [0]\n",
      "row: 31 label: [0]\n",
      "row: 32 label: [0]\n",
      "row: 33 label: [0]\n",
      "row: 34 label: [0]\n",
      "row: 35 label: [0]\n",
      "row: 36 label: [0]\n",
      "row: 37 label: [0]\n",
      "row: 38 label: [0]\n",
      "row: 39 label: [0]\n",
      "row: 40 label: [0]\n",
      "row: 41 label: [0]\n",
      "row: 42 label: [0]\n",
      "row: 43 label: [0]\n",
      "row: 44 label: [0]\n",
      "row: 45 label: [0]\n",
      "row: 46 label: [1]\n",
      "row: 47 label: [1]\n",
      "row: 48 label: [1]\n",
      "row: 49 label: [1]\n",
      "row: 50 label: [1]\n",
      "row: 51 label: [1]\n",
      "row: 52 label: [1]\n",
      "row: 53 label: [1]\n",
      "row: 54 label: [1]\n",
      "row: 55 label: [1]\n",
      "row: 56 label: [1]\n",
      "row: 57 label: [1]\n",
      "row: 58 label: [1]\n",
      "row: 59 label: [1]\n",
      "row: 60 label: [1]\n",
      "row: 61 label: [1]\n",
      "row: 62 label: [1]\n",
      "row: 63 label: [1]\n",
      "row: 64 label: [1]\n",
      "row: 65 label: [1]\n",
      "row: 66 label: [1]\n",
      "row: 67 label: [1]\n",
      "row: 68 label: [1]\n",
      "row: 69 label: [1]\n",
      "row: 70 label: [1]\n",
      "row: 71 label: [1]\n",
      "row: 72 label: [1]\n",
      "row: 73 label: [1]\n",
      "row: 74 label: [1]\n",
      "row: 75 label: [1]\n",
      "row: 76 label: [1]\n",
      "row: 77 label: [1]\n",
      "row: 78 label: [1]\n",
      "row: 79 label: [1]\n",
      "row: 80 label: [1]\n",
      "row: 81 label: [1]\n",
      "row: 82 label: [1]\n",
      "row: 83 label: [1]\n",
      "row: 84 label: [1]\n",
      "row: 85 label: [1]\n",
      "row: 86 label: [2]\n",
      "row: 87 label: [2]\n",
      "row: 88 label: [2]\n",
      "row: 89 label: [2]\n",
      "row: 90 label: [2]\n",
      "row: 91 label: [2]\n",
      "row: 92 label: [2]\n",
      "row: 93 label: [2]\n",
      "row: 94 label: [2]\n",
      "row: 95 label: [2]\n",
      "row: 96 label: [2]\n",
      "row: 97 label: [2]\n",
      "row: 98 label: [2]\n",
      "row: 99 label: [2]\n",
      "row: 100 label: [2]\n",
      "row: 101 label: [2]\n",
      "row: 102 label: [2]\n",
      "row: 103 label: [2]\n",
      "row: 104 label: [2]\n",
      "row: 105 label: [2]\n",
      "row: 106 label: [2]\n",
      "row: 107 label: [2]\n",
      "row: 108 label: [2]\n",
      "row: 109 label: [2]\n",
      "row: 110 label: [2]\n",
      "row: 111 label: [2]\n",
      "row: 112 label: [2]\n",
      "row: 113 label: [2]\n",
      "row: 114 label: [2]\n",
      "row: 115 label: [2]\n",
      "row: 116 label: [2]\n",
      "row: 117 label: [2]\n",
      "row: 118 label: [2]\n",
      "row: 119 label: [3]\n",
      "row: 120 label: [3]\n",
      "row: 121 label: [3]\n",
      "row: 122 label: [3]\n",
      "row: 123 label: [3]\n",
      "row: 124 label: [3]\n",
      "row: 125 label: [3]\n",
      "row: 126 label: [3]\n",
      "row: 127 label: [3]\n",
      "row: 128 label: [3]\n",
      "row: 129 label: [3]\n",
      "row: 130 label: [3]\n",
      "row: 131 label: [3]\n",
      "row: 132 label: [3]\n",
      "row: 133 label: [3]\n",
      "row: 134 label: [3]\n",
      "row: 135 label: [3]\n",
      "row: 136 label: [3]\n",
      "row: 137 label: [3]\n",
      "row: 138 label: [3]\n",
      "row: 139 label: [3]\n",
      "row: 140 label: [3]\n",
      "row: 141 label: [3]\n",
      "row: 142 label: [3]\n",
      "row: 143 label: [3]\n",
      "row: 144 label: [3]\n",
      "row: 145 label: [3]\n",
      "row: 146 label: [3]\n",
      "row: 147 label: [3]\n",
      "row: 148 label: [3]\n",
      "row: 149 label: [3]\n",
      "row: 150 label: [3]\n",
      "row: 151 label: [3]\n",
      "row: 152 label: [3]\n",
      "row: 153 label: [3]\n",
      "row: 154 label: [3]\n",
      "row: 155 label: [3]\n",
      "row: 156 label: [3]\n",
      "row: 157 label: [3]\n",
      "row: 158 label: [3]\n",
      "row: 159 label: [3]\n",
      "row: 160 label: [3]\n",
      "row: 161 label: [3]\n",
      "row: 162 label: [3]\n",
      "row: 163 label: [3]\n",
      "row: 164 label: [3]\n",
      "row: 165 label: [3]\n",
      "row: 166 label: [3]\n",
      "row: 167 label: [3]\n",
      "row: 168 label: [3]\n",
      "row: 169 label: [3]\n",
      "row: 170 label: [3]\n",
      "row: 171 label: [3]\n",
      "row: 172 label: [4]\n",
      "row: 173 label: [4]\n",
      "row: 174 label: [4]\n",
      "row: 175 label: [4]\n",
      "row: 176 label: [4]\n",
      "row: 177 label: [4]\n",
      "row: 178 label: [4]\n",
      "row: 179 label: [4]\n",
      "row: 180 label: [4]\n",
      "row: 181 label: [4]\n",
      "row: 182 label: [4]\n",
      "row: 183 label: [4]\n",
      "row: 184 label: [4]\n",
      "row: 185 label: [4]\n",
      "row: 186 label: [4]\n",
      "row: 187 label: [4]\n",
      "row: 188 label: [4]\n",
      "row: 189 label: [4]\n",
      "row: 190 label: [4]\n",
      "row: 191 label: [4]\n",
      "row: 192 label: [4]\n",
      "row: 193 label: [4]\n",
      "row: 194 label: [4]\n",
      "row: 195 label: [4]\n",
      "row: 196 label: [4]\n",
      "row: 197 label: [4]\n",
      "row: 198 label: [4]\n",
      "row: 199 label: [5]\n",
      "row: 200 label: [5]\n",
      "row: 201 label: [5]\n",
      "row: 202 label: [5]\n",
      "row: 203 label: [5]\n",
      "row: 204 label: [5]\n",
      "row: 205 label: [5]\n",
      "row: 206 label: [5]\n",
      "row: 207 label: [5]\n",
      "row: 208 label: [5]\n",
      "row: 209 label: [5]\n",
      "row: 210 label: [5]\n",
      "row: 211 label: [5]\n",
      "row: 212 label: [5]\n",
      "row: 213 label: [5]\n",
      "row: 214 label: [5]\n",
      "row: 215 label: [5]\n",
      "row: 216 label: [5]\n",
      "row: 217 label: [5]\n",
      "row: 218 label: [5]\n",
      "row: 219 label: [5]\n",
      "row: 220 label: [5]\n",
      "row: 221 label: [5]\n",
      "row: 222 label: [5]\n",
      "row: 223 label: [5]\n",
      "row: 224 label: [5]\n",
      "row: 225 label: [5]\n",
      "row: 226 label: [5]\n",
      "row: 227 label: [5]\n",
      "row: 228 label: [5]\n",
      "row: 229 label: [5]\n",
      "row: 230 label: [5]\n",
      "row: 231 label: [5]\n",
      "row: 232 label: [5]\n",
      "row: 233 label: [5]\n",
      "row: 234 label: [5]\n",
      "row: 235 label: [5]\n",
      "row: 236 label: [5]\n",
      "row: 237 label: [5]\n",
      "row: 238 label: [5]\n",
      "row: 239 label: [5]\n",
      "row: 240 label: [5]\n",
      "row: 241 label: [6]\n",
      "row: 242 label: [6]\n",
      "row: 243 label: [6]\n",
      "row: 244 label: [6]\n",
      "row: 245 label: [6]\n",
      "row: 246 label: [6]\n",
      "row: 247 label: [6]\n",
      "row: 248 label: [6]\n",
      "row: 249 label: [6]\n",
      "row: 250 label: [6]\n",
      "row: 251 label: [6]\n",
      "row: 252 label: [6]\n",
      "row: 253 label: [6]\n",
      "row: 254 label: [6]\n",
      "row: 255 label: [6]\n",
      "row: 256 label: [6]\n",
      "row: 257 label: [6]\n",
      "row: 258 label: [6]\n",
      "row: 259 label: [6]\n",
      "row: 260 label: [6]\n",
      "row: 261 label: [6]\n",
      "row: 262 label: [6]\n",
      "row: 263 label: [6]\n",
      "row: 264 label: [6]\n",
      "row: 265 label: [6]\n",
      "row: 266 label: [6]\n",
      "row: 267 label: [6]\n",
      "row: 268 label: [6]\n",
      "row: 269 label: [6]\n",
      "row: 270 label: [6]\n",
      "row: 271 label: [6]\n",
      "row: 272 label: [6]\n",
      "row: 273 label: [6]\n",
      "row: 274 label: [6]\n",
      "row: 275 label: [6]\n",
      "row: 276 label: [6]\n",
      "row: 277 label: [6]\n",
      "row: 278 label: [6]\n",
      "row: 279 label: [6]\n",
      "row: 280 label: [6]\n",
      "row: 281 label: [6]\n",
      "row: 282 label: [6]\n",
      "row: 283 label: [6]\n",
      "row: 284 label: [6]\n",
      "row: 285 label: [6]\n",
      "row: 286 label: [7]\n",
      "row: 287 label: [7]\n",
      "row: 288 label: [7]\n",
      "row: 289 label: [7]\n",
      "row: 290 label: [7]\n",
      "row: 291 label: [7]\n",
      "row: 292 label: [7]\n",
      "row: 293 label: [7]\n",
      "row: 294 label: [7]\n",
      "row: 295 label: [7]\n",
      "row: 296 label: [7]\n",
      "row: 297 label: [7]\n",
      "row: 298 label: [7]\n",
      "row: 299 label: [7]\n",
      "row: 300 label: [7]\n",
      "row: 301 label: [7]\n",
      "row: 302 label: [7]\n",
      "row: 303 label: [7]\n",
      "row: 304 label: [7]\n",
      "row: 305 label: [7]\n",
      "row: 306 label: [7]\n",
      "row: 307 label: [7]\n",
      "row: 308 label: [7]\n",
      "row: 309 label: [7]\n",
      "row: 310 label: [7]\n",
      "row: 311 label: [7]\n",
      "row: 312 label: [7]\n",
      "row: 313 label: [7]\n",
      "row: 314 label: [7]\n",
      "row: 315 label: [7]\n",
      "row: 316 label: [7]\n",
      "row: 317 label: [7]\n",
      "row: 318 label: [7]\n",
      "row: 319 label: [7]\n",
      "row: 320 label: [7]\n",
      "row: 321 label: [7]\n",
      "row: 322 label: [7]\n",
      "row: 323 label: [7]\n",
      "row: 324 label: [7]\n",
      "row: 325 label: [7]\n",
      "row: 326 label: [7]\n",
      "row: 327 label: [7]\n",
      "row: 328 label: [7]\n",
      "row: 329 label: [7]\n",
      "row: 330 label: [7]\n",
      "row: 331 label: [7]\n",
      "row: 332 label: [7]\n",
      "row: 333 label: [7]\n",
      "row: 334 label: [7]\n",
      "row: 335 label: [7]\n",
      "row: 336 label: [7]\n",
      "row: 337 label: [7]\n",
      "row: 338 label: [7]\n",
      "row: 339 label: [7]\n",
      "row: 340 label: [7]\n",
      "row: 341 label: [7]\n",
      "row: 342 label: [7]\n",
      "row: 343 label: [7]\n",
      "row: 344 label: [7]\n",
      "row: 345 label: [7]\n",
      "row: 346 label: [7]\n",
      "row: 347 label: [7]\n",
      "label: [[0.825 0.025 0.025 ... 0.025 0.025 0.025]\n",
      " [0.825 0.025 0.025 ... 0.025 0.025 0.025]\n",
      " [0.825 0.025 0.025 ... 0.025 0.025 0.025]\n",
      " ...\n",
      " [0.025 0.025 0.025 ... 0.025 0.025 0.825]\n",
      " [0.025 0.025 0.025 ... 0.025 0.025 0.825]\n",
      " [0.025 0.025 0.025 ... 0.025 0.025 0.825]]\n",
      "features: [[22 24 70 ... 15 39 41]\n",
      " [22 22 61 ... 16 38 44]\n",
      " [21 20 66 ... 16 39 40]\n",
      " ...\n",
      " [27 18 55 ... 18 41 41]\n",
      " [23 19 61 ... 16 41 43]\n",
      " [20 18 57 ... 18 41 47]]\n"
     ]
    }
   ],
   "source": [
    "#features_map,num_features=map_features()\n",
    "#print('num_features:',num_features)\n",
    "#n_class=16\n",
    "\"\"\"\n",
    "train_dataset = FMData(config.train_libfm,config.train_label,features_map)\n",
    "train_loader = data.DataLoader(train_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "validate_dataset = FMData(config.valid_libfm,config.valid_label,features_map)\n",
    "validate_loader = data.DataLoader(train_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "test_dataset = FMData(config.test_libfm,config.test_label,features_map)\n",
    "test_loader = data.DataLoader(test_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\"\"\"\n",
    "train_dataset = FMData(config.train_libfm,config.train_label)\n",
    "train_loader = data.DataLoader(train_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "#validate_dataset = FMData(config.valid_libfm,config.valid_label)\n",
    "#validate_loader = data.DataLoader(train_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "#test_dataset = FMData(config.test_libfm,config.test_label)\n",
    "#test_loader = data.DataLoader(test_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79048492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from basemodel import BaseModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BiInteractionPooling(nn.Module):\n",
    "    \"\"\"Bi-Interaction Layer used in Neural FM,compress the\n",
    "      pairwise element-wise product of features into one single vector.\n",
    "      Input shape\n",
    "        - A 3D tensor with shape:``(batch_size,field_size,embedding_size)``.\n",
    "      Output shape\n",
    "    http://127.0.0.1:3000/notebooks/NFM-pyorch-master/NFM-pyorch-master/%E6%9C%AA%E5%91%BD%E5%90%8D5.ipynb?kernel_name=python3#    - 3D tensor with shape: ``(batch_size,1,embedding_size)``.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(BiInteractionPooling, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        concated_embeds_value = inputs\n",
    "        square_of_sum = torch.pow(\n",
    "            torch.sum(concated_embeds_value, dim=1, keepdim=True), 2)\n",
    "        sum_of_square = torch.sum(\n",
    "            concated_embeds_value * concated_embeds_value, dim=1, keepdim=True)\n",
    "        cross_term = 0.5 * (square_of_sum - sum_of_square)\n",
    "        return cross_term\n",
    "\n",
    "class NFM(BaseModel):\n",
    "    def __init__(self, config, dense_features_cols=[]):#=[]为新增\n",
    "    #def __init__(self, config, dense_features_cols, sparse_features_cols):\n",
    "        super(NFM, self).__init__(config)\n",
    "        # 稠密和稀疏特征的数量\n",
    "        #self.num_dense_feature = dense_features_cols.__len__()\n",
    "        self.num_dense_feature = 0#修改\n",
    "        self.num_sparse_feature = 10149\n",
    "        #self.num_sparse_feature = 0##修改\n",
    "        # NFM的线性部分，对应 ∑WiXi\n",
    "        #self.linear_model = nn.Linear(self.num_dense_feature + self.num_sparse_feature, 1)\n",
    "        #self.linear_model = nn.Linear(self.num_dense_feature + self.num_sparse_feature, n_class)##修改\n",
    "        # NFM的Embedding层\n",
    "        \n",
    "        \"\"\"\n",
    "        self.dnn_layers = nn.ModuleList([\n",
    "            nn.Linear(in_features=layer[0], out_features=layer[1])\\\n",
    "                for layer in list(zip(self.hidden_layers[:-1], self.hidden_layers[1:])) \n",
    "        ])\n",
    "        \"\"\"\n",
    "        self.embedding_layers=nn.Embedding(1001,config['embed_dim'])\n",
    "        # B-Interaction 层\n",
    "        self.bi_pooling = BiInteractionPooling()\n",
    "        self.bi_dropout = config['bi_dropout']\n",
    "        if self.bi_dropout > 0:\n",
    "            self.dropout = nn.Dropout(self.bi_dropout)\n",
    "\n",
    "        # NFM的DNN部分\n",
    "        self.hidden_layers = [self.num_dense_feature + config['embed_dim']] + config['dnn_hidden_units']#是加还是乘\n",
    "        self.dnn_layers = nn.ModuleList([\n",
    "            nn.Linear(in_features=layer[0], out_features=layer[1])\\\n",
    "                for layer in list(zip(self.hidden_layers[:-1], self.hidden_layers[1:])) \n",
    "        ])\n",
    "        #self.dnn_linear = nn.Linear(self.hidden_layers[-1], 1, bias=False)\n",
    "        #self.dnn_linear = nn.Linear(self.hidden_layers[-1], n_class, bias=False)\n",
    "        \n",
    "        #增加\n",
    "        self.dnn_softmax=nn.Softmax(dim=1) # 按列SoftMax,列和为1  #注意nn.softmax的定义和调用\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 先区分出稀疏特征和稠密特征，这里是按照列来划分的，即所有的行都要进行筛选\n",
    "        dense_input, sparse_inputs = x[:, :self.num_dense_feature], x[:, self.num_dense_feature:]\n",
    "        sparse_inputs = sparse_inputs.long()\n",
    "\n",
    "        # 求出线性部分\n",
    "        #linear_output = self.linear_model(x)\n",
    "\n",
    "        # 求出稀疏特征的embedding向量\n",
    "        sparse_embeds = [self.embedding_layers(sparse_inputs[:, i]) for i in range(sparse_inputs.shape[1])]\n",
    "        sparse_embeds = torch.cat(sparse_embeds, axis=-1)\n",
    "\n",
    "        # 送入B-Interaction层\n",
    "        fm_input = sparse_embeds.view(-1, self.num_sparse_feature, self._config['embed_dim'])#整理成n行m列\n",
    "        # print(fm_input)\n",
    "        # print(fm_input.shape)\n",
    "\n",
    "        bi_out = self.bi_pooling(fm_input)\n",
    "        if self.bi_dropout:\n",
    "            bi_out = self.dropout(bi_out)\n",
    "\n",
    "        bi_out = bi_out.view(-1, self._config['embed_dim'])\n",
    "        # 将结果聚合起来\n",
    "        dnn_input = torch.cat((dense_input, bi_out), dim=-1)\n",
    "\n",
    "        # DNN 层\n",
    "        dnn_output = dnn_input\n",
    "        for dnn in self.dnn_layers:\n",
    "            dnn_output = dnn(dnn_output)#dnn_output为tensor\n",
    "            # dnn_output = nn.BatchNormalize(dnn_output)\n",
    "            dnn_output = torch.relu(dnn_output)\n",
    "        #dnn_output = self.dnn_linear(dnn_output)\n",
    "        \n",
    "        print(\"dnn_softmax_output:\",dnn_output.shape)\n",
    "        y_pred=self.dnn_softmax(dnn_output)#增加\n",
    "        \n",
    "        # Final\n",
    "        #output = linear_output + dnn_output#修改\n",
    "        #y_pred = self.dnn_softmax(output,dim=0)\n",
    "\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aba2fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from basemodel import BaseModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BiInteractionPooling(nn.Module):\n",
    "    \"\"\"Bi-Interaction Layer used in Neural FM,compress the\n",
    "      pairwise element-wise product of features into one single vector.\n",
    "      Input shape\n",
    "        - A 3D tensor with shape:``(batch_size,field_size,embedding_size)``.\n",
    "      Output shape\n",
    "    http://127.0.0.1:3000/notebooks/NFM-pyorch-master/NFM-pyorch-master/%E6%9C%AA%E5%91%BD%E5%90%8D5.ipynb?kernel_name=python3#    - 3D tensor with shape: ``(batch_size,1,embedding_size)``.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(BiInteractionPooling, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        concated_embeds_value = inputs\n",
    "        square_of_sum = torch.pow(\n",
    "            torch.sum(concated_embeds_value, dim=1, keepdim=True), 2)\n",
    "        sum_of_square = torch.sum(\n",
    "            concated_embeds_value * concated_embeds_value, dim=1, keepdim=True)\n",
    "        cross_term = 0.5 * (square_of_sum - sum_of_square)\n",
    "        return cross_term\n",
    "\n",
    "class NFM(BaseModel):\n",
    "    def __init__(self, config, dense_features_cols=[]):#=[]为新增\n",
    "    #def __init__(self, config, dense_features_cols, sparse_features_cols):\n",
    "        super(NFM, self).__init__(config)\n",
    "        # 稠密和稀疏特征的数量\n",
    "        #self.num_dense_feature = dense_features_cols.__len__()\n",
    "        self.num_dense_feature = 0#修改\n",
    "        self.num_sparse_feature = 537\n",
    "        #self.num_sparse_feature = 0##修改\n",
    "        # NFM的线性部分，对应 ∑WiXi\n",
    "        self.linear_model =nn.Linear(self.num_dense_feature + self.num_sparse_feature, 100)\n",
    "            \n",
    "         \n",
    "        #self.linear_model = nn.Linear(self.num_dense_feature + self.num_sparse_feature, n_class)##修改\n",
    "        # NFM的Embedding层\n",
    "        \n",
    "        \"\"\"\n",
    "        self.dnn_layers = nn.ModuleList([\n",
    "            nn.Linear(in_features=layer[0], out_features=layer[1])\\\n",
    "                for layer in list(zip(self.hidden_layers[:-1], self.hidden_layers[1:])) \n",
    "        ])\n",
    "        \"\"\"\n",
    "        self.embedding_layers=nn.Embedding(1001,config['embed_dim'])\n",
    "        # B-Interaction 层\n",
    "        self.bi_pooling = BiInteractionPooling()\n",
    "        self.bi_dropout = config['bi_dropout']\n",
    "        if self.bi_dropout > 0:\n",
    "            self.dropout = nn.Dropout(self.bi_dropout)\n",
    "\n",
    "        # NFM的DNN部分\n",
    "        self.hidden_layers = [self.num_dense_feature+100 + config['embed_dim']] + config['dnn_hidden_units']#是加还是乘\n",
    "        self.dnn_layers = nn.ModuleList([\n",
    "            nn.Linear(in_features=layer[0], out_features=layer[1])\\\n",
    "                for layer in list(zip(self.hidden_layers[:-1], self.hidden_layers[1:])) \n",
    "        ])\n",
    "        #self.dnn_linear = nn.Linear(self.hidden_layers[-1], 1, bias=False)\n",
    "        #self.dnn_linear = nn.Linear(self.hidden_layers[-1], n_class, bias=False)\n",
    "        \n",
    "        #增加\n",
    "        self.dnn_softmax=nn.Softmax(dim=1) # 按列SoftMax,列和为1  #注意nn.softmax的定义和调用\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 先区分出稀疏特征和稠密特征，这里是按照列来划分的，即所有的行都要进行筛选\n",
    "        dense_input, sparse_inputs = x[:, :self.num_dense_feature], x[:, self.num_dense_feature:]\n",
    "        sparse_inputs = sparse_inputs.long()\n",
    "\n",
    "        # 求出线性部分\n",
    "        linear_output = self.linear_model(x)\n",
    "        print(\"linear_output:\",linear_output)\n",
    "        # 求出稀疏特征的embedding向量\n",
    "        sparse_embeds = [self.embedding_layers(sparse_inputs[:, i]) for i in range(sparse_inputs.shape[1])]\n",
    "        sparse_embeds = torch.cat(sparse_embeds, axis=-1)\n",
    "\n",
    "        # 送入B-Interaction层\n",
    "        fm_input = sparse_embeds.view(-1, self.num_sparse_feature, self._config['embed_dim'])#整理成n行m列\n",
    "        # print(fm_input)\n",
    "        # print(fm_input.shape)\n",
    "\n",
    "        bi_out = self.bi_pooling(fm_input)\n",
    "        if self.bi_dropout:\n",
    "            bi_out = self.dropout(bi_out)\n",
    "\n",
    "        bi_out = bi_out.view(-1, self._config['embed_dim'])\n",
    "        \n",
    "        \n",
    "        # 将结果聚合起来\n",
    "        dnn_input = torch.cat((dense_input, bi_out,linear_output), dim=-1)\n",
    "\n",
    "        # DNN 层\n",
    "        dnn_output = dnn_input\n",
    "        for dnn in self.dnn_layers:\n",
    "            dnn_output = dnn(dnn_output)#dnn_output为tensor\n",
    "            # dnn_output = nn.BatchNormalize(dnn_output)\n",
    "            dnn_output = torch.relu(dnn_output)\n",
    "        #dnn_output = self.dnn_linear(dnn_output)\n",
    "        \n",
    "        print(\"dnn_softmax_output:\",dnn_output.shape)\n",
    "        y_pred=self.dnn_softmax(dnn_output)#增加\n",
    "        \n",
    "        # Final\n",
    "        #output = linear_output + y_pred#修改\n",
    "        #y_pred = self.dnn_softmax(output,dim=0)\n",
    "\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "236c62c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2000])\n",
      "torch.Size([100, 2000])\n",
      "torch.Size([100, 2000])\n",
      "torch.Size([100, 2000])\n",
      "torch.Size([100, 2000])\n",
      "torch.Size([100, 2000])\n",
      "torch.Size([100, 2000])\n"
     ]
    }
   ],
   "source": [
    "model=nn.Linear(10149, 2000)\n",
    "for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "            correct=0\n",
    "            #x=torch.from_numpy(x,d)\n",
    "            #x = Variable(x,requires_grad=True)\n",
    "            #x=Variable(torch.FloatTensor(x), requires_grad=True)\n",
    "            #x=torch.FloatTensor(x)\n",
    "            x=x.view(-1,10149)\n",
    "            x=torch.tensor(x,dtype=torch.float)\n",
    "            \n",
    "            y=model(x)\n",
    "            print(y.shape)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8575711b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nfm: NFM(\n",
      "  (linear_model): Linear(in_features=537, out_features=100, bias=True)\n",
      "  (embedding_layers): Embedding(1001, 100)\n",
      "  (bi_pooling): BiInteractionPooling()\n",
      "  (dropout): Dropout(p=0.8, inplace=False)\n",
      "  (dnn_layers): ModuleList(\n",
      "    (0): Linear(in_features=200, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=8, bias=True)\n",
      "  )\n",
      "  (dnn_softmax): Softmax(dim=1)\n",
      ")\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "y_predict: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 8.0941e-01, 0.0000e+00, 0.0000e+00, 1.9059e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.1918e-12, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9451e-16, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 3.1112e-41, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 1.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [5.9594e-16, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "y_pred: torch.Size([100, 8])\n",
      "label.shape: torch.Size([100, 8])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: tensor([[0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250],\n",
      "        [0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250],\n",
      "        [0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250],\n",
      "        [0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250],\n",
      "        [0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250],\n",
      "        [0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250],\n",
      "        [0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250],\n",
      "        [0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250],\n",
      "        [0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250],\n",
      "        [0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250],\n",
      "        [0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250],\n",
      "        [0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250],\n",
      "        [0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250],\n",
      "        [0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250, 0.8250],\n",
      "        [0.0250, 0.0250, 0.8250, 0.0250, 0.0250, 0.0250, 0.0250, 0.0250]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-56-4d194d9b0df9>:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x=torch.tensor(x,dtype=torch.float)#NameError: name 'dtype' is not defined:错误摘录，torch.Tensor()和torch.tensor()的区别\n",
      "<ipython-input-56-4d194d9b0df9>:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels=torch.tensor(labels,dtype=torch.float)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-4d194d9b0df9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;31m#loss = loss_func(y_predict.view(-1), labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"labels:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m    916\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2019\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2021\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1836\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1838\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1839\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "if __name__ == \"__main__\":\n",
    "    ####################################################################################\n",
    "    # NFM 模型\n",
    "    ####################################################################################\n",
    "    BATCH_SIZE=100\n",
    "    \"\"\"\n",
    "    training_data, training_label, dense_features_col, sparse_features_col = getTrainData(nfm_config['train_file'], nfm_config['fea_file'])\n",
    "    train_dataset = Data.TensorDataset(torch.tensor(training_data).float(), torch.tensor(training_label).float())\n",
    "\n",
    "    test_data = getTestData(nfm_config['test_file'])\n",
    "    test_dataset = Data.TensorDataset(torch.tensor(test_data).float())\n",
    "    \n",
    "    test_data = getTestData(nfm_config['test_file'])\n",
    "    test_dataset = Data.TensorDataset(torch.tensor(test_data).float())\n",
    "\n",
    "    \"\"\"\n",
    "    #device = torch.device('cuda:0')\n",
    "    epoch=0\n",
    "    \n",
    "    #model=nn.Linear(10149,16).to(device)\n",
    "    #model=nn.Linear(10149,16)\n",
    "    #model=nn.ReLU(nn.Linear(10149,16))#RuntimeError: all elements of input should be between 0 and 1\n",
    "    #print('model:',model)\n",
    "    nfm = NFM(nfm_config).cuda()#加了device防止出现GPU CPU两种设备的错误提示\n",
    "    print(\"nfm:\",nfm)\n",
    "    #print(nfm)\n",
    "    #nfm.train()\n",
    "    #u=nfm.parameters()\n",
    "    #print(u)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    optimizer = torch.optim.Adam(nfm.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    total = 0\n",
    "    #loss_func = torch.nn.BCELoss()\n",
    "    loss_func=torch.nn.CrossEntropyLoss()\n",
    "    #loss_func=torch.nn.LogSoftmax()\n",
    "    \n",
    "    #model=nn.Softmax(nn.Linear(10149,16)).to(device)\n",
    "    # 从DataLoader中获取小批量的id以及数据\n",
    "    for epoch in range(100):\n",
    "        correct=0\n",
    "        for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "            #correct=0\n",
    "            #x=torch.from_numpy(x,d)\n",
    "            #x = Variable(x,requires_grad=True)\n",
    "            #x=Variable(torch.FloatTensor(x), requires_grad=True)\n",
    "            #x=torch.FloatTensor(x)\n",
    "            x=x.view(-1,537)\n",
    "            #print('x.shape:',x.shape)\n",
    "            labels=labels.view(-1,8)\n",
    "            #print('labels.shape:',labels.shape)\n",
    "            #labels=labels.squeeze(0)\n",
    "            x=torch.tensor(x,dtype=torch.float)#NameError: name 'dtype' is not defined:错误摘录，torch.Tensor()和torch.tensor()的区别\n",
    "            labels=torch.tensor(labels,dtype=torch.float)\n",
    "            #labels=Variable(labels)\n",
    "            #labels=torch.tensor(labels)\n",
    "            #labels = Variable(torch.from_numpy().float(), requires_grad=True)\n",
    "            #if self._config['use_cuda'] is True:\n",
    "            x, labels = x.cuda(), labels.cuda()\n",
    "            y_batch=labels\n",
    "            X_batch=x\n",
    "            #model.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            #y_predict = nfm(x)\n",
    "            #y_predict=model(x)\n",
    "            y_predict=nfm(x)\n",
    "            print(\"y_predict:\",y_predict)\n",
    "            ##y_predict=y_predict.squeeze(-1)#再增加\n",
    "            print('y_pred:',y_predict.shape)\n",
    "            print(\"label.shape:\",labels.shape)\n",
    "            #loss = loss_func(y_predict.view(-1), labels)\n",
    "            print(\"labels:\",labels)\n",
    "            loss=loss_func(y_predict,labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss = loss.item()\n",
    "            #print(\"loss:\",loss)\n",
    "            \n",
    "            \n",
    "            #这两行代码是求准确率的地方\n",
    "            \"\"\"\n",
    "            2.torch..max(input, dim)\n",
    "            dim = 0 表示按列求最大值，并返回最大值的索引\n",
    "            dim = 1 表示按行求最大值，并返回最大值的索引\n",
    "            \n",
    "        \"\"\"\n",
    "            predicted = torch.max(y_predict.data,1)\n",
    "            #print(\"predicted:\",predicted)\n",
    "            \n",
    "            predicted = torch.max(y_predict.data,1)[1]\n",
    "            \n",
    "            \"\"\" \n",
    "            print(\"predicted[1]:\",predicted)\n",
    "            print(\"predicted.data:\",y_predict.data)\n",
    "            print(\"y_predicted.data【1】：\",y_predict.data[1])\n",
    "            print(\"y_predicted.data[1].shape:\",y_predict.data[1].shape)\n",
    "            print(\"predicted.shape:\",predicted.shape)\n",
    "            print(\"labels.shape:\",labels.shape)\n",
    "            print(\"label\",labels)\n",
    "            \"\"\"\n",
    "           \n",
    "            labels=torch.max(labels,1)\n",
    "            #print(\"labels:\",labels)\n",
    "            labels=labels[1]\n",
    "            #labels=labels.data\n",
    "            #labels=labels.data[1]\n",
    "            \n",
    "            #print(\"predicted.shape:\",predicted.shape)\n",
    "            #print(\"labels.shape:\",labels.shape)\n",
    "            #labels=labels.squeeze(-1)\n",
    "            #labels = torch.max(labels,1)\n",
    "            #print(\"predicted:\",predicted)\n",
    "            \n",
    "            #labels = torch.max(labels,1)[1]\n",
    "            #print(\"last_labels:\",labels)\n",
    "            \n",
    "            #print(predicted==labels)\n",
    "            correct += (predicted == labels).sum()\n",
    "            #print(\"correct:\",correct)\n",
    "            #correct=correct[0]\n",
    "            print(\"new_correct:\",float(correct))\n",
    "            correct=float(correct)   \n",
    "            #if batch_idx % 10 == 0:\n",
    "            print(\"batch_idx:\",batch_idx)\n",
    "            print(correct/(BATCH_SIZE*(batch_idx+1)))\n",
    "            #print(correct/(BATCH_SIZE))\n",
    "            #print('Epoch :{}[{}/{}({:.0f}%)]\\t Loss:{:.6f}\\t Accuracy:{:.3f}'.format(epoch,batch_idx * len(X_batch),len(train_loader.dataset),100.*batch_idx / len(train_loader),loss.data.item(),correct/(BATCH_SIZE)*(batch_idx+1)))\n",
    " \n",
    "    \n",
    "            \n",
    "            \n",
    "    ####################################################################################\n",
    "    # 模型训练阶段\n",
    "    ####################################################################################\n",
    "    # # 实例化模型训练器\n",
    "    #nfm(9)\n",
    "    #trainer = Trainer(nfm, nfm_config)\n",
    "    \n",
    "    # 训练\n",
    "    #trainer._train_an_epoch(train_loader,1)\n",
    "    #trainer.train(train_dataset\n",
    "    # 保存模型\n",
    "    #trainer.save()\n",
    "\n",
    "    ####################################################################################\n",
    "    # 模型测试阶段\n",
    "    ####################################################################################\n",
    "    \"\"\"\n",
    "    nfm.eval()\n",
    "    if nfm_config['use_cuda']:\n",
    "        nfm.loadModel(map_location=lambda storage, loc: storage.cuda(nfm_config['device_id']))\n",
    "        nfm = nfm.cuda()\n",
    "    else:\n",
    "        nfm.loadModel(map_location=torch.device('cpu'))\n",
    "\n",
    "    y_pred_probs = nfm(torch.tensor(test_data).float())\n",
    "    y_pred = torch.where(y_pred_probs>0.5, torch.ones_like(y_pred_probs), torch.zeros_like(y_pred_probs))\n",
    "    print(\"Test Data CTR Predict...\\n \", y_pred.view(-1))\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ba2b38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nfm: NFM(\n",
      "  (linear_model): Linear(in_features=537, out_features=100, bias=True)\n",
      "  (embedding_layers): Embedding(1001, 100)\n",
      "  (bi_pooling): BiInteractionPooling()\n",
      "  (dropout): Dropout(p=0.8, inplace=False)\n",
      "  (dnn_layers): ModuleList(\n",
      "    (0): Linear(in_features=200, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=8, bias=True)\n",
      "  )\n",
      "  (dnn_softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "only Tensors of floating point dtype can require gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-9323ab81a905>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m#x = torch.tensor(x, dtype=torch.float)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: only Tensors of floating point dtype can require gradients"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "if __name__ == \"__main__\":\n",
    "    ####################################################################################\n",
    "    # NFM 模型\n",
    "    ####################################################################################\n",
    "    BATCH_SIZE=100\n",
    "    \"\"\"\n",
    "    training_data, training_label, dense_features_col, sparse_features_col = getTrainData(nfm_config['train_file'], nfm_config['fea_file'])\n",
    "    train_dataset = Data.TensorDataset(torch.tensor(training_data).float(), torch.tensor(training_label).float())\n",
    "\n",
    "    test_data = getTestData(nfm_config['test_file'])\n",
    "    test_dataset = Data.TensorDataset(torch.tensor(test_data).float())\n",
    "    \n",
    "    test_data = getTestData(nfm_config['test_file'])\n",
    "    test_dataset = Data.TensorDataset(torch.tensor(test_data).float())\n",
    "\n",
    "    \"\"\"\n",
    "    #device = torch.device('cuda:0')\n",
    "    epoch=0\n",
    "    \n",
    "    #model=nn.Linear(10149,16).to(device)\n",
    "    #model=nn.Linear(10149,16)\n",
    "    #model=nn.ReLU(nn.Linear(10149,16))#RuntimeError: all elements of input should be between 0 and 1\n",
    "    #print('model:',model)\n",
    "    nfm = NFM(nfm_config).cuda()#加了device防止出现GPU CPU两种设备的错误提示\n",
    "    print(\"nfm:\",nfm)\n",
    "    #print(nfm)\n",
    "    #nfm.train()\n",
    "    #u=nfm.parameters()\n",
    "    #print(u)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    optimizer = torch.optim.Adam(nfm.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    total = 0\n",
    "    #loss_func = torch.nn.BCELoss()\n",
    "    loss_func=torch.nn.CrossEntropyLoss()\n",
    "    #loss_func=torch.nn.LogSoftmax()\n",
    "    \n",
    "    #model=nn.Softmax(nn.Linear(10149,16)).to(device)\n",
    "    # 从DataLoader中获取小批量的id以及数据\n",
    "    for epoch_id in range(100):\n",
    "        correct=0\n",
    "        total=0\n",
    "        for batch_id, (x, labels) in enumerate(train_loader):\n",
    "            x = Variable(x)\n",
    "            labels = Variable(labels)\n",
    "            \n",
    "            \n",
    "            #x = torch.tensor(x, dtype=torch.float)\n",
    "            x=x.clone().detach().requires_grad_(True)\n",
    "            labels=torch.tensor(labels,dtype=torch.float)\n",
    "            x, labels = x.cuda(), labels.cuda()\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_predict = nfm(x)\n",
    "            #loss = loss_func(y_predict.view(-1), labels)\n",
    "            loss = loss_func(y_predict, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss = loss.item()\n",
    "            #loss, predicted = self._train_single_batch(x, labels)\n",
    "\n",
    "            total += loss\n",
    "            \n",
    "            #y_predict.detach().numpy()\n",
    "            pred = y_predict\n",
    "            print(\"pred:\",pred.shape)\n",
    "            y=labels.clone().detach().requires_grad_(True)\n",
    "            print(\"y:\",y.shape)\n",
    "            #y=labels.data.cpu().numpy()\n",
    "            #y = labels.detach().numpy()\n",
    "            roc_auc_score(y, pred)\n",
    "            # print('[Training Epoch: {}] Batch: {}, Loss: {}'.format(epoch_id, batch_id, loss))\n",
    "        print(\"Training Epoch: %d, total loss: %f\" % (epoch_id, total))\n",
    "        print(\"auc:\",roc_auc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c33d283d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 1 (got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6165f92f2015>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#c=torch.tensor(c,dtype=int )#TypeError: squeeze(): argument 'input' (position 1) must be Tensor, not int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#c=torch.tensor(c,dtype=torch.int)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 3 at dim 1 (got 1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "c=torch.tensor([[1,2,3],[1,],[3,2,4],[1,3,6]])\n",
    "#c=torch.tensor(c,dtype=int )#TypeError: squeeze(): argument 'input' (position 1) must be Tensor, not int\n",
    "#c=torch.tensor(c,dtype=torch.int)\n",
    "print(c)\n",
    "c=c.squeeze(-1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa3a093",
   "metadata": {},
   "outputs": [],
   "source": [
    "#这两行代码是求准确率的地方\n",
    "        predicted = torch.max(outputs.data,1)[1]\n",
    "        correct += (predicted == y_batch).sum()\n",
    "        #print(correct)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Epoch :{}[{}/{}({:.0f}%)]\\t Loss:{:.6f}\\t Accuracy:{:.3f}'.format(epoch,batch_idx * len(X_batch),len(train_loader.dataset),100.*batch_idx / len(train_loader),loss.data.item(),float(correct)/float(BATCH_SIZE)*(batch_idx+1)))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8441076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: tensor([[0.0125, 0.0125, 0.0125,  ..., 0.0125, 0.0125, 0.0125],\n",
      "        [0.0125, 0.0125, 0.0125,  ..., 0.0125, 0.0125, 0.0125],\n",
      "        [0.0125, 0.0125, 0.8125,  ..., 0.0125, 0.0125, 0.0125],\n",
      "        ...,\n",
      "        [0.0125, 0.0125, 0.0125,  ..., 0.0125, 0.0125, 0.0125],\n",
      "        [0.8125, 0.0125, 0.0125,  ..., 0.0125, 0.0125, 0.0125],\n",
      "        [0.0125, 0.0125, 0.0125,  ..., 0.0125, 0.0125, 0.0125]],\n",
      "       device='cuda:0')\n",
      "predicted: torch.return_types.max(\n",
      "values=tensor([0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125,\n",
      "        0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125,\n",
      "        0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125,\n",
      "        0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125,\n",
      "        0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125,\n",
      "        0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125,\n",
      "        0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125,\n",
      "        0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125,\n",
      "        0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125,\n",
      "        0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125,\n",
      "        0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125, 0.8125,\n",
      "        0.8125], device='cuda:0'),\n",
      "indices=tensor([10, 11,  2, 11,  5,  2,  8,  9,  3, 10,  2,  8,  5,  3,  5,  3, 10, 11,\n",
      "         8,  3,  3,  8,  8,  9,  7,  3, 10,  3,  9, 10,  8,  8,  3,  7,  6, 11,\n",
      "         0, 11,  3,  2,  3,  6,  3,  6,  3, 11,  3,  6, 11,  0,  7,  9,  2,  3,\n",
      "        11, 11,  5,  3,  0,  3,  0, 10,  3,  8, 10,  6,  8,  6, 11,  8,  0, 10,\n",
      "         6,  3,  8, 10, 10,  2,  5, 10,  3, 10,  0,  2,  9,  2,  8,  0,  8,  8,\n",
      "         2,  3,  6, 10,  6, 10, 11,  8,  0,  3], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "print(\"labels:\",labels)\n",
    "predicted = torch.max(labels,1)\n",
    "print(\"predicted:\",predicted)\n",
    "            \n",
    "#predicted = torch.max(y_predict.data,1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89056b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n"
     ]
    }
   ],
   "source": [
    "m=torch.tensor(9)\n",
    "m=float(m)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaceaf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
