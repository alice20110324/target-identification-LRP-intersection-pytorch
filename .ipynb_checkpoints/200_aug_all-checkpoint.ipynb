{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ed2f8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bn0.weight\n",
      "bn0.bias\n",
      "bn0.running_mean\n",
      "bn0.running_var\n",
      "bn0.num_batches_tracked\n",
      "fc1.weight\n",
      "fc1.bias\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "bn1.running_mean\n",
      "bn1.running_var\n",
      "bn1.num_batches_tracked\n",
      "fc2.weight\n",
      "fc2.bias\n",
      "bn2.weight\n",
      "bn2.bias\n",
      "bn2.running_mean\n",
      "bn2.running_var\n",
      "bn2.num_batches_tracked\n",
      "fc3.weight\n",
      "fc3.bias\n",
      "bn3.weight\n",
      "bn3.bias\n",
      "bn3.running_mean\n",
      "bn3.running_var\n",
      "bn3.num_batches_tracked\n",
      "MLP(\n",
      "  (bn0): BatchNorm1d(253, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=253, out_features=1000, bias=True)\n",
      "  (bn1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=100, out_features=9, bias=True)\n",
      "  (bn3): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "relevance\n"
     ]
    }
   ],
   "source": [
    "#############  u===2   MLP_encode_100 and MLP_encode_1000  :654    ##################\n",
    "import torch\n",
    "#import Trainer\n",
    "from network import NFM\n",
    "import torch.utils.data as Data\n",
    "from Utils.criteo_loader import getTestData, getTrainData\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import sys \n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "import torchmetrics\n",
    "\n",
    "from innvestigator import InnvestigateModel\n",
    "from utils import Flatten\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from mnist_test import Net, train, test\n",
    "\n",
    "input_num=253\n",
    "# Network parameters\n",
    "class Params(object):\n",
    "    batch_size = 64\n",
    "    test_batch_size = 20\n",
    "    epochs = 5\n",
    "    lr = 0.01\n",
    "    momentum = 0.5\n",
    "    no_cuda = True\n",
    "    seed = 1\n",
    "    log_interval = 10\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "args = Params()\n",
    "torch.manual_seed(args.seed)\n",
    "#device = torch.device(\"cpu\")\n",
    "device=torch.device('cuda')\n",
    "kwargs = {}\n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':9,\n",
    "    \n",
    "    'num_epoch': 500,#训练epoch次数\n",
    "    'batch_size': 16,#batch_size\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    'epoch':1000,\n",
    "    \n",
    "   \n",
    "    'gene_name':'dataset/qiuguan/origin_800/gene_name.csv',\n",
    "    'label_name':'dataset/qiuguan/origin_800/gene_label.csv'\n",
    "    \n",
    "}\n",
    "\n",
    "#model definition\n",
    "import torch.nn as nn\n",
    "    \n",
    "    \n",
    "\n",
    "#model1 = MLP().cuda()\n",
    "#print(model1)\n",
    "\n",
    "class MLP_P(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn0=nn.BatchNorm1d(input_num)\n",
    "        self.fc1 = nn.Linear(input_num, 1000)\n",
    "        self.bn1= nn.BatchNorm1d(1000)\n",
    "        self.fc2 = nn.Linear(1000, 100)\n",
    "        self.bn2=nn.BatchNorm1d(100)\n",
    "        self.fc3=nn.Linear(100,9)\n",
    "        self.bn3=nn.BatchNorm1d(9)\n",
    "        \n",
    "        self.drop=nn.Dropout(0.3)    \n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn0=nn.BatchNorm1d(input_num)\n",
    "        self.fc1 = nn.Linear(input_num, 1000)\n",
    "        self.bn1= nn.BatchNorm1d(1000)\n",
    "        self.fc2 = nn.Linear(1000, 100)\n",
    "        self.bn2=nn.BatchNorm1d(100)\n",
    "        self.fc3=nn.Linear(100,9)\n",
    "        self.bn3=nn.BatchNorm1d(9)\n",
    "        \n",
    "        self.drop=nn.Dropout(0.3) \n",
    "        \n",
    "        #self.model1=MLP1().cuda() \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        y1=self.bn0(x)\n",
    "        y1 = F.relu(self.drop(self.bn1(self.fc1(y1))))\n",
    "        y1= F.relu(self.drop(self.bn2(self.fc2(y1))))\n",
    "        return F.softmax(self.bn3(self.fc3(y1)), dim=1)\n",
    "        \n",
    "           \n",
    "        \n",
    "        \n",
    "        \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "model = MLP().cuda()\n",
    "\n",
    "for i in model.state_dict():\n",
    "    print(i)\n",
    "print(model)\n",
    "mlp_paras=list(model.named_parameters())\n",
    "#print(mlp_paras)\n",
    "inn_model = InnvestigateModel(model, lrp_exponent=2,\n",
    "                              method=\"e-rule\",\n",
    "                              beta=.5)\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import torch.nn.functional as F  # 激励函数的库\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "\n",
    "def one_hot_smoothing(labels, classes, label_smoothing=0.2):\n",
    "    #n = len(labels)\n",
    "    n=labels.shape[0]\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    n = len(labels)\n",
    "    #eoff = label_smoothing / classes\n",
    "    output = np.zeros((n, classes), dtype=np.float32)\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_smoothing(labels, classes, label_smoothing=0.2):\n",
    "    #n = len(labels)\n",
    "    n=labels.shape[0]\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    n = len(labels)\n",
    "    #eoff = label_smoothing / classes\n",
    "    output = np.zeros((n, classes), dtype=np.float32)\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "class KZDatasetTest(data.Dataset):\n",
    "    \"\"\" Construct the FM pytorch dataset. \"\"\"\n",
    "    #def __init__(self, file,label_file, feature_map,n_class=16):\n",
    "    def __init__(self, csv_path):\n",
    "    \n",
    "       \n",
    "        self.data_info = self.get_data_info(csv_path)\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Dataset读取图片的函数\n",
    "        data, label = self.data_info[index]\n",
    "        #img = Image.open(img_pth).convert('RGB')\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_data_info(self,csv_path):\n",
    "        #解析路径\n",
    "        #转为一维list存储，每一位为【图片路径，图片类别】\n",
    "        labels=[]\n",
    "        data_info=[]\n",
    "        df=pd.read_csv(csv_path,sep=',',header=None)\n",
    "        #print(\"df:\",df)\n",
    "        df=df.iloc[2:,1:]\n",
    "        \n",
    "        rows,cols=df.shape\n",
    "        #print(rows,cols)\n",
    "        for i in df.iloc[:,-1]:\n",
    "            #print(i)\n",
    "            labels.append(int(float(i)))\n",
    "        #print('labels:',labels)\n",
    "        labels=np.array(labels)\n",
    "        \n",
    "        labels=one_hot_smoothing(labels,nfm_config['n_class'])\n",
    "        #print(labels)\n",
    "        for i in range(rows):\n",
    "            data=df.iloc[i,:-1]\n",
    "            data=data.astype(float)##############\n",
    "            \n",
    "            data=np.array(data)##\n",
    "            \n",
    "            label=labels[i]\n",
    "            \n",
    "            data=torch.from_numpy(data)#\n",
    "            label=torch.from_numpy(label)#\n",
    "           \n",
    "            \n",
    "            data_info.append((data,label))\n",
    "        return data_info\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import *\n",
    "from PIL import Image\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ast\n",
    "import torchvision\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "class KZDataset(Dataset):\n",
    "    def __init__(self, csv_path, K,n_class,ki=0, typ='train', transform=None, rand=False):\n",
    "        \n",
    "        self.all_data_info = self.get_data_info(csv_path)\n",
    "        \n",
    "        if rand:\n",
    "            random.seed(1)\n",
    "            random.shuffle(self.all_data_info)\n",
    "        leng = len(self.all_data_info)\n",
    "        every_z_len = leng // K\n",
    "        if typ == 'val':\n",
    "            self.data_info = self.all_data_info[every_z_len * ki : every_z_len * (ki+1)]\n",
    "        elif typ == 'train':\n",
    "            self.data_info = self.all_data_info[: every_z_len * ki] + self.all_data_info[every_z_len * (ki+1) :]\n",
    "            \n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Dataset读取图片的函数\n",
    "        data, label = self.data_info[index]\n",
    "        #img = Image.open(img_pth).convert('RGB')\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "       \n",
    "    \n",
    "    def get_data_info(self,csv_path):\n",
    "        #解析路径\n",
    "        #转为一维list存储，每一位为【图片路径，图片类别】\n",
    "        labels=[]\n",
    "        data_info=[]\n",
    "        df=pd.read_csv(csv_path,sep=',',header=None)\n",
    "        #print(\"df:\",df)\n",
    "        df=df.iloc[2:,1:]\n",
    "        #print(\"df:\",df)\n",
    "        print(df.shape)\n",
    "        #print(\"df:\",df)\n",
    "        #print(df.iloc[:,-1])\n",
    "        #df=df.applymap(ast.literal_eval)\n",
    "        rows,cols=df.shape\n",
    "        #print(rows,cols)\n",
    "        for i in df.iloc[:,-1]:\n",
    "            #print(i)\n",
    "            labels.append(int(float(i)))\n",
    "        #print('labels:',i,labels)\n",
    "        labels=np.array(labels)\n",
    "        #print('labels:',labels)\n",
    "        #labels=np.array(labels)\n",
    "        labels=one_hot_smoothing(labels,nfm_config['n_class'])\n",
    "        #print(labels)\n",
    "        for i in range(rows):\n",
    "            data=df.iloc[i,:-1]\n",
    "            data=data.astype(float)##############\n",
    "            #print(\"i,data:\",i,data)\n",
    "            #data=pd.DataFrame(data,dtype=float)###############\n",
    "            data=np.array(data)##\n",
    "            \n",
    "            label=labels[i]\n",
    "            #print(data.shape)\n",
    "            #print(label.shape)\n",
    "            #label=label.tolist()\n",
    "            data=torch.from_numpy(data)#\n",
    "            label=torch.from_numpy(label)#\n",
    "           \n",
    "            \n",
    "            data_info.append((data,label))\n",
    "        return data_info\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import sys \n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "import torchmetrics\n",
    "#LRP\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline\n",
    "\n",
    "from innvestigator import InnvestigateModel\n",
    "from utils import Flatten\n",
    "def standN(x):\n",
    "    row,col=x.shape\n",
    "    max_x=torch.max(x,1)\n",
    "    min_x=torch.min(x,1)\n",
    "    #print(max_x)\n",
    "    #print(min_x)\n",
    "    for i in range(row):\n",
    "        x[i]=(x[i]-min_x.values[i])/max_x.values[i]\n",
    "    return x\n",
    "\n",
    "def standNorm(x):\n",
    "    row,col=x.shape\n",
    "    mean=torch.mean(x,1)\n",
    "    std=torch.std(x,1)\n",
    "    #print(mean)\n",
    "    #print(std)\n",
    "    for i in range(row):\n",
    "        x[i]=(x[i]-mean[i])/std[i]\n",
    "    return x\n",
    "\n",
    "def augment_epoch(kii,train_loader,batch_size):\n",
    "    BATCH_SIZE=batch_size\n",
    "    total = 0\n",
    "    correct=0\n",
    "    total_loss=0\n",
    "    #loss_score=torch.tensor([[]]).cuda()\n",
    "    #\n",
    "    #loss_op=0\n",
    "    loss2_list=[]\n",
    "    model.train()\n",
    "    total_train_accuracy=0  \n",
    "    #BL=nn.BatchNorm1d(input_num)\n",
    "    #BL=BL.cuda()\n",
    "    y=[[0]*input_num]*batch_size\n",
    "    y=torch.tensor(y,dtype=torch.float)\n",
    "    y=y.cuda()\n",
    "    \n",
    "    for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "            \n",
    "        labels = Variable(labels)\n",
    "        x = Variable(x)\n",
    "            \n",
    "        x_row,x_col=x.shape   \n",
    "        x=torch.tensor(x,dtype=torch.float)\n",
    "        #print(x.shape)\n",
    "        labels=torch.tensor(labels,dtype=torch.float)\n",
    "        x, labels = x.cuda(), labels.cuda()\n",
    "        labels_int=labels=torch.max(labels,1)[1]#########\n",
    "        #print(labels_int)\n",
    "        #print('labels_int:',labels_int.shape)\n",
    "        #print('labels:',labels) \n",
    "        #print('x:',x.shape)\n",
    "        ll=labels.view(batch_size,-1)\n",
    "        #ll.dtype=torch.int\n",
    "        #print('ll:',ll)\n",
    "        if kii>=0 : \n",
    "            \n",
    "            inn_model.evaluate(in_tensor=x)\n",
    "        \n",
    "            model_prediction, only_max_score,org_shape = inn_model.innvestigatex()\n",
    "            model_prediction.cuda()\n",
    "            #print('only_max_score:',only_max_score)\n",
    "            #only_max_score1=only_max_score.detach().clone()\n",
    "            #print(labels.shape,labels)   \n",
    "        \n",
    "            #print(\"torch.argmax(labels[batch_idx]):\",torch.argmax(labels[batch_idx]))\n",
    "            #print(model_prediction.shape, model_prediction)\n",
    "            #model_prediction.cuda()\n",
    "            #print('input_relevance_values:')\n",
    "            rel_for_class_list=torch.argmax(model_prediction,1)\n",
    "            #print(rel_for_class_list)\n",
    "            para_list=[]\n",
    "            \n",
    "            '''  \n",
    "            rand=torch.linspace(0, 1, steps=input_num)\n",
    "            rand=rand.detach().numpy().tolist()\n",
    "            \n",
    "            para_list=[round(i,4) for i in rand]\n",
    "            '''\n",
    "            \n",
    "            #print(para_list)\n",
    "            \n",
    "            \n",
    "            for i in range(batch_size):\n",
    "            \n",
    "                max_pre=torch.argmax(model_prediction[i])\n",
    "                if max_pre!=labels[i]:\n",
    "                    #print('only_max_score1:',i,only_max_score)\n",
    "                    #only_max_score=only_max_score1\n",
    "                    #input_relevance_values=inn_model.compute_relevance_score(only_max_score,labels[i],org_shape,para=0.1)\n",
    "                    #rel_for_class=labels[i]\n",
    "                    #para=random.gauss(0.6,0.4)#########0.6，0.4：98  96\n",
    "                    \n",
    "                    #para=random.gauss(0.6,0.7)############0.6  0.7  96  95\n",
    "                    \n",
    "                    #para=random.gauss(0.7,0.4)###############\n",
    "                    #para=random.gauss(0.7,0.3)#############9110  96  98，0.2，0.1\n",
    "                    para=random.gauss(0.6,0.3)########96  98  0.1  0.1\n",
    "                    para=round(para,4)\n",
    "                    para_list.append(para)\n",
    "                    #print('device:',input_relevance_values.device)\n",
    "                    #input_relevance_values[labels[i]]=input_relevance_values[labels[i]].cuda()\n",
    "                    #input_relevance_values[labels[i]]=input_relevance_values[labels[i]].exp()\n",
    "                    #print('input_relevance_values:',i, input_relevance_values)\n",
    "                    #print('input_relevance_values[labels[i]]:',i,batch_idx,input_relevance_values[labels[i]])\n",
    "                    #cc[i]=torch.mul(x[i],input_relevance_values[labels[i]])+torch.tensor(0.1,dtype=torch.float)##############注意力\n",
    "                    #print('pre_target:',pre_target[i])\n",
    "                    #print('cc',i,cc[i].shape)\n",
    "                else:\n",
    "                    #print('only_max_score2:',i,only_max_score)\n",
    "                    #only_max_score=only_max_score1\n",
    "                    #input_relevance_values=inn_model.compute_relevance_score(only_max_score,labels[i],org_shape,para=0)\n",
    "                    #input_relevance_values[labels[i]]=input_relevance_values[labels[i]].exp()\n",
    "                    #print('input_relevance_values:',i, input_relevance_values)\n",
    "                    #print('input_relevance_values[i]:',i,batch_idx,input_relevance_values[labels[i]])\n",
    "                    #cc[i]=torch.mul(x[i],input_relevance_values[labels[i]])##############注意力\n",
    "                    #print('cc',i,cc[i].shape)\n",
    "                    para=random.gauss(0.1,0.1)\n",
    "                    para=round(para,4)\n",
    "                    para_list.append(para)\n",
    "            \n",
    "            \n",
    "            \n",
    "            input_relevance_values,layers=inn_model.compute_relevance_scorex(only_max_score,rel_for_class_list,org_shape,para_list)\n",
    "            input_len=len(input_relevance_values)\n",
    "            \n",
    "            ee=[[0]*input_num]*batch_size\n",
    "            ee=torch.tensor(ee,dtype=torch.float)\n",
    "        \n",
    "            ee=ee.cuda()\n",
    "            input_relevance_values[-1]=input_relevance_values[-1].exp()############\n",
    "            \n",
    "            \n",
    "            #sum_input_relevance_values=torch.sum(input_relevance_values,dim=0)\n",
    "            #input_relevance_values=F.softmax(input_relevance_values/sum_input_relevance_values,dim=1)\n",
    "            input_relevance_values[-1]=F.softmax(input_relevance_values[-1],dim=1)################\n",
    "        \n",
    "            '''\n",
    "            for i in range(batch_size):\n",
    "                        max_pre=torch.argmax(model_prediction[i])######\n",
    "                        if max_pre!=labels[i]: #and i==min_predict_mis:###\n",
    "                            \n",
    "                                \n",
    "            \n",
    "                                ee[i]=torch.mul(x[i],input_relevance_values[-1][i])##############注意力33333333333333333weight\n",
    "                                #dd=torch.mul(bias,input_relevance_values[-1][i])############bias\n",
    "                                #print('cc:',i,cc)\n",
    "                                #ee[i]=torch.mul(drop,input_relevance_values[i])\n",
    "                                ################\n",
    "            '''                    \n",
    "            ee=torch.mul(x,input_relevance_values[-1])\n",
    "            \n",
    "            \n",
    "            x1=torch.subtract(x,ee)\n",
    "            x2=torch.add(x,ee)\n",
    "            #print(ll.shape)\n",
    "            #print(x.shape)\n",
    "            #x1=torch.cat((x1,ll),dim=1)\n",
    "            #x2=torch.cat((x2,ll),dim=1)\n",
    "            \n",
    "            l1=ll\n",
    "            l2=ll\n",
    "            if batch_idx==0:\n",
    "                \n",
    "                y=torch.cat((x1,x2),dim=0)\n",
    "                l=torch.cat((l1,l2),dim=0)\n",
    "            y=torch.cat((y,x1),dim=0)\n",
    "            y=torch.cat((y,x2),dim=0)\n",
    "            \n",
    "            l=torch.cat((l,l1),dim=0)\n",
    "            l=torch.cat((l,l2),dim=0)\n",
    "    \n",
    "    \n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "                    \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "              \n",
    "            \n",
    "            \n",
    "                \n",
    "                \n",
    "                \n",
    "                        \n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "    '''  \n",
    "        #print('x_new:',x)\n",
    "        #print('new_x:',x)\n",
    "        optimizer.zero_grad()\n",
    "        y_predict=model(x)\n",
    "        #y_predict=model(x,labels)\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        loss1 = loss_func(y_predict, labels)\n",
    "        #print('loss1:',loss1)\n",
    "        #loss2=loss_func(y_predict1,labels)\n",
    "        #loss2=u*(1/loss_op)\n",
    "        #print('input_relevance_values:',input_relevance_values.shape)\n",
    "        #print('loss_score:',loss_score.shape)\n",
    "        #cc=1.0/torch.abs(torch.sum(cc))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #loss2=u*cc\n",
    "        #print('input_relevance_values:',input_relevance_values)\n",
    "        \n",
    "        #print('loss1:',loss1)\n",
    "        #print('loss2:',loss2)\n",
    "        #loss=loss1+loss2\n",
    "        loss=loss1\n",
    "        #loss = loss_func(y_predict, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #loss2_list.append(u*loss2)   \n",
    "        loss = loss.item()\n",
    "           \n",
    "\n",
    "        total_loss += loss\n",
    "            \n",
    "            \n",
    "            \n",
    "        batch_train_acc=torchmetrics.functional.accuracy(y_predict,labels_int)\n",
    "        total_train_accuracy+=batch_train_acc\n",
    "    #plotLoss(loss2_list,batch_idx+1)   #################################     \n",
    "    total_train_accuracy/=(batch_idx+1)\n",
    "    print('total_train_accuracy:',total_train_accuracy)\n",
    "    print(\"Training Epoch: %d, total loss: %f\" % (epoch_id, total_loss))\n",
    "    return total_loss,total_train_accuracy\n",
    "    \n",
    "    ''' \n",
    "    return y,l\n",
    "\n",
    "def val_epoch(test_loader,batch_size,optimizer): \n",
    "    batch_size_num=0\n",
    "    total_test_acc=0\n",
    "    model.eval()\n",
    "    for i , (inputs , targets) in enumerate(test_loader):   \n",
    "        print(\"val\")\n",
    "            \n",
    "            \n",
    "        inputs = Variable(inputs)   \n",
    "        targets = Variable(targets)     \n",
    "           \n",
    "        inputs=torch.tensor(inputs ,dtype=torch.float)   \n",
    "        targets=torch.tensor(targets ,dtype=torch.float)   \n",
    "        inputs , targets = inputs.cuda(),  targets.cuda()\n",
    "        targets=torch.max(targets,1)[1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #yhat = model1(inputs,targets)\n",
    "        yhat=model(inputs)\n",
    "            \n",
    "            \n",
    "            \n",
    "        #targets=torch.max(targets,1)[1]\n",
    "            \n",
    "            \n",
    "            \n",
    "        batch_test_acc=torchmetrics.functional.accuracy(yhat,targets)\n",
    "            \n",
    "        total_test_acc+=batch_test_acc\n",
    "            \n",
    "        batch_size_num=i\n",
    "    total_test_acc/=(batch_size_num+1)\n",
    "        ###print('total_test_accuracy:',total_test_acc/(batch_size+1))\n",
    "    print('total_test_accuracy:',total_test_acc)\n",
    "        \n",
    "                    \n",
    "                    \n",
    "            \n",
    "            \n",
    "    \n",
    "        \n",
    "   \n",
    "    \n",
    "    return total_test_acc\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotLoss(loss,epoch):\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    x=[i for i in range(epoch)]\n",
    "    #acc_train=acc_train.cpu()\n",
    "    #acc_test=acc_test.cpu()\n",
    "    plt.plot(x, loss, 'r-', mec='k', label='Logistic Loss', lw=2)\n",
    "    #plt.plot(x,acc_train,'b-',mec='k',label='accuracy Train',lw=2)\n",
    "    #plt.plot(x,acc_test,'g-',mec='k',label='accuracy Test',lw=2)\n",
    "    #plt.plot(x, y_01, 'g-', mec='k', label='0/1 Loss', lw=2)\n",
    "    #plt.plot(x, y_hinge, 'b-',mec='k', label='Hinge Loss', lw=2)\n",
    "    #plt.plot(x, boost, 'm--',mec='k', label='Adaboost Loss',lw=2)\n",
    "    plt.grid(True, ls='--')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('损失函数')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1458e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dbdb8926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (bn0): BatchNorm1d(253, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=253, out_features=1000, bias=True)\n",
      "  (bn1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=100, out_features=9, bias=True)\n",
      "  (bn3): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "relevance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:440: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:442: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1          2         3         4         5         6    \\\n",
      "0     4.713817  9.061473   7.784545  6.888749  5.106256  6.449218  8.771512   \n",
      "1     4.873114  9.197755   5.552020  6.863386  5.264899  7.234107  8.793950   \n",
      "2     5.122286  8.447052   5.010365  7.261227  4.499170  9.397468  8.356542   \n",
      "3     5.204596  8.750046   9.360844  8.070467  5.416446  7.979344  8.909450   \n",
      "4     5.094244  8.976845   8.816326  6.959654  5.042892  6.858524  8.818862   \n",
      "...        ...       ...        ...       ...       ...       ...       ...   \n",
      "1115  5.548376  8.640358  11.239311  7.622852  5.661729  7.784764  8.763618   \n",
      "1116  5.766167  9.042012  10.507083  6.776916  5.997128  7.693694  8.073849   \n",
      "1117  5.449224  9.504914   9.695319  8.000257  5.379730  6.582393  7.514043   \n",
      "1118  5.314301  9.223829  10.902714  6.792574  5.468862  6.245750  9.168919   \n",
      "1119  6.098722  9.830072  14.262702  7.167557  5.110237  7.490800  8.305342   \n",
      "\n",
      "            7         8         9    ...       244        245        246  \\\n",
      "0      9.621956  6.081521  6.673338  ...  4.708681   8.632880   9.687784   \n",
      "1      9.453402  5.713405  6.619789  ...  4.625470   9.006108   9.975176   \n",
      "2      9.286455  5.879249  6.795776  ...  5.622236   8.203370   9.227241   \n",
      "3      9.415679  6.051291  6.678167  ...  5.054687   8.696851   9.353734   \n",
      "4      9.615933  5.966228  6.624414  ...  4.961370   8.372217   9.778141   \n",
      "...         ...       ...       ...  ...       ...        ...        ...   \n",
      "1115   9.289927  6.583532  7.083311  ...  5.236454  10.055302   9.868288   \n",
      "1116   8.954114  6.556721  6.875843  ...  5.044769   7.190833  10.107918   \n",
      "1117  10.307979  7.434063  6.331275  ...  5.264178   9.277337  10.592884   \n",
      "1118   9.762880  6.136823  7.262049  ...  5.136260   8.325504  10.285226   \n",
      "1119   9.274586  6.363458  7.427974  ...  5.343798   7.798296  10.475674   \n",
      "\n",
      "           247       248       249       250       251       252  0    \n",
      "0     7.284761  5.915208  9.197577  6.923080  4.280621  7.494098    2  \n",
      "1     7.131264  6.031895  8.694609  8.121610  4.592810  7.889319    4  \n",
      "2     7.009006  5.443134  9.125780  8.358521  4.352551  7.014194    1  \n",
      "3     6.393789  5.799902  8.879117  7.522341  4.383837  7.416082    8  \n",
      "4     6.645982  6.141311  9.267976  7.162416  4.608670  7.309915    8  \n",
      "...        ...       ...       ...       ...       ...       ...  ...  \n",
      "1115  7.311229  5.861302  9.447378  9.085924  4.967437  7.599844    1  \n",
      "1116  6.757883  5.991660  8.893085  6.187199  5.413517  7.793299    4  \n",
      "1117  6.399415  6.014355  9.874236  7.756323  5.275237  8.066117    8  \n",
      "1118  6.231947  6.780967  9.070424  6.649499  4.890666  7.579297    6  \n",
      "1119  6.906374  5.793839  9.003146  6.699020  4.833356  7.837696    5  \n",
      "\n",
      "[1120 rows x 254 columns]\n"
     ]
    }
   ],
   "source": [
    "#_,model=MLPA().cuda\n",
    "import torch\n",
    "\n",
    "#功能：加载保存到path中的各层参数到神经网络\n",
    "\n",
    "path='dataset/qiuguan/origin_800/LRP/non_encode/200/without_attention/MLP1010.pkl'\n",
    "\n",
    "#nfm=NFM(nfm_config)\n",
    "mlp=MLP()\n",
    "#print(nfm)\n",
    "#net = nn.DataParallel(net)\n",
    "#net = net.to(device)\n",
    "mlp.load_state_dict(torch.load(path),strict=False)\n",
    "mlp.cuda()\n",
    "\n",
    "print(mlp)\n",
    "\n",
    "\n",
    "#print(model.state_dict())\n",
    "\n",
    "mlp_params = list(mlp.named_parameters())\n",
    "#print(mlp_params)\n",
    "model=mlp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inn_model = InnvestigateModel(model, lrp_exponent=2,\n",
    "                              method=\"e-rule\",\n",
    "                              beta=.5)\n",
    "\n",
    "\n",
    "import itertools\n",
    "\n",
    "K=10\n",
    "test_metrics=[]\n",
    "train_loss_total_list=[]\n",
    "\n",
    "testset= KZDatasetTest(csv_path='dataset/qiuguan/origin_800/LRP/200/selected_train_val_info.csv')\n",
    "   \n",
    "test_loader = data.DataLoader(\n",
    "         dataset=testset,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         drop_last=True,\n",
    "         batch_size=nfm_config['batch_size']\n",
    "        \n",
    "     )\n",
    "\n",
    "\n",
    "#model_path='dataset/qiuguan/origin_800/LRP/non_encode/200/attention0.01/'\n",
    "#BATCH_SIZE=batch_size\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "#total = 0\n",
    "    \n",
    "    \n",
    "#loss_func=torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "num=0\n",
    "   \n",
    "model_named_parameters=[j for j in model.named_parameters()]\n",
    "#print('model_parameters:',model_named_parameters)\n",
    "#print('model.state.dict:',model.state_dict())\n",
    "epoches=1\n",
    "for epoch_id in range(epoches):\n",
    "          \n",
    "        \n",
    "        \n",
    "        y,l=augment_epoch(0,test_loader,nfm_config['batch_size'])\n",
    "        #train_loss_total_list.append(train_loss_total)#\n",
    "        \n",
    "        '''  \n",
    "        if epoch_id %10==0:\n",
    "            num=num+1\n",
    "            path=os.path.join(model_path,'MLP'+str(num)+str(K)+'.pkl')\n",
    "            torch.save(model.state_dict(),path)\n",
    "            \n",
    "         '''  \n",
    "y=y.detach().cpu().numpy()\n",
    "y=pd.DataFrame(y)\n",
    "\n",
    "l=l.detach().cpu().numpy()\n",
    "l=pd.DataFrame(l)\n",
    "#print(y)\n",
    "#print(l)\n",
    "\n",
    "\n",
    "aug=pd.concat((y,l),axis=1)\n",
    "aug.to_csv('dataset/qiuguan/origin_800/LRP/200/aug_train_val_info.csv')\n",
    "print(aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2fc96fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            TYR               RPA3                REN               ABCC3  \\\n",
      "0        4.71382            9.06147            7.78455             6.88875   \n",
      "1        4.87311            9.19775            5.55202             6.86339   \n",
      "2        5.12229            8.44705            5.01036             7.26123   \n",
      "3         5.2046            8.75005            9.36084             8.07047   \n",
      "4        5.09424            8.97684            8.81633             6.95965   \n",
      "..           ...                ...                ...                 ...   \n",
      "541  5.743529064  9.006553647999999        10.46588395  6.7502614670000005   \n",
      "542  5.427847751  9.467610688999999  9.657287377000001         7.968845876   \n",
      "543  5.293333995        9.187666118        10.85997562   6.765459452999999   \n",
      "544   6.07443638        9.790993025        14.20590899         7.139406749   \n",
      "545   5.19511412  8.946676458999999         6.83466967  7.9497790739999985   \n",
      "\n",
      "0           LRP5           IGLV1-40       DNAJB1              ABCD3  \\\n",
      "0        5.10626            6.44922      8.77151            9.62196   \n",
      "1         5.2649            7.23411      8.79395             9.4534   \n",
      "2        4.49917            9.39747      8.35654            9.28646   \n",
      "3        5.41645            7.97934      8.90945            9.41568   \n",
      "4        5.04289            6.85852      8.81886            9.61593   \n",
      "..           ...                ...          ...                ...   \n",
      "541  5.973083531  7.663390122999999  8.041910695  8.918939803999999   \n",
      "542  5.358618036         6.55656512   7.48444839        10.26745748   \n",
      "543   5.44740957        6.220947165  9.132807376  9.724481127999999   \n",
      "544  5.089893268  7.461380137000001  8.272700125        9.238210411   \n",
      "545  5.215868235  6.744153932000001  8.668916388         8.96872861   \n",
      "\n",
      "0              EIF2AK2        FKBP4  ...               AADAC  \\\n",
      "0              6.08152      6.67334  ...             4.70868   \n",
      "1              5.71341      6.61979  ...             4.62547   \n",
      "2              5.87925      6.79578  ...             5.62224   \n",
      "3              6.05129      6.67817  ...             5.05469   \n",
      "4              5.96623      6.62441  ...             4.96137   \n",
      "..                 ...          ...  ...                 ...   \n",
      "541        6.531013439  6.848810165  ...         5.024974264   \n",
      "542        7.404914793  6.305334465  ...         5.243536465   \n",
      "543        6.112767223  7.233185653  ...         5.116125795   \n",
      "544        6.338502009  7.398263494  ...  5.3228251680000005   \n",
      "545  6.483501352999999  7.009632536  ...         5.092608074   \n",
      "\n",
      "0                FBLN5      ATP5IF1        MST1L             MRPL44  \\\n",
      "0              8.63288      9.68778      7.28476            5.91521   \n",
      "1              9.00611      9.97518      7.13126             6.0319   \n",
      "2              8.20337      9.22724      7.00901            5.44313   \n",
      "3              8.69685      9.35373      6.39379             5.7999   \n",
      "4              8.37222      9.77814      6.64598            6.14131   \n",
      "..                 ...          ...          ...                ...   \n",
      "541        7.162388038  10.06828642  6.731386081  5.968105592000001   \n",
      "542        9.240928392  10.55134956  6.374319513  5.990752387000001   \n",
      "543        8.292869487  10.24490888  6.207453766        6.751462625   \n",
      "544  7.767682237000001  10.43459007  6.879279593  5.770955197999999   \n",
      "545        7.895972466  9.684586981  5.813778255        5.715177177   \n",
      "\n",
      "0                CAMLG     SERPINF1        MUC16              ABCB7 label  \n",
      "0              9.19758      6.92308      4.28062             7.4941     2  \n",
      "1              8.69461      8.12161      4.59281            7.88932     4  \n",
      "2              9.12578      8.35852      4.35255            7.01419     1  \n",
      "3              8.87912      7.52234      4.38384            7.41608     8  \n",
      "4              9.26798      7.16242      4.60867            7.30992     8  \n",
      "..                 ...          ...          ...                ...   ...  \n",
      "541        8.857877495   6.16271403  5.392151009        7.762717611     4  \n",
      "542  9.835309092000001   7.72590708  5.254541275        8.034228317     8  \n",
      "543  9.034547868999999  6.623344071  4.871486101        7.549562876     6  \n",
      "544        8.967589079  6.672702295  4.814384327  7.806921347999999     5  \n",
      "545        9.273350113  7.532901706  4.789443717        7.185852079     7  \n",
      "\n",
      "[1665 rows x 254 columns]\n"
     ]
    }
   ],
   "source": [
    "x=pd.read_csv('dataset/qiuguan/origin_800/LRP/200/selected_train_val_info.csv',sep=',')\n",
    "#print(x)\n",
    "\n",
    "#x=x.iloc[:,1:]\n",
    "\n",
    "#print(x)\n",
    "\n",
    "column=x.iloc[0,1:]#列名\n",
    "#print(column)\n",
    "\n",
    "x_new=x.iloc[1:,1:]\n",
    "x_new.columns=column\n",
    "#print(x_new)\n",
    "aug.columns=column\n",
    "#print(aug)\n",
    "\n",
    "all=pd.concat((aug,x_new),axis=0)\n",
    "print(all)\n",
    "\n",
    "all.to_csv('dataset/qiuguan/origin_800/LRP/200/all_train_val_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3976a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19813b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca38a5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1           TYR\n",
      "2          RPA3\n",
      "3           REN\n",
      "4         ABCC3\n",
      "5          LRP5\n",
      "         ...   \n",
      "250       CAMLG\n",
      "251    SERPINF1\n",
      "252       MUC16\n",
      "253       ABCB7\n",
      "254       label\n",
      "Name: 1, Length: 254, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x=x.iloc[1:,:]\n",
    "column=x.iloc[0,1:]#列名\n",
    "print(column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac64554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8781225d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
