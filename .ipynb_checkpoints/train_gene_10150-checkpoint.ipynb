{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3015b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import Trainer\n",
    "from network import NFM\n",
    "import torch.utils.data as Data\n",
    "from Utils.criteo_loader import getTestData, getTrainData\n",
    "\n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':11,\n",
    "    'linear_hidden':100,#线性模型输出层（隐层个数）\n",
    "    'embed_input_dim':1001,#embed输入维度\n",
    "    'embed_dim': 100, # 用于控制稀疏特征经过Embedding层后的稠密特征大小，embed输出维度\n",
    "    'dnn_hidden_units': [100,11],#MLP隐层和输出层\n",
    "    'num_sparse_features_cols':4077,#the number of the gene columns\n",
    "    'num_dense_features': 0,#dense features number\n",
    "    'bi_dropout': 0.3,#Bi-Interaction 的dropout\n",
    "    'num_epoch': 500,#训练epoch次数\n",
    "    'batch_size': 100,#batch_size\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    #'train_file': '../Data/criteo/processed_data/train_set.csv',\n",
    "    #'fea_file': '../Data/criteo/processed_data/fea_col.npy',\n",
    "    #'validate_file': '../Data/criteo/processed_data/val_set.csv',\n",
    "    #'test_file': '../Data/criteo/processed_data/test_set.csv',\n",
    "    #'model_name': '../TrainedModels/NFM.model'\n",
    "    'train_file':'data/frappe/gene_4000/gene_4000_encode.csv',\n",
    "    'label_file':'data/frappe/gene_4000/label.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b67eaddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "#from utils import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bed954e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[190 220 490 ... 360 370 420]\n",
      " [110 220 500 ... 340 260 420]\n",
      " [100 300 500 ... 330 350 390]\n",
      " ...\n",
      " [450 360 500 ... 430 360 500]\n",
      " [420 350 530 ... 440 370 500]\n",
      " [420 360 540 ... 430 370 450]]\n",
      "row: 0 label: [0]\n",
      "row: 1 label: [0]\n",
      "row: 2 label: [0]\n",
      "row: 3 label: [0]\n",
      "row: 4 label: [0]\n",
      "row: 5 label: [0]\n",
      "row: 6 label: [0]\n",
      "row: 7 label: [0]\n",
      "row: 8 label: [0]\n",
      "row: 9 label: [0]\n",
      "row: 10 label: [0]\n",
      "row: 11 label: [0]\n",
      "row: 12 label: [0]\n",
      "row: 13 label: [0]\n",
      "row: 14 label: [0]\n",
      "row: 15 label: [0]\n",
      "row: 16 label: [0]\n",
      "row: 17 label: [0]\n",
      "row: 18 label: [0]\n",
      "row: 19 label: [0]\n",
      "row: 20 label: [0]\n",
      "row: 21 label: [0]\n",
      "row: 22 label: [0]\n",
      "row: 23 label: [0]\n",
      "row: 24 label: [0]\n",
      "row: 25 label: [0]\n",
      "row: 26 label: [0]\n",
      "row: 27 label: [0]\n",
      "row: 28 label: [0]\n",
      "row: 29 label: [0]\n",
      "row: 30 label: [0]\n",
      "row: 31 label: [0]\n",
      "row: 32 label: [0]\n",
      "row: 33 label: [0]\n",
      "row: 34 label: [0]\n",
      "row: 35 label: [0]\n",
      "row: 36 label: [0]\n",
      "row: 37 label: [0]\n",
      "row: 38 label: [0]\n",
      "row: 39 label: [0]\n",
      "row: 40 label: [0]\n",
      "row: 41 label: [0]\n",
      "row: 42 label: [0]\n",
      "row: 43 label: [0]\n",
      "row: 44 label: [0]\n",
      "row: 45 label: [0]\n",
      "row: 46 label: [0]\n",
      "row: 47 label: [0]\n",
      "row: 48 label: [0]\n",
      "row: 49 label: [0]\n",
      "row: 50 label: [0]\n",
      "row: 51 label: [0]\n",
      "row: 52 label: [0]\n",
      "row: 53 label: [0]\n",
      "row: 54 label: [0]\n",
      "row: 55 label: [0]\n",
      "row: 56 label: [0]\n",
      "row: 57 label: [1]\n",
      "row: 58 label: [1]\n",
      "row: 59 label: [1]\n",
      "row: 60 label: [1]\n",
      "row: 61 label: [1]\n",
      "row: 62 label: [1]\n",
      "row: 63 label: [1]\n",
      "row: 64 label: [1]\n",
      "row: 65 label: [1]\n",
      "row: 66 label: [1]\n",
      "row: 67 label: [1]\n",
      "row: 68 label: [1]\n",
      "row: 69 label: [1]\n",
      "row: 70 label: [1]\n",
      "row: 71 label: [1]\n",
      "row: 72 label: [1]\n",
      "row: 73 label: [1]\n",
      "row: 74 label: [1]\n",
      "row: 75 label: [1]\n",
      "row: 76 label: [1]\n",
      "row: 77 label: [1]\n",
      "row: 78 label: [1]\n",
      "row: 79 label: [1]\n",
      "row: 80 label: [1]\n",
      "row: 81 label: [1]\n",
      "row: 82 label: [1]\n",
      "row: 83 label: [1]\n",
      "row: 84 label: [1]\n",
      "row: 85 label: [1]\n",
      "row: 86 label: [1]\n",
      "row: 87 label: [1]\n",
      "row: 88 label: [1]\n",
      "row: 89 label: [1]\n",
      "row: 90 label: [1]\n",
      "row: 91 label: [1]\n",
      "row: 92 label: [1]\n",
      "row: 93 label: [1]\n",
      "row: 94 label: [1]\n",
      "row: 95 label: [1]\n",
      "row: 96 label: [1]\n",
      "row: 97 label: [1]\n",
      "row: 98 label: [1]\n",
      "row: 99 label: [1]\n",
      "row: 100 label: [1]\n",
      "row: 101 label: [1]\n",
      "row: 102 label: [1]\n",
      "row: 103 label: [1]\n",
      "row: 104 label: [1]\n",
      "row: 105 label: [1]\n",
      "row: 106 label: [1]\n",
      "row: 107 label: [1]\n",
      "row: 108 label: [1]\n",
      "row: 109 label: [1]\n",
      "row: 110 label: [1]\n",
      "row: 111 label: [1]\n",
      "row: 112 label: [1]\n",
      "row: 113 label: [1]\n",
      "row: 114 label: [2]\n",
      "row: 115 label: [2]\n",
      "row: 116 label: [2]\n",
      "row: 117 label: [2]\n",
      "row: 118 label: [2]\n",
      "row: 119 label: [2]\n",
      "row: 120 label: [2]\n",
      "row: 121 label: [2]\n",
      "row: 122 label: [2]\n",
      "row: 123 label: [2]\n",
      "row: 124 label: [2]\n",
      "row: 125 label: [2]\n",
      "row: 126 label: [2]\n",
      "row: 127 label: [2]\n",
      "row: 128 label: [2]\n",
      "row: 129 label: [2]\n",
      "row: 130 label: [2]\n",
      "row: 131 label: [2]\n",
      "row: 132 label: [2]\n",
      "row: 133 label: [2]\n",
      "row: 134 label: [2]\n",
      "row: 135 label: [3]\n",
      "row: 136 label: [3]\n",
      "row: 137 label: [3]\n",
      "row: 138 label: [3]\n",
      "row: 139 label: [3]\n",
      "row: 140 label: [3]\n",
      "row: 141 label: [3]\n",
      "row: 142 label: [3]\n",
      "row: 143 label: [3]\n",
      "row: 144 label: [3]\n",
      "row: 145 label: [3]\n",
      "row: 146 label: [3]\n",
      "row: 147 label: [3]\n",
      "row: 148 label: [3]\n",
      "row: 149 label: [3]\n",
      "row: 150 label: [3]\n",
      "row: 151 label: [3]\n",
      "row: 152 label: [3]\n",
      "row: 153 label: [3]\n",
      "row: 154 label: [3]\n",
      "row: 155 label: [3]\n",
      "row: 156 label: [3]\n",
      "row: 157 label: [3]\n",
      "row: 158 label: [3]\n",
      "row: 159 label: [3]\n",
      "row: 160 label: [3]\n",
      "row: 161 label: [3]\n",
      "row: 162 label: [3]\n",
      "row: 163 label: [3]\n",
      "row: 164 label: [3]\n",
      "row: 165 label: [3]\n",
      "row: 166 label: [3]\n",
      "row: 167 label: [3]\n",
      "row: 168 label: [3]\n",
      "row: 169 label: [3]\n",
      "row: 170 label: [3]\n",
      "row: 171 label: [3]\n",
      "row: 172 label: [3]\n",
      "row: 173 label: [3]\n",
      "row: 174 label: [3]\n",
      "row: 175 label: [3]\n",
      "row: 176 label: [3]\n",
      "row: 177 label: [3]\n",
      "row: 178 label: [3]\n",
      "row: 179 label: [3]\n",
      "row: 180 label: [3]\n",
      "row: 181 label: [3]\n",
      "row: 182 label: [3]\n",
      "row: 183 label: [3]\n",
      "row: 184 label: [3]\n",
      "row: 185 label: [3]\n",
      "row: 186 label: [3]\n",
      "row: 187 label: [3]\n",
      "row: 188 label: [3]\n",
      "row: 189 label: [3]\n",
      "row: 190 label: [3]\n",
      "row: 191 label: [3]\n",
      "row: 192 label: [3]\n",
      "row: 193 label: [3]\n",
      "row: 194 label: [3]\n",
      "row: 195 label: [3]\n",
      "row: 196 label: [3]\n",
      "row: 197 label: [3]\n",
      "row: 198 label: [3]\n",
      "row: 199 label: [3]\n",
      "row: 200 label: [3]\n",
      "row: 201 label: [3]\n",
      "row: 202 label: [3]\n",
      "row: 203 label: [3]\n",
      "row: 204 label: [3]\n",
      "row: 205 label: [3]\n",
      "row: 206 label: [3]\n",
      "row: 207 label: [3]\n",
      "row: 208 label: [3]\n",
      "row: 209 label: [3]\n",
      "row: 210 label: [3]\n",
      "row: 211 label: [3]\n",
      "row: 212 label: [3]\n",
      "row: 213 label: [3]\n",
      "row: 214 label: [3]\n",
      "row: 215 label: [3]\n",
      "row: 216 label: [3]\n",
      "row: 217 label: [3]\n",
      "row: 218 label: [3]\n",
      "row: 219 label: [3]\n",
      "row: 220 label: [3]\n",
      "row: 221 label: [3]\n",
      "row: 222 label: [3]\n",
      "row: 223 label: [3]\n",
      "row: 224 label: [3]\n",
      "row: 225 label: [3]\n",
      "row: 226 label: [3]\n",
      "row: 227 label: [3]\n",
      "row: 228 label: [3]\n",
      "row: 229 label: [3]\n",
      "row: 230 label: [3]\n",
      "row: 231 label: [3]\n",
      "row: 232 label: [3]\n",
      "row: 233 label: [3]\n",
      "row: 234 label: [3]\n",
      "row: 235 label: [3]\n",
      "row: 236 label: [3]\n",
      "row: 237 label: [3]\n",
      "row: 238 label: [3]\n",
      "row: 239 label: [3]\n",
      "row: 240 label: [3]\n",
      "row: 241 label: [3]\n",
      "row: 242 label: [3]\n",
      "row: 243 label: [3]\n",
      "row: 244 label: [3]\n",
      "row: 245 label: [3]\n",
      "row: 246 label: [3]\n",
      "row: 247 label: [3]\n",
      "row: 248 label: [3]\n",
      "row: 249 label: [3]\n",
      "row: 250 label: [3]\n",
      "row: 251 label: [4]\n",
      "row: 252 label: [4]\n",
      "row: 253 label: [4]\n",
      "row: 254 label: [4]\n",
      "row: 255 label: [4]\n",
      "row: 256 label: [4]\n",
      "row: 257 label: [4]\n",
      "row: 258 label: [4]\n",
      "row: 259 label: [4]\n",
      "row: 260 label: [4]\n",
      "row: 261 label: [4]\n",
      "row: 262 label: [4]\n",
      "row: 263 label: [4]\n",
      "row: 264 label: [4]\n",
      "row: 265 label: [4]\n",
      "row: 266 label: [4]\n",
      "row: 267 label: [4]\n",
      "row: 268 label: [4]\n",
      "row: 269 label: [4]\n",
      "row: 270 label: [4]\n",
      "row: 271 label: [4]\n",
      "row: 272 label: [4]\n",
      "row: 273 label: [4]\n",
      "row: 274 label: [4]\n",
      "row: 275 label: [4]\n",
      "row: 276 label: [4]\n",
      "row: 277 label: [4]\n",
      "row: 278 label: [4]\n",
      "row: 279 label: [4]\n",
      "row: 280 label: [4]\n",
      "row: 281 label: [4]\n",
      "row: 282 label: [4]\n",
      "row: 283 label: [4]\n",
      "row: 284 label: [4]\n",
      "row: 285 label: [4]\n",
      "row: 286 label: [4]\n",
      "row: 287 label: [4]\n",
      "row: 288 label: [4]\n",
      "row: 289 label: [4]\n",
      "row: 290 label: [4]\n",
      "row: 291 label: [4]\n",
      "row: 292 label: [4]\n",
      "row: 293 label: [4]\n",
      "row: 294 label: [4]\n",
      "row: 295 label: [4]\n",
      "row: 296 label: [4]\n",
      "row: 297 label: [4]\n",
      "row: 298 label: [4]\n",
      "row: 299 label: [4]\n",
      "row: 300 label: [4]\n",
      "row: 301 label: [4]\n",
      "row: 302 label: [4]\n",
      "row: 303 label: [4]\n",
      "row: 304 label: [4]\n",
      "row: 305 label: [4]\n",
      "row: 306 label: [4]\n",
      "row: 307 label: [4]\n",
      "row: 308 label: [5]\n",
      "row: 309 label: [5]\n",
      "row: 310 label: [5]\n",
      "row: 311 label: [5]\n",
      "row: 312 label: [5]\n",
      "row: 313 label: [5]\n",
      "row: 314 label: [5]\n",
      "row: 315 label: [5]\n",
      "row: 316 label: [5]\n",
      "row: 317 label: [5]\n",
      "row: 318 label: [5]\n",
      "row: 319 label: [5]\n",
      "row: 320 label: [5]\n",
      "row: 321 label: [5]\n",
      "row: 322 label: [5]\n",
      "row: 323 label: [5]\n",
      "row: 324 label: [5]\n",
      "row: 325 label: [5]\n",
      "row: 326 label: [5]\n",
      "row: 327 label: [5]\n",
      "row: 328 label: [5]\n",
      "row: 329 label: [5]\n",
      "row: 330 label: [5]\n",
      "row: 331 label: [5]\n",
      "row: 332 label: [5]\n",
      "row: 333 label: [5]\n",
      "row: 334 label: [5]\n",
      "row: 335 label: [5]\n",
      "row: 336 label: [5]\n",
      "row: 337 label: [5]\n",
      "row: 338 label: [5]\n",
      "row: 339 label: [5]\n",
      "row: 340 label: [5]\n",
      "row: 341 label: [5]\n",
      "row: 342 label: [5]\n",
      "row: 343 label: [5]\n",
      "row: 344 label: [5]\n",
      "row: 345 label: [5]\n",
      "row: 346 label: [5]\n",
      "row: 347 label: [5]\n",
      "row: 348 label: [5]\n",
      "row: 349 label: [5]\n",
      "row: 350 label: [5]\n",
      "row: 351 label: [5]\n",
      "row: 352 label: [5]\n",
      "row: 353 label: [5]\n",
      "row: 354 label: [5]\n",
      "row: 355 label: [5]\n",
      "row: 356 label: [5]\n",
      "row: 357 label: [5]\n",
      "row: 358 label: [5]\n",
      "row: 359 label: [5]\n",
      "row: 360 label: [5]\n",
      "row: 361 label: [5]\n",
      "row: 362 label: [5]\n",
      "row: 363 label: [5]\n",
      "row: 364 label: [5]\n",
      "row: 365 label: [5]\n",
      "row: 366 label: [5]\n",
      "row: 367 label: [5]\n",
      "row: 368 label: [5]\n",
      "row: 369 label: [5]\n",
      "row: 370 label: [5]\n",
      "row: 371 label: [5]\n",
      "row: 372 label: [5]\n",
      "row: 373 label: [5]\n",
      "row: 374 label: [5]\n",
      "row: 375 label: [5]\n",
      "row: 376 label: [5]\n",
      "row: 377 label: [5]\n",
      "row: 378 label: [5]\n",
      "row: 379 label: [5]\n",
      "row: 380 label: [5]\n",
      "row: 381 label: [5]\n",
      "row: 382 label: [5]\n",
      "row: 383 label: [5]\n",
      "row: 384 label: [6]\n",
      "row: 385 label: [6]\n",
      "row: 386 label: [6]\n",
      "row: 387 label: [6]\n",
      "row: 388 label: [6]\n",
      "row: 389 label: [6]\n",
      "row: 390 label: [6]\n",
      "row: 391 label: [6]\n",
      "row: 392 label: [6]\n",
      "row: 393 label: [6]\n",
      "row: 394 label: [6]\n",
      "row: 395 label: [6]\n",
      "row: 396 label: [6]\n",
      "row: 397 label: [6]\n",
      "row: 398 label: [6]\n",
      "row: 399 label: [6]\n",
      "row: 400 label: [6]\n",
      "row: 401 label: [6]\n",
      "row: 402 label: [6]\n",
      "row: 403 label: [6]\n",
      "row: 404 label: [6]\n",
      "row: 405 label: [6]\n",
      "row: 406 label: [6]\n",
      "row: 407 label: [6]\n",
      "row: 408 label: [6]\n",
      "row: 409 label: [7]\n",
      "row: 410 label: [7]\n",
      "row: 411 label: [7]\n",
      "row: 412 label: [7]\n",
      "row: 413 label: [7]\n",
      "row: 414 label: [7]\n",
      "row: 415 label: [7]\n",
      "row: 416 label: [7]\n",
      "row: 417 label: [7]\n",
      "row: 418 label: [7]\n",
      "row: 419 label: [7]\n",
      "row: 420 label: [7]\n",
      "row: 421 label: [7]\n",
      "row: 422 label: [7]\n",
      "row: 423 label: [7]\n",
      "row: 424 label: [7]\n",
      "row: 425 label: [7]\n",
      "row: 426 label: [7]\n",
      "row: 427 label: [7]\n",
      "row: 428 label: [7]\n",
      "row: 429 label: [7]\n",
      "row: 430 label: [7]\n",
      "row: 431 label: [7]\n",
      "row: 432 label: [7]\n",
      "row: 433 label: [7]\n",
      "row: 434 label: [7]\n",
      "row: 435 label: [7]\n",
      "row: 436 label: [7]\n",
      "row: 437 label: [7]\n",
      "row: 438 label: [7]\n",
      "row: 439 label: [7]\n",
      "row: 440 label: [7]\n",
      "row: 441 label: [8]\n",
      "row: 442 label: [8]\n",
      "row: 443 label: [8]\n",
      "row: 444 label: [8]\n",
      "row: 445 label: [8]\n",
      "row: 446 label: [8]\n",
      "row: 447 label: [8]\n",
      "row: 448 label: [8]\n",
      "row: 449 label: [8]\n",
      "row: 450 label: [8]\n",
      "row: 451 label: [8]\n",
      "row: 452 label: [8]\n",
      "row: 453 label: [8]\n",
      "row: 454 label: [8]\n",
      "row: 455 label: [8]\n",
      "row: 456 label: [8]\n",
      "row: 457 label: [8]\n",
      "row: 458 label: [8]\n",
      "row: 459 label: [8]\n",
      "row: 460 label: [8]\n",
      "row: 461 label: [8]\n",
      "row: 462 label: [8]\n",
      "row: 463 label: [8]\n",
      "row: 464 label: [8]\n",
      "row: 465 label: [8]\n",
      "row: 466 label: [8]\n",
      "row: 467 label: [8]\n",
      "row: 468 label: [8]\n",
      "row: 469 label: [8]\n",
      "row: 470 label: [8]\n",
      "row: 471 label: [8]\n",
      "row: 472 label: [8]\n",
      "row: 473 label: [8]\n",
      "row: 474 label: [8]\n",
      "row: 475 label: [8]\n",
      "row: 476 label: [8]\n",
      "row: 477 label: [8]\n",
      "row: 478 label: [8]\n",
      "row: 479 label: [8]\n",
      "row: 480 label: [8]\n",
      "row: 481 label: [8]\n",
      "row: 482 label: [8]\n",
      "row: 483 label: [8]\n",
      "row: 484 label: [8]\n",
      "row: 485 label: [8]\n",
      "row: 486 label: [8]\n",
      "row: 487 label: [8]\n",
      "row: 488 label: [8]\n",
      "row: 489 label: [8]\n",
      "row: 490 label: [8]\n",
      "row: 491 label: [8]\n",
      "row: 492 label: [8]\n",
      "row: 493 label: [8]\n",
      "row: 494 label: [8]\n",
      "row: 495 label: [8]\n",
      "row: 496 label: [8]\n",
      "row: 497 label: [8]\n",
      "row: 498 label: [8]\n",
      "row: 499 label: [8]\n",
      "row: 500 label: [8]\n",
      "row: 501 label: [8]\n",
      "row: 502 label: [8]\n",
      "row: 503 label: [8]\n",
      "row: 504 label: [8]\n",
      "row: 505 label: [8]\n",
      "row: 506 label: [8]\n",
      "row: 507 label: [8]\n",
      "row: 508 label: [8]\n",
      "row: 509 label: [8]\n",
      "row: 510 label: [8]\n",
      "row: 511 label: [8]\n",
      "row: 512 label: [8]\n",
      "row: 513 label: [8]\n",
      "row: 514 label: [8]\n",
      "row: 515 label: [8]\n",
      "row: 516 label: [8]\n",
      "row: 517 label: [8]\n",
      "row: 518 label: [8]\n",
      "row: 519 label: [8]\n",
      "row: 520 label: [8]\n",
      "row: 521 label: [8]\n",
      "row: 522 label: [8]\n",
      "row: 523 label: [8]\n",
      "row: 524 label: [8]\n",
      "row: 525 label: [8]\n",
      "row: 526 label: [8]\n",
      "row: 527 label: [8]\n",
      "row: 528 label: [8]\n",
      "row: 529 label: [8]\n",
      "row: 530 label: [8]\n",
      "row: 531 label: [8]\n",
      "row: 532 label: [8]\n",
      "row: 533 label: [8]\n",
      "row: 534 label: [8]\n",
      "row: 535 label: [8]\n",
      "row: 536 label: [8]\n",
      "row: 537 label: [8]\n",
      "row: 538 label: [8]\n",
      "row: 539 label: [8]\n",
      "row: 540 label: [8]\n",
      "row: 541 label: [8]\n",
      "row: 542 label: [8]\n",
      "row: 543 label: [8]\n",
      "row: 544 label: [8]\n",
      "row: 545 label: [8]\n",
      "row: 546 label: [8]\n",
      "row: 547 label: [8]\n",
      "row: 548 label: [8]\n",
      "row: 549 label: [8]\n",
      "row: 550 label: [8]\n",
      "row: 551 label: [8]\n",
      "row: 552 label: [8]\n",
      "row: 553 label: [8]\n",
      "row: 554 label: [8]\n",
      "row: 555 label: [8]\n",
      "row: 556 label: [8]\n",
      "row: 557 label: [8]\n",
      "row: 558 label: [8]\n",
      "row: 559 label: [8]\n",
      "row: 560 label: [8]\n",
      "row: 561 label: [8]\n",
      "row: 562 label: [8]\n",
      "row: 563 label: [8]\n",
      "row: 564 label: [8]\n",
      "row: 565 label: [8]\n",
      "row: 566 label: [8]\n",
      "row: 567 label: [8]\n",
      "row: 568 label: [8]\n",
      "row: 569 label: [8]\n",
      "row: 570 label: [8]\n",
      "row: 571 label: [8]\n",
      "row: 572 label: [8]\n",
      "row: 573 label: [8]\n",
      "row: 574 label: [8]\n",
      "row: 575 label: [8]\n",
      "row: 576 label: [8]\n",
      "row: 577 label: [8]\n",
      "row: 578 label: [8]\n",
      "row: 579 label: [8]\n",
      "row: 580 label: [8]\n",
      "row: 581 label: [8]\n",
      "row: 582 label: [8]\n",
      "row: 583 label: [8]\n",
      "row: 584 label: [8]\n",
      "row: 585 label: [8]\n",
      "row: 586 label: [8]\n",
      "row: 587 label: [8]\n",
      "row: 588 label: [8]\n",
      "row: 589 label: [8]\n",
      "row: 590 label: [8]\n",
      "row: 591 label: [8]\n",
      "row: 592 label: [8]\n",
      "row: 593 label: [8]\n",
      "row: 594 label: [8]\n",
      "row: 595 label: [8]\n",
      "row: 596 label: [8]\n",
      "row: 597 label: [8]\n",
      "row: 598 label: [8]\n",
      "row: 599 label: [8]\n",
      "row: 600 label: [8]\n",
      "row: 601 label: [8]\n",
      "row: 602 label: [8]\n",
      "row: 603 label: [8]\n",
      "row: 604 label: [8]\n",
      "row: 605 label: [8]\n",
      "row: 606 label: [8]\n",
      "row: 607 label: [8]\n",
      "row: 608 label: [8]\n",
      "row: 609 label: [8]\n",
      "row: 610 label: [8]\n",
      "row: 611 label: [8]\n",
      "row: 612 label: [8]\n",
      "row: 613 label: [8]\n",
      "row: 614 label: [8]\n",
      "row: 615 label: [8]\n",
      "row: 616 label: [8]\n",
      "row: 617 label: [8]\n",
      "row: 618 label: [8]\n",
      "row: 619 label: [9]\n",
      "row: 620 label: [9]\n",
      "row: 621 label: [9]\n",
      "row: 622 label: [9]\n",
      "row: 623 label: [9]\n",
      "row: 624 label: [9]\n",
      "row: 625 label: [9]\n",
      "row: 626 label: [9]\n",
      "row: 627 label: [9]\n",
      "row: 628 label: [9]\n",
      "row: 629 label: [9]\n",
      "row: 630 label: [9]\n",
      "row: 631 label: [9]\n",
      "row: 632 label: [9]\n",
      "row: 633 label: [9]\n",
      "row: 634 label: [9]\n",
      "row: 635 label: [9]\n",
      "row: 636 label: [9]\n",
      "row: 637 label: [9]\n",
      "row: 638 label: [9]\n",
      "row: 639 label: [9]\n",
      "row: 640 label: [9]\n",
      "row: 641 label: [9]\n",
      "row: 642 label: [9]\n",
      "row: 643 label: [9]\n",
      "row: 644 label: [9]\n",
      "row: 645 label: [9]\n",
      "row: 646 label: [9]\n",
      "row: 647 label: [9]\n",
      "row: 648 label: [9]\n",
      "row: 649 label: [9]\n",
      "row: 650 label: [9]\n",
      "row: 651 label: [9]\n",
      "row: 652 label: [9]\n",
      "row: 653 label: [9]\n",
      "row: 654 label: [9]\n",
      "row: 655 label: [9]\n",
      "row: 656 label: [9]\n",
      "row: 657 label: [9]\n",
      "row: 658 label: [9]\n",
      "row: 659 label: [9]\n",
      "row: 660 label: [10]\n",
      "row: 661 label: [10]\n",
      "row: 662 label: [10]\n",
      "row: 663 label: [10]\n",
      "row: 664 label: [10]\n",
      "row: 665 label: [10]\n",
      "row: 666 label: [10]\n",
      "row: 667 label: [10]\n",
      "row: 668 label: [10]\n",
      "row: 669 label: [10]\n",
      "row: 670 label: [10]\n",
      "row: 671 label: [10]\n",
      "row: 672 label: [10]\n",
      "row: 673 label: [10]\n",
      "row: 674 label: [10]\n",
      "row: 675 label: [10]\n",
      "row: 676 label: [10]\n",
      "row: 677 label: [10]\n",
      "row: 678 label: [10]\n",
      "row: 679 label: [10]\n",
      "row: 680 label: [10]\n",
      "row: 681 label: [10]\n",
      "row: 682 label: [10]\n",
      "row: 683 label: [10]\n",
      "row: 684 label: [10]\n",
      "row: 685 label: [10]\n",
      "row: 686 label: [10]\n",
      "row: 687 label: [10]\n",
      "row: 688 label: [10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row: 689 label: [10]\n",
      "row: 690 label: [10]\n",
      "row: 691 label: [10]\n",
      "row: 692 label: [10]\n",
      "row: 693 label: [10]\n",
      "row: 694 label: [10]\n",
      "row: 695 label: [10]\n",
      "row: 696 label: [10]\n",
      "row: 697 label: [10]\n",
      "row: 698 label: [10]\n",
      "row: 699 label: [10]\n",
      "row: 700 label: [10]\n",
      "row: 701 label: [10]\n",
      "row: 702 label: [10]\n",
      "row: 703 label: [10]\n",
      "row: 704 label: [10]\n",
      "row: 705 label: [10]\n",
      "row: 706 label: [10]\n",
      "row: 707 label: [10]\n",
      "row: 708 label: [10]\n",
      "row: 709 label: [10]\n",
      "row: 710 label: [10]\n",
      "row: 711 label: [10]\n",
      "row: 712 label: [10]\n",
      "row: 713 label: [10]\n",
      "row: 714 label: [10]\n",
      "row: 715 label: [10]\n",
      "row: 716 label: [10]\n",
      "row: 717 label: [10]\n",
      "row: 718 label: [10]\n",
      "row: 719 label: [10]\n",
      "row: 720 label: [10]\n",
      "row: 721 label: [10]\n",
      "row: 722 label: [10]\n",
      "row: 723 label: [10]\n",
      "row: 724 label: [10]\n",
      "row: 725 label: [10]\n",
      "row: 726 label: [10]\n",
      "row: 727 label: [10]\n",
      "row: 728 label: [10]\n",
      "row: 729 label: [10]\n",
      "row: 730 label: [10]\n",
      "row: 731 label: [10]\n",
      "row: 732 label: [10]\n",
      "row: 733 label: [10]\n",
      "row: 734 label: [10]\n",
      "row: 735 label: [10]\n",
      "row: 736 label: [10]\n",
      "row: 737 label: [10]\n",
      "row: 738 label: [10]\n",
      "row: 739 label: [10]\n",
      "row: 740 label: [10]\n",
      "row: 741 label: [10]\n",
      "row: 742 label: [10]\n",
      "row: 743 label: [10]\n",
      "row: 744 label: [10]\n",
      "row: 745 label: [10]\n",
      "row: 746 label: [10]\n",
      "row: 747 label: [10]\n",
      "row: 748 label: [10]\n",
      "row: 749 label: [10]\n",
      "row: 750 label: [10]\n",
      "row: 751 label: [10]\n",
      "row: 752 label: [10]\n",
      "row: 753 label: [10]\n",
      "row: 754 label: [10]\n",
      "row: 755 label: [10]\n",
      "row: 756 label: [10]\n",
      "row: 757 label: [10]\n",
      "row: 758 label: [10]\n",
      "row: 759 label: [10]\n",
      "label: [[0.8181818  0.01818182 0.01818182 ... 0.01818182 0.01818182 0.01818182]\n",
      " [0.8181818  0.01818182 0.01818182 ... 0.01818182 0.01818182 0.01818182]\n",
      " [0.8181818  0.01818182 0.01818182 ... 0.01818182 0.01818182 0.01818182]\n",
      " ...\n",
      " [0.01818182 0.01818182 0.01818182 ... 0.01818182 0.01818182 0.8181818 ]\n",
      " [0.01818182 0.01818182 0.01818182 ... 0.01818182 0.01818182 0.8181818 ]\n",
      " [0.01818182 0.01818182 0.01818182 ... 0.01818182 0.01818182 0.8181818 ]]\n",
      "features: [[33 34 46 ... 28 27 29]\n",
      " [36 35 47 ... 31 26 29]\n",
      " [36 35 46 ... 33 27 29]\n",
      " ...\n",
      " [10 36 38 ... 14 35 32]\n",
      " [ 9 32 42 ... 10 29 36]\n",
      " [ 9 37 46 ... 13 27 33]]\n"
     ]
    }
   ],
   "source": [
    "from dataset_process import FMData\n",
    "\n",
    "#train_dataset = FMData(config.train_libfm,config.train_label,nfm_config['n_class'])\n",
    "train_dataset = FMData(nfm_config['train_file'],nfm_config['label_file'],nfm_config['n_class'])\n",
    "train_loader = data.DataLoader(train_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "#validate_dataset = FMData(config.valid_libfm,config.valid_label)\n",
    "#validate_loader = data.DataLoader(train_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "#test_dataset = FMData(config.test_libfm,config.test_label)\n",
    "#test_loader = data.DataLoader(test_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18a4e1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nfm: NFM(\n",
      "  (drop): Dropout(p=0.7, inplace=False)\n",
      "  (linear_model): Linear(in_features=4077, out_features=100, bias=True)\n",
      "  (BN_linear): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (embedding_layers): Embedding(1001, 100)\n",
      "  (bi_pooling): BiInteractionPooling()\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (BN_bi): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dnn_layers): ModuleList(\n",
      "    (0): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (1): Linear(in_features=100, out_features=11, bias=True)\n",
      "  )\n",
      "  (dnn_softmax): Softmax(dim=1)\n",
      ")\n",
      "linear_output: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1265e+00,  4.7054e+00, -1.7301e+01,  ..., -1.3541e+01,\n",
      "         -1.0522e+01,  9.1082e-01],\n",
      "        [-1.9128e-02,  1.9574e+00, -1.4394e+01,  ..., -1.0566e+01,\n",
      "         -1.3954e+01, -3.7546e-01],\n",
      "        [-1.3816e+00, -3.0119e+00, -1.1583e+01,  ..., -1.1724e+01,\n",
      "         -1.3203e+01, -1.0619e+00],\n",
      "        ...,\n",
      "        [ 3.8880e-01,  3.3734e+00, -1.6886e+01,  ..., -1.5004e+01,\n",
      "         -1.0724e+01, -3.2508e+00],\n",
      "        [ 2.1179e+00, -4.5261e+00, -1.7205e+01,  ..., -1.6275e+01,\n",
      "         -1.9887e+01, -3.9783e+00],\n",
      "        [ 2.1329e+00, -5.6592e+00, -1.8273e+01,  ..., -1.5526e+01,\n",
      "         -1.9405e+01, -6.1251e+00]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 11])\n",
      "y_predict: tensor([[0.0985, 0.0817, 0.0922,  ..., 0.0885, 0.0817, 0.0944],\n",
      "        [0.0801, 0.0866, 0.0819,  ..., 0.0843, 0.1031, 0.1199],\n",
      "        [0.0817, 0.1099, 0.1013,  ..., 0.0817, 0.0817, 0.1066],\n",
      "        ...,\n",
      "        [0.0827, 0.0987, 0.0965,  ..., 0.0844, 0.0849, 0.1149],\n",
      "        [0.0870, 0.1152, 0.0853,  ..., 0.0877, 0.0838, 0.0976],\n",
      "        [0.0837, 0.0837, 0.1094,  ..., 0.0859, 0.0839, 0.1232]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "new_correct: 7.0\n",
      "batch_idx: 0\n",
      "0.07\n",
      "linear_output: tensor([[ 145.5117,  143.7841,  -60.8957,  ..., -117.3532, -133.5125,\n",
      "         -151.1083],\n",
      "        [ 190.8752,  182.7557,  -43.6145,  ..., -165.7589, -169.5797,\n",
      "         -192.0238],\n",
      "        [ 148.4772,  133.5356,  -62.5288,  ..., -121.9574, -127.7817,\n",
      "         -151.9609],\n",
      "        ...,\n",
      "        [ 225.5241,  205.7462,  -49.3463,  ..., -195.3702, -192.1292,\n",
      "         -226.2392],\n",
      "        [ 191.3993,  187.3705,  -38.0892,  ..., -173.5148, -169.2591,\n",
      "         -197.4441],\n",
      "        [ 143.3077,  144.4781,  -64.4946,  ..., -113.8741, -134.5023,\n",
      "         -146.3787]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 11])\n",
      "y_predict: tensor([[0.0788, 0.0900, 0.0838,  ..., 0.1370, 0.0788, 0.1063],\n",
      "        [0.0854, 0.0854, 0.1154,  ..., 0.0854, 0.0874, 0.1015],\n",
      "        [0.0787, 0.0787, 0.0975,  ..., 0.0864, 0.0787, 0.1106],\n",
      "        ...,\n",
      "        [0.0836, 0.0837, 0.1017,  ..., 0.0963, 0.0836, 0.0836],\n",
      "        [0.0738, 0.0738, 0.1118,  ..., 0.0862, 0.0738, 0.1008],\n",
      "        [0.0954, 0.0970, 0.1007,  ..., 0.0907, 0.0830, 0.0965]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f0702262fa95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;31m#loss = loss_func(y_predict.view(-1), labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from nfm_network import NFM\n",
    "if __name__ == \"__main__\":\n",
    "    ####################################################################################\n",
    "    # NFM 模型\n",
    "    ####################################################################################\n",
    "    BATCH_SIZE=100\n",
    "    \"\"\"\n",
    "    training_data, training_label, dense_features_col, sparse_features_col = getTrainData(nfm_config['train_file'], nfm_config['fea_file'])\n",
    "    train_dataset = Data.TensorDataset(torch.tensor(training_data).float(), torch.tensor(training_label).float())\n",
    "\n",
    "    test_data = getTestData(nfm_config['test_file'])\n",
    "    test_dataset = Data.TensorDataset(torch.tensor(test_data).float())\n",
    "    \n",
    "    test_data = getTestData(nfm_config['test_file'])\n",
    "    test_dataset = Data.TensorDataset(torch.tensor(test_data).float())\n",
    "\n",
    "    \"\"\"\n",
    "    #device = torch.device('cuda:0')\n",
    "    #epoch=0\n",
    "    \n",
    "    #model=nn.Linear(10149,16).to(device)\n",
    "    #model=nn.Linear(10149,16)\n",
    "    #model=nn.ReLU(nn.Linear(10149,16))#RuntimeError: all elements of input should be between 0 and 1\n",
    "    #print('model:',model)\n",
    "    nfm = NFM(nfm_config).cuda()#加了device防止出现GPU CPU两种设备的错误提示\n",
    "    print(\"nfm:\",nfm)\n",
    "    #print(nfm)\n",
    "    #nfm.train()\n",
    "    #u=nfm.parameters()\n",
    "    #print(u)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    optimizer = torch.optim.Adam(nfm.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    total = 0\n",
    "    #loss_func = torch.nn.BCELoss()\n",
    "    loss_func=torch.nn.CrossEntropyLoss()\n",
    "    #loss_func=torch.nn.LogSoftmax()\n",
    "    \n",
    "    #model=nn.Softmax(nn.Linear(10149,16)).to(device)\n",
    "    # 从DataLoader中获取小批量的id以及数据\n",
    "    for epoch_id in range(1000):\n",
    "        correct=0\n",
    "        total=0\n",
    "        for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "            x = Variable(x)\n",
    "            labels = Variable(labels)\n",
    "            \n",
    "            \n",
    "            #x = torch.tensor(x, dtype=torch.float)\n",
    "            #x=x.clone().detach().requires_grad_(True)\n",
    "            x=torch.tensor(x,dtype=torch.float)\n",
    "            labels=torch.tensor(labels,dtype=torch.float)\n",
    "            x, labels = x.cuda(), labels.cuda()\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_predict = nfm(x)\n",
    "            print(\"y_predict:\",y_predict)\n",
    "            #loss = loss_func(y_predict.view(-1), labels)\n",
    "            loss = loss_func(y_predict, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss = loss.item()\n",
    "            #loss, predicted = self._train_single_batch(x, labels)\n",
    "\n",
    "            total += loss\n",
    "            \n",
    "            \n",
    "            \n",
    "            predicted = torch.max(y_predict.data,1)\n",
    "            #print(\"predicted:\",predicted)\n",
    "            predicted = torch.max(y_predict.data,1)[1]\n",
    "            \n",
    "            \n",
    "            labels=torch.max(labels,1)\n",
    "            #print(\"labels:\",labels)\n",
    "            labels=labels[1]\n",
    "            \n",
    "            correct += (predicted == labels).sum()\n",
    "            #print(\"correct:\",correct)\n",
    "            #correct=correct[0]\n",
    "            print(\"new_correct:\",float(correct))\n",
    "            correct=float(correct)   \n",
    "            #if batch_idx % 10 == 0:\n",
    "            print(\"batch_idx:\",batch_idx)\n",
    "            print(correct/(BATCH_SIZE*(batch_idx+1)))\n",
    "            \n",
    "            \"\"\"\n",
    "            #y_predict.detach().numpy()\n",
    "            pred = y_predict\n",
    "            print(\"pred:\",pred.shape)\n",
    "            y=labels.clone().detach().requires_grad_(True)\n",
    "            print(\"y:\",y.shape)\n",
    "            #y=labels.data.cpu().numpy()\n",
    "            #y = labels.detach().numpy()\n",
    "            roc_auc_score(y, pred)\n",
    "            \"\"\"\n",
    "           \n",
    "            # print('[Training Epoch: {}] Batch: {}, Loss: {}'.format(epoch_id, batch_id, loss))\n",
    "        print(\"Training Epoch: %d, total loss: %f\" % (epoch_id, total))\n",
    "        #print(\"auc:\",roc_auc_score)\n",
    "\n",
    "        \n",
    "#功能：保存训练完的网络的各层参数（即weights和bias)\n",
    "path='data/frappe/model/model_param_gene_4000.pkl'\n",
    "torch.save(nfm.state_dict(),path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ba678d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#功能：保存训练完的网络的各层参数（即weights和bias)\n",
    "path='data/frappe/model/model_param.pkl'\n",
    "torch.save(nfm.state_dict(),path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22cc4c5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nfm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-25b3a309e0d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnfm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nfm' is not defined"
     ]
    }
   ],
   "source": [
    "nfm.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23a0cc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NFM(\n",
      "  (linear_model): Linear(in_features=10150, out_features=16, bias=True)\n",
      "  (embedding_layers): Embedding(1001, 100)\n",
      "  (bi_pooling): BiInteractionPooling()\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (dnn_layers): ModuleList(\n",
      "    (0): Linear(in_features=10150, out_features=100, bias=True)\n",
      "    (1): Linear(in_features=100, out_features=16, bias=True)\n",
      "  )\n",
      "  (dnn_linear): Linear(in_features=16, out_features=16, bias=False)\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for NFM:\n\tsize mismatch for linear_model.weight: copying a param with shape torch.Size([100, 10150]) from checkpoint, the shape in current model is torch.Size([16, 10150]).\n\tsize mismatch for linear_model.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for dnn_layers.0.weight: copying a param with shape torch.Size([100, 200]) from checkpoint, the shape in current model is torch.Size([100, 10150]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9de684dba1a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#net = nn.DataParallel(net)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#net = net.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1483\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1484\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for NFM:\n\tsize mismatch for linear_model.weight: copying a param with shape torch.Size([100, 10150]) from checkpoint, the shape in current model is torch.Size([16, 10150]).\n\tsize mismatch for linear_model.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for dnn_layers.0.weight: copying a param with shape torch.Size([100, 200]) from checkpoint, the shape in current model is torch.Size([100, 10150])."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "\n",
    "#功能：加载保存到path中的各层参数到神经网络\n",
    "path='data/frappe/model/model_param.pkl'\n",
    "net=NFM(nfm_config)\n",
    "print(net)\n",
    "#net = nn.DataParallel(net)\n",
    "#net = net.to(device)\n",
    "net.load_state_dict(torch.load(path),strict=False)\n",
    "net.cuda()\n",
    "\n",
    "print(net)\n",
    "\n",
    "params = list(net.named_parameters())\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3a4ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存模型\n",
    "\n",
    "torch.save(nfm,'model.pkl')\n",
    "torch.save(nfm_config.state_dict(),'model_param.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fb8859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载\n",
    "model = torch.load('model.pkl')\n",
    "# 加载参数\n",
    "model_param = torch.load('model_param.pkl')\n",
    "# 为模型设置参数\n",
    "model.load_state_dict(model_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d03c5cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row: 0 label: [0]\n",
      "row: 1 label: [0]\n",
      "row: 2 label: [1]\n",
      "row: 3 label: [1]\n",
      "row: 4 label: [2]\n",
      "row: 5 label: [2]\n",
      "row: 6 label: [3]\n",
      "row: 7 label: [3]\n",
      "row: 8 label: [4]\n",
      "row: 9 label: [4]\n",
      "row: 10 label: [5]\n",
      "row: 11 label: [5]\n",
      "row: 12 label: [6]\n",
      "row: 13 label: [6]\n",
      "row: 14 label: [7]\n",
      "row: 15 label: [7]\n",
      "row: 16 label: [8]\n",
      "row: 17 label: [8]\n",
      "row: 18 label: [9]\n",
      "row: 19 label: [9]\n",
      "row: 20 label: [10]\n",
      "row: 21 label: [10]\n",
      "row: 22 label: [11]\n",
      "row: 23 label: [11]\n",
      "row: 24 label: [12]\n",
      "row: 25 label: [12]\n",
      "row: 26 label: [13]\n",
      "row: 27 label: [13]\n",
      "row: 28 label: [14]\n",
      "row: 29 label: [14]\n",
      "row: 30 label: [15]\n",
      "row: 31 label: [15]\n",
      "label: [[0.8125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125\n",
      "  0.0125 0.0125 0.0125 0.0125 0.0125 0.0125]\n",
      " [0.8125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125\n",
      "  0.0125 0.0125 0.0125 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.8125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125\n",
      "  0.0125 0.0125 0.0125 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.8125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125\n",
      "  0.0125 0.0125 0.0125 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.8125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125\n",
      "  0.0125 0.0125 0.0125 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.8125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125\n",
      "  0.0125 0.0125 0.0125 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 0.8125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125\n",
      "  0.0125 0.0125 0.0125 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 0.8125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125\n",
      "  0.0125 0.0125 0.0125 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 0.0125 0.8125 0.0125 0.0125 0.0125 0.0125 0.0125\n",
      "  0.0125 0.0125 0.0125 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 0.0125 0.8125 0.0125 0.0125 0.0125 0.0125 0.0125\n",
      "  0.0125 0.0125 0.0125 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 0.0125 0.0125 0.8125 0.0125 0.0125 0.0125 0.0125\n",
      "  0.0125 0.0125 0.0125 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 0.0125 0.0125 0.8125 0.0125 0.0125 0.0125 0.0125\n",
      "  0.0125 0.0125 0.0125 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.8125 0.0125 0.0125 0.0125\n",
      "  0.0125 0.0125 0.0125 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.8125 0.0125 0.0125 0.0125\n",
      "  0.0125 0.0125 0.0125 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.8125 0.0125 0.0125\n",
      "  0.0125 0.0125 0.0125 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.8125 0.0125 0.0125\n",
      "  0.0125 0.0125 0.0125 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.8125 0.0125\n",
      "  0.0125 0.0125 0.0125 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.8125 0.0125\n",
      "  0.0125 0.0125 0.0125 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.8125\n",
      "  0.0125 0.0125 0.0125 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.8125\n",
      "  0.0125 0.0125 0.0125 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125\n",
      "  0.8125 0.0125 0.0125 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125\n",
      "  0.8125 0.0125 0.0125 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125\n",
      "  0.0125 0.8125 0.0125 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125\n",
      "  0.0125 0.8125 0.0125 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125\n",
      "  0.0125 0.0125 0.8125 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125\n",
      "  0.0125 0.0125 0.8125 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125\n",
      "  0.0125 0.0125 0.0125 0.8125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125\n",
      "  0.0125 0.0125 0.0125 0.8125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125\n",
      "  0.0125 0.0125 0.0125 0.0125 0.8125 0.0125]\n",
      " [0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125\n",
      "  0.0125 0.0125 0.0125 0.0125 0.8125 0.0125]\n",
      " [0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125\n",
      "  0.0125 0.0125 0.0125 0.0125 0.0125 0.8125]\n",
      " [0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125 0.0125\n",
      "  0.0125 0.0125 0.0125 0.0125 0.0125 0.8125]]\n",
      "features: [[120 200 480 ... 390 290 420]\n",
      " [130 290 500 ... 350 320 380]\n",
      " [430 360 520 ... 450 360 480]\n",
      " ...\n",
      " [460 460 500 ... 560 490 530]\n",
      " [470 470 490 ... 580 520 560]\n",
      " [450 360 500 ... 430 360 500]]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = FMData(config.test_libfm,config.test_label,nfm_config['n_class'])\n",
    "test_loader = data.DataLoader(test_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dca8f8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————\n",
    "#版权声明：本文为CSDN博主「山阴少年」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\n",
    "#原文链接：https://blog.csdn.net/jclian91/article/details/121708431#\n",
    "\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = [], []\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        #yhat = yhat.detach().numpy()\n",
    "        yhat = yhat.detach().cpu.numpy()#转换到cpu\n",
    "        print(\"yhat:\",yhat)\n",
    "        actual = targets.numpy()\n",
    "        print(\"actual:\",actual)\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # round to class values\n",
    "        yhat = yhat.round()\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    print(\"prediction:\",predictions)\n",
    "    predictions, actuals = np.vstack(predictions), np.vstack(actuals)\n",
    "    # calculate accuracy\n",
    "    acc_test = accuracy_score(actuals, predictions)\n",
    "    return acc_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d6a0a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: []\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3895d1856796>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnfm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-d8702bf4613d>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(test_dl, model)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mactuals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prediction:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactuals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactuals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;31m# calculate accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0macc_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactuals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "acc_test=evaluate_model(test_loader,nfm)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5a09dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————\n",
    "#版权声明：本文为CSDN博主「山阴少年」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\n",
    "#原文链接：https://blog.csdn.net/jclian91/article/details/121708431\n",
    "\n",
    "# make a class prediction for one row of data\n",
    "def predict(row, model):\n",
    "    # convert row to data\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().cpu.numpy()\n",
    "    return yhat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83215c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (inputs, targets) in enumerate(test_loader):\n",
    "        # evaluate the model on the test set\n",
    "        #yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        #yhat = yhat.detach().numpy()\n",
    "        #yhat = yhat.detach().cpu.numpy()#转换到cpu\n",
    "        #actual = targets.numpy()\n",
    "        ##actual = actual.reshape((len(actual), 1))\n",
    "        # round to class values\n",
    "        #yhat = yhat.round()\n",
    "        # store\n",
    "        #predictions.append(yhat)\n",
    "        #actuals.append(actual)\n",
    "        actual=targets.numpy()\n",
    "        yhat[i]=predict(inputs,nfm)\n",
    "        predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "        # calculate accuracy\n",
    "        acc_test = accuracy_score(actuals, predictions)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1dd125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————\n",
    "#版权声明：本文为CSDN博主「卅拓」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\n",
    "#原文链接：https://blog.csdn.net/weixin_45426939/article/details/120967776\n",
    "\n",
    "#计算AUC\n",
    "y_test_one_hot_hat = clf.predict_proba(x_test)  #测试集预测分类的概率 \n",
    "\n",
    "from sklearn import metrics\n",
    "#计算fpr tpr\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test_one_hot.ravel(), y_test_one_hot_hat.ravel())\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print ('Micro AUC:\\t', metrics.auc(fpr, tpr))    # AUC ROC意思是ROC曲线下方的面积(Area under the Curve of ROC)\n",
    "print( 'Micro AUC(System):\\t', metrics.roc_auc_score(y_test_one_hot, y_test_one_hot_hat, average='micro'))\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test_one_hot, y_test_one_hot_hat, average='macro')\n",
    "print ('Macro AUC:\\t', auc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8205ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    "计算获得fpr、tpr、auc,上述步骤中有\n",
    "假装已经得到~\n",
    "'''\n",
    "\n",
    "plt.figure(figsize=(8, 7), dpi=80, facecolor='w')    # dpi:每英寸长度的像素点数；facecolor 背景颜色\n",
    "plt.xlim((-0.01, 1.02))  # x,y 轴刻度的范围\n",
    "plt.ylim((-0.01, 1.02))\n",
    "plt.xticks(np.arange(0, 1.1, 0.1))  #绘制刻度\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.plot(fpr, tpr, 'r-', lw=2, label='AUC=%.4f' % auc)  # 绘制AUC 曲线\n",
    "plt.legend(loc='lower right')    # 设置显示标签的位置\n",
    "plt.xlabel('False Positive Rate', fontsize=14)   #绘制x,y 坐标轴对应的标签\n",
    "plt.ylabel('True Positive Rate', fontsize=14)\n",
    "plt.grid(b=True, ls=':')  # 绘制网格作为底板;b是否显示网格线；ls表示line style\n",
    "plt.title(u'DecisionTree ROC curve And  AUC', fontsize=18)  # 打印标题\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25ebc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————\n",
    "#版权声明：本文为CSDN博主「农民小飞侠」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\n",
    "#原文链接：https://blog.csdn.net/w5688414/article/details/106595892\n",
    "\n",
    "n_classes=y_predict.shape[1]\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(true_y[:, i], y_predict[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba74722",
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————\n",
    "#版权声明：本文为CSDN博主「Ramiro Luo」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\n",
    "#原文链接：https://blog.csdn.net/qq_37844142/article/details/108282190————————————————\n",
    "#版权声明：本文为CSDN博主「Ramiro Luo」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\n",
    "#原文链接：https://blog.csdn.net/qq_37844142/article/details/108282190————————————————\n",
    "#版权声明：本文为CSDN博主「Ramiro Luo」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\n",
    "#原文链接：https://blog.csdn.net/qq_37844142/article/details/108282190\n",
    "color=[\n",
    "'aliceblue',\n",
    "'antiquewhite',\n",
    "'aqua',\n",
    "'aquamarine',\n",
    "'azure',\n",
    "'beige',\n",
    "'bisque',\n",
    "'black',\n",
    "'blanchedalmond',\n",
    "'blue',\n",
    "'blueviolet',\n",
    "'brown',\n",
    "'burlywood',\n",
    "'cadetblue',\n",
    "'chartreuse',\n",
    "'chocolate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2730861",
   "metadata": {},
   "outputs": [],
   "source": [
    "#————————————————\n",
    "#33版权声明：本文为CSDN博主「农民小飞侠」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\n",
    "#原文链接：https://blog.csdn.net/w5688414/article/details/106595892\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "lw = 2\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "labels=['Category 0','Category 1','Category 2','Category 3','Category 4'，'Category 5','Category 6','Category 7','Category 8','Category 9'，'Category 10','Category 11','Category 12','Category 13','Category 14'，'Category 15']\n",
    "\n",
    "#colors = cycle(['aqua', 'darkorange', 'cornflowerblue','blue','yellow'])\n",
    "colors=cycle(color)\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label=labels[i]+'(area = {0:0.4f})'.format(roc_auc[i]))\n",
    " \n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1-Specificity (%)')\n",
    "plt.ylabel('Sensitivity (%)')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "def to_percent(temp, position):\n",
    "    return '%1.0f'%(100*temp)\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(to_percent))\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(to_percent))\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e25b73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
