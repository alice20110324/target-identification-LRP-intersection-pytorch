{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81411d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load main.py\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import model\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--lr\", \n",
    "    type=float, \n",
    "    default=0.05, \n",
    "    help=\"learning rate\")\n",
    "parser.add_argument(\"--dropout\", \n",
    "    default='[0.5, 0.2]',  \n",
    "    help=\"dropout rate for FM and MLP\")\n",
    "parser.add_argument(\"--batch_size\", \n",
    "    type=int, \n",
    "    default=128, \n",
    "    help=\"batch size for training\")\n",
    "parser.add_argument(\"--epochs\", \n",
    "    type=int,\n",
    "    default=100, \n",
    "    help=\"training epochs\")\n",
    "parser.add_argument(\"--hidden_factor\", \n",
    "    type=int,\n",
    "    default=64, #被修改，原始值为64\n",
    "    help=\"predictive factors numbers in the model\")\n",
    "parser.add_argument(\"--layers\", \n",
    "    default='[5000,1000]',#被修改，有两个隐层，之前值为64，一个隐层 \n",
    "    help=\"size of layers in MLP model, '[]' is NFM-0\")\n",
    "\n",
    "parser.add_argument(\"--nclass\",\n",
    "    type=int,\n",
    "    default=13,\n",
    "    help=\"number of class\")\n",
    "\n",
    "\n",
    "parser.add_argument(\"--lamda\", \n",
    "    type=float, \n",
    "    default=0.0, \n",
    "    help=\"regularizer for bilinear layers\")\n",
    "parser.add_argument(\"--batch_norm\", \n",
    "    default=True, \n",
    "    help=\"use batch_norm or not\")\n",
    "parser.add_argument(\"--pre_train\", \n",
    "    action='store_true', \n",
    "    default=False, \n",
    "    help=\"whether use the pre-train or not\")\n",
    "parser.add_argument(\"--out\", \n",
    "    default=True, \n",
    "    help=\"save model or not\")\n",
    "parser.add_argument(\"--gpu\", \n",
    "    type=str,\n",
    "    default=\"0\",  \n",
    "    help=\"gpu card ID\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "cudnn.benchmark = True\n",
    "\n",
    "\n",
    "#############################  PREPARE DATASET #########################\n",
    "features_map, num_features = data_utils.map_features()\n",
    "\n",
    "train_dataset = data_utils.FMData(config.train_libfm, features_map)\n",
    "valid_dataset = data_utils.FMData(config.valid_libfm, features_map)\n",
    "test_dataset = data_utils.FMData(config.test_libfm, features_map)\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, drop_last=True,\n",
    "            batch_size=args.batch_size, shuffle=True, num_workers=4)\n",
    "valid_loader = data.DataLoader(valid_dataset,\n",
    "            batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = data.DataLoader(test_dataset,\n",
    "            batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "##############################  CREATE MODEL ###########################\n",
    "if args.pre_train:\n",
    "    assert os.path.exists(config.FM_model_path), 'lack of FM model'\n",
    "    assert config.model == 'NFM', 'only support NFM for now'\n",
    "    FM_model = torch.load(config.FM_model_path)\n",
    "else:\n",
    "    FM_model = None\n",
    "\n",
    "if config.model == 'FM':\n",
    "    model = model.FM(num_features, args.hidden_factor,\n",
    "                    args.batch_norm, eval(args.dropout))\n",
    "else:\n",
    "    model = model.NFM(\n",
    "        num_features, args.hidden_factor, \n",
    "        config.activation_function, eval(args.layers), \n",
    "        args.batch_norm, eval(args.dropout), FM_model)\n",
    "model.cuda()\n",
    "if config.optimizer == 'Adagrad':\n",
    "    optimizer = optim.Adagrad(\n",
    "        model.parameters(), lr=args.lr, initial_accumulator_value=1e-8)\n",
    "elif config.optimizer == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "elif config.optimizer == 'SGD':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "elif config.optimizer == 'Momentum':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.95)\n",
    "\n",
    "if config.loss_type == 'square_loss':\n",
    "    criterion = nn.MSELoss(reduction='sum')\n",
    "elif config.loss_type=='cross_entropy_loss':#被修改，增加了交叉熵损失\n",
    "    criterion=nn.CrossEntropyLoss(reduction='mean')\n",
    "else: # log_loss\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "# writer = SummaryWriter() # for visualization\n",
    "\n",
    "###############################  TRAINING ############################\n",
    "count, best_rmse = 0, 100\n",
    "for epoch in range(args.epochs):\n",
    "    model.train() # Enable dropout and batch_norm\n",
    "    start_time = time.time()\n",
    "\n",
    "    for features, feature_values, label in train_loader:\n",
    "        features = features.cuda()\n",
    "        feature_values = feature_values.cuda()\n",
    "        label = label.cuda()\n",
    "\n",
    "        model.zero_grad()\n",
    "        prediction = model(features, feature_values)\n",
    "        loss = criterion(prediction, label) \n",
    "        loss += args.lamda * model.embeddings.weight.norm()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # writer.add_scalar('data/loss', loss.item(), count)\n",
    "        count += 1\n",
    "\n",
    "    model.eval()\n",
    "    train_result = evaluate.metrics(model, train_loader)\n",
    "    valid_result = evaluate.metrics(model, valid_loader)\n",
    "    test_result = evaluate.metrics(model, test_loader)\n",
    "\n",
    "    print(\"Runing Epoch {:03d} \".format(epoch) + \"costs \" + time.strftime(\n",
    "                        \"%H: %M: %S\", time.gmtime(time.time()-start_time)))\n",
    "    print(\"Train_RMSE: {:.3f}, Valid_RMSE: {:.3f}, Test_RMSE: {:.3f}\".format(\n",
    "                        train_result, valid_result, test_result))\n",
    "\n",
    "    if test_result < best_rmse:\n",
    "        best_rmse, best_epoch = test_result, epoch\n",
    "        if args.out:\n",
    "            if not os.path.exists(config.model_path):\n",
    "                os.mkdir(config.model_path)\n",
    "            torch.save(model, \n",
    "                '{}{}.pth'.format(config.model_path, config.model))\n",
    "\n",
    "print(\"End. Best epoch {:03d}: Test_RMSE is {:.3f}\".format(best_epoch, best_rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67199ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
