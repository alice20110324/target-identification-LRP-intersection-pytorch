{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4853fe45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bn0.weight\n",
      "bn0.bias\n",
      "bn0.running_mean\n",
      "bn0.running_var\n",
      "bn0.num_batches_tracked\n",
      "fc1.weight\n",
      "fc1.bias\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "bn1.running_mean\n",
      "bn1.running_var\n",
      "bn1.num_batches_tracked\n",
      "fc2.weight\n",
      "fc2.bias\n",
      "bn2.weight\n",
      "bn2.bias\n",
      "bn2.running_mean\n",
      "bn2.running_var\n",
      "bn2.num_batches_tracked\n",
      "fc3.weight\n",
      "fc3.bias\n",
      "bn3.weight\n",
      "bn3.bias\n",
      "bn3.running_mean\n",
      "bn3.running_var\n",
      "bn3.num_batches_tracked\n",
      "MLP(\n",
      "  (bn0): BatchNorm1d(3300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=3300, out_features=2000, bias=True)\n",
      "  (bn1): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=2000, out_features=100, bias=True)\n",
      "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=100, out_features=9, bias=True)\n",
      "  (bn3): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "relevance\n"
     ]
    }
   ],
   "source": [
    "#############  u===2   MLP_encode_100 and MLP_encode_1000  :654    ##################\n",
    "import torch\n",
    "#import Trainer\n",
    "from network import NFM\n",
    "import torch.utils.data as Data\n",
    "from Utils.criteo_loader import getTestData, getTrainData\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import sys \n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "import torchmetrics\n",
    "\n",
    "from innvestigator import InnvestigateModel\n",
    "from utils import Flatten\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from mnist_test import Net, train, test\n",
    "\n",
    "input_num=3300\n",
    "# Network parameters\n",
    "class Params(object):\n",
    "    batch_size = 64\n",
    "    test_batch_size = 20\n",
    "    epochs = 5\n",
    "    lr = 0.01\n",
    "    momentum = 0.5\n",
    "    no_cuda = True\n",
    "    seed = 1\n",
    "    log_interval = 10\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "args = Params()\n",
    "torch.manual_seed(args.seed)\n",
    "#device = torch.device(\"cpu\")\n",
    "device=torch.device('cuda')\n",
    "kwargs = {}\n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':9,\n",
    "    \n",
    "    'num_epoch': 500,#训练epoch次数\n",
    "    'batch_size': 16,#batch_size\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    'epoch':1000,\n",
    "    \n",
    "   \n",
    "    'gene_name':'dataset/qiuguan/origin_800/gene_name.csv',\n",
    "    'label_name':'dataset/qiuguan/origin_800/gene_label.csv'\n",
    "    \n",
    "}\n",
    "\n",
    "#model definition\n",
    "import torch.nn as nn\n",
    "    \n",
    "    \n",
    "\n",
    "#model1 = MLP().cuda()\n",
    "#print(model1)\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn0=nn.BatchNorm1d(input_num)\n",
    "        self.fc1 = nn.Linear(input_num, 2000)\n",
    "        self.bn1= nn.BatchNorm1d(2000)\n",
    "        self.fc2 = nn.Linear(2000, 100)\n",
    "        self.bn2=nn.BatchNorm1d(100)\n",
    "        self.fc3=nn.Linear(100,9)\n",
    "        self.bn3=nn.BatchNorm1d(9)\n",
    "        \n",
    "        self.drop=nn.Dropout(0.3) \n",
    "        \n",
    "        #self.model1=MLP1().cuda() \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        y1=self.bn0(x)\n",
    "        y1 = F.relu(self.drop(self.bn1(self.fc1(y1))))\n",
    "        y1= F.relu(self.drop(self.bn2(self.fc2(y1))))\n",
    "        return F.softmax(self.bn3(self.fc3(y1)), dim=1)\n",
    "        \n",
    "           \n",
    "        \n",
    "        \n",
    "        \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "model = MLP().cuda()\n",
    "\n",
    "for i in model.state_dict():\n",
    "    print(i)\n",
    "print(model)\n",
    "mlp_paras=list(model.named_parameters())\n",
    "#print(mlp_paras)\n",
    "inn_model = InnvestigateModel(model, lrp_exponent=2,\n",
    "                              method=\"e-rule\",\n",
    "                              beta=.5)\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import torch.nn.functional as F  # 激励函数的库\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "\n",
    "def one_hot_smoothing(labels, classes, label_smoothing=0.2):\n",
    "    #n = len(labels)\n",
    "    n=labels.shape[0]\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    n = len(labels)\n",
    "    #eoff = label_smoothing / classes\n",
    "    output = np.zeros((n, classes), dtype=np.float32)\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_smoothing(labels, classes, label_smoothing=0.2):\n",
    "    #n = len(labels)\n",
    "    n=labels.shape[0]\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    n = len(labels)\n",
    "    #eoff = label_smoothing / classes\n",
    "    output = np.zeros((n, classes), dtype=np.float32)\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "class KZDatasetTest(data.Dataset):\n",
    "    \"\"\" Construct the FM pytorch dataset. \"\"\"\n",
    "    #def __init__(self, file,label_file, feature_map,n_class=16):\n",
    "    def __init__(self, csv_path):\n",
    "    \n",
    "       \n",
    "        self.data_info = self.get_data_info(csv_path)\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Dataset读取图片的函数\n",
    "        data, label = self.data_info[index]\n",
    "        #img = Image.open(img_pth).convert('RGB')\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_data_info(self,csv_path):\n",
    "        #解析路径\n",
    "        #转为一维list存储，每一位为【图片路径，图片类别】\n",
    "        labels=[]\n",
    "        data_info=[]\n",
    "        df=pd.read_csv(csv_path,sep=',')\n",
    "        #print(\"df:\",df)\n",
    "        df=df.iloc[:,1:]\n",
    "        \n",
    "        rows,cols=df.shape\n",
    "        #print(rows,cols)\n",
    "        for i in df.iloc[:,-1]:\n",
    "            #print(i)\n",
    "            labels.append(int(float(i)))\n",
    "        #print('labels:',labels)\n",
    "        labels=np.array(labels)\n",
    "        \n",
    "        labels=one_hot_smoothing(labels,nfm_config['n_class'])\n",
    "        #print(labels)\n",
    "        for i in range(rows):\n",
    "            data=df.iloc[i,:-1]\n",
    "            data=data.astype(float)##############\n",
    "            \n",
    "            data=np.array(data)##\n",
    "            \n",
    "            label=labels[i]\n",
    "            \n",
    "            data=torch.from_numpy(data)#\n",
    "            label=torch.from_numpy(label)#\n",
    "           \n",
    "            \n",
    "            data_info.append((data,label))\n",
    "        return data_info\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import *\n",
    "from PIL import Image\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ast\n",
    "import torchvision\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "class KZDataset(Dataset):\n",
    "    def __init__(self, csv_path, K,n_class,ki=0, typ='train', transform=None, rand=False):\n",
    "        \n",
    "        self.all_data_info = self.get_data_info(csv_path)\n",
    "        \n",
    "        if rand:\n",
    "            random.seed(1)\n",
    "            random.shuffle(self.all_data_info)\n",
    "        leng = len(self.all_data_info)\n",
    "        every_z_len = leng // K\n",
    "        if typ == 'val':\n",
    "            self.data_info = self.all_data_info[every_z_len * ki : every_z_len * (ki+1)]\n",
    "        elif typ == 'train':\n",
    "            self.data_info = self.all_data_info[: every_z_len * ki] + self.all_data_info[every_z_len * (ki+1) :]\n",
    "            \n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Dataset读取图片的函数\n",
    "        data, label = self.data_info[index]\n",
    "        #img = Image.open(img_pth).convert('RGB')\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "       \n",
    "    \n",
    "    def get_data_info(self,csv_path):\n",
    "        #解析路径\n",
    "        #转为一维list存储，每一位为【图片路径，图片类别】\n",
    "        labels=[]\n",
    "        data_info=[]\n",
    "        df=pd.read_csv(csv_path,sep=',')\n",
    "        #print(\"df:\",df)\n",
    "        df=df.iloc[:,1:]\n",
    "        #print(\"df:\",df)\n",
    "        print(df.shape)\n",
    "        #print(\"df:\",df)\n",
    "        #print(df.iloc[:,-1])\n",
    "        #df=df.applymap(ast.literal_eval)\n",
    "        rows,cols=df.shape\n",
    "        #print(rows,cols)\n",
    "        for i in df.iloc[:,-1]:\n",
    "            #print(i)\n",
    "            labels.append(int(float(i)))\n",
    "        #print('labels:',i,labels)\n",
    "        labels=np.array(labels)\n",
    "        #print('labels:',labels)\n",
    "        #labels=np.array(labels)\n",
    "        labels=one_hot_smoothing(labels,nfm_config['n_class'])\n",
    "        #print(labels)\n",
    "        for i in range(rows):\n",
    "            data=df.iloc[i,:-1]\n",
    "            data=data.astype(float)##############\n",
    "            #print(\"i,data:\",i,data)\n",
    "            #data=pd.DataFrame(data,dtype=float)###############\n",
    "            data=np.array(data)##\n",
    "            \n",
    "            label=labels[i]\n",
    "            #print(data.shape)\n",
    "            #print(label.shape)\n",
    "            #label=label.tolist()\n",
    "            data=torch.from_numpy(data)#\n",
    "            label=torch.from_numpy(label)#\n",
    "           \n",
    "            \n",
    "            data_info.append((data,label))\n",
    "        return data_info\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import sys \n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "import torchmetrics\n",
    "#LRP\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline\n",
    "\n",
    "from innvestigator import InnvestigateModel\n",
    "from utils import Flatten\n",
    "def standN(x):\n",
    "    row,col=x.shape\n",
    "    max_x=torch.max(x,1)\n",
    "    min_x=torch.min(x,1)\n",
    "    #print(max_x)\n",
    "    #print(min_x)\n",
    "    for i in range(row):\n",
    "        x[i]=(x[i]-min_x.values[i])/max_x.values[i]\n",
    "    return x\n",
    "\n",
    "def standNorm(x):\n",
    "    row,col=x.shape\n",
    "    mean=torch.mean(x,1)\n",
    "    std=torch.std(x,1)\n",
    "    #print(mean)\n",
    "    #print(std)\n",
    "    for i in range(row):\n",
    "        x[i]=(x[i]-mean[i])/std[i]\n",
    "    return x\n",
    "\n",
    "def augment_epoch(kii,train_loader,batch_size):\n",
    "    BATCH_SIZE=batch_size\n",
    "    total = 0\n",
    "    correct=0\n",
    "    total_loss=0\n",
    "    #loss_score=torch.tensor([[]]).cuda()\n",
    "    #\n",
    "    #loss_op=0\n",
    "    loss2_list=[]\n",
    "    model.train()\n",
    "    total_train_accuracy=0  \n",
    "    #BL=nn.BatchNorm1d(input_num)\n",
    "    #BL=BL.cuda()\n",
    "    y=[[0]*input_num]*batch_size\n",
    "    y=torch.tensor(y,dtype=torch.float)\n",
    "    y=y.cuda()\n",
    "    \n",
    "    for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "            \n",
    "        labels = Variable(labels)\n",
    "        x = Variable(x)\n",
    "            \n",
    "        x_row,x_col=x.shape   \n",
    "        x=torch.tensor(x,dtype=torch.float)\n",
    "        #print(x.shape)\n",
    "        labels=torch.tensor(labels,dtype=torch.float)\n",
    "        \n",
    "        print('labels.shape:',labels.shape)\n",
    "        x, labels = x.cuda(), labels.cuda()\n",
    "        labels_int=labels=torch.max(labels,1)[1]#########\n",
    "        print('labels_int:',labels_int.shape)\n",
    "        #print(labels_int)\n",
    "        #print('labels_int:',labels_int.shape)\n",
    "        #print('labels:',labels) \n",
    "        #print('x:',x.shape)\n",
    "        l1=labels.view(batch_size,-1)\n",
    "        #ll.dtype=torch.int\n",
    "        #print('ll:',ll)\n",
    "        if kii>=0 : \n",
    "            \n",
    "            inn_model.evaluate(in_tensor=x)\n",
    "        \n",
    "            model_prediction, only_max_score,org_shape = inn_model.innvestigatex()\n",
    "            model_prediction.cuda()\n",
    "            print('model_prediction:',model_prediction.shape)\n",
    "            #print('only_max_score:',only_max_score)\n",
    "            #only_max_score1=only_max_score.detach().clone()\n",
    "            #print(labels.shape,labels)   \n",
    "        \n",
    "            #print(\"torch.argmax(labels[batch_idx]):\",torch.argmax(labels[batch_idx]))\n",
    "            #print(model_prediction.shape, model_prediction)\n",
    "            #model_prediction.cuda()\n",
    "            #print('input_relevance_values:')\n",
    "            #rel_for_class_list=torch.argmax(model_prediction,1)#######################\n",
    "            rel_for_class_list=labels_int###################\n",
    "            print('rel_for_class_list:',rel_for_class_list.shape)\n",
    "            para_list=[0 for i in range(batch_size)]\n",
    "            \n",
    "            '''  \n",
    "            rand=torch.linspace(0, 1, steps=input_num)\n",
    "            rand=rand.detach().numpy().tolist()\n",
    "            \n",
    "            para_list=[round(i,4) for i in rand]\n",
    "            '''\n",
    "            \n",
    "            #print(para_list)\n",
    "            \n",
    "            '''\n",
    "            for i in range(batch_size):\n",
    "            \n",
    "                max_pre=torch.argmax(model_prediction[i])\n",
    "                if max_pre!=labels[i]:\n",
    "                    #print('only_max_score1:',i,only_max_score)\n",
    "                    #only_max_score=only_max_score1\n",
    "                    #input_relevance_values=inn_model.compute_relevance_score(only_max_score,labels[i],org_shape,para=0.1)\n",
    "                    #rel_for_class=labels[i]\n",
    "                    #para=random.uniform(0.3,0.6)\n",
    "                    ##para=random.gauss(0.2,0.9)\n",
    "                    para=random.gauss(0.8,0.4)\n",
    "                    #para=random.uniform(0.4,0.8)\n",
    "                    #para=random.gauss(0.6,0.2)\n",
    "                    #para=0.4\n",
    "                    para=round(para,4)\n",
    "                    para_list.append(para)\n",
    "                    #print('device:',input_relevance_values.device)\n",
    "                    #input_relevance_values[labels[i]]=input_relevance_values[labels[i]].cuda()\n",
    "                    #input_relevance_values[labels[i]]=input_relevance_values[labels[i]].exp()\n",
    "                    #print('input_relevance_values:',i, input_relevance_values)\n",
    "                    #print('input_relevance_values[labels[i]]:',i,batch_idx,input_relevance_values[labels[i]])\n",
    "                    #cc[i]=torch.mul(x[i],input_relevance_values[labels[i]])+torch.tensor(0.1,dtype=torch.float)##############注意力\n",
    "                    #print('pre_target:',pre_target[i])\n",
    "                    #print('cc',i,cc[i].shape)\n",
    "                else:\n",
    "                    #print('only_max_score2:',i,only_max_score)\n",
    "                    #only_max_score=only_max_score1\n",
    "                    #input_relevance_values=inn_model.compute_relevance_score(only_max_score,labels[i],org_shape,para=0)\n",
    "                    #input_relevance_values[labels[i]]=input_relevance_values[labels[i]].exp()\n",
    "                    #print('input_relevance_values:',i, input_relevance_values)\n",
    "                    #print('input_relevance_values[i]:',i,batch_idx,input_relevance_values[labels[i]])\n",
    "                    #cc[i]=torch.mul(x[i],input_relevance_values[labels[i]])##############注意力\n",
    "                    #print('cc',i,cc[i].shape)\n",
    "                    #para=random.uniform(0.4,0.7)\n",
    "                    #para=round(para,4)\n",
    "                    para=random.gauss(0.3,0.1)\n",
    "                    #para=random.uniform(0.1,0.2)\n",
    "                    #para=round(para,4)\n",
    "                    #para=0\n",
    "                    para_list.append(para)\n",
    "            \n",
    "            '''\n",
    "            \n",
    "            input_relevance_values,layers=inn_model.compute_relevance_scorex(only_max_score,rel_for_class_list,org_shape,para_list)\n",
    "            input_len=len(input_relevance_values)\n",
    "            \n",
    "            ee=[[0]*input_num]*batch_size\n",
    "            ee=torch.tensor(ee,dtype=torch.float)\n",
    "        \n",
    "            ee=ee.cuda()\n",
    "            input_relevance_values[-1]=input_relevance_values[-1].exp()############\n",
    "            \n",
    "            \n",
    "            #sum_input_relevance_values=torch.sum(input_relevance_values,dim=0)\n",
    "            #input_relevance_values=F.softmax(input_relevance_values/sum_input_relevance_values,dim=1)\n",
    "            input_relevance_values[-1]=F.softmax(input_relevance_values[-1],dim=1)################\n",
    "        \n",
    "            '''  \n",
    "            for i in range(batch_size):\n",
    "                        max_pre=torch.argmax(model_prediction[i])######\n",
    "                        if max_pre!=labels[i]: #and i==min_predict_mis:###\n",
    "                            \n",
    "                                \n",
    "            \n",
    "                                ee[i]=torch.mul(x[i],input_relevance_values[-1][i])##############注意力33333333333333333weight\n",
    "                                #dd=torch.mul(bias,input_relevance_values[-1][i])############bias\n",
    "                                #print('cc:',i,cc)\n",
    "                                #ee[i]=torch.mul(drop,input_relevance_values[i])\n",
    "                                ################\n",
    "            \n",
    "            '''  \n",
    "            \n",
    "                                \n",
    "            ee=torch.mul(x,input_relevance_values[-1])\n",
    "            \n",
    "            \n",
    "            x1=torch.subtract(x,ee)\n",
    "            x2=torch.add(x,ee)\n",
    "            #print(ll.shape)\n",
    "            #print(x.shape)\n",
    "            #x1=torch.cat((x1,ll),dim=1)\n",
    "            #x2=torch.cat((x2,ll),dim=1)\n",
    "        \n",
    "            l1=l1\n",
    "            l2=l1\n",
    "            if batch_idx==0:\n",
    "                \n",
    "                y=torch.cat((x1,x2),dim=0)\n",
    "                l=torch.cat((l1,l2),dim=0)\n",
    "            y=torch.cat((y,x1),dim=0)\n",
    "            y=torch.cat((y,x2),dim=0)\n",
    "            \n",
    "            l=torch.cat((l,l1),dim=0)\n",
    "            l=torch.cat((l,l2),dim=0)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            '''\n",
    "            l2=l1\n",
    "            \n",
    "            if batch_idx==0:\n",
    "                y=x2\n",
    "                l=l2\n",
    "            y=torch.cat((y,x2),dim=0)\n",
    "            l=torch.cat((l,l2),dim=0)\n",
    "            \n",
    "            '''\n",
    "            \n",
    "    \n",
    "    \n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "                    \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "              \n",
    "            \n",
    "            \n",
    "                \n",
    "                \n",
    "                \n",
    "                        \n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "   \n",
    "    return y,l\n",
    "\n",
    "def val_epoch(test_loader,batch_size,optimizer): \n",
    "    batch_size_num=0\n",
    "    total_test_acc=0\n",
    "    model.eval()\n",
    "    for i , (inputs , targets) in enumerate(test_loader):   \n",
    "        print(\"val\")\n",
    "            \n",
    "            \n",
    "        inputs = Variable(inputs)   \n",
    "        targets = Variable(targets)     \n",
    "           \n",
    "        inputs=torch.tensor(inputs ,dtype=torch.float)   \n",
    "        targets=torch.tensor(targets ,dtype=torch.float)   \n",
    "        inputs , targets = inputs.cuda(),  targets.cuda()\n",
    "        targets=torch.max(targets,1)[1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #yhat = model1(inputs,targets)\n",
    "        yhat=model(inputs)\n",
    "            \n",
    "            \n",
    "            \n",
    "        #targets=torch.max(targets,1)[1]\n",
    "            \n",
    "            \n",
    "            \n",
    "        batch_test_acc=torchmetrics.functional.accuracy(yhat,targets)\n",
    "            \n",
    "        total_test_acc+=batch_test_acc\n",
    "            \n",
    "        batch_size_num=i\n",
    "    total_test_acc/=(batch_size_num+1)\n",
    "        ###print('total_test_accuracy:',total_test_acc/(batch_size+1))\n",
    "    print('total_test_accuracy:',total_test_acc)\n",
    "        \n",
    "                    \n",
    "                    \n",
    "            \n",
    "            \n",
    "    \n",
    "        \n",
    "   \n",
    "    \n",
    "    return total_test_acc\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotLoss(loss,epoch):\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    x=[i for i in range(epoch)]\n",
    "    #acc_train=acc_train.cpu()\n",
    "    #acc_test=acc_test.cpu()\n",
    "    plt.plot(x, loss, 'r-', mec='k', label='Logistic Loss', lw=2)\n",
    "    #plt.plot(x,acc_train,'b-',mec='k',label='accuracy Train',lw=2)\n",
    "    #plt.plot(x,acc_test,'g-',mec='k',label='accuracy Test',lw=2)\n",
    "    #plt.plot(x, y_01, 'g-', mec='k', label='0/1 Loss', lw=2)\n",
    "    #plt.plot(x, y_hinge, 'b-',mec='k', label='Hinge Loss', lw=2)\n",
    "    #plt.plot(x, boost, 'm--',mec='k', label='Adaboost Loss',lw=2)\n",
    "    plt.grid(True, ls='--')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('损失函数')\n",
    "    plt.show()\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee15295f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (bn0): BatchNorm1d(3300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=3300, out_features=2000, bias=True)\n",
      "  (bn1): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=2000, out_features=100, bias=True)\n",
      "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=100, out_features=9, bias=True)\n",
      "  (bn3): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "relevance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:428: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:430: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.shape: torch.Size([16, 9])\n",
      "labels_int: torch.Size([16])\n",
      "model_prediction: torch.Size([16, 9])\n",
      "rel_for_class_list: torch.Size([16])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6781cb35bd30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maugment_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnfm_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;31m#train_loss_total_list.append(train_loss_total)#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-86fbc5889c79>\u001b[0m in \u001b[0;36maugment_epoch\u001b[0;34m(kii, train_loader, batch_size)\u001b[0m\n\u001b[1;32m    514\u001b[0m             '''\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0minput_relevance_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_relevance_scorex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monly_max_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrel_for_class_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0morg_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpara_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m             \u001b[0minput_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_relevance_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NFM-pyorch-master/NFM-pyorch-master/innvestigator.py\u001b[0m in \u001b[0;36mcompute_relevance_scorex\u001b[0;34m(self, only_max_score, rel_for_class, org_shape, para)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;31m#print('prediction:',predictionx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morg_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                 \u001b[0monly_max_score\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_for_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictionx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_for_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpara\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#########################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0mrelevance_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monly_max_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morg_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#_,model=MLPA().cuda\n",
    "import torch\n",
    "\n",
    "#功能：加载保存到path中的各层参数到神经网络\n",
    "\n",
    "path='dataset/qiuguan/model_new_K_fold_RandomTree/MLP_non_encode/MLP9110.pkl'\n",
    "\n",
    "#nfm=NFM(nfm_config)\n",
    "mlp=MLP()\n",
    "#print(nfm)\n",
    "#net = nn.DataParallel(net)\n",
    "#net = net.to(device)\n",
    "mlp.load_state_dict(torch.load(path),strict=False)\n",
    "mlp.cuda()\n",
    "\n",
    "print(mlp)\n",
    "\n",
    "\n",
    "#print(model.state_dict())\n",
    "\n",
    "mlp_params = list(mlp.named_parameters())\n",
    "#print(mlp_params)\n",
    "model=mlp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inn_model = InnvestigateModel(model, lrp_exponent=2,\n",
    "                              method=\"e-rule\",\n",
    "                              beta=.5)\n",
    "\n",
    "\n",
    "import itertools\n",
    "\n",
    "K=10\n",
    "test_metrics=[]\n",
    "train_loss_total_list=[]\n",
    "\n",
    "testset= KZDatasetTest(csv_path='dataset/qiuguan/origin_800/xiaoqiu_xiaoguan/train_val_info.csv')\n",
    "   \n",
    "test_loader = data.DataLoader(\n",
    "         dataset=testset,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         drop_last=True,\n",
    "         batch_size=nfm_config['batch_size']\n",
    "        \n",
    "     )\n",
    "\n",
    "\n",
    "#model_path='dataset/qiuguan/origin_800/LRP/non_encode/200/attention0.01/'\n",
    "#BATCH_SIZE=batch_size\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "#total = 0\n",
    "    \n",
    "    \n",
    "#loss_func=torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "num=0\n",
    "   \n",
    "model_named_parameters=[j for j in model.named_parameters()]\n",
    "#print('model_parameters:',model_named_parameters)\n",
    "#print('model.state.dict:',model.state_dict())\n",
    "epoches=1\n",
    "for epoch_id in range(epoches):\n",
    "          \n",
    "        \n",
    "        \n",
    "        y,l=augment_epoch(0,test_loader,nfm_config['batch_size'])\n",
    "        #train_loss_total_list.append(train_loss_total)#\n",
    "        \n",
    "        '''  \n",
    "        if epoch_id %10==0:\n",
    "            num=num+1\n",
    "            path=os.path.join(model_path,'MLP'+str(num)+str(K)+'.pkl')\n",
    "            torch.save(model.state_dict(),path)\n",
    "            \n",
    "         '''  \n",
    "y=y.detach().cpu().numpy()\n",
    "y=pd.DataFrame(y)\n",
    "\n",
    "l=l.detach().cpu().numpy()\n",
    "l=pd.DataFrame(l)\n",
    "#print(y)\n",
    "#print(l)\n",
    "\n",
    "\n",
    "aug=pd.concat((y,l),axis=1)\n",
    "aug.to_csv('dataset/qiuguan/origin_800/xiaoqiu_xiaoguan/aug_et_some/para_0/aug_train_val_info_3301.csv')\n",
    "print(aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28657d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          RHOA      STX2      CISD1     WDR11     SCYL2   MGC2889     CCDC47  \\\n",
      "0    11.249338  6.118217   9.607851  7.381154  5.932984  4.568555   8.114415   \n",
      "1    10.898457  5.905008  10.043073  7.461420  6.575100  4.241700   8.489481   \n",
      "2    10.911331  6.069016  10.299607  7.182738  5.973380  4.809875   8.189254   \n",
      "3    10.993798  5.980454   9.853507  7.477239  6.933372  4.576204   8.669412   \n",
      "4    11.116181  6.223586  10.276357  7.721223  6.903274  4.518367   8.622838   \n",
      "..         ...       ...        ...       ...       ...       ...        ...   \n",
      "544  12.143129  8.564954   9.422492  8.952287  6.777686  5.370823  10.227443   \n",
      "545  12.726252  8.631955   9.292412  8.955966  7.429243  4.985910  10.532490   \n",
      "546  12.265202  8.325337   8.159097  8.508012  7.503678  5.299556   9.695292   \n",
      "547  12.524550  8.936505   9.154908  9.581060  6.652090  5.280697  10.231094   \n",
      "548  12.447256  9.023328   8.866550  9.026690  7.250243  5.410700  10.175947   \n",
      "\n",
      "         KLF8      CCL1   SLCO3A1  ...     PLAC4     NRBP1    LRRC23  \\\n",
      "0    5.157409  4.487112  5.820010  ...  5.712665  6.791393  5.808071   \n",
      "1    5.069326  4.479613  5.134880  ...  5.543574  6.929709  6.555523   \n",
      "2    4.585909  4.613632  4.791522  ...  5.539604  7.268553  6.802556   \n",
      "3    5.113865  4.746227  4.955312  ...  5.558108  7.189141  6.110985   \n",
      "4    5.049970  4.649294  4.941420  ...  5.424755  7.107507  6.351678   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "544  6.159603  5.333605  8.746401  ...  6.583877  8.862260  6.330797   \n",
      "545  6.284940  5.158891  8.242999  ...  6.583575  8.698243  5.957340   \n",
      "546  6.189908  5.617562  8.323383  ...  7.310800  8.698188  6.607848   \n",
      "547  5.984366  5.163376  7.675782  ...  6.630721  8.585757  6.092531   \n",
      "548  6.215468  5.346898  8.245499  ...  6.879922  8.878569  5.643788   \n",
      "\n",
      "        SPHK2  KIAA0513    SULT1A1       AMOT       CA1     GPR35  label  \n",
      "0    6.273794  6.089332   9.098081   9.535161  6.536145  5.161503      1  \n",
      "1    7.399356  5.808598   9.079245  10.257830  5.861706  5.222787      8  \n",
      "2    7.275560  5.743625   9.183698  10.225727  5.942001  5.328758      8  \n",
      "3    7.041728  5.643958   9.110606  10.427959  6.150741  5.243479      8  \n",
      "4    7.110633  5.816275   8.872020  10.179556  5.906155  4.811466      7  \n",
      "..        ...       ...        ...        ...       ...       ...    ...  \n",
      "544  7.773148  7.108987  10.593943  11.938976  6.926710  5.816790      1  \n",
      "545  7.751746  7.817693  11.096974  11.931336  6.692624  5.850543      7  \n",
      "546  7.872981  7.165746  11.475837  12.579405  7.116216  6.137149      1  \n",
      "547  7.831073  7.679095  10.628727  12.838763  6.821784  5.853294      4  \n",
      "548  7.767346  7.696756  10.827806  12.847613  7.241533  5.776671      7  \n",
      "\n",
      "[549 rows x 3301 columns]\n",
      "              0         1          2         3         4         5          6  \\\n",
      "0     11.245931  6.116363   9.604940  7.378918  5.931186  4.567172   8.111957   \n",
      "1     10.895157  5.903219  10.040030  7.459161  6.573109  4.240415   8.486910   \n",
      "2     10.908026  6.067177  10.296486  7.180562  5.971570  4.808418   8.186773   \n",
      "3     10.990468  5.978643   9.850522  7.474974  6.931272  4.574818   8.666785   \n",
      "4     11.112814  6.221701  10.273244  7.718884  6.901184  4.516998   8.620226   \n",
      "...         ...       ...        ...       ...       ...       ...        ...   \n",
      "1115  12.390919  8.915878   8.791835  9.095506  7.579767  5.234414  10.294180   \n",
      "1116  12.626436  8.723766   9.064002  8.902285  7.076329  5.073016  10.261233   \n",
      "1117  12.377678  8.822784   8.823498  9.287048  6.989891  5.286879  10.280223   \n",
      "1118  12.685410  8.780883   8.785497  9.093746  7.402497  5.242527  10.598632   \n",
      "1119  12.192116  8.775008   9.566997  8.382664  6.595394  5.211708  10.687550   \n",
      "\n",
      "             7         8         9  ...      3291      3292      3293  \\\n",
      "0     5.155847  4.485752  5.818247  ...  5.710934  6.789335  5.806304   \n",
      "1     5.067791  4.478257  5.133325  ...  5.541895  6.927610  6.553536   \n",
      "2     4.584520  4.612234  4.790070  ...  5.537926  7.266352  6.800485   \n",
      "3     5.112315  4.744790  4.953811  ...  5.556425  7.186963  6.109128   \n",
      "4     5.048440  4.647886  4.939923  ...  5.423111  7.105354  6.349754   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "1115  6.035849  5.423981  7.575852  ...  6.491421  8.657205  6.278894   \n",
      "1116  5.936676  5.103812  7.739202  ...  6.720247  8.747729  6.422568   \n",
      "1117  5.907369  5.328851  8.289339  ...  6.702363  8.859281  6.081217   \n",
      "1118  6.079669  5.644705  7.805773  ...  6.663282  8.507225  6.484153   \n",
      "1119  5.863449  5.309286  8.198456  ...  6.519610  9.013555  6.647990   \n",
      "\n",
      "          3294      3295       3296       3297      3298      3299  0.1  \n",
      "0     6.271893  6.087487   9.095325   9.532273  6.534165  5.159939    1  \n",
      "1     7.397115  5.806839   9.076495  10.254723  5.859931  5.221205    8  \n",
      "2     7.273356  5.741885   9.180917  10.222630  5.940200  5.327144    8  \n",
      "3     7.039595  5.642249   9.107846  10.424800  6.148879  5.241890    8  \n",
      "4     7.108479  5.814513   8.869332  10.176473  5.904365  4.810009    7  \n",
      "...        ...       ...        ...        ...       ...       ...  ...  \n",
      "1115  7.983522  7.520989  11.329852  12.624511  6.649924  5.515790    7  \n",
      "1116  7.880569  7.500274  11.168436  11.992241  6.964818  5.601811    8  \n",
      "1117  7.521110  7.791753  10.487171  11.909148  7.454988  5.945383    7  \n",
      "1118  7.766120  7.505548  10.385175  12.151476  6.852107  5.702806    8  \n",
      "1119  7.745537  7.492159  11.478963  11.366694  6.449205  5.392934    3  \n",
      "\n",
      "[1120 rows x 3301 columns]\n",
      "           RHOA      STX2      CISD1     WDR11     SCYL2   MGC2889     CCDC47  \\\n",
      "0     11.245931  6.116363   9.604940  7.378918  5.931186  4.567172   8.111957   \n",
      "1     10.895157  5.903219  10.040030  7.459161  6.573109  4.240415   8.486910   \n",
      "2     10.908026  6.067177  10.296486  7.180562  5.971570  4.808418   8.186773   \n",
      "3     10.990468  5.978643   9.850522  7.474974  6.931272  4.574818   8.666785   \n",
      "4     11.112814  6.221701  10.273244  7.718884  6.901184  4.516998   8.620226   \n",
      "...         ...       ...        ...       ...       ...       ...        ...   \n",
      "1115  12.390919  8.915878   8.791835  9.095506  7.579767  5.234414  10.294180   \n",
      "1116  12.626436  8.723766   9.064002  8.902285  7.076329  5.073016  10.261233   \n",
      "1117  12.377678  8.822784   8.823498  9.287048  6.989891  5.286879  10.280223   \n",
      "1118  12.685410  8.780883   8.785497  9.093746  7.402497  5.242527  10.598632   \n",
      "1119  12.192116  8.775008   9.566997  8.382664  6.595394  5.211708  10.687550   \n",
      "\n",
      "          KLF8      CCL1   SLCO3A1  ...     PLAC4     NRBP1    LRRC23  \\\n",
      "0     5.155847  4.485752  5.818247  ...  5.710934  6.789335  5.806304   \n",
      "1     5.067791  4.478257  5.133325  ...  5.541895  6.927610  6.553536   \n",
      "2     4.584520  4.612234  4.790070  ...  5.537926  7.266352  6.800485   \n",
      "3     5.112315  4.744790  4.953811  ...  5.556425  7.186963  6.109128   \n",
      "4     5.048440  4.647886  4.939923  ...  5.423111  7.105354  6.349754   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "1115  6.035849  5.423981  7.575852  ...  6.491421  8.657205  6.278894   \n",
      "1116  5.936676  5.103812  7.739202  ...  6.720247  8.747729  6.422568   \n",
      "1117  5.907369  5.328851  8.289339  ...  6.702363  8.859281  6.081217   \n",
      "1118  6.079669  5.644705  7.805773  ...  6.663282  8.507225  6.484153   \n",
      "1119  5.863449  5.309286  8.198456  ...  6.519610  9.013555  6.647990   \n",
      "\n",
      "         SPHK2  KIAA0513    SULT1A1       AMOT       CA1     GPR35  label  \n",
      "0     6.271893  6.087487   9.095325   9.532273  6.534165  5.159939      1  \n",
      "1     7.397115  5.806839   9.076495  10.254723  5.859931  5.221205      8  \n",
      "2     7.273356  5.741885   9.180917  10.222630  5.940200  5.327144      8  \n",
      "3     7.039595  5.642249   9.107846  10.424800  6.148879  5.241890      8  \n",
      "4     7.108479  5.814513   8.869332  10.176473  5.904365  4.810009      7  \n",
      "...        ...       ...        ...        ...       ...       ...    ...  \n",
      "1115  7.983522  7.520989  11.329852  12.624511  6.649924  5.515790      7  \n",
      "1116  7.880569  7.500274  11.168436  11.992241  6.964818  5.601811      8  \n",
      "1117  7.521110  7.791753  10.487171  11.909148  7.454988  5.945383      7  \n",
      "1118  7.766120  7.505548  10.385175  12.151476  6.852107  5.702806      8  \n",
      "1119  7.745537  7.492159  11.478963  11.366694  6.449205  5.392934      3  \n",
      "\n",
      "[1120 rows x 3301 columns]\n",
      "          RHOA      STX2      CISD1     WDR11     SCYL2   MGC2889     CCDC47  \\\n",
      "0    11.245931  6.116363   9.604940  7.378918  5.931186  4.567172   8.111957   \n",
      "1    10.895157  5.903219  10.040030  7.459161  6.573109  4.240415   8.486910   \n",
      "2    10.908026  6.067177  10.296486  7.180562  5.971570  4.808418   8.186773   \n",
      "3    10.990468  5.978643   9.850522  7.474974  6.931272  4.574818   8.666785   \n",
      "4    11.112814  6.221701  10.273244  7.718884  6.901184  4.516998   8.620226   \n",
      "..         ...       ...        ...       ...       ...       ...        ...   \n",
      "544  12.143129  8.564954   9.422492  8.952287  6.777686  5.370823  10.227443   \n",
      "545  12.726252  8.631955   9.292412  8.955966  7.429243  4.985910  10.532490   \n",
      "546  12.265202  8.325337   8.159097  8.508012  7.503678  5.299556   9.695292   \n",
      "547  12.524550  8.936505   9.154908  9.581060  6.652090  5.280697  10.231094   \n",
      "548  12.447256  9.023328   8.866550  9.026690  7.250243  5.410700  10.175947   \n",
      "\n",
      "         KLF8      CCL1   SLCO3A1  ...     PLAC4     NRBP1    LRRC23  \\\n",
      "0    5.155847  4.485752  5.818247  ...  5.710934  6.789335  5.806304   \n",
      "1    5.067791  4.478257  5.133325  ...  5.541895  6.927610  6.553536   \n",
      "2    4.584520  4.612234  4.790070  ...  5.537926  7.266352  6.800485   \n",
      "3    5.112315  4.744790  4.953811  ...  5.556425  7.186963  6.109128   \n",
      "4    5.048440  4.647886  4.939923  ...  5.423111  7.105354  6.349754   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "544  6.159603  5.333605  8.746401  ...  6.583877  8.862260  6.330797   \n",
      "545  6.284940  5.158891  8.242999  ...  6.583575  8.698243  5.957340   \n",
      "546  6.189908  5.617562  8.323383  ...  7.310800  8.698188  6.607848   \n",
      "547  5.984366  5.163376  7.675782  ...  6.630721  8.585757  6.092531   \n",
      "548  6.215468  5.346898  8.245499  ...  6.879922  8.878569  5.643788   \n",
      "\n",
      "        SPHK2  KIAA0513    SULT1A1       AMOT       CA1     GPR35  label  \n",
      "0    6.271893  6.087487   9.095325   9.532273  6.534165  5.159939      1  \n",
      "1    7.397115  5.806839   9.076495  10.254723  5.859931  5.221205      8  \n",
      "2    7.273356  5.741885   9.180917  10.222630  5.940200  5.327144      8  \n",
      "3    7.039595  5.642249   9.107846  10.424800  6.148879  5.241890      8  \n",
      "4    7.108479  5.814513   8.869332  10.176473  5.904365  4.810009      7  \n",
      "..        ...       ...        ...        ...       ...       ...    ...  \n",
      "544  7.773148  7.108987  10.593943  11.938976  6.926710  5.816790      1  \n",
      "545  7.751746  7.817693  11.096974  11.931336  6.692624  5.850543      7  \n",
      "546  7.872981  7.165746  11.475837  12.579405  7.116216  6.137149      1  \n",
      "547  7.831073  7.679095  10.628727  12.838763  6.821784  5.853294      4  \n",
      "548  7.767346  7.696756  10.827806  12.847613  7.241533  5.776671      7  \n",
      "\n",
      "[1669 rows x 3301 columns]\n"
     ]
    }
   ],
   "source": [
    "x=pd.read_csv('dataset/qiuguan/origin_800/xiaoqiu_xiaoguan/train_val_info.csv',sep=',')\n",
    "\n",
    "\n",
    "aug=pd.read_csv('dataset/qiuguan/origin_800/xiaoqiu_xiaoguan/aug_et/para_gause_0.8_0.4_gause_0.3_0.1/aug_train_val_info_3301.csv',sep=',')\n",
    "#print(x)\n",
    "\n",
    "x=x.iloc[:,1:]\n",
    "\n",
    "print(x)\n",
    "\n",
    "aug=aug.iloc[:,1:]\n",
    "print(aug)\n",
    "\n",
    "aug.columns=x.columns \n",
    "\n",
    "print(aug)\n",
    "\n",
    "\n",
    "all=pd.concat((aug,x),axis=0)\n",
    "print(all)\n",
    "\n",
    "all.to_csv('dataset/qiuguan/origin_800/xiaoqiu_xiaoguan/aug_et/para_gause_0.8_0.4_gause_0.3_0.1/all_aug_train_val_info_3301.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d041d78e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
