{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e83c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load main.py\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--lr\", \n",
    "    type=float, \n",
    "    default=0.05, \n",
    "    help=\"learning rate\")\n",
    "parser.add_argument(\"--dropout\", \n",
    "    default='[0.5, 0.2]',  \n",
    "    help=\"dropout rate for FM and MLP\")\n",
    "parser.add_argument(\"--batch_size\", \n",
    "    type=int, \n",
    "    default=128, \n",
    "    help=\"batch size for training\")\n",
    "parser.add_argument(\"--epochs\", \n",
    "    type=int,\n",
    "    default=100, \n",
    "    help=\"training epochs\")\n",
    "parser.add_argument(\"--hidden_factor\", \n",
    "    type=int,\n",
    "    default=64, #被修改，原始值为64\n",
    "    help=\"predictive factors numbers in the model\")\n",
    "parser.add_argument(\"--layers\", \n",
    "    default='[5000,1000]',#被修改，有两个隐层，之前值为64，一个隐层 \n",
    "    help=\"size of layers in MLP model, '[]' is NFM-0\")\n",
    "\n",
    "parser.add_argument(\"--n_class\",\n",
    "    type=int,\n",
    "    default=16,\n",
    "    help=\"number of class\")\n",
    "\n",
    "\n",
    "parser.add_argument(\"--lamda\", \n",
    "    type=float, \n",
    "    default=0.0, \n",
    "    help=\"regularizer for bilinear layers\")\n",
    "parser.add_argument(\"--batch_norm\", \n",
    "    default=True, \n",
    "    help=\"use batch_norm or not\")\n",
    "parser.add_argument(\"--pre_train\", \n",
    "    action='store_true', \n",
    "    default=False, \n",
    "    help=\"whether use the pre-train or not\")\n",
    "parser.add_argument(\"--out\", \n",
    "    default=True, \n",
    "    help=\"save model or not\")\n",
    "\n",
    "parser.add_argument(\"--gpu\", \n",
    "    type=str,\n",
    "    default=\"0\",  \n",
    "    help=\"gpu card ID\")\n",
    "args = parser.parse_args(args=[])#括号中增加args[]\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "cudnn.benchmark = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3067b05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "import config\n",
    "\n",
    "\n",
    "def read_features(file, features):\n",
    "    \"\"\" Read features from the given file. \"\"\"\n",
    "    #file=\"data/frappe/c_df_v_fff2000.csv\"\n",
    "    num = len(features)\n",
    "    fd=pd.read_csv(file,sep=',')\n",
    "    nrow=fd.shape[0]\n",
    "    ncol=fd.shape[1]\n",
    "    n_fd=np.array(fd)\n",
    "    #print(n_fd)\n",
    "    n_fd=n_fd[:,2:]\n",
    "    print('n_fd.shape:')\n",
    "    print(n_fd.shape)\n",
    "    #print(n_fd)\n",
    "    #features={}\n",
    "\n",
    "    for i, row_list in enumerate(n_fd):\n",
    "        for j, col_value in enumerate(row_list):\n",
    "            #print(col_value)\n",
    "            if col_value not in features:\n",
    "                features[col_value]=num\n",
    "                num=num+1\n",
    "\n",
    "    #print(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4fd02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_features():\n",
    "    \"\"\" Get the number of existing features in all the three files. \"\"\"\n",
    "    features = {}\n",
    "    #features = read_features(csv_data, features)\n",
    "    \n",
    "    features = read_features(config.train_libfm, features)\n",
    "    features = read_features(config.valid_libfm, features)\n",
    "    features = read_features(config.test_libfm, features)\n",
    "    \n",
    "    \n",
    "    print(\"number of features: {}\".format(len(features)))\n",
    "    return features, len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb049c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_features():\n",
    "    \"\"\" Get the number of existing features in all the three files. \"\"\"\n",
    "    features = {}\n",
    "    #features = read_features(csv_data, features)\n",
    "    \n",
    "    features = read_features(config.train_libfm, features)\n",
    "    features = read_features(config.valid_libfm, features)\n",
    "    features = read_features(config.test_libfm, features)\n",
    "    \n",
    "    \n",
    "    print(\"number of features: {}\".format(len(features)))\n",
    "    return features, len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8886e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, classes, label_smoothing=0.2):\n",
    "    n = len(labels)\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        print(\"row:\",row,\"label:\",label)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dddd4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMData(data.Dataset):\n",
    "    \"\"\" Construct the FM pytorch dataset. \"\"\"\n",
    "    def __init__(self, file,label_file, feature_map,n_class=16):\n",
    "        super(FMData, self).__init__()\n",
    "        self.label = []\n",
    "        self.features = []\n",
    "        self.feature_values = []\n",
    "        \n",
    "        features=[]\n",
    "        #feature_map.keys()\n",
    "        #self.features=np.array(feature_map)\n",
    "        #feature_map\n",
    "        #file=\"data/frappe/c_df_v_fff2000.csv\"\n",
    "        #num = len(features)\n",
    "        fd=pd.read_csv(file,sep=',')\n",
    "        #nrow=fd.shape[0]\n",
    "        #ncol=fd.shape[1]\n",
    "        n_fd=np.array(fd)\n",
    "        #print(n_fd)\n",
    "        n_fd=n_fd[:,2:]\n",
    "        for i, item in enumerate(n_fd):\n",
    "            u=[feature_map[x] for x in item]\n",
    "            features.append(u)\n",
    "        \n",
    "        \n",
    "        self.features=np.array(features)\n",
    "        #self.features=features.tolist()\n",
    "        \n",
    "        \n",
    "        nrow,ncol=n_fd.shape\n",
    "        #ncol=10150\n",
    "        feature_v=[]\n",
    "        \"\"\"\n",
    "            feature_v=[1 for i in range(ncol)]\n",
    "            #print(feature_v)\n",
    "            #feature_values=[feature_v for j in range(nrow)]\n",
    "\n",
    "            for item in range(nrow):\n",
    "            feature_values.append(feature_v)\n",
    "        \"\"\"\n",
    "\n",
    "        feature_v=[[1 for i in range(ncol)] for i in range(nrow)]\n",
    "        self.feature_values=np.array(feature_v)\n",
    "        #print(feature_value)\n",
    "        #self.feature_values=feature_value.tolist()\n",
    "        #feature_map,lenth=map_features()\n",
    "        #raw = [item for item in  enumerate(n_fd)]\n",
    "        #print(raw)\n",
    "        #raw=raw.tolist()\n",
    "        \n",
    "        #label_file=[]\n",
    "        label_fd=pd.read_csv(label_file,sep=',')\n",
    "        #print(features)\n",
    "        #print(label_fd)\n",
    "        label=np.array(label_fd)\n",
    "        label=label[:,1:]\n",
    "        label=one_hot(label,n_class)\n",
    "        self.label=label\n",
    "        print(\"label:\",label)\n",
    "        #print(label)\n",
    "        # convert labels\n",
    "        \"\"\"if config.loss_type == 'square_loss':\n",
    "            self.label.append(np.float32(items[0]))\n",
    "        else: # log_loss\n",
    "            label = 1 if float(items[0]) > 0 else 0\n",
    "            self.label.append(label)\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        assert all(len(item) == len(self.features[0]\n",
    "            ) for item in self.features), 'features are of different length'\n",
    "        \"\"\"\n",
    "        print(len(self.features))\n",
    "        print(len(self.feature_values))\n",
    "        print(len(self.label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.label[idx]\n",
    "        features = self.features[idx]\n",
    "        feature_values = self.feature_values[idx]\n",
    "        return features, feature_values, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bc3dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################  PREPARE DATASET #########################\n",
    "\"\"\"\n",
    "features_map, num_features = data_utils.map_features()\n",
    "\n",
    "train_dataset = data_utils.FMData(config.train_libfm, features_map)\n",
    "valid_dataset = data_utils.FMData(config.valid_libfm, features_map)\n",
    "test_dataset = data_utils.FMData(config.test_libfm, features_map)\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, drop_last=True,\n",
    "            batch_size=args.batch_size, shuffle=True, num_workers=4)\n",
    "valid_loader = data.DataLoader(valid_dataset,\n",
    "            batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = data.DataLoader(test_dataset,\n",
    "            batch_size=args.batch_size, shuffle=False, num_workers=0)\n",
    "\"\"\"\n",
    "\n",
    "features_map,num_features=map_features()\n",
    "print('num_features:',num_features)\n",
    "#n_class=16\n",
    "train_dataset = FMData(config.train_libfm,config.train_label,features_map)\n",
    "train_loader = data.DataLoader(train_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "validate_dataset = FMData(config.valid_libfm,config.valid_label,features_map)\n",
    "validate_loader = data.DataLoader(train_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "test_dataset = FMData(config.test_libfm,config.test_label,features_map)\n",
    "test_loader = data.DataLoader(test_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b556c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################  CREATE MODEL ###########################\n",
    "if args.pre_train:\n",
    "    assert os.path.exists(config.FM_model_path), 'lack of FM model'\n",
    "    assert config.model == 'NFM', 'only support NFM for now'\n",
    "    FM_model = torch.load(config.FM_model_path)\n",
    "else:\n",
    "    FM_model = None\n",
    "\n",
    "if config.model == 'FM':\n",
    "    model = model.FM(num_features, args.hidden_factor,\n",
    "                    args.batch_norm, eval(args.dropout))\n",
    "else:\n",
    "    \"\"\"\n",
    "    model = model.NFM(\n",
    "        num_features, args.hidden_factor, \n",
    "        config.activation_function, eval(args.layers), \n",
    "        args.batch_norm, eval(args.dropout), args.n_class,FM_model)\n",
    "    \"\"\"\n",
    "    model=network.NFM(config,[],train_dataset,args.n_class)#\n",
    "model.cuda()\n",
    "if config.optimizer == 'Adagrad':\n",
    "    optimizer = optim.Adagrad(\n",
    "        model.parameters(), lr=args.lr, initial_accumulator_value=1e-8)\n",
    "elif config.optimizer == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "elif config.optimizer == 'SGD':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "elif config.optimizer == 'Momentum':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.95)\n",
    "\n",
    "if config.loss_type == 'square_loss':\n",
    "    criterion = nn.MSELoss(reduction='sum')\n",
    "elif config.loss_type=='cross_entropy_loss':#被修改，增加了交叉熵损失\n",
    "    criterion=nn.CrossEntropyLoss(reduction='mean')\n",
    "else: # log_loss\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "# writer = SummaryWriter() # for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cdc566",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################  TRAINING ############################\n",
    "count, best_rmse = 0, 100\n",
    "print(model)\n",
    "for epoch in range(args.epochs):\n",
    "    model.train() # Enable dropout and batch_norm\n",
    "    start_time = time.time()\n",
    "\n",
    "    for features, feature_values, label in train_loader:\n",
    "        features = features.cuda()\n",
    "        feature_values = feature_values.cuda()\n",
    "        label = label.cuda()\n",
    "\n",
    "        model.zero_grad()\n",
    "        prediction = model(features, feature_values)\n",
    "        loss = criterion(prediction, label) \n",
    "        loss += args.lamda * model.embeddings.weight.norm()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # writer.add_scalar('data/loss', loss.item(), count)\n",
    "        count += 1\n",
    "\n",
    "    model.eval()\n",
    "    train_result = evaluate.metrics(model, train_loader)\n",
    "    valid_result = evaluate.metrics(model, valid_loader)\n",
    "    test_result = evaluate.metrics(model, test_loader)\n",
    "\n",
    "    print(\"Runing Epoch {:03d} \".format(epoch) + \"costs \" + time.strftime(\n",
    "                        \"%H: %M: %S\", time.gmtime(time.time()-start_time)))\n",
    "    print(\"Train_RMSE: {:.3f}, Valid_RMSE: {:.3f}, Test_RMSE: {:.3f}\".format(\n",
    "                        train_result, valid_result, test_result))\n",
    "\n",
    "    if test_result < best_rmse:\n",
    "        best_rmse, best_epoch = test_result, epoch\n",
    "        if args.out:\n",
    "            if not os.path.exists(config.model_path):\n",
    "                os.mkdir(config.model_path)\n",
    "            torch.save(model, \n",
    "                '{}{}.pth'.format(config.model_path, config.model))\n",
    "\n",
    "print(\"End. Best epoch {:03d}: Test_RMSE is {:.3f}\".format(best_epoch, best_rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
