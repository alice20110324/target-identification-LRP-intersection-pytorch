{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56782c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import Trainer\n",
    "#from network import NFM\n",
    "import torch.utils.data as Data\n",
    "from Utils.criteo_loader import getTestData, getTrainData\n",
    "\n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':8,\n",
    "    'linear_hidden':100,#线性模型输出层（隐层个数）\n",
    "    'embed_input_dim':1001,#embed输入维度\n",
    "    'embed_dim': 100, # 用于控制稀疏特征经过Embedding层后的稠密特征大小，embed输出维度\n",
    "    'dnn_hidden_units': [50,8],#MLP隐层和输出层\n",
    "    'num_sparse_features_cols':538,#the number of the gene columns\n",
    "    'num_dense_features': 0,#dense features number\n",
    "    'bi_dropout': 0.5,#Bi-Interaction 的dropout\n",
    "    'num_epoch': 500,#训练epoch次数\n",
    "    'batch_size': 100,#batch_size\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    'train_label': 'data/xiaoqiu/train/gene_500_train_labels.csv',\n",
    "    'train_data':'data/xiaoqiu/train/final_500_encode_100.csv'\n",
    "    #'fea_file': '../Data/criteo/processed_data/fea_col.npy',\n",
    "    #'validate_file': '../Data/criteo/processed_data/val_set.csv',\n",
    "    #'test_file': '../Data/criteo/processed_data/test_set.csv',\n",
    "    #'model_name': '../TrainedModels/NFM.model'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a85bb269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "#from utils import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07442345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "205edc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row: 0 label: [0]\n",
      "row: 1 label: [0]\n",
      "row: 2 label: [0]\n",
      "row: 3 label: [0]\n",
      "row: 4 label: [0]\n",
      "row: 5 label: [0]\n",
      "row: 6 label: [0]\n",
      "row: 7 label: [0]\n",
      "row: 8 label: [0]\n",
      "row: 9 label: [0]\n",
      "row: 10 label: [0]\n",
      "row: 11 label: [0]\n",
      "row: 12 label: [0]\n",
      "row: 13 label: [0]\n",
      "row: 14 label: [0]\n",
      "row: 15 label: [0]\n",
      "row: 16 label: [0]\n",
      "row: 17 label: [0]\n",
      "row: 18 label: [0]\n",
      "row: 19 label: [0]\n",
      "row: 20 label: [0]\n",
      "row: 21 label: [0]\n",
      "row: 22 label: [0]\n",
      "row: 23 label: [0]\n",
      "row: 24 label: [0]\n",
      "row: 25 label: [0]\n",
      "row: 26 label: [0]\n",
      "row: 27 label: [0]\n",
      "row: 28 label: [0]\n",
      "row: 29 label: [0]\n",
      "row: 30 label: [0]\n",
      "row: 31 label: [0]\n",
      "row: 32 label: [0]\n",
      "row: 33 label: [0]\n",
      "row: 34 label: [0]\n",
      "row: 35 label: [0]\n",
      "row: 36 label: [0]\n",
      "row: 37 label: [0]\n",
      "row: 38 label: [0]\n",
      "row: 39 label: [0]\n",
      "row: 40 label: [0]\n",
      "row: 41 label: [0]\n",
      "row: 42 label: [0]\n",
      "row: 43 label: [0]\n",
      "row: 44 label: [0]\n",
      "row: 45 label: [0]\n",
      "row: 46 label: [1]\n",
      "row: 47 label: [1]\n",
      "row: 48 label: [1]\n",
      "row: 49 label: [1]\n",
      "row: 50 label: [1]\n",
      "row: 51 label: [1]\n",
      "row: 52 label: [1]\n",
      "row: 53 label: [1]\n",
      "row: 54 label: [1]\n",
      "row: 55 label: [1]\n",
      "row: 56 label: [1]\n",
      "row: 57 label: [1]\n",
      "row: 58 label: [1]\n",
      "row: 59 label: [1]\n",
      "row: 60 label: [1]\n",
      "row: 61 label: [1]\n",
      "row: 62 label: [1]\n",
      "row: 63 label: [1]\n",
      "row: 64 label: [1]\n",
      "row: 65 label: [1]\n",
      "row: 66 label: [1]\n",
      "row: 67 label: [1]\n",
      "row: 68 label: [1]\n",
      "row: 69 label: [1]\n",
      "row: 70 label: [1]\n",
      "row: 71 label: [1]\n",
      "row: 72 label: [1]\n",
      "row: 73 label: [1]\n",
      "row: 74 label: [1]\n",
      "row: 75 label: [1]\n",
      "row: 76 label: [1]\n",
      "row: 77 label: [1]\n",
      "row: 78 label: [1]\n",
      "row: 79 label: [1]\n",
      "row: 80 label: [1]\n",
      "row: 81 label: [1]\n",
      "row: 82 label: [1]\n",
      "row: 83 label: [1]\n",
      "row: 84 label: [1]\n",
      "row: 85 label: [1]\n",
      "row: 86 label: [2]\n",
      "row: 87 label: [2]\n",
      "row: 88 label: [2]\n",
      "row: 89 label: [2]\n",
      "row: 90 label: [2]\n",
      "row: 91 label: [2]\n",
      "row: 92 label: [2]\n",
      "row: 93 label: [2]\n",
      "row: 94 label: [2]\n",
      "row: 95 label: [2]\n",
      "row: 96 label: [2]\n",
      "row: 97 label: [2]\n",
      "row: 98 label: [2]\n",
      "row: 99 label: [2]\n",
      "row: 100 label: [2]\n",
      "row: 101 label: [2]\n",
      "row: 102 label: [2]\n",
      "row: 103 label: [2]\n",
      "row: 104 label: [2]\n",
      "row: 105 label: [2]\n",
      "row: 106 label: [2]\n",
      "row: 107 label: [2]\n",
      "row: 108 label: [2]\n",
      "row: 109 label: [2]\n",
      "row: 110 label: [2]\n",
      "row: 111 label: [2]\n",
      "row: 112 label: [2]\n",
      "row: 113 label: [2]\n",
      "row: 114 label: [2]\n",
      "row: 115 label: [2]\n",
      "row: 116 label: [2]\n",
      "row: 117 label: [2]\n",
      "row: 118 label: [2]\n",
      "row: 119 label: [3]\n",
      "row: 120 label: [3]\n",
      "row: 121 label: [3]\n",
      "row: 122 label: [3]\n",
      "row: 123 label: [3]\n",
      "row: 124 label: [3]\n",
      "row: 125 label: [3]\n",
      "row: 126 label: [3]\n",
      "row: 127 label: [3]\n",
      "row: 128 label: [3]\n",
      "row: 129 label: [3]\n",
      "row: 130 label: [3]\n",
      "row: 131 label: [3]\n",
      "row: 132 label: [3]\n",
      "row: 133 label: [3]\n",
      "row: 134 label: [3]\n",
      "row: 135 label: [3]\n",
      "row: 136 label: [3]\n",
      "row: 137 label: [3]\n",
      "row: 138 label: [3]\n",
      "row: 139 label: [3]\n",
      "row: 140 label: [3]\n",
      "row: 141 label: [3]\n",
      "row: 142 label: [3]\n",
      "row: 143 label: [3]\n",
      "row: 144 label: [3]\n",
      "row: 145 label: [3]\n",
      "row: 146 label: [3]\n",
      "row: 147 label: [3]\n",
      "row: 148 label: [3]\n",
      "row: 149 label: [3]\n",
      "row: 150 label: [3]\n",
      "row: 151 label: [3]\n",
      "row: 152 label: [3]\n",
      "row: 153 label: [3]\n",
      "row: 154 label: [3]\n",
      "row: 155 label: [3]\n",
      "row: 156 label: [3]\n",
      "row: 157 label: [3]\n",
      "row: 158 label: [3]\n",
      "row: 159 label: [3]\n",
      "row: 160 label: [3]\n",
      "row: 161 label: [3]\n",
      "row: 162 label: [3]\n",
      "row: 163 label: [3]\n",
      "row: 164 label: [3]\n",
      "row: 165 label: [3]\n",
      "row: 166 label: [3]\n",
      "row: 167 label: [3]\n",
      "row: 168 label: [3]\n",
      "row: 169 label: [3]\n",
      "row: 170 label: [3]\n",
      "row: 171 label: [3]\n",
      "row: 172 label: [4]\n",
      "row: 173 label: [4]\n",
      "row: 174 label: [4]\n",
      "row: 175 label: [4]\n",
      "row: 176 label: [4]\n",
      "row: 177 label: [4]\n",
      "row: 178 label: [4]\n",
      "row: 179 label: [4]\n",
      "row: 180 label: [4]\n",
      "row: 181 label: [4]\n",
      "row: 182 label: [4]\n",
      "row: 183 label: [4]\n",
      "row: 184 label: [4]\n",
      "row: 185 label: [4]\n",
      "row: 186 label: [4]\n",
      "row: 187 label: [4]\n",
      "row: 188 label: [4]\n",
      "row: 189 label: [4]\n",
      "row: 190 label: [4]\n",
      "row: 191 label: [4]\n",
      "row: 192 label: [4]\n",
      "row: 193 label: [4]\n",
      "row: 194 label: [4]\n",
      "row: 195 label: [4]\n",
      "row: 196 label: [4]\n",
      "row: 197 label: [4]\n",
      "row: 198 label: [4]\n",
      "row: 199 label: [5]\n",
      "row: 200 label: [5]\n",
      "row: 201 label: [5]\n",
      "row: 202 label: [5]\n",
      "row: 203 label: [5]\n",
      "row: 204 label: [5]\n",
      "row: 205 label: [5]\n",
      "row: 206 label: [5]\n",
      "row: 207 label: [5]\n",
      "row: 208 label: [5]\n",
      "row: 209 label: [5]\n",
      "row: 210 label: [5]\n",
      "row: 211 label: [5]\n",
      "row: 212 label: [5]\n",
      "row: 213 label: [5]\n",
      "row: 214 label: [5]\n",
      "row: 215 label: [5]\n",
      "row: 216 label: [5]\n",
      "row: 217 label: [5]\n",
      "row: 218 label: [5]\n",
      "row: 219 label: [5]\n",
      "row: 220 label: [5]\n",
      "row: 221 label: [5]\n",
      "row: 222 label: [5]\n",
      "row: 223 label: [5]\n",
      "row: 224 label: [5]\n",
      "row: 225 label: [5]\n",
      "row: 226 label: [5]\n",
      "row: 227 label: [5]\n",
      "row: 228 label: [5]\n",
      "row: 229 label: [5]\n",
      "row: 230 label: [5]\n",
      "row: 231 label: [5]\n",
      "row: 232 label: [5]\n",
      "row: 233 label: [5]\n",
      "row: 234 label: [5]\n",
      "row: 235 label: [5]\n",
      "row: 236 label: [5]\n",
      "row: 237 label: [5]\n",
      "row: 238 label: [5]\n",
      "row: 239 label: [5]\n",
      "row: 240 label: [5]\n",
      "row: 241 label: [6]\n",
      "row: 242 label: [6]\n",
      "row: 243 label: [6]\n",
      "row: 244 label: [6]\n",
      "row: 245 label: [6]\n",
      "row: 246 label: [6]\n",
      "row: 247 label: [6]\n",
      "row: 248 label: [6]\n",
      "row: 249 label: [6]\n",
      "row: 250 label: [6]\n",
      "row: 251 label: [6]\n",
      "row: 252 label: [6]\n",
      "row: 253 label: [6]\n",
      "row: 254 label: [6]\n",
      "row: 255 label: [6]\n",
      "row: 256 label: [6]\n",
      "row: 257 label: [6]\n",
      "row: 258 label: [6]\n",
      "row: 259 label: [6]\n",
      "row: 260 label: [6]\n",
      "row: 261 label: [6]\n",
      "row: 262 label: [6]\n",
      "row: 263 label: [6]\n",
      "row: 264 label: [6]\n",
      "row: 265 label: [6]\n",
      "row: 266 label: [6]\n",
      "row: 267 label: [6]\n",
      "row: 268 label: [6]\n",
      "row: 269 label: [6]\n",
      "row: 270 label: [6]\n",
      "row: 271 label: [6]\n",
      "row: 272 label: [6]\n",
      "row: 273 label: [6]\n",
      "row: 274 label: [6]\n",
      "row: 275 label: [6]\n",
      "row: 276 label: [6]\n",
      "row: 277 label: [6]\n",
      "row: 278 label: [6]\n",
      "row: 279 label: [6]\n",
      "row: 280 label: [6]\n",
      "row: 281 label: [6]\n",
      "row: 282 label: [6]\n",
      "row: 283 label: [6]\n",
      "row: 284 label: [6]\n",
      "row: 285 label: [6]\n",
      "row: 286 label: [7]\n",
      "row: 287 label: [7]\n",
      "row: 288 label: [7]\n",
      "row: 289 label: [7]\n",
      "row: 290 label: [7]\n",
      "row: 291 label: [7]\n",
      "row: 292 label: [7]\n",
      "row: 293 label: [7]\n",
      "row: 294 label: [7]\n",
      "row: 295 label: [7]\n",
      "row: 296 label: [7]\n",
      "row: 297 label: [7]\n",
      "row: 298 label: [7]\n",
      "row: 299 label: [7]\n",
      "row: 300 label: [7]\n",
      "row: 301 label: [7]\n",
      "row: 302 label: [7]\n",
      "row: 303 label: [7]\n",
      "row: 304 label: [7]\n",
      "row: 305 label: [7]\n",
      "row: 306 label: [7]\n",
      "row: 307 label: [7]\n",
      "row: 308 label: [7]\n",
      "row: 309 label: [7]\n",
      "row: 310 label: [7]\n",
      "row: 311 label: [7]\n",
      "row: 312 label: [7]\n",
      "row: 313 label: [7]\n",
      "row: 314 label: [7]\n",
      "row: 315 label: [7]\n",
      "row: 316 label: [7]\n",
      "row: 317 label: [7]\n",
      "row: 318 label: [7]\n",
      "row: 319 label: [7]\n",
      "row: 320 label: [7]\n",
      "row: 321 label: [7]\n",
      "row: 322 label: [7]\n",
      "row: 323 label: [7]\n",
      "row: 324 label: [7]\n",
      "row: 325 label: [7]\n",
      "row: 326 label: [7]\n",
      "row: 327 label: [7]\n",
      "row: 328 label: [7]\n",
      "row: 329 label: [7]\n",
      "row: 330 label: [7]\n",
      "row: 331 label: [7]\n",
      "row: 332 label: [7]\n",
      "row: 333 label: [7]\n",
      "row: 334 label: [7]\n",
      "row: 335 label: [7]\n",
      "row: 336 label: [7]\n",
      "row: 337 label: [7]\n",
      "row: 338 label: [7]\n",
      "row: 339 label: [7]\n",
      "row: 340 label: [7]\n",
      "row: 341 label: [7]\n",
      "row: 342 label: [7]\n",
      "row: 343 label: [7]\n",
      "row: 344 label: [7]\n",
      "row: 345 label: [7]\n",
      "row: 346 label: [7]\n",
      "row: 347 label: [7]\n",
      "label: [[0.825 0.025 0.025 ... 0.025 0.025 0.025]\n",
      " [0.825 0.025 0.025 ... 0.025 0.025 0.025]\n",
      " [0.825 0.025 0.025 ... 0.025 0.025 0.025]\n",
      " ...\n",
      " [0.025 0.025 0.025 ... 0.025 0.025 0.825]\n",
      " [0.025 0.025 0.025 ... 0.025 0.025 0.825]\n",
      " [0.025 0.025 0.025 ... 0.025 0.025 0.825]]\n",
      "features: [[80 22 22 ... 16 38 44]\n",
      " [75 21 20 ... 16 39 40]\n",
      " [80 22 19 ... 16 42 43]\n",
      " ...\n",
      " [91 27 18 ... 18 41 41]\n",
      " [87 23 19 ... 16 41 43]\n",
      " [87 20 18 ... 18 41 47]]\n"
     ]
    }
   ],
   "source": [
    "from dataset_process import FMData\n",
    "\n",
    "train_dataset = FMData(nfm_config['train_data'],nfm_config['train_label'],nfm_config['n_class'])\n",
    "train_loader = data.DataLoader(train_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "#validate_dataset = FMData(config.valid_libfm,config.valid_label)\n",
    "#validate_loader = data.DataLoader(train_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "#test_dataset = FMData(config.test_libfm,config.test_label)\n",
    "#test_loader = data.DataLoader(test_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdb60202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nfm: NFM(\n",
      "  (drop): Dropout(p=0.7, inplace=False)\n",
      "  (linear_model): Linear(in_features=538, out_features=100, bias=True)\n",
      "  (BN_linear): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (embedding_layers): Embedding(1001, 100)\n",
      "  (bi_pooling): BiInteractionPooling()\n",
      "  (dropout): Dropout(p=0.8, inplace=False)\n",
      "  (BN_bi): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dnn_layers): ModuleList(\n",
      "    (0): Linear(in_features=200, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=8, bias=True)\n",
      "  )\n",
      "  (dnn_softmax): Softmax(dim=1)\n",
      ")\n",
      "linear_output: tensor([[  8.4310, -41.9103,  59.1874,  ..., -17.0939,   3.6718,   5.7403],\n",
      "        [  7.2743, -44.6808,  57.0746,  ..., -16.8052,   0.9713,   7.0045],\n",
      "        [ 12.3580, -44.6484,  58.1197,  ..., -17.2183,   0.2298,   6.3521],\n",
      "        ...,\n",
      "        [  9.5725, -45.3784,  60.9817,  ..., -15.3034,   2.5707,   5.7345],\n",
      "        [ 10.6882, -46.6872,  57.6645,  ..., -14.8445,   3.8977,   4.6217],\n",
      "        [ 12.6595, -44.9266,  62.4390,  ..., -17.8202,  -0.1650,   8.1198]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_correct: 11.0\n",
      "batch_idx: 0\n",
      "0.11\n",
      "linear_output: tensor([[ -7.7358, -44.8852,  63.5563,  ..., -22.7466,  19.4784, -16.8485],\n",
      "        [ -8.7641, -42.7902,  68.0102,  ..., -28.3100,  22.0382, -13.7183],\n",
      "        [ -4.3129, -44.9389,  63.8524,  ..., -29.9928,  25.5944, -15.5684],\n",
      "        ...,\n",
      "        [-10.3793, -42.4226,  65.7335,  ..., -28.5700,  20.4510, -13.3965],\n",
      "        [ -3.8775, -42.7998,  63.8204,  ..., -23.0881,  26.3206,  -9.4421],\n",
      "        [-13.7255, -41.5972,  66.6449,  ..., -28.4955,  21.8080, -12.6050]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 27.0\n",
      "batch_idx: 1\n",
      "0.135\n",
      "linear_output: tensor([[-30.0333, -51.0423,  66.6781,  ..., -27.7956,  36.6250, -27.4532],\n",
      "        [-22.9220, -54.0779,  66.5648,  ..., -25.1965,  34.2372, -27.0708],\n",
      "        [-26.1933, -48.5267,  67.1640,  ..., -21.9358,  34.6486, -27.7991],\n",
      "        ...,\n",
      "        [-27.1044, -46.3408,  64.7177,  ..., -26.9050,  35.0925, -26.9587],\n",
      "        [-23.7058, -45.1476,  66.8583,  ..., -24.3705,  33.6725, -26.2056],\n",
      "        [-30.1014, -54.7347,  68.5645,  ..., -28.6285,  37.4803, -26.6194]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 41.0\n",
      "batch_idx: 2\n",
      "0.13666666666666666\n",
      "Training Epoch: 0, total loss: 6.237349\n",
      "linear_output: tensor([[-36.3567, -61.2807,  64.7287,  ..., -18.3002,  49.6834, -40.5915],\n",
      "        [-46.5986, -61.5187,  68.9897,  ..., -22.4167,  55.5982, -40.5142],\n",
      "        [-42.1416, -54.9044,  67.3863,  ..., -17.7682,  48.4564, -34.9193],\n",
      "        ...,\n",
      "        [-38.7335, -52.2348,  63.3122,  ..., -13.5436,  52.4859, -38.9405],\n",
      "        [-42.5257, -52.5830,  66.5437,  ..., -16.3587,  45.5708, -37.1883],\n",
      "        [-43.4213, -52.9976,  70.9814,  ..., -13.4141,  48.6268, -36.4128]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 11.0\n",
      "batch_idx: 0\n",
      "0.11\n",
      "linear_output: tensor([[-55.7195, -54.7445,  64.2022,  ...,  -4.7317,  53.7737, -46.0311],\n",
      "        [-52.5173, -55.1814,  61.7178,  ...,  -7.5762,  54.8786, -43.9861],\n",
      "        [-55.8853, -51.7254,  63.1746,  ...,  -2.4455,  56.2893, -43.5117],\n",
      "        ...,\n",
      "        [-56.2059, -55.3545,  64.7167,  ...,  -3.6465,  55.5458, -44.5471],\n",
      "        [-59.0671, -56.4329,  58.9254,  ...,  -6.6560,  52.6045, -44.2891],\n",
      "        [-47.6046, -64.2071,  58.3908,  ..., -10.7809,  66.7121, -48.7566]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 21.0\n",
      "batch_idx: 1\n",
      "0.105\n",
      "linear_output: tensor([[-67.2094, -59.5676,  63.6185,  ...,   8.3815,  60.2465, -52.0377],\n",
      "        [-66.9945, -56.1312,  59.8249,  ...,   7.7782,  59.4056, -50.0380],\n",
      "        [-64.3367, -51.6008,  58.4756,  ...,   8.6737,  63.2780, -53.9989],\n",
      "        ...,\n",
      "        [-67.6837, -57.6269,  59.1335,  ...,   6.9965,  63.6364, -55.6432],\n",
      "        [-61.3414, -61.5916,  52.4544,  ...,   7.1176,  67.6645, -58.9237],\n",
      "        [-60.9966, -61.2195,  54.4005,  ...,   6.5437,  64.1581, -54.1675]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 37.0\n",
      "batch_idx: 2\n",
      "0.12333333333333334\n",
      "Training Epoch: 1, total loss: 6.236520\n",
      "linear_output: tensor([[-74.8447, -62.1849,  57.4061,  ...,  18.7793,  70.8913, -57.4851],\n",
      "        [-79.2045, -64.4392,  53.9846,  ...,  12.9999,  67.5412, -60.3096],\n",
      "        [-78.2402, -67.5990,  56.7036,  ...,  18.2396,  71.2964, -60.1396],\n",
      "        ...,\n",
      "        [-68.6527, -56.6632,  55.5559,  ...,  21.8750,  67.5319, -54.5129],\n",
      "        [-74.9881, -64.3937,  56.7976,  ...,  20.9111,  68.8255, -57.7694],\n",
      "        [-69.9533, -55.8401,  57.9687,  ...,  23.0918,  65.6840, -54.9750]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 12.0\n",
      "batch_idx: 0\n",
      "0.12\n",
      "linear_output: tensor([[-85.8010, -54.9444,  54.4114,  ...,  29.2933,  71.3258, -68.3789],\n",
      "        [-81.4024, -54.4924,  55.3182,  ...,  30.7503,  74.8719, -64.9649],\n",
      "        [-81.0731, -59.1610,  53.7760,  ...,  32.6845,  74.5145, -57.6527],\n",
      "        ...,\n",
      "        [-79.4770, -64.4713,  53.9101,  ...,  27.1438,  76.8609, -62.5009],\n",
      "        [-79.2608, -63.5582,  52.7620,  ...,  29.0057,  75.4093, -63.8601],\n",
      "        [-82.9561, -62.4414,  54.5667,  ...,  30.8887,  79.8664, -65.2901]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 22.0\n",
      "batch_idx: 1\n",
      "0.11\n",
      "linear_output: tensor([[-91.6017, -60.7864,  54.7046,  ...,  42.6886,  76.0858, -65.4586],\n",
      "        [-85.7144, -56.7616,  53.6006,  ...,  41.0395,  78.4641, -62.6781],\n",
      "        [-92.7125, -58.7188,  55.0257,  ...,  40.7560,  77.4052, -72.7570],\n",
      "        ...,\n",
      "        [-87.7421, -59.5165,  53.4718,  ...,  38.6469,  76.6402, -62.9410],\n",
      "        [-85.8077, -60.9343,  55.5272,  ...,  42.1376,  73.8340, -63.0039],\n",
      "        [-80.4538, -55.5508,  54.5452,  ...,  41.1039,  72.6498, -61.2653]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 38.0\n",
      "batch_idx: 2\n",
      "0.12666666666666668\n",
      "Training Epoch: 2, total loss: 6.236149\n",
      "linear_output: tensor([[-92.2076, -62.9327,  54.9268,  ...,  48.0165,  84.7696, -73.8798],\n",
      "        [-91.0477, -59.8127,  51.9073,  ...,  44.3514,  80.2393, -70.3960],\n",
      "        [-91.8336, -58.4286,  53.0051,  ...,  49.1440,  82.6066, -66.5212],\n",
      "        ...,\n",
      "        [-94.5511, -60.7622,  49.3238,  ...,  47.9908,  84.2069, -72.9714],\n",
      "        [-94.1605, -67.6227,  50.1766,  ...,  48.4737,  80.8758, -76.2940],\n",
      "        [-97.4370, -63.5449,  53.0374,  ...,  48.0317,  81.8753, -72.9699]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 15.0\n",
      "batch_idx: 0\n",
      "0.15\n",
      "linear_output: tensor([[-105.0579,  -62.7901,   50.6256,  ...,   59.3738,   87.3162,\n",
      "          -74.2032],\n",
      "        [ -99.9019,  -64.4531,   41.7319,  ...,   58.6986,   86.2348,\n",
      "          -76.2895],\n",
      "        [ -96.9909,  -59.5052,   49.8246,  ...,   60.9914,   87.5195,\n",
      "          -70.4622],\n",
      "        ...,\n",
      "        [-110.9998,  -75.9671,   43.9538,  ...,   53.1629,   98.1655,\n",
      "          -81.7581],\n",
      "        [-105.1458,  -66.9531,   49.4825,  ...,   54.3910,   88.6442,\n",
      "          -75.0883],\n",
      "        [-101.2628,  -64.8313,   48.1490,  ...,   58.2076,   82.6940,\n",
      "          -77.3299]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 33.0\n",
      "batch_idx: 1\n",
      "0.165\n",
      "linear_output: tensor([[-109.0068,  -66.9820,   43.9568,  ...,   62.5733,   92.9354,\n",
      "          -83.6991],\n",
      "        [-111.4891,  -73.9880,   40.9217,  ...,   61.1131,   87.0728,\n",
      "          -77.9124],\n",
      "        [-107.6366,  -71.4900,   45.1805,  ...,   63.2738,   90.8478,\n",
      "          -78.5245],\n",
      "        ...,\n",
      "        [-110.9181,  -66.6467,   45.6477,  ...,   60.3349,   85.8750,\n",
      "          -76.3079],\n",
      "        [ -97.3064,  -61.3689,   45.3376,  ...,   60.5935,   84.5891,\n",
      "          -73.0344],\n",
      "        [-108.5188,  -66.4730,   43.1584,  ...,   60.5869,   87.8483,\n",
      "          -78.7805]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 47.0\n",
      "batch_idx: 2\n",
      "0.15666666666666668\n",
      "Training Epoch: 3, total loss: 6.236641\n",
      "linear_output: tensor([[-104.9048,  -70.6142,   36.6906,  ...,   70.2579,   89.6898,\n",
      "          -81.2191],\n",
      "        [-117.1282,  -80.7217,   35.2134,  ...,   65.7854,   95.9758,\n",
      "          -84.4682],\n",
      "        [-111.7779,  -76.2831,   37.6070,  ...,   68.3740,   90.4819,\n",
      "          -77.9749],\n",
      "        ...,\n",
      "        [-114.0318,  -71.2523,   40.5094,  ...,   68.0729,   90.8661,\n",
      "          -77.5856],\n",
      "        [-104.4821,  -82.5897,   33.9941,  ...,   66.4416,   95.2015,\n",
      "          -80.0496],\n",
      "        [-110.0841,  -73.3383,   41.7473,  ...,   67.4569,   88.2129,\n",
      "          -84.5971]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 13.0\n",
      "batch_idx: 0\n",
      "0.13\n",
      "linear_output: tensor([[-116.8677,  -71.2601,   42.1744,  ...,   75.9718,   95.2761,\n",
      "          -81.5443],\n",
      "        [-113.7071,  -75.0746,   29.2771,  ...,   69.3235,   96.9099,\n",
      "          -81.7378],\n",
      "        [-113.4829,  -78.1461,   31.8718,  ...,   73.1180,   95.4030,\n",
      "          -85.2565],\n",
      "        ...,\n",
      "        [-121.5552,  -75.1951,   28.5446,  ...,   67.2592,   96.8867,\n",
      "          -86.4413],\n",
      "        [-110.3209,  -85.3539,   26.3344,  ...,   70.7431,   98.6942,\n",
      "          -88.8943],\n",
      "        [-120.5807,  -81.3954,   31.2994,  ...,   69.8604,   95.7312,\n",
      "          -82.3867]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 33.0\n",
      "batch_idx: 1\n",
      "0.165\n",
      "linear_output: tensor([[-115.6516,  -82.3162,   18.0704,  ...,   85.3531,  100.6615,\n",
      "          -94.6613],\n",
      "        [-122.8850,  -80.6129,   22.8442,  ...,   75.9966,  102.1933,\n",
      "          -88.9457],\n",
      "        [-112.4762,  -73.1877,   26.7713,  ...,   74.2348,   94.5965,\n",
      "          -77.3692],\n",
      "        ...,\n",
      "        [-118.6640,  -77.5285,   26.7842,  ...,   77.7594,   97.0079,\n",
      "          -83.7422],\n",
      "        [-119.6679,  -76.9278,   25.7974,  ...,   80.2604,   96.8149,\n",
      "          -86.3208],\n",
      "        [-120.2975,  -78.3070,   23.9806,  ...,   78.3808,   99.7188,\n",
      "          -84.5683]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_correct: 53.0\n",
      "batch_idx: 2\n",
      "0.17666666666666667\n",
      "Training Epoch: 4, total loss: 6.232770\n",
      "linear_output: tensor([[-117.8762,  -76.5241,   17.0533,  ...,   80.2363,  103.3568,\n",
      "          -86.7920],\n",
      "        [-120.3036,  -84.3971,   11.1821,  ...,   80.9208,   98.6986,\n",
      "          -93.2277],\n",
      "        [-109.7259,  -72.2331,   21.4191,  ...,   77.0818,   94.0838,\n",
      "          -79.7231],\n",
      "        ...,\n",
      "        [-112.1802,  -73.4446,   18.1636,  ...,   78.3908,   97.2747,\n",
      "          -86.7582],\n",
      "        [-126.0844,  -79.6749,    9.0435,  ...,   82.1466,  111.7178,\n",
      "          -95.9870],\n",
      "        [-120.7353,  -80.5036,   15.2220,  ...,   80.5014,   95.6068,\n",
      "          -91.4804]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 13.0\n",
      "batch_idx: 0\n",
      "0.13\n",
      "linear_output: tensor([[-132.3848,  -87.5766,   -6.8284,  ...,   83.2756,  102.6564,\n",
      "          -92.1690],\n",
      "        [-128.7431,  -78.1070,   -1.5905,  ...,   87.9920,  101.9887,\n",
      "          -92.7131],\n",
      "        [-124.9523,  -82.8487,    2.4503,  ...,   93.0472,  100.0915,\n",
      "          -98.2745],\n",
      "        ...,\n",
      "        [-126.5079,  -81.5271,    1.3340,  ...,   82.9384,  104.2839,\n",
      "          -88.6096],\n",
      "        [-123.0610,  -85.3804,   -6.1617,  ...,   83.7537,  113.2776,\n",
      "          -95.8201],\n",
      "        [-122.3753,  -85.2018,   -0.3000,  ...,   82.3584,  103.5008,\n",
      "          -93.7106]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 33.0\n",
      "batch_idx: 1\n",
      "0.165\n",
      "linear_output: tensor([[-132.7469,  -81.2561,  -15.5482,  ...,   88.6433,  104.3412,\n",
      "          -99.0166],\n",
      "        [-121.8768,  -77.3997,   -7.9437,  ...,   85.6288,  103.0830,\n",
      "          -86.1463],\n",
      "        [-130.7919,  -81.5016,  -16.9260,  ...,   84.5285,  103.7680,\n",
      "          -93.0853],\n",
      "        ...,\n",
      "        [-117.2951,  -77.1739,   -3.3643,  ...,   92.0213,  104.2225,\n",
      "          -88.2084],\n",
      "        [-128.1244,  -80.8872,  -16.1701,  ...,   90.9156,  103.3300,\n",
      "          -91.8943],\n",
      "        [-124.4828,  -83.1414,  -12.0018,  ...,   88.8031,  103.3692,\n",
      "          -93.0770]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 47.0\n",
      "batch_idx: 2\n",
      "0.15666666666666668\n",
      "Training Epoch: 5, total loss: 6.232657\n",
      "linear_output: tensor([[-125.6061,  -79.1219,  -21.0901,  ...,   86.0779,  107.1287,\n",
      "          -93.2158],\n",
      "        [-129.7044,  -87.7295,  -29.5999,  ...,   89.8317,  110.1754,\n",
      "         -100.4264],\n",
      "        [-131.2267,  -78.2674,  -27.2034,  ...,   86.4825,  108.8300,\n",
      "          -90.7374],\n",
      "        ...,\n",
      "        [-124.9058,  -73.6265,  -20.1162,  ...,   88.4906,  102.9590,\n",
      "          -91.2517],\n",
      "        [-128.5829,  -84.4316,  -33.5522,  ...,   89.9605,  116.7265,\n",
      "          -99.1246],\n",
      "        [-134.6962,  -79.1936,  -30.2666,  ...,   92.2354,  117.3764,\n",
      "         -101.2548]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 17.0\n",
      "batch_idx: 0\n",
      "0.17\n",
      "linear_output: tensor([[-124.5703,  -74.7369,  -25.2123,  ...,   91.2215,  104.3813,\n",
      "          -92.8278],\n",
      "        [-132.9072,  -83.4834,  -32.1554,  ...,   95.9464,  109.7449,\n",
      "          -96.2519],\n",
      "        [-132.7978,  -77.6270,  -27.8631,  ...,   92.7979,  106.1074,\n",
      "          -93.5899],\n",
      "        ...,\n",
      "        [-126.8856,  -72.1737,  -28.1238,  ...,   93.7163,  107.4447,\n",
      "          -89.5500],\n",
      "        [-130.1521,  -79.1938,  -35.9206,  ...,   92.0447,  104.7028,\n",
      "          -93.6429],\n",
      "        [-128.1727,  -72.9168,  -33.0676,  ...,   91.0749,   99.7770,\n",
      "          -92.8190]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 26.0\n",
      "batch_idx: 1\n",
      "0.13\n",
      "linear_output: tensor([[-131.3749,  -80.3701,  -40.8803,  ...,   96.7199,  108.0556,\n",
      "          -97.2832],\n",
      "        [-131.8047,  -74.9841,  -36.3493,  ...,   97.4082,  108.4539,\n",
      "          -98.9726],\n",
      "        [-126.5270,  -76.4795,  -39.0123,  ...,   92.8641,  105.9117,\n",
      "          -93.1213],\n",
      "        ...,\n",
      "        [-136.8215,  -77.5364,  -37.5076,  ...,   91.5402,  107.5102,\n",
      "          -95.8843],\n",
      "        [-136.0144,  -76.5262,  -38.9851,  ...,   93.2998,  106.3449,\n",
      "          -96.2614],\n",
      "        [-148.8356,  -89.6897,  -58.8257,  ...,   96.2402,  122.5804,\n",
      "         -106.6196]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 45.0\n",
      "batch_idx: 2\n",
      "0.15\n",
      "Training Epoch: 6, total loss: 6.232764\n",
      "linear_output: tensor([[-133.5040,  -73.1367,  -37.7467,  ...,  103.0862,  113.6617,\n",
      "          -95.6027],\n",
      "        [-136.2503,  -74.4882,  -45.3268,  ...,   98.1746,  108.8806,\n",
      "          -94.7113],\n",
      "        [-135.6084,  -74.9346,  -45.5179,  ...,   95.1548,  111.6579,\n",
      "          -95.0375],\n",
      "        ...,\n",
      "        [-138.4276,  -73.7973,  -48.8121,  ...,   96.1363,  108.2255,\n",
      "          -97.6058],\n",
      "        [-134.8068,  -72.1830,  -44.6553,  ...,   98.0636,  112.3786,\n",
      "         -102.4550],\n",
      "        [-133.2083,  -71.7049,  -45.3870,  ...,   96.5162,  109.6653,\n",
      "          -97.1397]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 17.0\n",
      "batch_idx: 0\n",
      "0.17\n",
      "linear_output: tensor([[-134.8225,  -73.2654,  -58.4600,  ...,   97.6113,  113.5281,\n",
      "         -100.7444],\n",
      "        [-134.6056,  -73.0557,  -55.6558,  ...,   97.7975,  115.5862,\n",
      "         -103.2857],\n",
      "        [-141.8129,  -73.6315,  -58.3829,  ...,   96.7200,  111.6605,\n",
      "         -102.0867],\n",
      "        ...,\n",
      "        [-134.6761,  -68.7369,  -52.1224,  ...,   99.0879,  109.8782,\n",
      "          -94.7463],\n",
      "        [-140.2607,  -75.1567,  -57.7889,  ...,   95.4887,  114.7547,\n",
      "          -99.8872],\n",
      "        [-142.1451,  -77.9687,  -64.9972,  ...,   98.2530,  124.4972,\n",
      "         -110.5261]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 38.0\n",
      "batch_idx: 1\n",
      "0.19\n",
      "linear_output: tensor([[-141.7905,  -65.8330,  -61.1426,  ...,  100.5271,  111.3445,\n",
      "         -101.2421],\n",
      "        [-132.4329,  -65.3917,  -55.4833,  ...,  102.7457,  108.3497,\n",
      "          -98.4065],\n",
      "        [-142.7915,  -70.0039,  -57.4278,  ...,  102.8217,  115.2299,\n",
      "         -103.4625],\n",
      "        ...,\n",
      "        [-129.8927,  -59.3148,  -43.4604,  ...,  101.2869,  110.6878,\n",
      "          -93.4834],\n",
      "        [-140.8878,  -73.3068,  -60.5953,  ...,  108.3463,  115.5352,\n",
      "         -106.7416],\n",
      "        [-137.8205,  -67.1479,  -52.4300,  ...,  100.2962,  107.2762,\n",
      "         -102.0149]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 56.0\n",
      "batch_idx: 2\n",
      "0.18666666666666668\n",
      "Training Epoch: 7, total loss: 6.231408\n",
      "linear_output: tensor([[-146.2803,  -63.0780,  -65.1256,  ...,  102.8818,  112.8831,\n",
      "         -107.1820],\n",
      "        [-139.2489,  -62.7056,  -56.9311,  ...,   95.7009,  111.4748,\n",
      "         -101.6568],\n",
      "        [-142.6961,  -57.2969,  -59.3976,  ...,  101.6033,  112.5616,\n",
      "          -98.4556],\n",
      "        ...,\n",
      "        [-135.3801,  -71.8147,  -69.5479,  ...,  100.5587,  114.4017,\n",
      "          -98.8104],\n",
      "        [-142.3260,  -68.2881,  -69.8086,  ...,   99.0294,  117.6026,\n",
      "         -107.9218],\n",
      "        [-137.0647,  -65.7127,  -65.5690,  ...,  100.3294,  115.0591,\n",
      "         -105.4575]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 15.0\n",
      "batch_idx: 0\n",
      "0.15\n",
      "linear_output: tensor([[-140.3423,  -50.8867,  -61.9450,  ...,   98.4565,  112.0088,\n",
      "          -98.6727],\n",
      "        [-139.7929,  -61.7514,  -66.4789,  ...,  103.5875,  112.4255,\n",
      "         -103.4227],\n",
      "        [-141.9281,  -63.0719,  -74.1284,  ...,  112.0182,  116.5287,\n",
      "         -111.4313],\n",
      "        ...,\n",
      "        [-144.5228,  -56.2908,  -69.0418,  ...,  102.8997,  112.9971,\n",
      "         -102.8132],\n",
      "        [-140.2106,  -56.9807,  -66.1249,  ...,   98.8240,  109.7028,\n",
      "          -95.2004],\n",
      "        [-142.6298,  -55.7500,  -65.1825,  ...,  104.0985,  112.5703,\n",
      "          -98.6912]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 32.0\n",
      "batch_idx: 1\n",
      "0.16\n",
      "linear_output: tensor([[-133.5707,  -42.9723,  -53.0595,  ...,  104.8826,  113.0612,\n",
      "          -95.6461],\n",
      "        [-137.5648,  -51.9603,  -58.9347,  ...,  101.6220,  111.9385,\n",
      "          -96.4250],\n",
      "        [-140.6239,  -59.7791,  -79.7731,  ...,  102.3950,  116.8204,\n",
      "         -106.6016],\n",
      "        ...,\n",
      "        [-146.3185,  -47.8322,  -67.3231,  ...,  105.2263,  112.9843,\n",
      "          -97.6403],\n",
      "        [-137.6937,  -46.5980,  -58.3838,  ...,  109.1395,  117.7430,\n",
      "         -100.1605],\n",
      "        [-140.9483,  -55.5683,  -76.6591,  ...,   98.3470,  113.5193,\n",
      "         -105.9395]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_correct: 55.0\n",
      "batch_idx: 2\n",
      "0.18333333333333332\n",
      "Training Epoch: 8, total loss: 6.227640\n",
      "linear_output: tensor([[-146.1794,  -46.7712,  -68.9459,  ...,  107.3713,  116.7264,\n",
      "         -102.3444],\n",
      "        [-133.6562,  -36.1557,  -61.2167,  ...,  102.0660,  111.6573,\n",
      "          -99.8251],\n",
      "        [-139.5769,  -40.7837,  -66.9087,  ...,  108.4172,  111.2118,\n",
      "         -100.8406],\n",
      "        ...,\n",
      "        [-139.6283,  -37.7090,  -55.7693,  ...,  104.1245,  113.1001,\n",
      "          -95.0790],\n",
      "        [-144.8700,  -42.0974,  -74.1063,  ...,  108.1656,  113.4486,\n",
      "         -102.1918],\n",
      "        [-141.0115,  -42.5961,  -70.3365,  ...,  103.5086,  116.8314,\n",
      "         -102.6661]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 24.0\n",
      "batch_idx: 0\n",
      "0.24\n",
      "linear_output: tensor([[-137.4582,  -31.3498,  -63.0218,  ...,  102.7467,  111.8954,\n",
      "          -92.9225],\n",
      "        [-145.9095,  -31.3141,  -69.1719,  ...,  109.2963,  114.3754,\n",
      "         -102.3542],\n",
      "        [-142.6501,  -26.4605,  -65.6759,  ...,  100.7293,  113.3007,\n",
      "          -97.3815],\n",
      "        ...,\n",
      "        [-140.3362,  -31.2204,  -67.4385,  ...,  101.2145,  116.9146,\n",
      "         -102.0449],\n",
      "        [-142.1758,  -30.1939,  -70.8122,  ...,  107.9954,  114.4207,\n",
      "          -99.0624],\n",
      "        [-145.7260,  -30.0133,  -72.0182,  ...,  106.0532,  114.9276,\n",
      "         -104.5729]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 36.0\n",
      "batch_idx: 1\n",
      "0.18\n",
      "linear_output: tensor([[-135.3327,  -21.3826,  -61.4965,  ...,   99.6710,  112.0791,\n",
      "          -95.1277],\n",
      "        [-142.8309,  -24.4448,  -76.1604,  ...,  102.0525,  118.8431,\n",
      "         -102.2201],\n",
      "        [-149.4461,  -29.6656,  -88.3772,  ...,  104.2852,  122.3187,\n",
      "         -112.3307],\n",
      "        ...,\n",
      "        [-137.1982,  -22.8232,  -58.3794,  ...,  107.1861,  113.4773,\n",
      "          -99.3442],\n",
      "        [-141.2079,  -34.0188,  -82.8352,  ...,  105.1553,  117.3299,\n",
      "         -101.6152],\n",
      "        [-143.7034,  -20.8084,  -71.3710,  ...,  106.2553,  118.1564,\n",
      "         -106.0258]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 59.0\n",
      "batch_idx: 2\n",
      "0.19666666666666666\n",
      "Training Epoch: 9, total loss: 6.224935\n",
      "linear_output: tensor([[-138.0119,  -12.3934,  -61.6607,  ...,  105.3517,  112.0179,\n",
      "          -98.7371],\n",
      "        [-134.7061,  -12.2791,  -60.3951,  ...,  105.2485,  109.3521,\n",
      "          -95.8712],\n",
      "        [-140.0485,  -12.2385,  -67.8535,  ...,  104.3903,  114.2095,\n",
      "         -103.4420],\n",
      "        ...,\n",
      "        [-141.1319,   -6.4930,  -70.7527,  ...,  105.6710,  113.9932,\n",
      "         -106.8806],\n",
      "        [-134.3871,  -11.6979,  -63.0814,  ...,  103.4858,  110.0591,\n",
      "          -96.4672],\n",
      "        [-149.8502,  -10.2149,  -76.0876,  ...,  105.0393,  113.6959,\n",
      "         -104.8908]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 17.0\n",
      "batch_idx: 0\n",
      "0.17\n",
      "linear_output: tensor([[-149.9444,    2.0705,  -80.8990,  ...,  107.1620,  116.1280,\n",
      "         -105.7003],\n",
      "        [-144.1715,   -5.1365,  -78.9652,  ...,  105.4564,  124.7196,\n",
      "         -107.2564],\n",
      "        [-145.2534,    3.4806,  -79.9927,  ...,  110.5456,  116.1432,\n",
      "         -104.2839],\n",
      "        ...,\n",
      "        [-144.0993,    4.7108,  -74.2212,  ...,  107.8253,  119.0023,\n",
      "         -106.1260],\n",
      "        [-141.7276,   -0.7432,  -65.7094,  ...,  112.4898,  120.3261,\n",
      "         -102.4436],\n",
      "        [-145.9957,    5.0946,  -77.0418,  ...,  107.0662,  117.5880,\n",
      "         -104.7603]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n",
      "new_correct: 39.0\n",
      "batch_idx: 1\n",
      "0.195\n",
      "linear_output: tensor([[-144.9406,   12.1944,  -70.0669,  ...,  102.5996,  116.2136,\n",
      "         -106.1297],\n",
      "        [-141.5544,   16.3644,  -65.4345,  ...,  106.8256,  116.1037,\n",
      "          -98.4871],\n",
      "        [-148.3871,   19.3502,  -77.5796,  ...,  104.1752,  113.4133,\n",
      "         -101.0490],\n",
      "        ...,\n",
      "        [-150.5202,   14.9961,  -72.5349,  ...,  111.4448,  120.2183,\n",
      "         -104.3250],\n",
      "        [-142.2049,   10.4302,  -65.5430,  ...,  106.8900,  114.0651,\n",
      "          -93.6057],\n",
      "        [-163.3229,   11.7219,  -96.0475,  ...,  108.8462,  130.2295,\n",
      "         -112.9637]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "dnn_softmax_output: torch.Size([100, 8])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ff14c47fceff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m#loss = loss_func(y_predict.view(-1), labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from nfm_network import NFM\n",
    "if __name__ == \"__main__\":\n",
    "    ####################################################################################\n",
    "    # NFM 模型\n",
    "    ####################################################################################\n",
    "    BATCH_SIZE=100\n",
    "    \"\"\"\n",
    "    training_data, training_label, dense_features_col, sparse_features_col = getTrainData(nfm_config['train_file'], nfm_config['fea_file'])\n",
    "    train_dataset = Data.TensorDataset(torch.tensor(training_data).float(), torch.tensor(training_label).float())\n",
    "\n",
    "    test_data = getTestData(nfm_config['test_file'])\n",
    "    test_dataset = Data.TensorDataset(torch.tensor(test_data).float())\n",
    "    \n",
    "    test_data = getTestData(nfm_config['test_file'])\n",
    "    test_dataset = Data.TensorDataset(torch.tensor(test_data).float())\n",
    "\n",
    "    \"\"\"\n",
    "    #device = torch.device('cuda:0')\n",
    "    #epoch=0\n",
    "    \n",
    "    #model=nn.Linear(10149,16).to(device)\n",
    "    #model=nn.Linear(10149,16)\n",
    "    #model=nn.ReLU(nn.Linear(10149,16))#RuntimeError: all elements of input should be between 0 and 1\n",
    "    #print('model:',model)\n",
    "    nfm = NFM(nfm_config).cuda()#加了device防止出现GPU CPU两种设备的错误提示\n",
    "    print(\"nfm:\",nfm)\n",
    "    #print(nfm)\n",
    "    #nfm.train()\n",
    "    #u=nfm.parameters()\n",
    "    #print(u)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    optimizer = torch.optim.Adam(nfm.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    total = 0\n",
    "    #loss_func = torch.nn.BCELoss()\n",
    "    loss_func=torch.nn.CrossEntropyLoss()\n",
    "    #loss_func=torch.nn.LogSoftmax()\n",
    "    \n",
    "    #model=nn.Softmax(nn.Linear(10149,16)).to(device)\n",
    "    # 从DataLoader中获取小批量的id以及数据\n",
    "    for epoch_id in range(1000):\n",
    "        correct=0\n",
    "        total=0\n",
    "        for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "            x = Variable(x)\n",
    "            labels = Variable(labels)\n",
    "            \n",
    "            \n",
    "            #x = torch.tensor(x, dtype=torch.float)\n",
    "            #x=x.clone().detach().requires_grad_(True)\n",
    "            x=torch.tensor(x,dtype=torch.float)\n",
    "            labels=torch.tensor(labels,dtype=torch.float)\n",
    "            x, labels = x.cuda(), labels.cuda()\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_predict = nfm(x)\n",
    "            #loss = loss_func(y_predict.view(-1), labels)\n",
    "            loss = loss_func(y_predict, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss = loss.item()\n",
    "            #loss, predicted = self._train_single_batch(x, labels)\n",
    "\n",
    "            total += loss\n",
    "            \n",
    "            \n",
    "            \n",
    "            predicted = torch.max(y_predict.data,1)\n",
    "            #print(\"predicted:\",predicted)\n",
    "            predicted = torch.max(y_predict.data,1)[1]\n",
    "            \n",
    "            \n",
    "            labels=torch.max(labels,1)\n",
    "            #print(\"labels:\",labels)\n",
    "            labels=labels[1]\n",
    "            \n",
    "            correct += (predicted == labels).sum()\n",
    "            #print(\"correct:\",correct)\n",
    "            #correct=correct[0]\n",
    "            print(\"new_correct:\",float(correct))\n",
    "            correct=float(correct)   \n",
    "            #if batch_idx % 10 == 0:\n",
    "            print(\"batch_idx:\",batch_idx)\n",
    "            print(correct/(BATCH_SIZE*(batch_idx+1)))\n",
    "            \n",
    "            \"\"\"\n",
    "            #y_predict.detach().numpy()\n",
    "            pred = y_predict\n",
    "            print(\"pred:\",pred.shape)\n",
    "            y=labels.clone().detach().requires_grad_(True)\n",
    "            print(\"y:\",y.shape)\n",
    "            #y=labels.data.cpu().numpy()\n",
    "            #y = labels.detach().numpy()\n",
    "            roc_auc_score(y, pred)\n",
    "            \"\"\"\n",
    "           \n",
    "            # print('[Training Epoch: {}] Batch: {}, Loss: {}'.format(epoch_id, batch_id, loss))\n",
    "        print(\"Training Epoch: %d, total loss: %f\" % (epoch_id, total))\n",
    "        #print(\"auc:\",roc_auc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236e69fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
