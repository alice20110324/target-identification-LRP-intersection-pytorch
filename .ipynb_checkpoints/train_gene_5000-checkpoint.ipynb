{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ee9c26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import Trainer\n",
    "from network import NFM\n",
    "import torch.utils.data as Data\n",
    "from Utils.criteo_loader import getTestData, getTrainData\n",
    "'''  \n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':9,\n",
    "    'linear_hidden1':1000,\n",
    "    'linear_hidden':100,#线性模型输出层（隐层个数）\n",
    "    'embed_input_dim':1001,#embed输入维度\n",
    "    'embed_dim': 100, # 用于控制稀疏特征经过Embedding层后的稠密特征大小，embed输出维度\n",
    "    #'dnn_hidden_units': [100,11],#MLP隐层和输出层\n",
    "    \n",
    "    'dnn_hidden_units':[100,9],#MLP隐层\n",
    "    'num_sparse_features_cols':5510,#the number of the gene columns\n",
    "    'num_dense_features': 0,#dense features number\n",
    "    'bi_dropout': 0.3,#Bi-Interaction 的dropout\n",
    "    'num_epoch': 500,#训练epoch次数\n",
    "    'batch_size': 100,#batch_size\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    \n",
    "    #'train_file': '../Data/criteo/processed_data/train_set.csv',\n",
    "    #'fea_file': '../Data/criteo/processed_data/fea_col.npy',\n",
    "    #'validate_file': '../Data/criteo/processed_data/val_set.csv',\n",
    "    #'test_file': '../Data/criteo/processed_data/test_set.csv',\n",
    "    #'model_name': '../TrainedModels/NFM.model'\n",
    "    'train_file':'data/xiaoqiu_gene_5000/train/final_5000_encode_100x.csv',\n",
    "    'label_file':'data/xiaoqiu_gene_5000/train/gene_5000_labels.csv',\n",
    "    'test_file':'data/frappe/gene_4000/test/test_file.csv',\n",
    "    'test_label':'data/frappe/gene_4000/test/test_label.csv',\n",
    "    'title':'data/xiaoqiu_gene_5000/train/gene_5000_gene_name.csv',\n",
    "    'all':'data/xiaoqiu_gene_5000/train/gene_5000_label_name.csv'\n",
    "}\n",
    "\n",
    " '''\n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':9,\n",
    "    'linear_hidden1':1000,\n",
    "    'linear_hidden':100,#线性模型输出层（隐层个数）\n",
    "    'embed_input_dim':1001,#embed输入维度\n",
    "    'embed_dim': 100, # 用于控制稀疏特征经过Embedding层后的稠密特征大小，embed输出维度\n",
    "    #'dnn_hidden_units': [100,11],#MLP隐层和输出层\n",
    "    \n",
    "    'dnn_hidden_units':[100,9],#MLP隐层\n",
    "    'num_sparse_features_cols':4225,#the number of the gene columns\n",
    "    'num_dense_features': 0,#dense features number\n",
    "    'bi_dropout': 0.5,#Bi-Interaction 的dropout\n",
    "    'num_epoch': 500,#训练epoch次数\n",
    "    'batch_size': 100,#batch_size\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    \n",
    "    #'train_file': '../Data/criteo/processed_data/train_set.csv',\n",
    "    #'fea_file': '../Data/criteo/processed_data/fea_col.npy',\n",
    "    #'validate_file': '../Data/criteo/processed_data/val_set.csv',\n",
    "    #'test_file': '../Data/criteo/processed_data/test_set.csv',\n",
    "    #'model_name': '../TrainedModels/NFM.model'\n",
    "    #'train_file':'data/xiaoqiu_gene_5000/train/final_5000_encode_100x.csv',\n",
    "    'train_file':'dataset/xiaoguan/RF/RF_for_train/train_class_9/train/train_encode_data.csv',\n",
    "    'label_file':'dataset/xiaoguan/RF/RF_for_train/train_class_9/train/train_label.csv',\n",
    "    'test_file':'dataset/xiaoguan/RF/RF_for_train/train_class_9/test/test_encode_data.csv',\n",
    "    'test_label':'dataset/xiaoguan/RF/RF_for_train/train_class_9/test/test_label.csv',\n",
    "    #'title':'dataset/xiaoguan/RF/RF_for_train/train_class_9/test/test_data.csv',\n",
    "    \n",
    "    #'all':''\n",
    "    #'title':'data/xiaoqiu_gene_5000/train/gene_5000_gene_name.csv',\n",
    "    #'all':'data/xiaoqiu_gene_5000/train/gene_5000_label_name.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d3a64ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "#from utils import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c786e2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[190 220 490 ... 360 370 420]\n",
      " [110 220 500 ... 340 260 420]\n",
      " [100 300 500 ... 330 350 390]\n",
      " ...\n",
      " [450 360 500 ... 430 360 500]\n",
      " [420 350 530 ... 440 370 500]\n",
      " [420 360 540 ... 430 370 450]]\n",
      "row: 0 label: [0]\n",
      "row: 1 label: [0]\n",
      "row: 2 label: [0]\n",
      "row: 3 label: [0]\n",
      "row: 4 label: [0]\n",
      "row: 5 label: [0]\n",
      "row: 6 label: [0]\n",
      "row: 7 label: [0]\n",
      "row: 8 label: [0]\n",
      "row: 9 label: [0]\n",
      "row: 10 label: [0]\n",
      "row: 11 label: [0]\n",
      "row: 12 label: [0]\n",
      "row: 13 label: [0]\n",
      "row: 14 label: [0]\n",
      "row: 15 label: [0]\n",
      "row: 16 label: [0]\n",
      "row: 17 label: [0]\n",
      "row: 18 label: [0]\n",
      "row: 19 label: [0]\n",
      "row: 20 label: [0]\n",
      "row: 21 label: [0]\n",
      "row: 22 label: [0]\n",
      "row: 23 label: [0]\n",
      "row: 24 label: [0]\n",
      "row: 25 label: [0]\n",
      "row: 26 label: [0]\n",
      "row: 27 label: [0]\n",
      "row: 28 label: [0]\n",
      "row: 29 label: [0]\n",
      "row: 30 label: [0]\n",
      "row: 31 label: [0]\n",
      "row: 32 label: [0]\n",
      "row: 33 label: [0]\n",
      "row: 34 label: [0]\n",
      "row: 35 label: [0]\n",
      "row: 36 label: [0]\n",
      "row: 37 label: [0]\n",
      "row: 38 label: [0]\n",
      "row: 39 label: [0]\n",
      "row: 40 label: [0]\n",
      "row: 41 label: [0]\n",
      "row: 42 label: [0]\n",
      "row: 43 label: [1]\n",
      "row: 44 label: [1]\n",
      "row: 45 label: [1]\n",
      "row: 46 label: [1]\n",
      "row: 47 label: [1]\n",
      "row: 48 label: [1]\n",
      "row: 49 label: [1]\n",
      "row: 50 label: [1]\n",
      "row: 51 label: [1]\n",
      "row: 52 label: [1]\n",
      "row: 53 label: [1]\n",
      "row: 54 label: [1]\n",
      "row: 55 label: [1]\n",
      "row: 56 label: [1]\n",
      "row: 57 label: [1]\n",
      "row: 58 label: [1]\n",
      "row: 59 label: [1]\n",
      "row: 60 label: [1]\n",
      "row: 61 label: [1]\n",
      "row: 62 label: [1]\n",
      "row: 63 label: [1]\n",
      "row: 64 label: [1]\n",
      "row: 65 label: [1]\n",
      "row: 66 label: [1]\n",
      "row: 67 label: [1]\n",
      "row: 68 label: [1]\n",
      "row: 69 label: [1]\n",
      "row: 70 label: [1]\n",
      "row: 71 label: [1]\n",
      "row: 72 label: [1]\n",
      "row: 73 label: [1]\n",
      "row: 74 label: [1]\n",
      "row: 75 label: [1]\n",
      "row: 76 label: [1]\n",
      "row: 77 label: [1]\n",
      "row: 78 label: [1]\n",
      "row: 79 label: [1]\n",
      "row: 80 label: [1]\n",
      "row: 81 label: [1]\n",
      "row: 82 label: [1]\n",
      "row: 83 label: [1]\n",
      "row: 84 label: [1]\n",
      "row: 85 label: [1]\n",
      "row: 86 label: [1]\n",
      "row: 87 label: [1]\n",
      "row: 88 label: [1]\n",
      "row: 89 label: [1]\n",
      "row: 90 label: [1]\n",
      "row: 91 label: [1]\n",
      "row: 92 label: [1]\n",
      "row: 93 label: [1]\n",
      "row: 94 label: [1]\n",
      "row: 95 label: [1]\n",
      "row: 96 label: [1]\n",
      "row: 97 label: [1]\n",
      "row: 98 label: [1]\n",
      "row: 99 label: [1]\n",
      "row: 100 label: [1]\n",
      "row: 101 label: [1]\n",
      "row: 102 label: [1]\n",
      "row: 103 label: [1]\n",
      "row: 104 label: [1]\n",
      "row: 105 label: [1]\n",
      "row: 106 label: [1]\n",
      "row: 107 label: [1]\n",
      "row: 108 label: [1]\n",
      "row: 109 label: [1]\n",
      "row: 110 label: [1]\n",
      "row: 111 label: [1]\n",
      "row: 112 label: [1]\n",
      "row: 113 label: [1]\n",
      "row: 114 label: [1]\n",
      "row: 115 label: [1]\n",
      "row: 116 label: [1]\n",
      "row: 117 label: [1]\n",
      "row: 118 label: [1]\n",
      "row: 119 label: [1]\n",
      "row: 120 label: [1]\n",
      "row: 121 label: [1]\n",
      "row: 122 label: [1]\n",
      "row: 123 label: [1]\n",
      "row: 124 label: [1]\n",
      "row: 125 label: [1]\n",
      "row: 126 label: [1]\n",
      "row: 127 label: [1]\n",
      "row: 128 label: [1]\n",
      "row: 129 label: [1]\n",
      "row: 130 label: [1]\n",
      "row: 131 label: [1]\n",
      "row: 132 label: [1]\n",
      "row: 133 label: [1]\n",
      "row: 134 label: [1]\n",
      "row: 135 label: [1]\n",
      "row: 136 label: [1]\n",
      "row: 137 label: [1]\n",
      "row: 138 label: [1]\n",
      "row: 139 label: [1]\n",
      "row: 140 label: [1]\n",
      "row: 141 label: [1]\n",
      "row: 142 label: [1]\n",
      "row: 143 label: [1]\n",
      "row: 144 label: [1]\n",
      "row: 145 label: [1]\n",
      "row: 146 label: [2]\n",
      "row: 147 label: [2]\n",
      "row: 148 label: [2]\n",
      "row: 149 label: [2]\n",
      "row: 150 label: [2]\n",
      "row: 151 label: [2]\n",
      "row: 152 label: [2]\n",
      "row: 153 label: [2]\n",
      "row: 154 label: [2]\n",
      "row: 155 label: [2]\n",
      "row: 156 label: [2]\n",
      "row: 157 label: [2]\n",
      "row: 158 label: [2]\n",
      "row: 159 label: [2]\n",
      "row: 160 label: [2]\n",
      "row: 161 label: [2]\n",
      "row: 162 label: [2]\n",
      "row: 163 label: [2]\n",
      "row: 164 label: [2]\n",
      "row: 165 label: [2]\n",
      "row: 166 label: [2]\n",
      "row: 167 label: [2]\n",
      "row: 168 label: [2]\n",
      "row: 169 label: [2]\n",
      "row: 170 label: [2]\n",
      "row: 171 label: [2]\n",
      "row: 172 label: [2]\n",
      "row: 173 label: [2]\n",
      "row: 174 label: [2]\n",
      "row: 175 label: [2]\n",
      "row: 176 label: [2]\n",
      "row: 177 label: [2]\n",
      "row: 178 label: [2]\n",
      "row: 179 label: [2]\n",
      "row: 180 label: [2]\n",
      "row: 181 label: [2]\n",
      "row: 182 label: [2]\n",
      "row: 183 label: [2]\n",
      "row: 184 label: [2]\n",
      "row: 185 label: [2]\n",
      "row: 186 label: [2]\n",
      "row: 187 label: [2]\n",
      "row: 188 label: [2]\n",
      "row: 189 label: [2]\n",
      "row: 190 label: [2]\n",
      "row: 191 label: [2]\n",
      "row: 192 label: [2]\n",
      "row: 193 label: [2]\n",
      "row: 194 label: [2]\n",
      "row: 195 label: [2]\n",
      "row: 196 label: [2]\n",
      "row: 197 label: [2]\n",
      "row: 198 label: [2]\n",
      "row: 199 label: [2]\n",
      "row: 200 label: [3]\n",
      "row: 201 label: [3]\n",
      "row: 202 label: [3]\n",
      "row: 203 label: [3]\n",
      "row: 204 label: [3]\n",
      "row: 205 label: [3]\n",
      "row: 206 label: [3]\n",
      "row: 207 label: [3]\n",
      "row: 208 label: [3]\n",
      "row: 209 label: [3]\n",
      "row: 210 label: [3]\n",
      "row: 211 label: [3]\n",
      "row: 212 label: [3]\n",
      "row: 213 label: [3]\n",
      "row: 214 label: [3]\n",
      "row: 215 label: [3]\n",
      "row: 216 label: [3]\n",
      "row: 217 label: [3]\n",
      "row: 218 label: [3]\n",
      "row: 219 label: [3]\n",
      "row: 220 label: [3]\n",
      "row: 221 label: [3]\n",
      "row: 222 label: [3]\n",
      "row: 223 label: [3]\n",
      "row: 224 label: [3]\n",
      "row: 225 label: [3]\n",
      "row: 226 label: [3]\n",
      "row: 227 label: [3]\n",
      "row: 228 label: [3]\n",
      "row: 229 label: [3]\n",
      "row: 230 label: [3]\n",
      "row: 231 label: [3]\n",
      "row: 232 label: [3]\n",
      "row: 233 label: [3]\n",
      "row: 234 label: [3]\n",
      "row: 235 label: [3]\n",
      "row: 236 label: [3]\n",
      "row: 237 label: [3]\n",
      "row: 238 label: [3]\n",
      "row: 239 label: [3]\n",
      "row: 240 label: [3]\n",
      "row: 241 label: [3]\n",
      "row: 242 label: [3]\n",
      "row: 243 label: [3]\n",
      "row: 244 label: [3]\n",
      "row: 245 label: [3]\n",
      "row: 246 label: [3]\n",
      "row: 247 label: [3]\n",
      "row: 248 label: [3]\n",
      "row: 249 label: [3]\n",
      "row: 250 label: [3]\n",
      "row: 251 label: [3]\n",
      "row: 252 label: [3]\n",
      "row: 253 label: [3]\n",
      "row: 254 label: [3]\n",
      "row: 255 label: [3]\n",
      "row: 256 label: [3]\n",
      "row: 257 label: [3]\n",
      "row: 258 label: [3]\n",
      "row: 259 label: [3]\n",
      "row: 260 label: [3]\n",
      "row: 261 label: [3]\n",
      "row: 262 label: [3]\n",
      "row: 263 label: [3]\n",
      "row: 264 label: [3]\n",
      "row: 265 label: [3]\n",
      "row: 266 label: [3]\n",
      "row: 267 label: [3]\n",
      "row: 268 label: [3]\n",
      "row: 269 label: [3]\n",
      "row: 270 label: [3]\n",
      "row: 271 label: [3]\n",
      "row: 272 label: [3]\n",
      "row: 273 label: [4]\n",
      "row: 274 label: [4]\n",
      "row: 275 label: [4]\n",
      "row: 276 label: [4]\n",
      "row: 277 label: [4]\n",
      "row: 278 label: [4]\n",
      "row: 279 label: [4]\n",
      "row: 280 label: [4]\n",
      "row: 281 label: [4]\n",
      "row: 282 label: [4]\n",
      "row: 283 label: [4]\n",
      "row: 284 label: [4]\n",
      "row: 285 label: [4]\n",
      "row: 286 label: [4]\n",
      "row: 287 label: [4]\n",
      "row: 288 label: [4]\n",
      "row: 289 label: [4]\n",
      "row: 290 label: [4]\n",
      "row: 291 label: [4]\n",
      "row: 292 label: [4]\n",
      "row: 293 label: [4]\n",
      "row: 294 label: [4]\n",
      "row: 295 label: [4]\n",
      "row: 296 label: [5]\n",
      "row: 297 label: [5]\n",
      "row: 298 label: [5]\n",
      "row: 299 label: [5]\n",
      "row: 300 label: [5]\n",
      "row: 301 label: [5]\n",
      "row: 302 label: [5]\n",
      "row: 303 label: [5]\n",
      "row: 304 label: [5]\n",
      "row: 305 label: [5]\n",
      "row: 306 label: [5]\n",
      "row: 307 label: [5]\n",
      "row: 308 label: [5]\n",
      "row: 309 label: [5]\n",
      "row: 310 label: [5]\n",
      "row: 311 label: [5]\n",
      "row: 312 label: [5]\n",
      "row: 313 label: [5]\n",
      "row: 314 label: [5]\n",
      "row: 315 label: [5]\n",
      "row: 316 label: [5]\n",
      "row: 317 label: [5]\n",
      "row: 318 label: [5]\n",
      "row: 319 label: [5]\n",
      "row: 320 label: [5]\n",
      "row: 321 label: [5]\n",
      "row: 322 label: [5]\n",
      "row: 323 label: [5]\n",
      "row: 324 label: [5]\n",
      "row: 325 label: [5]\n",
      "row: 326 label: [6]\n",
      "row: 327 label: [6]\n",
      "row: 328 label: [6]\n",
      "row: 329 label: [6]\n",
      "row: 330 label: [6]\n",
      "row: 331 label: [6]\n",
      "row: 332 label: [6]\n",
      "row: 333 label: [6]\n",
      "row: 334 label: [6]\n",
      "row: 335 label: [6]\n",
      "row: 336 label: [6]\n",
      "row: 337 label: [6]\n",
      "row: 338 label: [6]\n",
      "row: 339 label: [6]\n",
      "row: 340 label: [6]\n",
      "row: 341 label: [6]\n",
      "row: 342 label: [6]\n",
      "row: 343 label: [6]\n",
      "row: 344 label: [6]\n",
      "row: 345 label: [6]\n",
      "row: 346 label: [6]\n",
      "row: 347 label: [6]\n",
      "row: 348 label: [6]\n",
      "row: 349 label: [6]\n",
      "row: 350 label: [6]\n",
      "row: 351 label: [6]\n",
      "row: 352 label: [6]\n",
      "row: 353 label: [6]\n",
      "row: 354 label: [6]\n",
      "row: 355 label: [6]\n",
      "row: 356 label: [6]\n",
      "row: 357 label: [6]\n",
      "row: 358 label: [6]\n",
      "row: 359 label: [6]\n",
      "row: 360 label: [6]\n",
      "row: 361 label: [6]\n",
      "row: 362 label: [6]\n",
      "row: 363 label: [6]\n",
      "row: 364 label: [6]\n",
      "row: 365 label: [6]\n",
      "row: 366 label: [6]\n",
      "row: 367 label: [6]\n",
      "row: 368 label: [6]\n",
      "row: 369 label: [6]\n",
      "row: 370 label: [6]\n",
      "row: 371 label: [6]\n",
      "row: 372 label: [6]\n",
      "row: 373 label: [6]\n",
      "row: 374 label: [6]\n",
      "row: 375 label: [6]\n",
      "row: 376 label: [6]\n",
      "row: 377 label: [6]\n",
      "row: 378 label: [6]\n",
      "row: 379 label: [6]\n",
      "row: 380 label: [6]\n",
      "row: 381 label: [6]\n",
      "row: 382 label: [6]\n",
      "row: 383 label: [6]\n",
      "row: 384 label: [6]\n",
      "row: 385 label: [6]\n",
      "row: 386 label: [6]\n",
      "row: 387 label: [6]\n",
      "row: 388 label: [6]\n",
      "row: 389 label: [6]\n",
      "row: 390 label: [6]\n",
      "row: 391 label: [6]\n",
      "row: 392 label: [6]\n",
      "row: 393 label: [6]\n",
      "row: 394 label: [6]\n",
      "row: 395 label: [6]\n",
      "row: 396 label: [6]\n",
      "row: 397 label: [6]\n",
      "row: 398 label: [6]\n",
      "row: 399 label: [6]\n",
      "row: 400 label: [6]\n",
      "row: 401 label: [6]\n",
      "row: 402 label: [6]\n",
      "row: 403 label: [6]\n",
      "row: 404 label: [6]\n",
      "row: 405 label: [6]\n",
      "row: 406 label: [6]\n",
      "row: 407 label: [6]\n",
      "row: 408 label: [6]\n",
      "row: 409 label: [6]\n",
      "row: 410 label: [6]\n",
      "row: 411 label: [6]\n",
      "row: 412 label: [6]\n",
      "row: 413 label: [6]\n",
      "row: 414 label: [6]\n",
      "row: 415 label: [6]\n",
      "row: 416 label: [6]\n",
      "row: 417 label: [6]\n",
      "row: 418 label: [6]\n",
      "row: 419 label: [6]\n",
      "row: 420 label: [6]\n",
      "row: 421 label: [6]\n",
      "row: 422 label: [6]\n",
      "row: 423 label: [6]\n",
      "row: 424 label: [6]\n",
      "row: 425 label: [6]\n",
      "row: 426 label: [6]\n",
      "row: 427 label: [6]\n",
      "row: 428 label: [6]\n",
      "row: 429 label: [6]\n",
      "row: 430 label: [6]\n",
      "row: 431 label: [6]\n",
      "row: 432 label: [6]\n",
      "row: 433 label: [6]\n",
      "row: 434 label: [6]\n",
      "row: 435 label: [6]\n",
      "row: 436 label: [6]\n",
      "row: 437 label: [6]\n",
      "row: 438 label: [6]\n",
      "row: 439 label: [6]\n",
      "row: 440 label: [6]\n",
      "row: 441 label: [6]\n",
      "row: 442 label: [6]\n",
      "row: 443 label: [6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row: 444 label: [6]\n",
      "row: 445 label: [6]\n",
      "row: 446 label: [6]\n",
      "row: 447 label: [6]\n",
      "row: 448 label: [6]\n",
      "row: 449 label: [6]\n",
      "row: 450 label: [6]\n",
      "row: 451 label: [6]\n",
      "row: 452 label: [6]\n",
      "row: 453 label: [6]\n",
      "row: 454 label: [6]\n",
      "row: 455 label: [6]\n",
      "row: 456 label: [6]\n",
      "row: 457 label: [6]\n",
      "row: 458 label: [6]\n",
      "row: 459 label: [6]\n",
      "row: 460 label: [6]\n",
      "row: 461 label: [6]\n",
      "row: 462 label: [6]\n",
      "row: 463 label: [6]\n",
      "row: 464 label: [6]\n",
      "row: 465 label: [6]\n",
      "row: 466 label: [6]\n",
      "row: 467 label: [6]\n",
      "row: 468 label: [6]\n",
      "row: 469 label: [6]\n",
      "row: 470 label: [6]\n",
      "row: 471 label: [6]\n",
      "row: 472 label: [6]\n",
      "row: 473 label: [6]\n",
      "row: 474 label: [6]\n",
      "row: 475 label: [6]\n",
      "row: 476 label: [6]\n",
      "row: 477 label: [6]\n",
      "row: 478 label: [6]\n",
      "row: 479 label: [7]\n",
      "row: 480 label: [7]\n",
      "row: 481 label: [7]\n",
      "row: 482 label: [7]\n",
      "row: 483 label: [7]\n",
      "row: 484 label: [7]\n",
      "row: 485 label: [7]\n",
      "row: 486 label: [7]\n",
      "row: 487 label: [7]\n",
      "row: 488 label: [7]\n",
      "row: 489 label: [7]\n",
      "row: 490 label: [7]\n",
      "row: 491 label: [7]\n",
      "row: 492 label: [7]\n",
      "row: 493 label: [7]\n",
      "row: 494 label: [7]\n",
      "row: 495 label: [7]\n",
      "row: 496 label: [7]\n",
      "row: 497 label: [7]\n",
      "row: 498 label: [7]\n",
      "row: 499 label: [7]\n",
      "row: 500 label: [7]\n",
      "row: 501 label: [7]\n",
      "row: 502 label: [7]\n",
      "row: 503 label: [7]\n",
      "row: 504 label: [7]\n",
      "row: 505 label: [7]\n",
      "row: 506 label: [7]\n",
      "row: 507 label: [7]\n",
      "row: 508 label: [7]\n",
      "row: 509 label: [7]\n",
      "row: 510 label: [7]\n",
      "row: 511 label: [7]\n",
      "row: 512 label: [7]\n",
      "row: 513 label: [7]\n",
      "row: 514 label: [7]\n",
      "row: 515 label: [7]\n",
      "row: 516 label: [8]\n",
      "row: 517 label: [8]\n",
      "row: 518 label: [8]\n",
      "row: 519 label: [8]\n",
      "row: 520 label: [8]\n",
      "row: 521 label: [8]\n",
      "row: 522 label: [8]\n",
      "row: 523 label: [8]\n",
      "row: 524 label: [8]\n",
      "row: 525 label: [8]\n",
      "row: 526 label: [8]\n",
      "row: 527 label: [8]\n",
      "row: 528 label: [8]\n",
      "row: 529 label: [8]\n",
      "row: 530 label: [8]\n",
      "row: 531 label: [8]\n",
      "row: 532 label: [8]\n",
      "row: 533 label: [8]\n",
      "row: 534 label: [8]\n",
      "row: 535 label: [8]\n",
      "row: 536 label: [8]\n",
      "row: 537 label: [8]\n",
      "row: 538 label: [8]\n",
      "row: 539 label: [8]\n",
      "row: 540 label: [8]\n",
      "row: 541 label: [8]\n",
      "row: 542 label: [8]\n",
      "row: 543 label: [8]\n",
      "row: 544 label: [8]\n",
      "row: 545 label: [8]\n",
      "row: 546 label: [8]\n",
      "row: 547 label: [8]\n",
      "row: 548 label: [8]\n",
      "row: 549 label: [8]\n",
      "row: 550 label: [8]\n",
      "row: 551 label: [8]\n",
      "row: 552 label: [8]\n",
      "row: 553 label: [8]\n",
      "row: 554 label: [8]\n",
      "row: 555 label: [8]\n",
      "row: 556 label: [8]\n",
      "row: 557 label: [8]\n",
      "row: 558 label: [8]\n",
      "row: 559 label: [8]\n",
      "row: 560 label: [8]\n",
      "row: 561 label: [8]\n",
      "row: 562 label: [8]\n",
      "row: 563 label: [8]\n",
      "row: 564 label: [8]\n",
      "row: 565 label: [8]\n",
      "row: 566 label: [8]\n",
      "row: 567 label: [8]\n",
      "row: 568 label: [8]\n",
      "row: 569 label: [8]\n",
      "row: 570 label: [8]\n",
      "row: 571 label: [8]\n",
      "row: 572 label: [8]\n",
      "row: 573 label: [8]\n",
      "row: 574 label: [8]\n",
      "row: 575 label: [8]\n",
      "row: 576 label: [8]\n",
      "row: 577 label: [8]\n",
      "row: 578 label: [8]\n",
      "row: 579 label: [8]\n",
      "row: 580 label: [8]\n",
      "row: 581 label: [8]\n",
      "row: 582 label: [8]\n",
      "row: 583 label: [8]\n",
      "row: 584 label: [8]\n",
      "row: 585 label: [8]\n",
      "row: 586 label: [8]\n",
      "row: 587 label: [8]\n",
      "row: 588 label: [8]\n",
      "row: 589 label: [8]\n",
      "row: 590 label: [8]\n",
      "row: 591 label: [8]\n",
      "row: 592 label: [8]\n",
      "row: 593 label: [8]\n",
      "row: 594 label: [8]\n",
      "row: 595 label: [8]\n",
      "row: 596 label: [8]\n",
      "row: 597 label: [8]\n",
      "row: 598 label: [8]\n",
      "row: 599 label: [8]\n",
      "row: 600 label: [8]\n",
      "row: 601 label: [8]\n",
      "row: 602 label: [8]\n",
      "label: [[0.82222223 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      " ...\n",
      " [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.82222223]]\n",
      "features: [[  0  35  45 ...  31  27  28]\n",
      " [  1  35  46 ...  30  27  28]\n",
      " [  2  35  41 ...  32  26  29]\n",
      " ...\n",
      " [601  36  38 ...  14  35  32]\n",
      " [602  32  42 ...  10  29  36]\n",
      " [603  37  46 ...  13  27  33]]\n"
     ]
    }
   ],
   "source": [
    "#from new_dataset_processed import FMData\n",
    "from dataset_process import FMData\n",
    "\n",
    "#train_dataset = FMData(config.train_libfm,config.train_label,nfm_config['n_class'])\n",
    "train_dataset = FMData(nfm_config['train_file'],nfm_config['label_file'],nfm_config['n_class'])\n",
    "train_loader = data.DataLoader(train_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "#validate_dataset = FMData(config.valid_libfm,config.valid_label)\n",
    "#validate_loader = data.DataLoader(train_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "#test_dataset = FMData(config.test_libfm,config.test_label)\n",
    "#test_loader = data.DataLoader(test_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f09b881d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nfm: NFM(\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (linear_model1): Linear(in_features=4225, out_features=1000, bias=True)\n",
      "  (BN_linear1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear_model2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (BN_linear2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (embedding_layers): Embedding(1001, 100)\n",
      "  (bi_pooling): BiInteractionPooling()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (BN_bi): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dnn_layers): ModuleList(\n",
      "    (0): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (1): Linear(in_features=100, out_features=9, bias=True)\n",
      "  )\n",
      "  (dnn_softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.16\n",
      "batch_idx: 1\n",
      "0.19\n",
      "batch_idx: 2\n",
      "0.21333333333333335\n",
      "batch_idx: 3\n",
      "0.25\n",
      "batch_idx: 4\n",
      "0.26\n",
      "batch_idx: 5\n",
      "0.27166666666666667\n",
      "Training Epoch: 0, total loss: 13.060806\n",
      "batch_idx: 0\n",
      "0.32\n",
      "batch_idx: 1\n",
      "0.34\n",
      "batch_idx: 2\n",
      "0.33666666666666667\n",
      "batch_idx: 3\n",
      "0.3375\n",
      "batch_idx: 4\n",
      "0.332\n",
      "batch_idx: 5\n",
      "0.3333333333333333\n",
      "Training Epoch: 1, total loss: 12.738219\n",
      "batch_idx: 0\n",
      "0.41\n",
      "batch_idx: 1\n",
      "0.385\n",
      "batch_idx: 2\n",
      "0.38333333333333336\n",
      "batch_idx: 3\n",
      "0.365\n",
      "batch_idx: 4\n",
      "0.344\n",
      "batch_idx: 5\n",
      "0.33\n",
      "Training Epoch: 2, total loss: 12.514790\n",
      "batch_idx: 0\n",
      "0.38\n",
      "batch_idx: 1\n",
      "0.37\n",
      "batch_idx: 2\n",
      "0.3433333333333333\n",
      "batch_idx: 3\n",
      "0.3375\n",
      "batch_idx: 4\n",
      "0.336\n",
      "batch_idx: 5\n",
      "0.3383333333333333\n",
      "Training Epoch: 3, total loss: 12.403606\n",
      "batch_idx: 0\n",
      "0.31\n",
      "batch_idx: 1\n",
      "0.32\n",
      "batch_idx: 2\n",
      "0.33666666666666667\n",
      "batch_idx: 3\n",
      "0.3325\n",
      "batch_idx: 4\n",
      "0.342\n",
      "batch_idx: 5\n",
      "0.3416666666666667\n",
      "Training Epoch: 4, total loss: 12.279730\n",
      "batch_idx: 0\n",
      "0.33\n",
      "batch_idx: 1\n",
      "0.36\n",
      "batch_idx: 2\n",
      "0.37666666666666665\n",
      "batch_idx: 3\n",
      "0.3775\n",
      "batch_idx: 4\n",
      "0.374\n",
      "batch_idx: 5\n",
      "0.395\n",
      "Training Epoch: 5, total loss: 12.143439\n",
      "batch_idx: 0\n",
      "0.55\n",
      "batch_idx: 1\n",
      "0.545\n",
      "batch_idx: 2\n",
      "0.51\n",
      "batch_idx: 3\n",
      "0.5025\n",
      "batch_idx: 4\n",
      "0.51\n",
      "batch_idx: 5\n",
      "0.5216666666666666\n",
      "Training Epoch: 6, total loss: 11.864643\n",
      "batch_idx: 0\n",
      "0.53\n",
      "batch_idx: 1\n",
      "0.54\n",
      "batch_idx: 2\n",
      "0.5433333333333333\n",
      "batch_idx: 3\n",
      "0.545\n",
      "batch_idx: 4\n",
      "0.568\n",
      "batch_idx: 5\n",
      "0.5766666666666667\n",
      "Training Epoch: 7, total loss: 11.520071\n",
      "batch_idx: 0\n",
      "0.53\n",
      "batch_idx: 1\n",
      "0.56\n",
      "batch_idx: 2\n",
      "0.5533333333333333\n",
      "batch_idx: 3\n",
      "0.575\n",
      "batch_idx: 4\n",
      "0.598\n",
      "batch_idx: 5\n",
      "0.5916666666666667\n",
      "Training Epoch: 8, total loss: 11.337448\n",
      "batch_idx: 0\n",
      "0.54\n",
      "batch_idx: 1\n",
      "0.545\n",
      "batch_idx: 2\n",
      "0.5933333333333334\n",
      "batch_idx: 3\n",
      "0.59\n",
      "batch_idx: 4\n",
      "0.592\n",
      "batch_idx: 5\n",
      "0.5816666666666667\n",
      "Training Epoch: 9, total loss: 11.288629\n",
      "batch_idx: 0\n",
      "0.59\n",
      "batch_idx: 1\n",
      "0.57\n",
      "batch_idx: 2\n",
      "0.5733333333333334\n",
      "batch_idx: 3\n",
      "0.605\n",
      "batch_idx: 4\n",
      "0.594\n",
      "batch_idx: 5\n",
      "0.6033333333333334\n",
      "Training Epoch: 10, total loss: 11.197091\n",
      "batch_idx: 0\n",
      "0.61\n",
      "batch_idx: 1\n",
      "0.6\n",
      "batch_idx: 2\n",
      "0.6133333333333333\n",
      "batch_idx: 3\n",
      "0.6175\n",
      "batch_idx: 4\n",
      "0.614\n",
      "batch_idx: 5\n",
      "0.615\n",
      "Training Epoch: 11, total loss: 11.178310\n",
      "batch_idx: 0\n",
      "0.6\n",
      "batch_idx: 1\n",
      "0.6\n",
      "batch_idx: 2\n",
      "0.58\n",
      "batch_idx: 3\n",
      "0.5975\n",
      "batch_idx: 4\n",
      "0.596\n",
      "batch_idx: 5\n",
      "0.5983333333333334\n",
      "Training Epoch: 12, total loss: 11.206364\n",
      "batch_idx: 0\n",
      "0.64\n",
      "batch_idx: 1\n",
      "0.55\n",
      "batch_idx: 2\n",
      "0.5433333333333333\n",
      "batch_idx: 3\n",
      "0.5625\n",
      "batch_idx: 4\n",
      "0.582\n",
      "batch_idx: 5\n",
      "0.6016666666666667\n",
      "Training Epoch: 13, total loss: 11.193061\n",
      "batch_idx: 0\n",
      "0.61\n",
      "batch_idx: 1\n",
      "0.615\n",
      "batch_idx: 2\n",
      "0.6266666666666667\n",
      "batch_idx: 3\n",
      "0.64\n",
      "batch_idx: 4\n",
      "0.646\n",
      "batch_idx: 5\n",
      "0.6416666666666667\n",
      "Training Epoch: 14, total loss: 11.085431\n",
      "batch_idx: 0\n",
      "0.63\n",
      "batch_idx: 1\n",
      "0.655\n",
      "batch_idx: 2\n",
      "0.6466666666666666\n",
      "batch_idx: 3\n",
      "0.625\n",
      "batch_idx: 4\n",
      "0.634\n",
      "batch_idx: 5\n",
      "0.6416666666666667\n",
      "Training Epoch: 15, total loss: 11.056747\n",
      "batch_idx: 0\n",
      "0.63\n",
      "batch_idx: 1\n",
      "0.62\n",
      "batch_idx: 2\n",
      "0.64\n",
      "batch_idx: 3\n",
      "0.645\n",
      "batch_idx: 4\n",
      "0.642\n",
      "batch_idx: 5\n",
      "0.65\n",
      "Training Epoch: 16, total loss: 11.042400\n",
      "batch_idx: 0\n",
      "0.65\n",
      "batch_idx: 1\n",
      "0.64\n",
      "batch_idx: 2\n",
      "0.63\n",
      "batch_idx: 3\n",
      "0.63\n",
      "batch_idx: 4\n",
      "0.65\n",
      "batch_idx: 5\n",
      "0.64\n",
      "Training Epoch: 17, total loss: 11.052306\n",
      "batch_idx: 0\n",
      "0.67\n",
      "batch_idx: 1\n",
      "0.665\n",
      "batch_idx: 2\n",
      "0.6533333333333333\n",
      "batch_idx: 3\n",
      "0.6475\n",
      "batch_idx: 4\n",
      "0.654\n",
      "batch_idx: 5\n",
      "0.6616666666666666\n",
      "Training Epoch: 18, total loss: 10.955853\n",
      "batch_idx: 0\n",
      "0.6\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6366666666666667\n",
      "batch_idx: 3\n",
      "0.625\n",
      "batch_idx: 4\n",
      "0.636\n",
      "batch_idx: 5\n",
      "0.6533333333333333\n",
      "Training Epoch: 19, total loss: 10.972201\n",
      "batch_idx: 0\n",
      "0.7\n",
      "batch_idx: 1\n",
      "0.68\n",
      "batch_idx: 2\n",
      "0.6833333333333333\n",
      "batch_idx: 3\n",
      "0.675\n",
      "batch_idx: 4\n",
      "0.672\n",
      "batch_idx: 5\n",
      "0.6633333333333333\n",
      "Training Epoch: 20, total loss: 10.952485\n",
      "batch_idx: 0\n",
      "0.63\n",
      "batch_idx: 1\n",
      "0.66\n",
      "batch_idx: 2\n",
      "0.6733333333333333\n",
      "batch_idx: 3\n",
      "0.6525\n",
      "batch_idx: 4\n",
      "0.654\n",
      "batch_idx: 5\n",
      "0.655\n",
      "Training Epoch: 21, total loss: 10.963440\n",
      "batch_idx: 0\n",
      "0.58\n",
      "batch_idx: 1\n",
      "0.605\n",
      "batch_idx: 2\n",
      "0.6533333333333333\n",
      "batch_idx: 3\n",
      "0.6775\n",
      "batch_idx: 4\n",
      "0.684\n",
      "batch_idx: 5\n",
      "0.695\n",
      "Training Epoch: 22, total loss: 10.889970\n",
      "batch_idx: 0\n",
      "0.7\n",
      "batch_idx: 1\n",
      "0.71\n",
      "batch_idx: 2\n",
      "0.6966666666666667\n",
      "batch_idx: 3\n",
      "0.69\n",
      "batch_idx: 4\n",
      "0.668\n",
      "batch_idx: 5\n",
      "0.6783333333333333\n",
      "Training Epoch: 23, total loss: 10.930030\n",
      "batch_idx: 0\n",
      "0.65\n",
      "batch_idx: 1\n",
      "0.685\n",
      "batch_idx: 2\n",
      "0.6733333333333333\n",
      "batch_idx: 3\n",
      "0.6825\n",
      "batch_idx: 4\n",
      "0.684\n",
      "batch_idx: 5\n",
      "0.6866666666666666\n",
      "Training Epoch: 24, total loss: 10.838794\n",
      "batch_idx: 0\n",
      "0.72\n",
      "batch_idx: 1\n",
      "0.69\n",
      "batch_idx: 2\n",
      "0.6966666666666667\n",
      "batch_idx: 3\n",
      "0.705\n",
      "batch_idx: 4\n",
      "0.702\n",
      "batch_idx: 5\n",
      "0.7133333333333334\n",
      "Training Epoch: 25, total loss: 10.762823\n",
      "batch_idx: 0\n",
      "0.72\n",
      "batch_idx: 1\n",
      "0.76\n",
      "batch_idx: 2\n",
      "0.7466666666666667\n",
      "batch_idx: 3\n",
      "0.7275\n",
      "batch_idx: 4\n",
      "0.728\n",
      "batch_idx: 5\n",
      "0.72\n",
      "Training Epoch: 26, total loss: 10.745891\n",
      "batch_idx: 0\n",
      "0.81\n",
      "batch_idx: 1\n",
      "0.775\n",
      "batch_idx: 2\n",
      "0.7366666666666667\n",
      "batch_idx: 3\n",
      "0.715\n",
      "batch_idx: 4\n",
      "0.716\n",
      "batch_idx: 5\n",
      "0.7216666666666667\n",
      "Training Epoch: 27, total loss: 10.683900\n",
      "batch_idx: 0\n",
      "0.64\n",
      "batch_idx: 1\n",
      "0.655\n",
      "batch_idx: 2\n",
      "0.69\n",
      "batch_idx: 3\n",
      "0.6975\n",
      "batch_idx: 4\n",
      "0.696\n",
      "batch_idx: 5\n",
      "0.7016666666666667\n",
      "Training Epoch: 28, total loss: 10.800201\n",
      "batch_idx: 0\n",
      "0.81\n",
      "batch_idx: 1\n",
      "0.76\n",
      "batch_idx: 2\n",
      "0.7133333333333334\n",
      "batch_idx: 3\n",
      "0.7075\n",
      "batch_idx: 4\n",
      "0.718\n",
      "batch_idx: 5\n",
      "0.71\n",
      "Training Epoch: 29, total loss: 10.744527\n",
      "batch_idx: 0\n",
      "0.78\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7333333333333333\n",
      "batch_idx: 3\n",
      "0.71\n",
      "batch_idx: 4\n",
      "0.706\n",
      "batch_idx: 5\n",
      "0.6966666666666667\n",
      "Training Epoch: 30, total loss: 10.790212\n",
      "batch_idx: 0\n",
      "0.73\n",
      "batch_idx: 1\n",
      "0.735\n",
      "batch_idx: 2\n",
      "0.73\n",
      "batch_idx: 3\n",
      "0.745\n",
      "batch_idx: 4\n",
      "0.736\n",
      "batch_idx: 5\n",
      "0.7216666666666667\n",
      "Training Epoch: 31, total loss: 10.697271\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.775\n",
      "batch_idx: 2\n",
      "0.77\n",
      "batch_idx: 3\n",
      "0.7725\n",
      "batch_idx: 4\n",
      "0.764\n",
      "batch_idx: 5\n",
      "0.7683333333333333\n",
      "Training Epoch: 32, total loss: 10.479445\n",
      "batch_idx: 0\n",
      "0.81\n",
      "batch_idx: 1\n",
      "0.775\n",
      "batch_idx: 2\n",
      "0.7733333333333333\n",
      "batch_idx: 3\n",
      "0.765\n",
      "batch_idx: 4\n",
      "0.774\n",
      "batch_idx: 5\n",
      "0.77\n",
      "Training Epoch: 33, total loss: 10.500987\n",
      "batch_idx: 0\n",
      "0.79\n",
      "batch_idx: 1\n",
      "0.77\n",
      "batch_idx: 2\n",
      "0.7933333333333333\n",
      "batch_idx: 3\n",
      "0.79\n",
      "batch_idx: 4\n",
      "0.796\n",
      "batch_idx: 5\n",
      "0.795\n",
      "Training Epoch: 34, total loss: 10.424607\n",
      "batch_idx: 0\n",
      "0.71\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.76\n",
      "batch_idx: 3\n",
      "0.78\n",
      "batch_idx: 4\n",
      "0.792\n",
      "batch_idx: 5\n",
      "0.7966666666666666\n",
      "Training Epoch: 35, total loss: 10.380259\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.855\n",
      "batch_idx: 2\n",
      "0.8466666666666667\n",
      "batch_idx: 3\n",
      "0.8075\n",
      "batch_idx: 4\n",
      "0.804\n",
      "batch_idx: 5\n",
      "0.8016666666666666\n",
      "Training Epoch: 36, total loss: 10.323030\n",
      "batch_idx: 0\n",
      "0.8\n",
      "batch_idx: 1\n",
      "0.825\n",
      "batch_idx: 2\n",
      "0.8166666666666667\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.816\n",
      "batch_idx: 5\n",
      "0.8166666666666667\n",
      "Training Epoch: 37, total loss: 10.252122\n",
      "batch_idx: 0\n",
      "0.81\n",
      "batch_idx: 1\n",
      "0.835\n",
      "batch_idx: 2\n",
      "0.8033333333333333\n",
      "batch_idx: 3\n",
      "0.8025\n",
      "batch_idx: 4\n",
      "0.788\n",
      "batch_idx: 5\n",
      "0.8066666666666666\n",
      "Training Epoch: 38, total loss: 10.301769\n",
      "batch_idx: 0\n",
      "0.81\n",
      "batch_idx: 1\n",
      "0.835\n",
      "batch_idx: 2\n",
      "0.8066666666666666\n",
      "batch_idx: 3\n",
      "0.8025\n",
      "batch_idx: 4\n",
      "0.82\n",
      "batch_idx: 5\n",
      "0.8133333333333334\n",
      "Training Epoch: 39, total loss: 10.276756\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.84\n",
      "batch_idx: 2\n",
      "0.81\n",
      "batch_idx: 3\n",
      "0.7875\n",
      "batch_idx: 4\n",
      "0.802\n",
      "batch_idx: 5\n",
      "0.8083333333333333\n",
      "Training Epoch: 40, total loss: 10.275363\n",
      "batch_idx: 0\n",
      "0.82\n",
      "batch_idx: 1\n",
      "0.8\n",
      "batch_idx: 2\n",
      "0.8133333333333334\n",
      "batch_idx: 3\n",
      "0.81\n",
      "batch_idx: 4\n",
      "0.81\n",
      "batch_idx: 5\n",
      "0.815\n",
      "Training Epoch: 41, total loss: 10.237622\n",
      "batch_idx: 0\n",
      "0.83\n",
      "batch_idx: 1\n",
      "0.84\n",
      "batch_idx: 2\n",
      "0.8433333333333334\n",
      "batch_idx: 3\n",
      "0.825\n",
      "batch_idx: 4\n",
      "0.824\n",
      "batch_idx: 5\n",
      "0.82\n",
      "Training Epoch: 42, total loss: 10.218614\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.82\n",
      "batch_idx: 2\n",
      "0.8266666666666667\n",
      "batch_idx: 3\n",
      "0.8375\n",
      "batch_idx: 4\n",
      "0.836\n",
      "batch_idx: 5\n",
      "0.8316666666666667\n",
      "Training Epoch: 43, total loss: 10.136854\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.84\n",
      "batch_idx: 2\n",
      "0.82\n",
      "batch_idx: 3\n",
      "0.8175\n",
      "batch_idx: 4\n",
      "0.828\n",
      "batch_idx: 5\n",
      "0.8383333333333334\n",
      "Training Epoch: 44, total loss: 10.174664\n",
      "batch_idx: 0\n",
      "0.77\n",
      "batch_idx: 1\n",
      "0.805\n",
      "batch_idx: 2\n",
      "0.84\n",
      "batch_idx: 3\n",
      "0.835\n",
      "batch_idx: 4\n",
      "0.842\n",
      "batch_idx: 5\n",
      "0.8383333333333334\n",
      "Training Epoch: 45, total loss: 10.164557\n",
      "batch_idx: 0\n",
      "0.8\n",
      "batch_idx: 1\n",
      "0.83\n",
      "batch_idx: 2\n",
      "0.83\n",
      "batch_idx: 3\n",
      "0.8275\n",
      "batch_idx: 4\n",
      "0.824\n",
      "batch_idx: 5\n",
      "0.8366666666666667\n",
      "Training Epoch: 46, total loss: 10.147870\n",
      "batch_idx: 0\n",
      "0.82\n",
      "batch_idx: 1\n",
      "0.815\n",
      "batch_idx: 2\n",
      "0.7933333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 3\n",
      "0.82\n",
      "batch_idx: 4\n",
      "0.822\n",
      "batch_idx: 5\n",
      "0.83\n",
      "Training Epoch: 47, total loss: 10.181272\n",
      "batch_idx: 0\n",
      "0.81\n",
      "batch_idx: 1\n",
      "0.82\n",
      "batch_idx: 2\n",
      "0.8\n",
      "batch_idx: 3\n",
      "0.8075\n",
      "batch_idx: 4\n",
      "0.818\n",
      "batch_idx: 5\n",
      "0.8216666666666667\n",
      "Training Epoch: 48, total loss: 10.203150\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.8\n",
      "batch_idx: 2\n",
      "0.8266666666666667\n",
      "batch_idx: 3\n",
      "0.8325\n",
      "batch_idx: 4\n",
      "0.838\n",
      "batch_idx: 5\n",
      "0.8316666666666667\n",
      "Training Epoch: 49, total loss: 10.194099\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.85\n",
      "batch_idx: 2\n",
      "0.83\n",
      "batch_idx: 3\n",
      "0.8325\n",
      "batch_idx: 4\n",
      "0.824\n",
      "batch_idx: 5\n",
      "0.815\n",
      "Training Epoch: 50, total loss: 10.195781\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8433333333333334\n",
      "batch_idx: 3\n",
      "0.84\n",
      "batch_idx: 4\n",
      "0.844\n",
      "batch_idx: 5\n",
      "0.8466666666666667\n",
      "Training Epoch: 51, total loss: 10.123291\n",
      "batch_idx: 0\n",
      "0.83\n",
      "batch_idx: 1\n",
      "0.815\n",
      "batch_idx: 2\n",
      "0.8233333333333334\n",
      "batch_idx: 3\n",
      "0.8425\n",
      "batch_idx: 4\n",
      "0.836\n",
      "batch_idx: 5\n",
      "0.8416666666666667\n",
      "Training Epoch: 52, total loss: 10.144822\n",
      "batch_idx: 0\n",
      "0.83\n",
      "batch_idx: 1\n",
      "0.855\n",
      "batch_idx: 2\n",
      "0.8533333333333334\n",
      "batch_idx: 3\n",
      "0.8475\n",
      "batch_idx: 4\n",
      "0.846\n",
      "batch_idx: 5\n",
      "0.8483333333333334\n",
      "Training Epoch: 53, total loss: 10.084977\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.84\n",
      "batch_idx: 3\n",
      "0.835\n",
      "batch_idx: 4\n",
      "0.83\n",
      "batch_idx: 5\n",
      "0.8283333333333334\n",
      "Training Epoch: 54, total loss: 10.146230\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.8533333333333334\n",
      "batch_idx: 3\n",
      "0.84\n",
      "batch_idx: 4\n",
      "0.846\n",
      "batch_idx: 5\n",
      "0.8383333333333334\n",
      "Training Epoch: 55, total loss: 10.121052\n",
      "batch_idx: 0\n",
      "0.83\n",
      "batch_idx: 1\n",
      "0.82\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.845\n",
      "batch_idx: 4\n",
      "0.836\n",
      "batch_idx: 5\n",
      "0.84\n",
      "Training Epoch: 56, total loss: 10.093935\n",
      "batch_idx: 0\n",
      "0.81\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.855\n",
      "batch_idx: 4\n",
      "0.854\n",
      "batch_idx: 5\n",
      "0.8466666666666667\n",
      "Training Epoch: 57, total loss: 10.068259\n",
      "batch_idx: 0\n",
      "0.81\n",
      "batch_idx: 1\n",
      "0.805\n",
      "batch_idx: 2\n",
      "0.8\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.818\n",
      "batch_idx: 5\n",
      "0.8166666666666667\n",
      "Training Epoch: 58, total loss: 10.197084\n",
      "batch_idx: 0\n",
      "0.8\n",
      "batch_idx: 1\n",
      "0.81\n",
      "batch_idx: 2\n",
      "0.8066666666666666\n",
      "batch_idx: 3\n",
      "0.8025\n",
      "batch_idx: 4\n",
      "0.802\n",
      "batch_idx: 5\n",
      "0.815\n",
      "Training Epoch: 59, total loss: 10.191514\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.85\n",
      "batch_idx: 2\n",
      "0.8566666666666667\n",
      "batch_idx: 3\n",
      "0.8575\n",
      "batch_idx: 4\n",
      "0.856\n",
      "batch_idx: 5\n",
      "0.8366666666666667\n",
      "Training Epoch: 60, total loss: 10.117864\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.85\n",
      "batch_idx: 3\n",
      "0.8525\n",
      "batch_idx: 4\n",
      "0.854\n",
      "batch_idx: 5\n",
      "0.8433333333333334\n",
      "Training Epoch: 61, total loss: 10.055839\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.805\n",
      "batch_idx: 2\n",
      "0.8233333333333334\n",
      "batch_idx: 3\n",
      "0.8275\n",
      "batch_idx: 4\n",
      "0.82\n",
      "batch_idx: 5\n",
      "0.8283333333333334\n",
      "Training Epoch: 62, total loss: 10.138070\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.855\n",
      "batch_idx: 2\n",
      "0.8366666666666667\n",
      "batch_idx: 3\n",
      "0.8475\n",
      "batch_idx: 4\n",
      "0.854\n",
      "batch_idx: 5\n",
      "0.8516666666666667\n",
      "Training Epoch: 63, total loss: 10.053632\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.835\n",
      "batch_idx: 2\n",
      "0.8566666666666667\n",
      "batch_idx: 3\n",
      "0.85\n",
      "batch_idx: 4\n",
      "0.842\n",
      "batch_idx: 5\n",
      "0.8466666666666667\n",
      "Training Epoch: 64, total loss: 10.105075\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.82\n",
      "batch_idx: 2\n",
      "0.8466666666666667\n",
      "batch_idx: 3\n",
      "0.8475\n",
      "batch_idx: 4\n",
      "0.854\n",
      "batch_idx: 5\n",
      "0.85\n",
      "Training Epoch: 65, total loss: 10.048190\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.83\n",
      "batch_idx: 2\n",
      "0.85\n",
      "batch_idx: 3\n",
      "0.8575\n",
      "batch_idx: 4\n",
      "0.86\n",
      "batch_idx: 5\n",
      "0.8633333333333333\n",
      "Training Epoch: 66, total loss: 10.015105\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8633333333333333\n",
      "batch_idx: 3\n",
      "0.8675\n",
      "batch_idx: 4\n",
      "0.874\n",
      "batch_idx: 5\n",
      "0.8616666666666667\n",
      "Training Epoch: 67, total loss: 10.020991\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.86\n",
      "batch_idx: 4\n",
      "0.876\n",
      "batch_idx: 5\n",
      "0.86\n",
      "Training Epoch: 68, total loss: 10.018433\n",
      "batch_idx: 0\n",
      "0.83\n",
      "batch_idx: 1\n",
      "0.83\n",
      "batch_idx: 2\n",
      "0.83\n",
      "batch_idx: 3\n",
      "0.8475\n",
      "batch_idx: 4\n",
      "0.842\n",
      "batch_idx: 5\n",
      "0.85\n",
      "Training Epoch: 69, total loss: 10.060218\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.85\n",
      "batch_idx: 2\n",
      "0.8666666666666667\n",
      "batch_idx: 3\n",
      "0.86\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8483333333333334\n",
      "Training Epoch: 70, total loss: 10.042676\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.855\n",
      "batch_idx: 2\n",
      "0.8566666666666667\n",
      "batch_idx: 3\n",
      "0.85\n",
      "batch_idx: 4\n",
      "0.842\n",
      "batch_idx: 5\n",
      "0.84\n",
      "Training Epoch: 71, total loss: 10.098237\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.8533333333333334\n",
      "batch_idx: 3\n",
      "0.84\n",
      "batch_idx: 4\n",
      "0.844\n",
      "batch_idx: 5\n",
      "0.8483333333333334\n",
      "Training Epoch: 72, total loss: 10.059709\n",
      "batch_idx: 0\n",
      "0.77\n",
      "batch_idx: 1\n",
      "0.795\n",
      "batch_idx: 2\n",
      "0.81\n",
      "batch_idx: 3\n",
      "0.8225\n",
      "batch_idx: 4\n",
      "0.84\n",
      "batch_idx: 5\n",
      "0.8416666666666667\n",
      "Training Epoch: 73, total loss: 10.086830\n",
      "batch_idx: 0\n",
      "0.78\n",
      "batch_idx: 1\n",
      "0.8\n",
      "batch_idx: 2\n",
      "0.8033333333333333\n",
      "batch_idx: 3\n",
      "0.815\n",
      "batch_idx: 4\n",
      "0.83\n",
      "batch_idx: 5\n",
      "0.8383333333333334\n",
      "Training Epoch: 74, total loss: 10.087560\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.835\n",
      "batch_idx: 2\n",
      "0.82\n",
      "batch_idx: 3\n",
      "0.855\n",
      "batch_idx: 4\n",
      "0.848\n",
      "batch_idx: 5\n",
      "0.845\n",
      "Training Epoch: 75, total loss: 10.047008\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.86\n",
      "batch_idx: 2\n",
      "0.8566666666666667\n",
      "batch_idx: 3\n",
      "0.85\n",
      "batch_idx: 4\n",
      "0.854\n",
      "batch_idx: 5\n",
      "0.8516666666666667\n",
      "Training Epoch: 76, total loss: 10.057368\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.855\n",
      "batch_idx: 2\n",
      "0.86\n",
      "batch_idx: 3\n",
      "0.85\n",
      "batch_idx: 4\n",
      "0.848\n",
      "batch_idx: 5\n",
      "0.8466666666666667\n",
      "Training Epoch: 77, total loss: 10.083795\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.84\n",
      "batch_idx: 2\n",
      "0.8266666666666667\n",
      "batch_idx: 3\n",
      "0.8275\n",
      "batch_idx: 4\n",
      "0.836\n",
      "batch_idx: 5\n",
      "0.8416666666666667\n",
      "Training Epoch: 78, total loss: 10.066673\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.866\n",
      "batch_idx: 5\n",
      "0.8683333333333333\n",
      "Training Epoch: 79, total loss: 9.977902\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.86\n",
      "batch_idx: 2\n",
      "0.8666666666666667\n",
      "batch_idx: 3\n",
      "0.86\n",
      "batch_idx: 4\n",
      "0.864\n",
      "batch_idx: 5\n",
      "0.8666666666666667\n",
      "Training Epoch: 80, total loss: 10.001404\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.8566666666666667\n",
      "batch_idx: 3\n",
      "0.87\n",
      "batch_idx: 4\n",
      "0.862\n",
      "batch_idx: 5\n",
      "0.8616666666666667\n",
      "Training Epoch: 81, total loss: 9.997385\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.8866666666666667\n",
      "batch_idx: 3\n",
      "0.8775\n",
      "batch_idx: 4\n",
      "0.87\n",
      "batch_idx: 5\n",
      "0.87\n",
      "Training Epoch: 82, total loss: 9.956862\n",
      "batch_idx: 0\n",
      "0.77\n",
      "batch_idx: 1\n",
      "0.82\n",
      "batch_idx: 2\n",
      "0.8266666666666667\n",
      "batch_idx: 3\n",
      "0.85\n",
      "batch_idx: 4\n",
      "0.86\n",
      "batch_idx: 5\n",
      "0.8666666666666667\n",
      "Training Epoch: 83, total loss: 9.969034\n",
      "batch_idx: 0\n",
      "0.71\n",
      "batch_idx: 1\n",
      "0.755\n",
      "batch_idx: 2\n",
      "0.7666666666666667\n",
      "batch_idx: 3\n",
      "0.8025\n",
      "batch_idx: 4\n",
      "0.826\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "Training Epoch: 84, total loss: 10.114095\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.8533333333333334\n",
      "batch_idx: 3\n",
      "0.845\n",
      "batch_idx: 4\n",
      "0.856\n",
      "batch_idx: 5\n",
      "0.8616666666666667\n",
      "Training Epoch: 85, total loss: 9.999857\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.8775\n",
      "batch_idx: 4\n",
      "0.87\n",
      "batch_idx: 5\n",
      "0.8716666666666667\n",
      "Training Epoch: 86, total loss: 9.964275\n",
      "batch_idx: 0\n",
      "0.82\n",
      "batch_idx: 1\n",
      "0.855\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.8725\n",
      "batch_idx: 4\n",
      "0.876\n",
      "batch_idx: 5\n",
      "0.8683333333333333\n",
      "Training Epoch: 87, total loss: 9.970850\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.855\n",
      "batch_idx: 2\n",
      "0.8633333333333333\n",
      "batch_idx: 3\n",
      "0.8625\n",
      "batch_idx: 4\n",
      "0.86\n",
      "batch_idx: 5\n",
      "0.865\n",
      "Training Epoch: 88, total loss: 9.984071\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8733333333333333\n",
      "batch_idx: 3\n",
      "0.8875\n",
      "batch_idx: 4\n",
      "0.884\n",
      "batch_idx: 5\n",
      "0.8766666666666667\n",
      "Training Epoch: 89, total loss: 9.930221\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.88\n",
      "batch_idx: 4\n",
      "0.864\n",
      "batch_idx: 5\n",
      "0.86\n",
      "Training Epoch: 90, total loss: 9.997524\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.855\n",
      "batch_idx: 2\n",
      "0.8533333333333334\n",
      "batch_idx: 3\n",
      "0.86\n",
      "batch_idx: 4\n",
      "0.868\n",
      "batch_idx: 5\n",
      "0.875\n",
      "Training Epoch: 91, total loss: 9.939278\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.855\n",
      "batch_idx: 2\n",
      "0.8566666666666667\n",
      "batch_idx: 3\n",
      "0.8625\n",
      "batch_idx: 4\n",
      "0.854\n",
      "batch_idx: 5\n",
      "0.8516666666666667\n",
      "Training Epoch: 92, total loss: 10.036072\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8566666666666667\n",
      "batch_idx: 3\n",
      "0.8425\n",
      "batch_idx: 4\n",
      "0.852\n",
      "batch_idx: 5\n",
      "0.855\n",
      "Training Epoch: 93, total loss: 9.997777\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.87\n",
      "batch_idx: 4\n",
      "0.864\n",
      "batch_idx: 5\n",
      "0.8666666666666667\n",
      "Training Epoch: 94, total loss: 9.967979\n",
      "batch_idx: 0\n",
      "0.82\n",
      "batch_idx: 1\n",
      "0.835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 2\n",
      "0.84\n",
      "batch_idx: 3\n",
      "0.845\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8566666666666667\n",
      "Training Epoch: 95, total loss: 10.014199\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.8766666666666667\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.87\n",
      "batch_idx: 5\n",
      "0.8666666666666667\n",
      "Training Epoch: 96, total loss: 9.955601\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.835\n",
      "batch_idx: 2\n",
      "0.8533333333333334\n",
      "batch_idx: 3\n",
      "0.8675\n",
      "batch_idx: 4\n",
      "0.876\n",
      "batch_idx: 5\n",
      "0.8666666666666667\n",
      "Training Epoch: 97, total loss: 9.949980\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.86\n",
      "batch_idx: 2\n",
      "0.8566666666666667\n",
      "batch_idx: 3\n",
      "0.8675\n",
      "batch_idx: 4\n",
      "0.868\n",
      "batch_idx: 5\n",
      "0.865\n",
      "Training Epoch: 98, total loss: 9.955297\n",
      "batch_idx: 0\n",
      "0.82\n",
      "batch_idx: 1\n",
      "0.83\n",
      "batch_idx: 2\n",
      "0.8466666666666667\n",
      "batch_idx: 3\n",
      "0.85\n",
      "batch_idx: 4\n",
      "0.852\n",
      "batch_idx: 5\n",
      "0.86\n",
      "Training Epoch: 99, total loss: 10.015342\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.8633333333333333\n",
      "batch_idx: 3\n",
      "0.8675\n",
      "batch_idx: 4\n",
      "0.862\n",
      "batch_idx: 5\n",
      "0.8633333333333333\n",
      "Training Epoch: 100, total loss: 9.981070\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.855\n",
      "batch_idx: 2\n",
      "0.8466666666666667\n",
      "batch_idx: 3\n",
      "0.845\n",
      "batch_idx: 4\n",
      "0.852\n",
      "batch_idx: 5\n",
      "0.8616666666666667\n",
      "Training Epoch: 101, total loss: 9.986322\n",
      "batch_idx: 0\n",
      "0.83\n",
      "batch_idx: 1\n",
      "0.815\n",
      "batch_idx: 2\n",
      "0.8066666666666666\n",
      "batch_idx: 3\n",
      "0.8225\n",
      "batch_idx: 4\n",
      "0.828\n",
      "batch_idx: 5\n",
      "0.825\n",
      "Training Epoch: 102, total loss: 10.130275\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.86\n",
      "batch_idx: 2\n",
      "0.8566666666666667\n",
      "batch_idx: 3\n",
      "0.8525\n",
      "batch_idx: 4\n",
      "0.852\n",
      "batch_idx: 5\n",
      "0.8433333333333334\n",
      "Training Epoch: 103, total loss: 10.063429\n",
      "batch_idx: 0\n",
      "0.81\n",
      "batch_idx: 1\n",
      "0.85\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.8675\n",
      "batch_idx: 4\n",
      "0.876\n",
      "batch_idx: 5\n",
      "0.8683333333333333\n",
      "Training Epoch: 104, total loss: 9.970656\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.8866666666666667\n",
      "batch_idx: 3\n",
      "0.8675\n",
      "batch_idx: 4\n",
      "0.854\n",
      "batch_idx: 5\n",
      "0.85\n",
      "Training Epoch: 105, total loss: 10.031628\n",
      "batch_idx: 0\n",
      "0.81\n",
      "batch_idx: 1\n",
      "0.82\n",
      "batch_idx: 2\n",
      "0.82\n",
      "batch_idx: 3\n",
      "0.8325\n",
      "batch_idx: 4\n",
      "0.844\n",
      "batch_idx: 5\n",
      "0.8516666666666667\n",
      "Training Epoch: 106, total loss: 10.010148\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.88\n",
      "batch_idx: 4\n",
      "0.876\n",
      "batch_idx: 5\n",
      "0.87\n",
      "Training Epoch: 107, total loss: 9.927637\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.87\n",
      "batch_idx: 2\n",
      "0.8533333333333334\n",
      "batch_idx: 3\n",
      "0.8525\n",
      "batch_idx: 4\n",
      "0.866\n",
      "batch_idx: 5\n",
      "0.8683333333333333\n",
      "Training Epoch: 108, total loss: 9.973437\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.8866666666666667\n",
      "batch_idx: 3\n",
      "0.86\n",
      "batch_idx: 4\n",
      "0.86\n",
      "batch_idx: 5\n",
      "0.865\n",
      "Training Epoch: 109, total loss: 9.974982\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.8733333333333333\n",
      "batch_idx: 3\n",
      "0.865\n",
      "batch_idx: 4\n",
      "0.862\n",
      "batch_idx: 5\n",
      "0.8616666666666667\n",
      "Training Epoch: 110, total loss: 9.996888\n",
      "batch_idx: 0\n",
      "0.81\n",
      "batch_idx: 1\n",
      "0.84\n",
      "batch_idx: 2\n",
      "0.86\n",
      "batch_idx: 3\n",
      "0.8675\n",
      "batch_idx: 4\n",
      "0.868\n",
      "batch_idx: 5\n",
      "0.8733333333333333\n",
      "Training Epoch: 111, total loss: 9.922026\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.855\n",
      "batch_idx: 2\n",
      "0.8533333333333334\n",
      "batch_idx: 3\n",
      "0.8625\n",
      "batch_idx: 4\n",
      "0.858\n",
      "batch_idx: 5\n",
      "0.86\n",
      "Training Epoch: 112, total loss: 9.986501\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.8766666666666667\n",
      "batch_idx: 3\n",
      "0.8675\n",
      "batch_idx: 4\n",
      "0.856\n",
      "batch_idx: 5\n",
      "0.86\n",
      "Training Epoch: 113, total loss: 9.972600\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.87\n",
      "batch_idx: 2\n",
      "0.8666666666666667\n",
      "batch_idx: 3\n",
      "0.865\n",
      "batch_idx: 4\n",
      "0.866\n",
      "batch_idx: 5\n",
      "0.87\n",
      "Training Epoch: 114, total loss: 9.964913\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.86\n",
      "batch_idx: 2\n",
      "0.8633333333333333\n",
      "batch_idx: 3\n",
      "0.87\n",
      "batch_idx: 4\n",
      "0.856\n",
      "batch_idx: 5\n",
      "0.8516666666666667\n",
      "Training Epoch: 115, total loss: 10.018833\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.8533333333333334\n",
      "batch_idx: 3\n",
      "0.8675\n",
      "batch_idx: 4\n",
      "0.872\n",
      "batch_idx: 5\n",
      "0.8766666666666667\n",
      "Training Epoch: 116, total loss: 9.925046\n",
      "batch_idx: 0\n",
      "0.78\n",
      "batch_idx: 1\n",
      "0.835\n",
      "batch_idx: 2\n",
      "0.8466666666666667\n",
      "batch_idx: 3\n",
      "0.8475\n",
      "batch_idx: 4\n",
      "0.854\n",
      "batch_idx: 5\n",
      "0.8633333333333333\n",
      "Training Epoch: 117, total loss: 9.993272\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.8875\n",
      "batch_idx: 4\n",
      "0.892\n",
      "batch_idx: 5\n",
      "0.8816666666666667\n",
      "Training Epoch: 118, total loss: 9.912410\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.855\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.85\n",
      "batch_idx: 4\n",
      "0.86\n",
      "batch_idx: 5\n",
      "0.8616666666666667\n",
      "Training Epoch: 119, total loss: 9.979782\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8766666666666667\n",
      "batch_idx: 3\n",
      "0.88\n",
      "batch_idx: 4\n",
      "0.874\n",
      "batch_idx: 5\n",
      "0.8733333333333333\n",
      "Training Epoch: 120, total loss: 9.911273\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.866\n",
      "batch_idx: 5\n",
      "0.8666666666666667\n",
      "Training Epoch: 121, total loss: 9.947083\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.85\n",
      "batch_idx: 3\n",
      "0.86\n",
      "batch_idx: 4\n",
      "0.856\n",
      "batch_idx: 5\n",
      "0.8666666666666667\n",
      "Training Epoch: 122, total loss: 9.988105\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.86\n",
      "batch_idx: 3\n",
      "0.85\n",
      "batch_idx: 4\n",
      "0.858\n",
      "batch_idx: 5\n",
      "0.86\n",
      "Training Epoch: 123, total loss: 9.981395\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.8766666666666667\n",
      "batch_idx: 3\n",
      "0.8775\n",
      "batch_idx: 4\n",
      "0.88\n",
      "batch_idx: 5\n",
      "0.8816666666666667\n",
      "Training Epoch: 124, total loss: 9.892461\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.8775\n",
      "batch_idx: 4\n",
      "0.87\n",
      "batch_idx: 5\n",
      "0.865\n",
      "Training Epoch: 125, total loss: 9.969777\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.8766666666666667\n",
      "batch_idx: 3\n",
      "0.8375\n",
      "batch_idx: 4\n",
      "0.83\n",
      "batch_idx: 5\n",
      "0.84\n",
      "Training Epoch: 126, total loss: 10.043240\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.86\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.8625\n",
      "batch_idx: 4\n",
      "0.872\n",
      "batch_idx: 5\n",
      "0.8666666666666667\n",
      "Training Epoch: 127, total loss: 9.939687\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.8675\n",
      "batch_idx: 4\n",
      "0.864\n",
      "batch_idx: 5\n",
      "0.85\n",
      "Training Epoch: 128, total loss: 10.031940\n",
      "batch_idx: 0\n",
      "0.83\n",
      "batch_idx: 1\n",
      "0.84\n",
      "batch_idx: 2\n",
      "0.8466666666666667\n",
      "batch_idx: 3\n",
      "0.8525\n",
      "batch_idx: 4\n",
      "0.858\n",
      "batch_idx: 5\n",
      "0.8566666666666667\n",
      "Training Epoch: 129, total loss: 10.003049\n",
      "batch_idx: 0\n",
      "0.83\n",
      "batch_idx: 1\n",
      "0.86\n",
      "batch_idx: 2\n",
      "0.8766666666666667\n",
      "batch_idx: 3\n",
      "0.8625\n",
      "batch_idx: 4\n",
      "0.87\n",
      "batch_idx: 5\n",
      "0.8783333333333333\n",
      "Training Epoch: 130, total loss: 9.909531\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.8675\n",
      "batch_idx: 4\n",
      "0.866\n",
      "batch_idx: 5\n",
      "0.87\n",
      "Training Epoch: 131, total loss: 9.936697\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.84\n",
      "batch_idx: 2\n",
      "0.8533333333333334\n",
      "batch_idx: 3\n",
      "0.8575\n",
      "batch_idx: 4\n",
      "0.86\n",
      "batch_idx: 5\n",
      "0.8666666666666667\n",
      "Training Epoch: 132, total loss: 9.956873\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.8733333333333333\n",
      "batch_idx: 3\n",
      "0.88\n",
      "batch_idx: 4\n",
      "0.886\n",
      "batch_idx: 5\n",
      "0.87\n",
      "Training Epoch: 133, total loss: 9.938146\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.89\n",
      "batch_idx: 3\n",
      "0.89\n",
      "batch_idx: 4\n",
      "0.894\n",
      "batch_idx: 5\n",
      "0.8833333333333333\n",
      "Training Epoch: 134, total loss: 9.892782\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.86\n",
      "batch_idx: 2\n",
      "0.8733333333333333\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.882\n",
      "batch_idx: 5\n",
      "0.8766666666666667\n",
      "Training Epoch: 135, total loss: 9.950131\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.87\n",
      "batch_idx: 2\n",
      "0.8866666666666667\n",
      "batch_idx: 3\n",
      "0.8725\n",
      "batch_idx: 4\n",
      "0.876\n",
      "batch_idx: 5\n",
      "0.86\n",
      "Training Epoch: 136, total loss: 9.977977\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.87\n",
      "batch_idx: 2\n",
      "0.8733333333333333\n",
      "batch_idx: 3\n",
      "0.8575\n",
      "batch_idx: 4\n",
      "0.872\n",
      "batch_idx: 5\n",
      "0.87\n",
      "Training Epoch: 137, total loss: 9.936676\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.8666666666666667\n",
      "batch_idx: 3\n",
      "0.8725\n",
      "batch_idx: 4\n",
      "0.872\n",
      "batch_idx: 5\n",
      "0.8733333333333333\n",
      "Training Epoch: 138, total loss: 9.924130\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.8566666666666667\n",
      "batch_idx: 3\n",
      "0.86\n",
      "batch_idx: 4\n",
      "0.87\n",
      "batch_idx: 5\n",
      "0.8783333333333333\n",
      "Training Epoch: 139, total loss: 9.906594\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8733333333333333\n",
      "batch_idx: 3\n",
      "0.88\n",
      "batch_idx: 4\n",
      "0.868\n",
      "batch_idx: 5\n",
      "0.87\n",
      "Training Epoch: 140, total loss: 9.935248\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.87\n",
      "batch_idx: 4\n",
      "0.87\n",
      "batch_idx: 5\n",
      "0.875\n",
      "Training Epoch: 141, total loss: 9.903629\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.8725\n",
      "batch_idx: 4\n",
      "0.876\n",
      "batch_idx: 5\n",
      "0.875\n",
      "Training Epoch: 142, total loss: 9.896477\n",
      "batch_idx: 0\n",
      "0.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.8766666666666667\n",
      "batch_idx: 3\n",
      "0.8775\n",
      "batch_idx: 4\n",
      "0.88\n",
      "batch_idx: 5\n",
      "0.8666666666666667\n",
      "Training Epoch: 143, total loss: 9.957104\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.8766666666666667\n",
      "batch_idx: 3\n",
      "0.88\n",
      "batch_idx: 4\n",
      "0.876\n",
      "batch_idx: 5\n",
      "0.8816666666666667\n",
      "Training Epoch: 144, total loss: 9.863834\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.8725\n",
      "batch_idx: 4\n",
      "0.866\n",
      "batch_idx: 5\n",
      "0.87\n",
      "Training Epoch: 145, total loss: 9.930391\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.8733333333333333\n",
      "batch_idx: 3\n",
      "0.8725\n",
      "batch_idx: 4\n",
      "0.874\n",
      "batch_idx: 5\n",
      "0.88\n",
      "Training Epoch: 146, total loss: 9.882071\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.89\n",
      "batch_idx: 4\n",
      "0.886\n",
      "batch_idx: 5\n",
      "0.88\n",
      "Training Epoch: 147, total loss: 9.912830\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.88\n",
      "batch_idx: 4\n",
      "0.882\n",
      "batch_idx: 5\n",
      "0.8783333333333333\n",
      "Training Epoch: 148, total loss: 9.900432\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.86\n",
      "batch_idx: 2\n",
      "0.85\n",
      "batch_idx: 3\n",
      "0.8425\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8616666666666667\n",
      "Training Epoch: 149, total loss: 9.987378\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.888\n",
      "batch_idx: 5\n",
      "0.8883333333333333\n",
      "Training Epoch: 150, total loss: 9.845856\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.85\n",
      "batch_idx: 3\n",
      "0.8625\n",
      "batch_idx: 4\n",
      "0.872\n",
      "batch_idx: 5\n",
      "0.8733333333333333\n",
      "Training Epoch: 151, total loss: 9.912342\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.86\n",
      "batch_idx: 2\n",
      "0.8766666666666667\n",
      "batch_idx: 3\n",
      "0.87\n",
      "batch_idx: 4\n",
      "0.864\n",
      "batch_idx: 5\n",
      "0.8733333333333333\n",
      "Training Epoch: 152, total loss: 9.913910\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.89\n",
      "batch_idx: 3\n",
      "0.885\n",
      "batch_idx: 4\n",
      "0.884\n",
      "batch_idx: 5\n",
      "0.885\n",
      "Training Epoch: 153, total loss: 9.886567\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.885\n",
      "batch_idx: 4\n",
      "0.882\n",
      "batch_idx: 5\n",
      "0.8733333333333333\n",
      "Training Epoch: 154, total loss: 9.913847\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8733333333333333\n",
      "batch_idx: 3\n",
      "0.88\n",
      "batch_idx: 4\n",
      "0.874\n",
      "batch_idx: 5\n",
      "0.8733333333333333\n",
      "Training Epoch: 155, total loss: 9.915774\n",
      "batch_idx: 0\n",
      "0.96\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.8825\n",
      "batch_idx: 4\n",
      "0.874\n",
      "batch_idx: 5\n",
      "0.8783333333333333\n",
      "Training Epoch: 156, total loss: 9.876966\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.87\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.8775\n",
      "batch_idx: 4\n",
      "0.87\n",
      "batch_idx: 5\n",
      "0.87\n",
      "Training Epoch: 157, total loss: 9.914826\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.89\n",
      "batch_idx: 3\n",
      "0.8725\n",
      "batch_idx: 4\n",
      "0.858\n",
      "batch_idx: 5\n",
      "0.865\n",
      "Training Epoch: 158, total loss: 9.947088\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.8875\n",
      "batch_idx: 4\n",
      "0.888\n",
      "batch_idx: 5\n",
      "0.8816666666666667\n",
      "Training Epoch: 159, total loss: 9.879503\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.8925\n",
      "batch_idx: 4\n",
      "0.886\n",
      "batch_idx: 5\n",
      "0.8666666666666667\n",
      "Training Epoch: 160, total loss: 9.934863\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.8666666666666667\n",
      "batch_idx: 3\n",
      "0.8725\n",
      "batch_idx: 4\n",
      "0.876\n",
      "batch_idx: 5\n",
      "0.8733333333333333\n",
      "Training Epoch: 161, total loss: 9.908742\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.8633333333333333\n",
      "batch_idx: 3\n",
      "0.87\n",
      "batch_idx: 4\n",
      "0.874\n",
      "batch_idx: 5\n",
      "0.8766666666666667\n",
      "Training Epoch: 162, total loss: 9.888394\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.8775\n",
      "batch_idx: 4\n",
      "0.882\n",
      "batch_idx: 5\n",
      "0.8683333333333333\n",
      "Training Epoch: 163, total loss: 9.947852\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8633333333333333\n",
      "batch_idx: 3\n",
      "0.8725\n",
      "batch_idx: 4\n",
      "0.866\n",
      "batch_idx: 5\n",
      "0.87\n",
      "Training Epoch: 164, total loss: 9.948703\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.8825\n",
      "batch_idx: 4\n",
      "0.87\n",
      "batch_idx: 5\n",
      "0.86\n",
      "Training Epoch: 165, total loss: 9.997005\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.8566666666666667\n",
      "batch_idx: 3\n",
      "0.855\n",
      "batch_idx: 4\n",
      "0.866\n",
      "batch_idx: 5\n",
      "0.8633333333333333\n",
      "Training Epoch: 166, total loss: 9.952835\n",
      "batch_idx: 0\n",
      "0.83\n",
      "batch_idx: 1\n",
      "0.845\n",
      "batch_idx: 2\n",
      "0.8366666666666667\n",
      "batch_idx: 3\n",
      "0.86\n",
      "batch_idx: 4\n",
      "0.864\n",
      "batch_idx: 5\n",
      "0.8683333333333333\n",
      "Training Epoch: 167, total loss: 9.921696\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.86\n",
      "batch_idx: 2\n",
      "0.8666666666666667\n",
      "batch_idx: 3\n",
      "0.8775\n",
      "batch_idx: 4\n",
      "0.886\n",
      "batch_idx: 5\n",
      "0.88\n",
      "Training Epoch: 168, total loss: 9.907375\n",
      "batch_idx: 0\n",
      "0.82\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.89\n",
      "batch_idx: 3\n",
      "0.8975\n",
      "batch_idx: 4\n",
      "0.89\n",
      "batch_idx: 5\n",
      "0.885\n",
      "Training Epoch: 169, total loss: 9.870857\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.845\n",
      "batch_idx: 2\n",
      "0.8666666666666667\n",
      "batch_idx: 3\n",
      "0.88\n",
      "batch_idx: 4\n",
      "0.88\n",
      "batch_idx: 5\n",
      "0.8783333333333333\n",
      "Training Epoch: 170, total loss: 9.904493\n",
      "batch_idx: 0\n",
      "0.78\n",
      "batch_idx: 1\n",
      "0.83\n",
      "batch_idx: 2\n",
      "0.84\n",
      "batch_idx: 3\n",
      "0.8375\n",
      "batch_idx: 4\n",
      "0.856\n",
      "batch_idx: 5\n",
      "0.8533333333333334\n",
      "Training Epoch: 171, total loss: 10.023502\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.89\n",
      "batch_idx: 4\n",
      "0.882\n",
      "batch_idx: 5\n",
      "0.88\n",
      "Training Epoch: 172, total loss: 9.890556\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.855\n",
      "batch_idx: 2\n",
      "0.8633333333333333\n",
      "batch_idx: 3\n",
      "0.8675\n",
      "batch_idx: 4\n",
      "0.872\n",
      "batch_idx: 5\n",
      "0.8716666666666667\n",
      "Training Epoch: 173, total loss: 9.911015\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.8766666666666667\n",
      "batch_idx: 3\n",
      "0.8825\n",
      "batch_idx: 4\n",
      "0.882\n",
      "batch_idx: 5\n",
      "0.88\n",
      "Training Epoch: 174, total loss: 9.888963\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.89\n",
      "batch_idx: 3\n",
      "0.8825\n",
      "batch_idx: 4\n",
      "0.884\n",
      "batch_idx: 5\n",
      "0.8783333333333333\n",
      "Training Epoch: 175, total loss: 9.903015\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.89\n",
      "batch_idx: 4\n",
      "0.886\n",
      "batch_idx: 5\n",
      "0.8833333333333333\n",
      "Training Epoch: 176, total loss: 9.883020\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.83\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.85\n",
      "batch_idx: 4\n",
      "0.86\n",
      "batch_idx: 5\n",
      "0.8583333333333333\n",
      "Training Epoch: 177, total loss: 9.993081\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.85\n",
      "batch_idx: 2\n",
      "0.8533333333333334\n",
      "batch_idx: 3\n",
      "0.86\n",
      "batch_idx: 4\n",
      "0.862\n",
      "batch_idx: 5\n",
      "0.865\n",
      "Training Epoch: 178, total loss: 9.976622\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.8733333333333333\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.866\n",
      "batch_idx: 5\n",
      "0.875\n",
      "Training Epoch: 179, total loss: 9.919416\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.8625\n",
      "batch_idx: 4\n",
      "0.862\n",
      "batch_idx: 5\n",
      "0.8766666666666667\n",
      "Training Epoch: 180, total loss: 9.910586\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.87\n",
      "batch_idx: 2\n",
      "0.86\n",
      "batch_idx: 3\n",
      "0.87\n",
      "batch_idx: 4\n",
      "0.878\n",
      "batch_idx: 5\n",
      "0.8816666666666667\n",
      "Training Epoch: 181, total loss: 9.885744\n",
      "batch_idx: 0\n",
      "0.83\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.88\n",
      "batch_idx: 4\n",
      "0.874\n",
      "batch_idx: 5\n",
      "0.8766666666666667\n",
      "Training Epoch: 182, total loss: 9.894400\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.8766666666666667\n",
      "batch_idx: 3\n",
      "0.855\n",
      "batch_idx: 4\n",
      "0.862\n",
      "batch_idx: 5\n",
      "0.86\n",
      "Training Epoch: 183, total loss: 9.971104\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.86\n",
      "batch_idx: 2\n",
      "0.8733333333333333\n",
      "batch_idx: 3\n",
      "0.8825\n",
      "batch_idx: 4\n",
      "0.884\n",
      "batch_idx: 5\n",
      "0.875\n",
      "Training Epoch: 184, total loss: 9.915296\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.87\n",
      "batch_idx: 2\n",
      "0.8733333333333333\n",
      "batch_idx: 3\n",
      "0.88\n",
      "batch_idx: 4\n",
      "0.882\n",
      "batch_idx: 5\n",
      "0.885\n",
      "Training Epoch: 185, total loss: 9.876107\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.84\n",
      "batch_idx: 2\n",
      "0.86\n",
      "batch_idx: 3\n",
      "0.865\n",
      "batch_idx: 4\n",
      "0.866\n",
      "batch_idx: 5\n",
      "0.875\n",
      "Training Epoch: 186, total loss: 9.912348\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.835\n",
      "batch_idx: 2\n",
      "0.8533333333333334\n",
      "batch_idx: 3\n",
      "0.8625\n",
      "batch_idx: 4\n",
      "0.866\n",
      "batch_idx: 5\n",
      "0.8666666666666667\n",
      "Training Epoch: 187, total loss: 9.936446\n",
      "batch_idx: 0\n",
      "0.77\n",
      "batch_idx: 1\n",
      "0.84\n",
      "batch_idx: 2\n",
      "0.86\n",
      "batch_idx: 3\n",
      "0.8525\n",
      "batch_idx: 4\n",
      "0.86\n",
      "batch_idx: 5\n",
      "0.8516666666666667\n",
      "Training Epoch: 188, total loss: 10.030390\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.86\n",
      "batch_idx: 3\n",
      "0.8725\n",
      "batch_idx: 4\n",
      "0.858\n",
      "batch_idx: 5\n",
      "0.8616666666666667\n",
      "Training Epoch: 189, total loss: 9.972000\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.8533333333333334\n",
      "batch_idx: 3\n",
      "0.8575\n",
      "batch_idx: 4\n",
      "0.86\n",
      "batch_idx: 5\n",
      "0.86\n",
      "Training Epoch: 190, total loss: 9.967265\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.86\n",
      "batch_idx: 2\n",
      "0.8766666666666667\n",
      "batch_idx: 3\n",
      "0.8725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 4\n",
      "0.876\n",
      "batch_idx: 5\n",
      "0.8683333333333333\n",
      "Training Epoch: 191, total loss: 9.934884\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.8975\n",
      "batch_idx: 4\n",
      "0.892\n",
      "batch_idx: 5\n",
      "0.8816666666666667\n",
      "Training Epoch: 192, total loss: 9.867403\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.8725\n",
      "batch_idx: 4\n",
      "0.876\n",
      "batch_idx: 5\n",
      "0.8783333333333333\n",
      "Training Epoch: 193, total loss: 9.903853\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.8766666666666667\n",
      "batch_idx: 3\n",
      "0.885\n",
      "batch_idx: 4\n",
      "0.892\n",
      "batch_idx: 5\n",
      "0.8833333333333333\n",
      "Training Epoch: 194, total loss: 9.865639\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.8825\n",
      "batch_idx: 4\n",
      "0.87\n",
      "batch_idx: 5\n",
      "0.8766666666666667\n",
      "Training Epoch: 195, total loss: 9.885958\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.8775\n",
      "batch_idx: 4\n",
      "0.884\n",
      "batch_idx: 5\n",
      "0.8816666666666667\n",
      "Training Epoch: 196, total loss: 9.874805\n",
      "batch_idx: 0\n",
      "0.81\n",
      "batch_idx: 1\n",
      "0.86\n",
      "batch_idx: 2\n",
      "0.86\n",
      "batch_idx: 3\n",
      "0.8675\n",
      "batch_idx: 4\n",
      "0.868\n",
      "batch_idx: 5\n",
      "0.8783333333333333\n",
      "Training Epoch: 197, total loss: 9.897766\n",
      "batch_idx: 0\n",
      "0.83\n",
      "batch_idx: 1\n",
      "0.825\n",
      "batch_idx: 2\n",
      "0.8633333333333333\n",
      "batch_idx: 3\n",
      "0.8775\n",
      "batch_idx: 4\n",
      "0.884\n",
      "batch_idx: 5\n",
      "0.88\n",
      "Training Epoch: 198, total loss: 9.901415\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8633333333333333\n",
      "batch_idx: 3\n",
      "0.8725\n",
      "batch_idx: 4\n",
      "0.878\n",
      "batch_idx: 5\n",
      "0.88\n",
      "Training Epoch: 199, total loss: 9.890104\n",
      "batch_idx: 0\n",
      "0.83\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.8566666666666667\n",
      "batch_idx: 3\n",
      "0.8725\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.86\n",
      "Training Epoch: 200, total loss: 9.974666\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.87\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.87\n",
      "batch_idx: 5\n",
      "0.8683333333333333\n",
      "Training Epoch: 201, total loss: 9.917930\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.8925\n",
      "batch_idx: 4\n",
      "0.888\n",
      "batch_idx: 5\n",
      "0.8816666666666667\n",
      "Training Epoch: 202, total loss: 9.890042\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.876\n",
      "batch_idx: 5\n",
      "0.8766666666666667\n",
      "Training Epoch: 203, total loss: 9.905554\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.8866666666666667\n",
      "batch_idx: 3\n",
      "0.8875\n",
      "batch_idx: 4\n",
      "0.878\n",
      "batch_idx: 5\n",
      "0.8766666666666667\n",
      "Training Epoch: 204, total loss: 9.908402\n",
      "batch_idx: 0\n",
      "0.83\n",
      "batch_idx: 1\n",
      "0.84\n",
      "batch_idx: 2\n",
      "0.86\n",
      "batch_idx: 3\n",
      "0.8575\n",
      "batch_idx: 4\n",
      "0.864\n",
      "batch_idx: 5\n",
      "0.8666666666666667\n",
      "Training Epoch: 205, total loss: 9.949722\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.8866666666666667\n",
      "batch_idx: 3\n",
      "0.8775\n",
      "batch_idx: 4\n",
      "0.886\n",
      "batch_idx: 5\n",
      "0.87\n",
      "Training Epoch: 206, total loss: 9.930385\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.882\n",
      "batch_idx: 5\n",
      "0.8766666666666667\n",
      "Training Epoch: 207, total loss: 9.896051\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.8725\n",
      "batch_idx: 4\n",
      "0.886\n",
      "batch_idx: 5\n",
      "0.88\n",
      "Training Epoch: 208, total loss: 9.876058\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.8925\n",
      "batch_idx: 4\n",
      "0.882\n",
      "batch_idx: 5\n",
      "0.8833333333333333\n",
      "Training Epoch: 209, total loss: 9.867666\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8866666666666667\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.878\n",
      "batch_idx: 5\n",
      "0.8816666666666667\n",
      "Training Epoch: 210, total loss: 9.881764\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.87\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.8675\n",
      "batch_idx: 4\n",
      "0.88\n",
      "batch_idx: 5\n",
      "0.8833333333333333\n",
      "Training Epoch: 211, total loss: 9.861580\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.888\n",
      "batch_idx: 5\n",
      "0.8883333333333333\n",
      "Training Epoch: 212, total loss: 9.856447\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8666666666666667\n",
      "batch_idx: 3\n",
      "0.86\n",
      "batch_idx: 4\n",
      "0.864\n",
      "batch_idx: 5\n",
      "0.875\n",
      "Training Epoch: 213, total loss: 9.902636\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.8866666666666667\n",
      "batch_idx: 3\n",
      "0.88\n",
      "batch_idx: 4\n",
      "0.886\n",
      "batch_idx: 5\n",
      "0.885\n",
      "Training Epoch: 214, total loss: 9.861441\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.87\n",
      "batch_idx: 4\n",
      "0.878\n",
      "batch_idx: 5\n",
      "0.88\n",
      "Training Epoch: 215, total loss: 9.874976\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.892\n",
      "batch_idx: 5\n",
      "0.88\n",
      "Training Epoch: 216, total loss: 9.873291\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.8866666666666667\n",
      "batch_idx: 3\n",
      "0.88\n",
      "batch_idx: 4\n",
      "0.882\n",
      "batch_idx: 5\n",
      "0.8883333333333333\n",
      "Training Epoch: 217, total loss: 9.838654\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.892\n",
      "batch_idx: 5\n",
      "0.8933333333333333\n",
      "Training Epoch: 218, total loss: 9.843988\n",
      "batch_idx: 0\n",
      "0.82\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.8733333333333333\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.876\n",
      "batch_idx: 5\n",
      "0.8833333333333333\n",
      "Training Epoch: 219, total loss: 9.869589\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.8825\n",
      "batch_idx: 4\n",
      "0.886\n",
      "batch_idx: 5\n",
      "0.8883333333333333\n",
      "Training Epoch: 220, total loss: 9.846064\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.8975\n",
      "batch_idx: 4\n",
      "0.896\n",
      "batch_idx: 5\n",
      "0.8916666666666667\n",
      "Training Epoch: 221, total loss: 9.835147\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.835\n",
      "batch_idx: 2\n",
      "0.8733333333333333\n",
      "batch_idx: 3\n",
      "0.8725\n",
      "batch_idx: 4\n",
      "0.88\n",
      "batch_idx: 5\n",
      "0.8733333333333333\n",
      "Training Epoch: 222, total loss: 9.917987\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.8775\n",
      "batch_idx: 4\n",
      "0.88\n",
      "batch_idx: 5\n",
      "0.8883333333333333\n",
      "Training Epoch: 223, total loss: 9.859218\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.89\n",
      "batch_idx: 3\n",
      "0.89\n",
      "batch_idx: 4\n",
      "0.886\n",
      "batch_idx: 5\n",
      "0.875\n",
      "Training Epoch: 224, total loss: 9.897464\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.882\n",
      "batch_idx: 5\n",
      "0.8783333333333333\n",
      "Training Epoch: 225, total loss: 9.895487\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.885\n",
      "batch_idx: 4\n",
      "0.88\n",
      "batch_idx: 5\n",
      "0.8816666666666667\n",
      "Training Epoch: 226, total loss: 9.881601\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.89\n",
      "batch_idx: 4\n",
      "0.876\n",
      "batch_idx: 5\n",
      "0.8766666666666667\n",
      "Training Epoch: 227, total loss: 9.890112\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.87\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.885\n",
      "batch_idx: 4\n",
      "0.89\n",
      "batch_idx: 5\n",
      "0.8866666666666667\n",
      "Training Epoch: 228, total loss: 9.855064\n",
      "batch_idx: 0\n",
      "0.82\n",
      "batch_idx: 1\n",
      "0.835\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8575\n",
      "batch_idx: 4\n",
      "0.868\n",
      "batch_idx: 5\n",
      "0.875\n",
      "Training Epoch: 229, total loss: 9.906734\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.8566666666666667\n",
      "batch_idx: 3\n",
      "0.86\n",
      "batch_idx: 4\n",
      "0.866\n",
      "batch_idx: 5\n",
      "0.8633333333333333\n",
      "Training Epoch: 230, total loss: 9.959444\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.87\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.8825\n",
      "batch_idx: 4\n",
      "0.888\n",
      "batch_idx: 5\n",
      "0.8816666666666667\n",
      "Training Epoch: 231, total loss: 9.882010\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.865\n",
      "batch_idx: 4\n",
      "0.862\n",
      "batch_idx: 5\n",
      "0.8616666666666667\n",
      "Training Epoch: 232, total loss: 9.989945\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.8866666666666667\n",
      "batch_idx: 3\n",
      "0.8675\n",
      "batch_idx: 4\n",
      "0.866\n",
      "batch_idx: 5\n",
      "0.8733333333333333\n",
      "Training Epoch: 233, total loss: 9.918186\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.8775\n",
      "batch_idx: 4\n",
      "0.882\n",
      "batch_idx: 5\n",
      "0.8783333333333333\n",
      "Training Epoch: 234, total loss: 9.899750\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.89\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.89\n",
      "batch_idx: 5\n",
      "0.875\n",
      "Training Epoch: 235, total loss: 9.903896\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8633333333333333\n",
      "batch_idx: 3\n",
      "0.865\n",
      "batch_idx: 4\n",
      "0.858\n",
      "batch_idx: 5\n",
      "0.86\n",
      "Training Epoch: 236, total loss: 9.972504\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8633333333333333\n",
      "batch_idx: 3\n",
      "0.8775\n",
      "batch_idx: 4\n",
      "0.876\n",
      "batch_idx: 5\n",
      "0.875\n",
      "Training Epoch: 237, total loss: 9.904103\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.892\n",
      "batch_idx: 5\n",
      "0.885\n",
      "Training Epoch: 238, total loss: 9.863697\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 2\n",
      "0.8533333333333334\n",
      "batch_idx: 3\n",
      "0.8575\n",
      "batch_idx: 4\n",
      "0.856\n",
      "batch_idx: 5\n",
      "0.865\n",
      "Training Epoch: 239, total loss: 9.957563\n",
      "batch_idx: 0\n",
      "0.79\n",
      "batch_idx: 1\n",
      "0.84\n",
      "batch_idx: 2\n",
      "0.8633333333333333\n",
      "batch_idx: 3\n",
      "0.865\n",
      "batch_idx: 4\n",
      "0.866\n",
      "batch_idx: 5\n",
      "0.87\n",
      "Training Epoch: 240, total loss: 9.933216\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.885\n",
      "batch_idx: 4\n",
      "0.882\n",
      "batch_idx: 5\n",
      "0.8766666666666667\n",
      "Training Epoch: 241, total loss: 9.896226\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.85\n",
      "batch_idx: 2\n",
      "0.86\n",
      "batch_idx: 3\n",
      "0.8725\n",
      "batch_idx: 4\n",
      "0.868\n",
      "batch_idx: 5\n",
      "0.8666666666666667\n",
      "Training Epoch: 242, total loss: 9.926574\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.885\n",
      "batch_idx: 4\n",
      "0.882\n",
      "batch_idx: 5\n",
      "0.8883333333333333\n",
      "Training Epoch: 243, total loss: 9.853159\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.87\n",
      "batch_idx: 2\n",
      "0.8766666666666667\n",
      "batch_idx: 3\n",
      "0.8775\n",
      "batch_idx: 4\n",
      "0.888\n",
      "batch_idx: 5\n",
      "0.8866666666666667\n",
      "Training Epoch: 244, total loss: 9.862261\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.874\n",
      "batch_idx: 5\n",
      "0.88\n",
      "Training Epoch: 245, total loss: 9.885851\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.8733333333333333\n",
      "batch_idx: 3\n",
      "0.8725\n",
      "batch_idx: 4\n",
      "0.872\n",
      "batch_idx: 5\n",
      "0.875\n",
      "Training Epoch: 246, total loss: 9.905266\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.89\n",
      "batch_idx: 4\n",
      "0.888\n",
      "batch_idx: 5\n",
      "0.8866666666666667\n",
      "Training Epoch: 247, total loss: 9.857426\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.885\n",
      "batch_idx: 4\n",
      "0.878\n",
      "batch_idx: 5\n",
      "0.8733333333333333\n",
      "Training Epoch: 248, total loss: 9.902865\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.88\n",
      "batch_idx: 4\n",
      "0.888\n",
      "batch_idx: 5\n",
      "0.8816666666666667\n",
      "Training Epoch: 249, total loss: 9.876967\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.885\n",
      "batch_idx: 4\n",
      "0.878\n",
      "batch_idx: 5\n",
      "0.8816666666666667\n",
      "Training Epoch: 250, total loss: 9.891832\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8633333333333333\n",
      "batch_idx: 3\n",
      "0.8725\n",
      "batch_idx: 4\n",
      "0.88\n",
      "batch_idx: 5\n",
      "0.8716666666666667\n",
      "Training Epoch: 251, total loss: 9.933105\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.8775\n",
      "batch_idx: 4\n",
      "0.882\n",
      "batch_idx: 5\n",
      "0.8716666666666667\n",
      "Training Epoch: 252, total loss: 9.918294\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.8875\n",
      "batch_idx: 4\n",
      "0.896\n",
      "batch_idx: 5\n",
      "0.8816666666666667\n",
      "Training Epoch: 253, total loss: 9.880148\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.86\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.885\n",
      "batch_idx: 4\n",
      "0.886\n",
      "batch_idx: 5\n",
      "0.89\n",
      "Training Epoch: 254, total loss: 9.843905\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.86\n",
      "batch_idx: 2\n",
      "0.89\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.888\n",
      "batch_idx: 5\n",
      "0.8883333333333333\n",
      "Training Epoch: 255, total loss: 9.842638\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.8675\n",
      "batch_idx: 4\n",
      "0.872\n",
      "batch_idx: 5\n",
      "0.8783333333333333\n",
      "Training Epoch: 256, total loss: 9.894553\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.87\n",
      "batch_idx: 2\n",
      "0.8733333333333333\n",
      "batch_idx: 3\n",
      "0.88\n",
      "batch_idx: 4\n",
      "0.88\n",
      "batch_idx: 5\n",
      "0.8816666666666667\n",
      "Training Epoch: 257, total loss: 9.872844\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.8825\n",
      "batch_idx: 4\n",
      "0.872\n",
      "batch_idx: 5\n",
      "0.8833333333333333\n",
      "Training Epoch: 258, total loss: 9.860840\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.865\n",
      "batch_idx: 4\n",
      "0.878\n",
      "batch_idx: 5\n",
      "0.8866666666666667\n",
      "Training Epoch: 259, total loss: 9.859989\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.855\n",
      "batch_idx: 2\n",
      "0.8666666666666667\n",
      "batch_idx: 3\n",
      "0.8775\n",
      "batch_idx: 4\n",
      "0.88\n",
      "batch_idx: 5\n",
      "0.8816666666666667\n",
      "Training Epoch: 260, total loss: 9.890067\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.88\n",
      "batch_idx: 4\n",
      "0.888\n",
      "batch_idx: 5\n",
      "0.8816666666666667\n",
      "Training Epoch: 261, total loss: 9.868745\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.8875\n",
      "batch_idx: 4\n",
      "0.882\n",
      "batch_idx: 5\n",
      "0.8766666666666667\n",
      "Training Epoch: 262, total loss: 9.904779\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.87\n",
      "batch_idx: 2\n",
      "0.8733333333333333\n",
      "batch_idx: 3\n",
      "0.8875\n",
      "batch_idx: 4\n",
      "0.888\n",
      "batch_idx: 5\n",
      "0.8816666666666667\n",
      "Training Epoch: 263, total loss: 9.901729\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.8575\n",
      "batch_idx: 4\n",
      "0.864\n",
      "batch_idx: 5\n",
      "0.8666666666666667\n",
      "Training Epoch: 264, total loss: 9.947327\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8733333333333333\n",
      "batch_idx: 3\n",
      "0.88\n",
      "batch_idx: 4\n",
      "0.886\n",
      "batch_idx: 5\n",
      "0.8833333333333333\n",
      "Training Epoch: 265, total loss: 9.873720\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.84\n",
      "batch_idx: 2\n",
      "0.8633333333333333\n",
      "batch_idx: 3\n",
      "0.8625\n",
      "batch_idx: 4\n",
      "0.866\n",
      "batch_idx: 5\n",
      "0.8733333333333333\n",
      "Training Epoch: 266, total loss: 9.906298\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.855\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.87\n",
      "batch_idx: 4\n",
      "0.864\n",
      "batch_idx: 5\n",
      "0.875\n",
      "Training Epoch: 267, total loss: 9.903920\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.8875\n",
      "batch_idx: 4\n",
      "0.878\n",
      "batch_idx: 5\n",
      "0.875\n",
      "Training Epoch: 268, total loss: 9.906348\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.8725\n",
      "batch_idx: 4\n",
      "0.878\n",
      "batch_idx: 5\n",
      "0.8783333333333333\n",
      "Training Epoch: 269, total loss: 9.885725\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.89\n",
      "batch_idx: 5\n",
      "0.8883333333333333\n",
      "Training Epoch: 270, total loss: 9.851540\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.8766666666666667\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.872\n",
      "batch_idx: 5\n",
      "0.875\n",
      "Training Epoch: 271, total loss: 9.890762\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8766666666666667\n",
      "batch_idx: 3\n",
      "0.88\n",
      "batch_idx: 4\n",
      "0.876\n",
      "batch_idx: 5\n",
      "0.8766666666666667\n",
      "Training Epoch: 272, total loss: 9.889730\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.885\n",
      "batch_idx: 4\n",
      "0.888\n",
      "batch_idx: 5\n",
      "0.885\n",
      "Training Epoch: 273, total loss: 9.860868\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.8733333333333333\n",
      "batch_idx: 3\n",
      "0.8725\n",
      "batch_idx: 4\n",
      "0.862\n",
      "batch_idx: 5\n",
      "0.8616666666666667\n",
      "Training Epoch: 274, total loss: 9.957773\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.87\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.8925\n",
      "batch_idx: 4\n",
      "0.882\n",
      "batch_idx: 5\n",
      "0.885\n",
      "Training Epoch: 275, total loss: 9.849884\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.878\n",
      "batch_idx: 5\n",
      "0.88\n",
      "Training Epoch: 276, total loss: 9.896305\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.8875\n",
      "batch_idx: 4\n",
      "0.884\n",
      "batch_idx: 5\n",
      "0.885\n",
      "Training Epoch: 277, total loss: 9.873988\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.89\n",
      "batch_idx: 3\n",
      "0.8925\n",
      "batch_idx: 4\n",
      "0.888\n",
      "batch_idx: 5\n",
      "0.8833333333333333\n",
      "Training Epoch: 278, total loss: 9.869693\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.8725\n",
      "batch_idx: 4\n",
      "0.852\n",
      "batch_idx: 5\n",
      "0.86\n",
      "Training Epoch: 279, total loss: 9.985607\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.885\n",
      "batch_idx: 4\n",
      "0.886\n",
      "batch_idx: 5\n",
      "0.885\n",
      "Training Epoch: 280, total loss: 9.861316\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.85\n",
      "batch_idx: 2\n",
      "0.8533333333333334\n",
      "batch_idx: 3\n",
      "0.855\n",
      "batch_idx: 4\n",
      "0.858\n",
      "batch_idx: 5\n",
      "0.8666666666666667\n",
      "Training Epoch: 281, total loss: 9.937749\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.8766666666666667\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.884\n",
      "batch_idx: 5\n",
      "0.8833333333333333\n",
      "Training Epoch: 282, total loss: 9.851505\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.8975\n",
      "batch_idx: 4\n",
      "0.888\n",
      "batch_idx: 5\n",
      "0.885\n",
      "Training Epoch: 283, total loss: 9.868706\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.89\n",
      "batch_idx: 3\n",
      "0.88\n",
      "batch_idx: 4\n",
      "0.888\n",
      "batch_idx: 5\n",
      "0.87\n",
      "Training Epoch: 284, total loss: 9.940328\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.8866666666666667\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.9\n",
      "Training Epoch: 285, total loss: 9.801277\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.87\n",
      "batch_idx: 2\n",
      "0.89\n",
      "batch_idx: 3\n",
      "0.885\n",
      "batch_idx: 4\n",
      "0.89\n",
      "batch_idx: 5\n",
      "0.8933333333333333\n",
      "Training Epoch: 286, total loss: 9.815317\n",
      "batch_idx: 0\n",
      "0.82\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.8866666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 3\n",
      "0.8875\n",
      "batch_idx: 4\n",
      "0.89\n",
      "batch_idx: 5\n",
      "0.8916666666666667\n",
      "Training Epoch: 287, total loss: 9.810291\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.86\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.898\n",
      "batch_idx: 5\n",
      "0.9\n",
      "Training Epoch: 288, total loss: 9.796686\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8766666666666667\n",
      "batch_idx: 3\n",
      "0.8875\n",
      "batch_idx: 4\n",
      "0.894\n",
      "batch_idx: 5\n",
      "0.8983333333333333\n",
      "Training Epoch: 289, total loss: 9.799798\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.8866666666666667\n",
      "batch_idx: 3\n",
      "0.8875\n",
      "batch_idx: 4\n",
      "0.892\n",
      "batch_idx: 5\n",
      "0.8966666666666666\n",
      "Training Epoch: 290, total loss: 9.811973\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.8975\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 291, total loss: 9.767022\n",
      "batch_idx: 0\n",
      "0.81\n",
      "batch_idx: 1\n",
      "0.85\n",
      "batch_idx: 2\n",
      "0.8466666666666667\n",
      "batch_idx: 3\n",
      "0.865\n",
      "batch_idx: 4\n",
      "0.87\n",
      "batch_idx: 5\n",
      "0.8816666666666667\n",
      "Training Epoch: 292, total loss: 9.876977\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.845\n",
      "batch_idx: 2\n",
      "0.8666666666666667\n",
      "batch_idx: 3\n",
      "0.8775\n",
      "batch_idx: 4\n",
      "0.872\n",
      "batch_idx: 5\n",
      "0.8766666666666667\n",
      "Training Epoch: 293, total loss: 9.890067\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.8866666666666667\n",
      "batch_idx: 3\n",
      "0.88\n",
      "batch_idx: 4\n",
      "0.88\n",
      "batch_idx: 5\n",
      "0.8883333333333333\n",
      "Training Epoch: 294, total loss: 9.828603\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.905\n",
      "Training Epoch: 295, total loss: 9.774756\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8866666666666667\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.895\n",
      "Training Epoch: 296, total loss: 9.816758\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.8975\n",
      "batch_idx: 4\n",
      "0.902\n",
      "batch_idx: 5\n",
      "0.9\n",
      "Training Epoch: 297, total loss: 9.775751\n",
      "batch_idx: 0\n",
      "0.8\n",
      "batch_idx: 1\n",
      "0.835\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.876\n",
      "batch_idx: 5\n",
      "0.8716666666666667\n",
      "Training Epoch: 298, total loss: 9.937099\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.888\n",
      "batch_idx: 5\n",
      "0.8933333333333333\n",
      "Training Epoch: 299, total loss: 9.814524\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 300, total loss: 9.775760\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.8966666666666666\n",
      "Training Epoch: 301, total loss: 9.807068\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.886\n",
      "batch_idx: 5\n",
      "0.89\n",
      "Training Epoch: 302, total loss: 9.822411\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.8766666666666667\n",
      "batch_idx: 3\n",
      "0.8925\n",
      "batch_idx: 4\n",
      "0.892\n",
      "batch_idx: 5\n",
      "0.8966666666666666\n",
      "Training Epoch: 303, total loss: 9.795224\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.894\n",
      "batch_idx: 5\n",
      "0.8933333333333333\n",
      "Training Epoch: 304, total loss: 9.831109\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.89\n",
      "batch_idx: 4\n",
      "0.878\n",
      "batch_idx: 5\n",
      "0.8816666666666667\n",
      "Training Epoch: 305, total loss: 9.863170\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.905\n",
      "Training Epoch: 306, total loss: 9.780127\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.898\n",
      "batch_idx: 5\n",
      "0.8983333333333333\n",
      "Training Epoch: 307, total loss: 9.784828\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.89\n",
      "batch_idx: 4\n",
      "0.886\n",
      "batch_idx: 5\n",
      "0.8866666666666667\n",
      "Training Epoch: 308, total loss: 9.845904\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8733333333333333\n",
      "batch_idx: 3\n",
      "0.87\n",
      "batch_idx: 4\n",
      "0.878\n",
      "batch_idx: 5\n",
      "0.8866666666666667\n",
      "Training Epoch: 309, total loss: 9.841280\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 310, total loss: 9.784593\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.8975\n",
      "batch_idx: 4\n",
      "0.902\n",
      "batch_idx: 5\n",
      "0.895\n",
      "Training Epoch: 311, total loss: 9.823302\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.89\n",
      "batch_idx: 3\n",
      "0.8925\n",
      "batch_idx: 4\n",
      "0.894\n",
      "batch_idx: 5\n",
      "0.8983333333333333\n",
      "Training Epoch: 312, total loss: 9.785963\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.8966666666666666\n",
      "Training Epoch: 313, total loss: 9.810933\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.885\n",
      "batch_idx: 4\n",
      "0.894\n",
      "batch_idx: 5\n",
      "0.8866666666666667\n",
      "Training Epoch: 314, total loss: 9.840430\n",
      "batch_idx: 0\n",
      "0.83\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.86\n",
      "batch_idx: 3\n",
      "0.8675\n",
      "batch_idx: 4\n",
      "0.87\n",
      "batch_idx: 5\n",
      "0.8816666666666667\n",
      "Training Epoch: 315, total loss: 9.873098\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.87\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.885\n",
      "batch_idx: 4\n",
      "0.888\n",
      "batch_idx: 5\n",
      "0.885\n",
      "Training Epoch: 316, total loss: 9.839285\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.896\n",
      "batch_idx: 5\n",
      "0.9\n",
      "Training Epoch: 317, total loss: 9.789708\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.8766666666666667\n",
      "batch_idx: 3\n",
      "0.8925\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.8966666666666666\n",
      "Training Epoch: 318, total loss: 9.802521\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.87\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.885\n",
      "batch_idx: 4\n",
      "0.878\n",
      "batch_idx: 5\n",
      "0.8733333333333333\n",
      "Training Epoch: 319, total loss: 9.880228\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.8925\n",
      "batch_idx: 4\n",
      "0.886\n",
      "batch_idx: 5\n",
      "0.8783333333333333\n",
      "Training Epoch: 320, total loss: 9.881624\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.89\n",
      "batch_idx: 4\n",
      "0.888\n",
      "batch_idx: 5\n",
      "0.8866666666666667\n",
      "Training Epoch: 321, total loss: 9.855344\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.8975\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 322, total loss: 9.766090\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.894\n",
      "batch_idx: 5\n",
      "0.885\n",
      "Training Epoch: 323, total loss: 9.870616\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.88\n",
      "batch_idx: 4\n",
      "0.888\n",
      "batch_idx: 5\n",
      "0.89\n",
      "Training Epoch: 324, total loss: 9.831241\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.896\n",
      "batch_idx: 5\n",
      "0.8933333333333333\n",
      "Training Epoch: 325, total loss: 9.817372\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.8975\n",
      "batch_idx: 4\n",
      "0.896\n",
      "batch_idx: 5\n",
      "0.8983333333333333\n",
      "Training Epoch: 326, total loss: 9.784882\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.905\n",
      "Training Epoch: 327, total loss: 9.780103\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8666666666666667\n",
      "batch_idx: 3\n",
      "0.885\n",
      "batch_idx: 4\n",
      "0.892\n",
      "batch_idx: 5\n",
      "0.8883333333333333\n",
      "Training Epoch: 328, total loss: 9.831532\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.8975\n",
      "batch_idx: 4\n",
      "0.896\n",
      "batch_idx: 5\n",
      "0.8983333333333333\n",
      "Training Epoch: 329, total loss: 9.815391\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 330, total loss: 9.753106\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 331, total loss: 9.761384\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.8975\n",
      "batch_idx: 4\n",
      "0.89\n",
      "batch_idx: 5\n",
      "0.8916666666666667\n",
      "Training Epoch: 332, total loss: 9.813599\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 333, total loss: 9.780039\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.89\n",
      "batch_idx: 3\n",
      "0.88\n",
      "batch_idx: 4\n",
      "0.894\n",
      "batch_idx: 5\n",
      "0.895\n",
      "Training Epoch: 334, total loss: 9.800752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 335, total loss: 9.774308\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.8875\n",
      "batch_idx: 4\n",
      "0.894\n",
      "batch_idx: 5\n",
      "0.8983333333333333\n",
      "Training Epoch: 336, total loss: 9.798980\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.8825\n",
      "batch_idx: 4\n",
      "0.882\n",
      "batch_idx: 5\n",
      "0.89\n",
      "Training Epoch: 337, total loss: 9.839159\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.8966666666666666\n",
      "Training Epoch: 338, total loss: 9.789545\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.8925\n",
      "batch_idx: 4\n",
      "0.898\n",
      "batch_idx: 5\n",
      "0.9016666666666666\n",
      "Training Epoch: 339, total loss: 9.771534\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.8933333333333333\n",
      "Training Epoch: 340, total loss: 9.811897\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.8833333333333333\n",
      "Training Epoch: 341, total loss: 9.863487\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.896\n",
      "batch_idx: 5\n",
      "0.895\n",
      "Training Epoch: 342, total loss: 9.811860\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.8725\n",
      "batch_idx: 4\n",
      "0.88\n",
      "batch_idx: 5\n",
      "0.8783333333333333\n",
      "Training Epoch: 343, total loss: 9.862982\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8666666666666667\n",
      "batch_idx: 3\n",
      "0.8675\n",
      "batch_idx: 4\n",
      "0.884\n",
      "batch_idx: 5\n",
      "0.8866666666666667\n",
      "Training Epoch: 344, total loss: 9.838172\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.895\n",
      "Training Epoch: 345, total loss: 9.814385\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9066666666666666\n",
      "Training Epoch: 346, total loss: 9.776750\n",
      "batch_idx: 0\n",
      "0.83\n",
      "batch_idx: 1\n",
      "0.86\n",
      "batch_idx: 2\n",
      "0.8666666666666667\n",
      "batch_idx: 3\n",
      "0.8875\n",
      "batch_idx: 4\n",
      "0.882\n",
      "batch_idx: 5\n",
      "0.8933333333333333\n",
      "Training Epoch: 347, total loss: 9.833675\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 348, total loss: 9.762417\n",
      "batch_idx: 0\n",
      "0.83\n",
      "batch_idx: 1\n",
      "0.87\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.8825\n",
      "batch_idx: 4\n",
      "0.884\n",
      "batch_idx: 5\n",
      "0.8833333333333333\n",
      "Training Epoch: 349, total loss: 9.871785\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 350, total loss: 9.791072\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 351, total loss: 9.773319\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.8875\n",
      "batch_idx: 4\n",
      "0.89\n",
      "batch_idx: 5\n",
      "0.89\n",
      "Training Epoch: 352, total loss: 9.832535\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.8975\n",
      "batch_idx: 4\n",
      "0.894\n",
      "batch_idx: 5\n",
      "0.8983333333333333\n",
      "Training Epoch: 353, total loss: 9.808157\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.87\n",
      "batch_idx: 2\n",
      "0.8866666666666667\n",
      "batch_idx: 3\n",
      "0.8825\n",
      "batch_idx: 4\n",
      "0.894\n",
      "batch_idx: 5\n",
      "0.895\n",
      "Training Epoch: 354, total loss: 9.810397\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.89\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.8983333333333333\n",
      "Training Epoch: 355, total loss: 9.811646\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.896\n",
      "batch_idx: 5\n",
      "0.8916666666666667\n",
      "Training Epoch: 356, total loss: 9.820730\n",
      "batch_idx: 0\n",
      "0.8\n",
      "batch_idx: 1\n",
      "0.85\n",
      "batch_idx: 2\n",
      "0.8733333333333333\n",
      "batch_idx: 3\n",
      "0.8825\n",
      "batch_idx: 4\n",
      "0.894\n",
      "batch_idx: 5\n",
      "0.895\n",
      "Training Epoch: 357, total loss: 9.813861\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.8866666666666667\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.894\n",
      "batch_idx: 5\n",
      "0.8966666666666666\n",
      "Training Epoch: 358, total loss: 9.809190\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9016666666666666\n",
      "Training Epoch: 359, total loss: 9.784526\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.8625\n",
      "batch_idx: 4\n",
      "0.872\n",
      "batch_idx: 5\n",
      "0.8783333333333333\n",
      "Training Epoch: 360, total loss: 9.918406\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.86\n",
      "batch_idx: 2\n",
      "0.8766666666666667\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.896\n",
      "batch_idx: 5\n",
      "0.9016666666666666\n",
      "Training Epoch: 361, total loss: 9.774917\n",
      "batch_idx: 0\n",
      "0.97\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9066666666666666\n",
      "Training Epoch: 362, total loss: 9.747697\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.894\n",
      "batch_idx: 5\n",
      "0.9016666666666666\n",
      "Training Epoch: 363, total loss: 9.768284\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 364, total loss: 9.740796\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.8983333333333333\n",
      "Training Epoch: 365, total loss: 9.780892\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.88\n",
      "batch_idx: 4\n",
      "0.882\n",
      "batch_idx: 5\n",
      "0.885\n",
      "Training Epoch: 366, total loss: 9.860381\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.8825\n",
      "batch_idx: 4\n",
      "0.89\n",
      "batch_idx: 5\n",
      "0.8933333333333333\n",
      "Training Epoch: 367, total loss: 9.841303\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.8625\n",
      "batch_idx: 4\n",
      "0.874\n",
      "batch_idx: 5\n",
      "0.875\n",
      "Training Epoch: 368, total loss: 9.902655\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.8775\n",
      "batch_idx: 4\n",
      "0.882\n",
      "batch_idx: 5\n",
      "0.89\n",
      "Training Epoch: 369, total loss: 9.847403\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.8925\n",
      "batch_idx: 4\n",
      "0.892\n",
      "batch_idx: 5\n",
      "0.89\n",
      "Training Epoch: 370, total loss: 9.845165\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.86\n",
      "batch_idx: 3\n",
      "0.8525\n",
      "batch_idx: 4\n",
      "0.86\n",
      "batch_idx: 5\n",
      "0.86\n",
      "Training Epoch: 371, total loss: 9.981469\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.89\n",
      "batch_idx: 3\n",
      "0.8825\n",
      "batch_idx: 4\n",
      "0.894\n",
      "batch_idx: 5\n",
      "0.9016666666666666\n",
      "Training Epoch: 372, total loss: 9.790832\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.835\n",
      "batch_idx: 2\n",
      "0.85\n",
      "batch_idx: 3\n",
      "0.8775\n",
      "batch_idx: 4\n",
      "0.876\n",
      "batch_idx: 5\n",
      "0.8883333333333333\n",
      "Training Epoch: 373, total loss: 9.846666\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 374, total loss: 9.760483\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 375, total loss: 9.753275\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9016666666666666\n",
      "Training Epoch: 376, total loss: 9.770814\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.905\n",
      "Training Epoch: 377, total loss: 9.772070\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.835\n",
      "batch_idx: 2\n",
      "0.8733333333333333\n",
      "batch_idx: 3\n",
      "0.885\n",
      "batch_idx: 4\n",
      "0.884\n",
      "batch_idx: 5\n",
      "0.8866666666666667\n",
      "Training Epoch: 378, total loss: 9.828305\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.8983333333333333\n",
      "Training Epoch: 379, total loss: 9.794210\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 380, total loss: 9.762321\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 381, total loss: 9.739159\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.87\n",
      "batch_idx: 2\n",
      "0.8566666666666667\n",
      "batch_idx: 3\n",
      "0.8825\n",
      "batch_idx: 4\n",
      "0.894\n",
      "batch_idx: 5\n",
      "0.8983333333333333\n",
      "Training Epoch: 382, total loss: 9.805831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.8866666666666667\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.8966666666666666\n",
      "Training Epoch: 383, total loss: 9.786934\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.894\n",
      "batch_idx: 5\n",
      "0.9016666666666666\n",
      "Training Epoch: 384, total loss: 9.784966\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 385, total loss: 9.724315\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.8733333333333333\n",
      "batch_idx: 3\n",
      "0.8825\n",
      "batch_idx: 4\n",
      "0.87\n",
      "batch_idx: 5\n",
      "0.8833333333333333\n",
      "Training Epoch: 386, total loss: 9.860682\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 387, total loss: 9.735329\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.9016666666666666\n",
      "Training Epoch: 388, total loss: 9.791579\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.8925\n",
      "batch_idx: 4\n",
      "0.896\n",
      "batch_idx: 5\n",
      "0.89\n",
      "Training Epoch: 389, total loss: 9.827368\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.902\n",
      "batch_idx: 5\n",
      "0.905\n",
      "Training Epoch: 390, total loss: 9.764213\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 391, total loss: 9.776460\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 392, total loss: 9.739190\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.898\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 393, total loss: 9.761204\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.9016666666666666\n",
      "Training Epoch: 394, total loss: 9.790451\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 395, total loss: 9.721272\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.896\n",
      "batch_idx: 5\n",
      "0.89\n",
      "Training Epoch: 396, total loss: 9.835225\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.8975\n",
      "batch_idx: 4\n",
      "0.898\n",
      "batch_idx: 5\n",
      "0.8916666666666667\n",
      "Training Epoch: 397, total loss: 9.853817\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 398, total loss: 9.751207\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.8966666666666666\n",
      "Training Epoch: 399, total loss: 9.812071\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.902\n",
      "batch_idx: 5\n",
      "0.9016666666666666\n",
      "Training Epoch: 400, total loss: 9.779696\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.8925\n",
      "batch_idx: 4\n",
      "0.896\n",
      "batch_idx: 5\n",
      "0.89\n",
      "Training Epoch: 401, total loss: 9.821515\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.9\n",
      "Training Epoch: 402, total loss: 9.769027\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9333333333333333\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 403, total loss: 9.719179\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 404, total loss: 9.743440\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9066666666666666\n",
      "Training Epoch: 405, total loss: 9.743662\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 406, total loss: 9.720437\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.87\n",
      "batch_idx: 2\n",
      "0.89\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 407, total loss: 9.760629\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.8975\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 408, total loss: 9.761430\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.9\n",
      "Training Epoch: 409, total loss: 9.788456\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 410, total loss: 9.715893\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 411, total loss: 9.746341\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.905\n",
      "Training Epoch: 412, total loss: 9.741167\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 413, total loss: 9.746024\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.905\n",
      "Training Epoch: 414, total loss: 9.751090\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 415, total loss: 9.734560\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 416, total loss: 9.737555\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 417, total loss: 9.680500\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 418, total loss: 9.735816\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 419, total loss: 9.726528\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "if __name__ == \"__main__\":\n",
    "    ####################################################################################\n",
    "    # NFM 模型\n",
    "    ####################################################################################\n",
    "    BATCH_SIZE=100\n",
    "    \"\"\"\n",
    "    training_data, training_label, dense_features_col, sparse_features_col = getTrainData(nfm_config['train_file'], nfm_config['fea_file'])\n",
    "    train_dataset = Data.TensorDataset(torch.tensor(training_data).float(), torch.tensor(training_label).float())\n",
    "\n",
    "    test_data = getTestData(nfm_config['test_file'])\n",
    "    test_dataset = Data.TensorDataset(torch.tensor(test_data).float())\n",
    "    \n",
    "    test_data = getTestData(nfm_config['test_file'])\n",
    "    test_dataset = Data.TensorDataset(torch.tensor(test_data).float())\n",
    "\n",
    "    \"\"\"\n",
    "    #device = torch.device('cuda:0')\n",
    "    #epoch=0\n",
    "    \n",
    "    #model=nn.Linear(10149,16).to(device)\n",
    "    #model=nn.Linear(10149,16)\n",
    "    #model=nn.ReLU(nn.Linear(10149,16))#RuntimeError: all elements of input should be between 0 and 1\n",
    "    #print('model:',model)\n",
    "    nfm = NFM(nfm_config).cuda()#加了device防止出现GPU CPU两种设备的错误提示\n",
    "    print(\"nfm:\",nfm)\n",
    "    #print(nfm)\n",
    "    #nfm.train()\n",
    "    #u=nfm.parameters()\n",
    "    #print(u)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    optimizer = torch.optim.Adam(nfm.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    total = 0\n",
    "    #loss_func = torch.nn.BCELoss()\n",
    "    loss_func=torch.nn.CrossEntropyLoss()\n",
    "    #loss_func=nn.MultiLabelSoftMarginLoss()\n",
    "    #loss_func=torch.nn.LogSoftmax()\n",
    "    \n",
    "    #model=nn.Softmax(nn.Linear(10149,16)).to(device)\n",
    "    # 从DataLoader中获取小批量的id以及数据\n",
    "    for epoch_id in range(420):\n",
    "        correct=0\n",
    "        total=0\n",
    "        for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "            x = Variable(x)\n",
    "            labels = Variable(labels)\n",
    "            \n",
    "            \n",
    "            #x = torch.tensor(x, dtype=torch.float)\n",
    "            #x=x.clone().detach().requires_grad_(True)\n",
    "            x=torch.tensor(x,dtype=torch.float)\n",
    "            labels=torch.tensor(labels,dtype=torch.float)\n",
    "            x, labels = x.cuda(), labels.cuda()\n",
    "            \n",
    "            #print(\"labels:\",labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_predict = nfm(x)\n",
    "            #print(\"y_predict:\",y_predict)\n",
    "            #loss = loss_func(y_predict.view(-1), labels)\n",
    "            loss = loss_func(y_predict, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss = loss.item()\n",
    "            #loss, predicted = self._train_single_batch(x, labels)\n",
    "\n",
    "            total += loss\n",
    "            \n",
    "            \n",
    "            \n",
    "            predicted = torch.max(y_predict.data,1)\n",
    "            #print(\"predicted:\",predicted)\n",
    "            predicted = torch.max(y_predict.data,1)[1]\n",
    "            \n",
    "            \n",
    "            labels=torch.max(labels,1)\n",
    "            #print(\"labels:\",labels)\n",
    "            labels=labels[1]\n",
    "            \n",
    "            correct += (predicted == labels).sum()\n",
    "            #print(\"correct:\",correct)\n",
    "            #correct=correct[0]\n",
    "            #print(\"new_correct:\",float(correct))\n",
    "            correct=float(correct)   \n",
    "            #if batch_idx % 10 == 0:\n",
    "            print(\"batch_idx:\",batch_idx)\n",
    "            print(correct/(BATCH_SIZE*(batch_idx+1)))\n",
    "            \n",
    "            \"\"\"\n",
    "            #y_predict.detach().numpy()\n",
    "            pred = y_predict\n",
    "            print(\"pred:\",pred.shape)\n",
    "            y=labels.clone().detach().requires_grad_(True)\n",
    "            print(\"y:\",y.shape)\n",
    "            #y=labels.data.cpu().numpy()\n",
    "            #y = labels.detach().numpy()\n",
    "            roc_auc_score(y, pred)\n",
    "            \"\"\"\n",
    "           \n",
    "            # print('[Training Epoch: {}] Batch: {}, Loss: {}'.format(epoch_id, batch_id, loss))\n",
    "        print(\"Training Epoch: %d, total loss: %f\" % (epoch_id, total))\n",
    "        #print(\"auc:\",roc_auc_score)\n",
    "#功能：保存训练完的网络的各层参数（即weights和bias)\n",
    "path='dataset/xiaoguan/RF/RF_for_train/train_class_9/model/gene_4000_NFM.pkl'\n",
    "torch.save(nfm.state_dict(),path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e849b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NFM(\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (linear_model1): Linear(in_features=4225, out_features=1000, bias=True)\n",
      "  (BN_linear1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear_model2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (BN_linear2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (embedding_layers): Embedding(1001, 100)\n",
      "  (bi_pooling): BiInteractionPooling()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (BN_bi): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dnn_layers): ModuleList(\n",
      "    (0): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (1): Linear(in_features=100, out_features=9, bias=True)\n",
      "  )\n",
      "  (dnn_softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.905\n",
      "Training Epoch: 0, total loss: 9.773653\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.8925\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.8983333333333333\n",
      "Training Epoch: 1, total loss: 9.808403\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 2, total loss: 9.752872\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 3, total loss: 9.726529\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 4, total loss: 9.707275\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.93\n",
      "batch_idx: 4\n",
      "0.932\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 5, total loss: 9.682479\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 6, total loss: 9.661366\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 7, total loss: 9.686193\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.94\n",
      "batch_idx: 3\n",
      "0.9325\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 8, total loss: 9.745925\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 9, total loss: 9.749588\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 10, total loss: 9.725340\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9066666666666666\n",
      "Training Epoch: 11, total loss: 9.759891\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9016666666666666\n",
      "Training Epoch: 12, total loss: 9.783574\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.902\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 13, total loss: 9.772373\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.8975\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9016666666666666\n",
      "Training Epoch: 14, total loss: 9.798840\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 15, total loss: 9.748029\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 16, total loss: 9.694140\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 17, total loss: 9.715737\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 18, total loss: 9.738862\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 19, total loss: 9.721918\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.902\n",
      "batch_idx: 5\n",
      "0.8983333333333333\n",
      "Training Epoch: 20, total loss: 9.770497\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 21, total loss: 9.707957\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 22, total loss: 9.711547\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.902\n",
      "batch_idx: 5\n",
      "0.9016666666666666\n",
      "Training Epoch: 23, total loss: 9.766119\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 24, total loss: 9.717227\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 25, total loss: 9.764164\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 26, total loss: 9.740082\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.87\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.8925\n",
      "batch_idx: 4\n",
      "0.896\n",
      "batch_idx: 5\n",
      "0.9016666666666666\n",
      "Training Epoch: 27, total loss: 9.768220\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 28, total loss: 9.712578\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 29, total loss: 9.765541\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.8975\n",
      "batch_idx: 4\n",
      "0.878\n",
      "batch_idx: 5\n",
      "0.8833333333333333\n",
      "Training Epoch: 30, total loss: 9.861830\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 31, total loss: 9.742737\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9366666666666666\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 32, total loss: 9.746591\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 33, total loss: 9.775400\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.928\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 34, total loss: 9.714994\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 35, total loss: 9.749330\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.8866666666666667\n",
      "batch_idx: 3\n",
      "0.8925\n",
      "batch_idx: 4\n",
      "0.902\n",
      "batch_idx: 5\n",
      "0.9066666666666666\n",
      "Training Epoch: 36, total loss: 9.736741\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.9433333333333334\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 37, total loss: 9.735628\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 38, total loss: 9.739178\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 39, total loss: 9.720382\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 40, total loss: 9.732670\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 41, total loss: 9.707328\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.894\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 42, total loss: 9.753506\n",
      "batch_idx: 0\n",
      "0.96\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.94\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.932\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 43, total loss: 9.688997\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 44, total loss: 9.718628\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 45, total loss: 9.687495\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.8975\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 46, total loss: 9.725379\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.8966666666666666\n",
      "Training Epoch: 47, total loss: 9.775776\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 48, total loss: 9.741647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 49, total loss: 9.701480\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 50, total loss: 9.710079\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 51, total loss: 9.724454\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 52, total loss: 9.697643\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 53, total loss: 9.755448\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.902\n",
      "batch_idx: 5\n",
      "0.905\n",
      "Training Epoch: 54, total loss: 9.779339\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 55, total loss: 9.701471\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.8925\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 56, total loss: 9.733983\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 57, total loss: 9.685948\n",
      "batch_idx: 0\n",
      "0.79\n",
      "batch_idx: 1\n",
      "0.86\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.8875\n",
      "batch_idx: 4\n",
      "0.896\n",
      "batch_idx: 5\n",
      "0.905\n",
      "Training Epoch: 58, total loss: 9.754759\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.89\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.905\n",
      "Training Epoch: 59, total loss: 9.753577\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 60, total loss: 9.723084\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 61, total loss: 9.722437\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 62, total loss: 9.719506\n",
      "batch_idx: 0\n",
      "0.97\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 63, total loss: 9.715731\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.905\n",
      "Training Epoch: 64, total loss: 9.769917\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.925\n",
      "Training Epoch: 65, total loss: 9.673790\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.8975\n",
      "batch_idx: 4\n",
      "0.898\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 66, total loss: 9.765164\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.945\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9066666666666666\n",
      "Training Epoch: 67, total loss: 9.748654\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.905\n",
      "Training Epoch: 68, total loss: 9.759732\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9066666666666666\n",
      "Training Epoch: 69, total loss: 9.749167\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.95\n",
      "batch_idx: 2\n",
      "0.9333333333333333\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 70, total loss: 9.741029\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 71, total loss: 9.710985\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.8766666666666667\n",
      "batch_idx: 3\n",
      "0.8875\n",
      "batch_idx: 4\n",
      "0.888\n",
      "batch_idx: 5\n",
      "0.8966666666666666\n",
      "Training Epoch: 72, total loss: 9.799101\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.9066666666666666\n",
      "Training Epoch: 73, total loss: 9.763292\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 74, total loss: 9.720242\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 75, total loss: 9.762946\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 76, total loss: 9.725998\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 77, total loss: 9.728202\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 78, total loss: 9.743390\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 79, total loss: 9.720035\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 80, total loss: 9.751296\n",
      "batch_idx: 0\n",
      "0.97\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 81, total loss: 9.716015\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.902\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 82, total loss: 9.776043\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 83, total loss: 9.702377\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 84, total loss: 9.720670\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 85, total loss: 9.734073\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 86, total loss: 9.723708\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9066666666666666\n",
      "Training Epoch: 87, total loss: 9.738562\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.9433333333333334\n",
      "batch_idx: 3\n",
      "0.9375\n",
      "batch_idx: 4\n",
      "0.936\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 88, total loss: 9.711559\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.905\n",
      "Training Epoch: 89, total loss: 9.769076\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.905\n",
      "Training Epoch: 90, total loss: 9.761431\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 91, total loss: 9.738228\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.9\n",
      "Training Epoch: 92, total loss: 9.777279\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.89\n",
      "batch_idx: 3\n",
      "0.885\n",
      "batch_idx: 4\n",
      "0.894\n",
      "batch_idx: 5\n",
      "0.8983333333333333\n",
      "Training Epoch: 93, total loss: 9.791658\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 94, total loss: 9.748696\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.8975\n",
      "batch_idx: 4\n",
      "0.902\n",
      "batch_idx: 5\n",
      "0.9016666666666666\n",
      "Training Epoch: 95, total loss: 9.795416\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.8775\n",
      "batch_idx: 4\n",
      "0.872\n",
      "batch_idx: 5\n",
      "0.8866666666666667\n",
      "Training Epoch: 96, total loss: 9.857573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 97, total loss: 9.722718\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 98, total loss: 9.699782\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 99, total loss: 9.728489\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.93\n",
      "batch_idx: 5\n",
      "0.93\n",
      "Training Epoch: 100, total loss: 9.649209\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 101, total loss: 9.710133\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 102, total loss: 9.692852\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 103, total loss: 9.735292\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 104, total loss: 9.699321\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.894\n",
      "batch_idx: 5\n",
      "0.8966666666666666\n",
      "Training Epoch: 105, total loss: 9.793823\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.9066666666666666\n",
      "Training Epoch: 106, total loss: 9.742886\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.9066666666666666\n",
      "Training Epoch: 107, total loss: 9.757671\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 108, total loss: 9.720575\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.9466666666666667\n",
      "batch_idx: 3\n",
      "0.94\n",
      "batch_idx: 4\n",
      "0.934\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 109, total loss: 9.692315\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 110, total loss: 9.760041\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 111, total loss: 9.732241\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 112, total loss: 9.701525\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 113, total loss: 9.712461\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.925\n",
      "Training Epoch: 114, total loss: 9.685622\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 115, total loss: 9.696018\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 116, total loss: 9.730085\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 117, total loss: 9.706996\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 118, total loss: 9.706084\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.905\n",
      "Training Epoch: 119, total loss: 9.747552\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 120, total loss: 9.763379\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9016666666666666\n",
      "Training Epoch: 121, total loss: 9.762113\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 122, total loss: 9.696820\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 123, total loss: 9.699239\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 124, total loss: 9.725591\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 125, total loss: 9.673969\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 126, total loss: 9.730503\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.87\n",
      "batch_idx: 4\n",
      "0.88\n",
      "batch_idx: 5\n",
      "0.8883333333333333\n",
      "Training Epoch: 127, total loss: 9.825177\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.8983333333333333\n",
      "Training Epoch: 128, total loss: 9.785912\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 129, total loss: 9.742764\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 130, total loss: 9.716374\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 131, total loss: 9.713442\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.945\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 132, total loss: 9.726028\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 133, total loss: 9.686342\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 134, total loss: 9.735348\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 135, total loss: 9.717155\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.8975\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.8933333333333333\n",
      "Training Epoch: 136, total loss: 9.805588\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 137, total loss: 9.714924\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.902\n",
      "batch_idx: 5\n",
      "0.905\n",
      "Training Epoch: 138, total loss: 9.759828\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 139, total loss: 9.704913\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.905\n",
      "Training Epoch: 140, total loss: 9.781098\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 141, total loss: 9.742776\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 142, total loss: 9.725364\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.896\n",
      "batch_idx: 5\n",
      "0.8983333333333333\n",
      "Training Epoch: 143, total loss: 9.803259\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 144, total loss: 9.712015\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 145, total loss: 9.737158\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 146, total loss: 9.727640\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.9066666666666666\n",
      "Training Epoch: 147, total loss: 9.736292\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.76\n",
      "batch_idx: 2\n",
      "0.82\n",
      "batch_idx: 3\n",
      "0.8425\n",
      "batch_idx: 4\n",
      "0.852\n",
      "batch_idx: 5\n",
      "0.86\n",
      "Training Epoch: 148, total loss: 9.983304\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 149, total loss: 9.751449\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 150, total loss: 9.719547\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 151, total loss: 9.704318\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9333333333333333\n",
      "batch_idx: 3\n",
      "0.93\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 152, total loss: 9.704501\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.9016666666666666\n",
      "Training Epoch: 153, total loss: 9.778437\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 154, total loss: 9.681544\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 155, total loss: 9.699309\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.8975\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 156, total loss: 9.793789\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.902\n",
      "batch_idx: 5\n",
      "0.9016666666666666\n",
      "Training Epoch: 157, total loss: 9.787384\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.896\n",
      "batch_idx: 5\n",
      "0.9\n",
      "Training Epoch: 158, total loss: 9.801766\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.902\n",
      "batch_idx: 5\n",
      "0.8966666666666666\n",
      "Training Epoch: 159, total loss: 9.804959\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 160, total loss: 9.730711\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 161, total loss: 9.721290\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.9066666666666666\n",
      "Training Epoch: 162, total loss: 9.728791\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.87\n",
      "batch_idx: 3\n",
      "0.8825\n",
      "batch_idx: 4\n",
      "0.89\n",
      "batch_idx: 5\n",
      "0.9\n",
      "Training Epoch: 163, total loss: 9.764589\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 164, total loss: 9.694182\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 165, total loss: 9.722601\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 166, total loss: 9.717970\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.93\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 167, total loss: 9.654003\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9066666666666666\n",
      "Training Epoch: 168, total loss: 9.750576\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.9366666666666666\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 169, total loss: 9.691805\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 170, total loss: 9.718090\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.89\n",
      "batch_idx: 3\n",
      "0.8975\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 171, total loss: 9.712205\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.93\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 172, total loss: 9.696916\n",
      "batch_idx: 0\n",
      "0.82\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 173, total loss: 9.748183\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 174, total loss: 9.686441\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 175, total loss: 9.729778\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9333333333333333\n",
      "batch_idx: 3\n",
      "0.935\n",
      "batch_idx: 4\n",
      "0.932\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 176, total loss: 9.694693\n",
      "batch_idx: 0\n",
      "0.96\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9333333333333333\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.925\n",
      "Training Epoch: 177, total loss: 9.674986\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.928\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 178, total loss: 9.670008\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 179, total loss: 9.704096\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 180, total loss: 9.704264\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 181, total loss: 9.694958\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 182, total loss: 9.778860\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 183, total loss: 9.702058\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 184, total loss: 9.722631\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9366666666666666\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 185, total loss: 9.717449\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 186, total loss: 9.721017\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 187, total loss: 9.718794\n",
      "batch_idx: 0\n",
      "0.97\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 188, total loss: 9.674301\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 189, total loss: 9.737950\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 190, total loss: 9.697997\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9433333333333334\n",
      "batch_idx: 3\n",
      "0.9375\n",
      "batch_idx: 4\n",
      "0.932\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 191, total loss: 9.664499\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.9066666666666666\n",
      "Training Epoch: 192, total loss: 9.744431\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 193, total loss: 9.676123\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.8875\n",
      "batch_idx: 4\n",
      "0.902\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 194, total loss: 9.731406\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 195, total loss: 9.696017\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.945\n",
      "batch_idx: 2\n",
      "0.9466666666666667\n",
      "batch_idx: 3\n",
      "0.9375\n",
      "batch_idx: 4\n",
      "0.93\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 196, total loss: 9.663596\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.9325\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 197, total loss: 9.708017\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 198, total loss: 9.678636\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 199, total loss: 9.675041\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 200, total loss: 9.661971\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 201, total loss: 9.656007\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 202, total loss: 9.685577\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 203, total loss: 9.726195\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9366666666666666\n",
      "batch_idx: 3\n",
      "0.93\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 204, total loss: 9.694919\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 205, total loss: 9.726659\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.93\n",
      "batch_idx: 4\n",
      "0.932\n",
      "batch_idx: 5\n",
      "0.925\n",
      "Training Epoch: 206, total loss: 9.678840\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.89\n",
      "batch_idx: 4\n",
      "0.902\n",
      "batch_idx: 5\n",
      "0.905\n",
      "Training Epoch: 207, total loss: 9.771205\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 208, total loss: 9.713010\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 209, total loss: 9.714903\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 210, total loss: 9.728854\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 211, total loss: 9.705220\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.9016666666666666\n",
      "Training Epoch: 212, total loss: 9.765326\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 213, total loss: 9.711915\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 214, total loss: 9.700261\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.945\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.9\n",
      "Training Epoch: 215, total loss: 9.791747\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 216, total loss: 9.707885\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9433333333333334\n",
      "batch_idx: 3\n",
      "0.94\n",
      "batch_idx: 4\n",
      "0.942\n",
      "batch_idx: 5\n",
      "0.9283333333333333\n",
      "Training Epoch: 217, total loss: 9.652139\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 218, total loss: 9.701370\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 219, total loss: 9.665494\n",
      "batch_idx: 0\n",
      "0.96\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.9066666666666666\n",
      "Training Epoch: 220, total loss: 9.731461\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 221, total loss: 9.692634\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 222, total loss: 9.695702\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 223, total loss: 9.715418\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 224, total loss: 9.737096\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 225, total loss: 9.754751\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 226, total loss: 9.704168\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 227, total loss: 9.763607\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9433333333333334\n",
      "batch_idx: 3\n",
      "0.9475\n",
      "batch_idx: 4\n",
      "0.93\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 228, total loss: 9.669699\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 229, total loss: 9.674343\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 230, total loss: 9.692055\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 231, total loss: 9.683804\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 232, total loss: 9.731548\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 233, total loss: 9.735308\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 234, total loss: 9.733890\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 235, total loss: 9.735545\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 236, total loss: 9.668706\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.93\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 237, total loss: 9.703088\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 238, total loss: 9.716346\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 239, total loss: 9.669416\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.925\n",
      "Training Epoch: 240, total loss: 9.666412\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 241, total loss: 9.692890\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 242, total loss: 9.667778\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 243, total loss: 9.681163\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 244, total loss: 9.681703\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.945\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.9325\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 245, total loss: 9.682984\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.902\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 246, total loss: 9.704896\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.9016666666666666\n",
      "Training Epoch: 247, total loss: 9.774226\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.945\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 248, total loss: 9.681765\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.928\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 249, total loss: 9.688607\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 250, total loss: 9.739848\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 251, total loss: 9.695104\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.945\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 252, total loss: 9.710284\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 253, total loss: 9.671303\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 254, total loss: 9.683511\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.89\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 255, total loss: 9.690735\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.9016666666666666\n",
      "Training Epoch: 256, total loss: 9.756122\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 257, total loss: 9.675153\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 258, total loss: 9.708107\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.94\n",
      "batch_idx: 3\n",
      "0.9375\n",
      "batch_idx: 4\n",
      "0.932\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 259, total loss: 9.692307\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.95\n",
      "batch_idx: 2\n",
      "0.95\n",
      "batch_idx: 3\n",
      "0.9375\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 260, total loss: 9.707749\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 261, total loss: 9.686459\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 262, total loss: 9.729596\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 263, total loss: 9.725067\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.965\n",
      "batch_idx: 2\n",
      "0.9566666666666667\n",
      "batch_idx: 3\n",
      "0.94\n",
      "batch_idx: 4\n",
      "0.934\n",
      "batch_idx: 5\n",
      "0.9316666666666666\n",
      "Training Epoch: 264, total loss: 9.654586\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 265, total loss: 9.688786\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9066666666666666\n",
      "Training Epoch: 266, total loss: 9.745744\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.93\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 267, total loss: 9.736899\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.9283333333333333\n",
      "Training Epoch: 268, total loss: 9.657618\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.9366666666666666\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.93\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 269, total loss: 9.675453\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.89\n",
      "batch_idx: 5\n",
      "0.8916666666666667\n",
      "Training Epoch: 270, total loss: 9.819247\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.8975\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 271, total loss: 9.696941\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 272, total loss: 9.713457\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.945\n",
      "batch_idx: 2\n",
      "0.9433333333333334\n",
      "batch_idx: 3\n",
      "0.9475\n",
      "batch_idx: 4\n",
      "0.93\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 273, total loss: 9.679779\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 274, total loss: 9.692894\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 275, total loss: 9.732951\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 276, total loss: 9.707671\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.905\n",
      "Training Epoch: 277, total loss: 9.746390\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 278, total loss: 9.669257\n",
      "batch_idx: 0\n",
      "0.96\n",
      "batch_idx: 1\n",
      "0.95\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 279, total loss: 9.696565\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9333333333333333\n",
      "batch_idx: 3\n",
      "0.935\n",
      "batch_idx: 4\n",
      "0.928\n",
      "batch_idx: 5\n",
      "0.925\n",
      "Training Epoch: 280, total loss: 9.667174\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9333333333333333\n",
      "batch_idx: 3\n",
      "0.93\n",
      "batch_idx: 4\n",
      "0.93\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 281, total loss: 9.658479\n",
      "batch_idx: 0\n",
      "0.97\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 282, total loss: 9.698552\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.965\n",
      "batch_idx: 2\n",
      "0.9466666666666667\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.925\n",
      "Training Epoch: 283, total loss: 9.664163\n",
      "batch_idx: 0\n",
      "0.96\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.94\n",
      "batch_idx: 3\n",
      "0.9325\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 284, total loss: 9.690393\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 285, total loss: 9.707215\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 286, total loss: 9.718951\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 287, total loss: 9.706876\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 288, total loss: 9.707558\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.928\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 289, total loss: 9.685062\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 290, total loss: 9.708694\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 291, total loss: 9.734029\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 292, total loss: 9.688957\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 293, total loss: 9.706444\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 294, total loss: 9.701329\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.89\n",
      "batch_idx: 4\n",
      "0.896\n",
      "batch_idx: 5\n",
      "0.9066666666666666\n",
      "Training Epoch: 295, total loss: 9.757051\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 296, total loss: 9.705487\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9016666666666666\n",
      "Training Epoch: 297, total loss: 9.749613\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 298, total loss: 9.717036\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9066666666666666\n",
      "Training Epoch: 299, total loss: 9.747678\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.93\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 300, total loss: 9.680499\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 301, total loss: 9.719766\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.925\n",
      "Training Epoch: 302, total loss: 9.671845\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9333333333333333\n",
      "batch_idx: 3\n",
      "0.9375\n",
      "batch_idx: 4\n",
      "0.928\n",
      "batch_idx: 5\n",
      "0.9283333333333333\n",
      "Training Epoch: 303, total loss: 9.650691\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9333333333333333\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.925\n",
      "Training Epoch: 304, total loss: 9.683275\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.9333333333333333\n",
      "batch_idx: 3\n",
      "0.9325\n",
      "batch_idx: 4\n",
      "0.93\n",
      "batch_idx: 5\n",
      "0.9283333333333333\n",
      "Training Epoch: 305, total loss: 9.654081\n",
      "batch_idx: 0\n",
      "0.96\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.93\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 306, total loss: 9.693465\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 307, total loss: 9.745539\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 308, total loss: 9.689350\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 309, total loss: 9.715368\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 310, total loss: 9.703466\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9333333333333333\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 311, total loss: 9.702067\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 312, total loss: 9.700905\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 313, total loss: 9.680178\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 314, total loss: 9.736613\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 315, total loss: 9.717263\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.932\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 316, total loss: 9.688004\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 317, total loss: 9.740627\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.89\n",
      "batch_idx: 4\n",
      "0.896\n",
      "batch_idx: 5\n",
      "0.9\n",
      "Training Epoch: 318, total loss: 9.783535\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 319, total loss: 9.690806\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 320, total loss: 9.702028\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 321, total loss: 9.666266\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 322, total loss: 9.695818\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 323, total loss: 9.697875\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 324, total loss: 9.679528\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 325, total loss: 9.686216\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.896\n",
      "batch_idx: 5\n",
      "0.895\n",
      "Training Epoch: 326, total loss: 9.799707\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 327, total loss: 9.703912\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.8975\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.8966666666666666\n",
      "Training Epoch: 328, total loss: 9.794155\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.95\n",
      "batch_idx: 2\n",
      "0.9466666666666667\n",
      "batch_idx: 3\n",
      "0.9325\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 329, total loss: 9.708991\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 330, total loss: 9.674421\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9283333333333333\n",
      "Training Epoch: 331, total loss: 9.673115\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 332, total loss: 9.737269\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.886\n",
      "batch_idx: 5\n",
      "0.8983333333333333\n",
      "Training Epoch: 333, total loss: 9.785876\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 334, total loss: 9.734503\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 335, total loss: 9.700601\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 336, total loss: 9.690004\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 337, total loss: 9.704762\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 338, total loss: 9.683605\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 339, total loss: 9.707880\n",
      "batch_idx: 0\n",
      "0.97\n",
      "batch_idx: 1\n",
      "0.955\n",
      "batch_idx: 2\n",
      "0.95\n",
      "batch_idx: 3\n",
      "0.93\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 340, total loss: 9.664305\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.945\n",
      "batch_idx: 2\n",
      "0.94\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 341, total loss: 9.765223\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.902\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 342, total loss: 9.755406\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 343, total loss: 9.722695\n",
      "batch_idx: 0\n",
      "0.84\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.8875\n",
      "batch_idx: 4\n",
      "0.892\n",
      "batch_idx: 5\n",
      "0.885\n",
      "Training Epoch: 344, total loss: 9.840306\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.8733333333333333\n",
      "batch_idx: 3\n",
      "0.88\n",
      "batch_idx: 4\n",
      "0.886\n",
      "batch_idx: 5\n",
      "0.8933333333333333\n",
      "Training Epoch: 345, total loss: 9.817640\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.88\n",
      "batch_idx: 3\n",
      "0.8775\n",
      "batch_idx: 4\n",
      "0.884\n",
      "batch_idx: 5\n",
      "0.885\n",
      "Training Epoch: 346, total loss: 9.853327\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.8866666666666667\n",
      "batch_idx: 3\n",
      "0.885\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.905\n",
      "Training Epoch: 347, total loss: 9.756601\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.8925\n",
      "batch_idx: 4\n",
      "0.898\n",
      "batch_idx: 5\n",
      "0.8966666666666666\n",
      "Training Epoch: 348, total loss: 9.778934\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 349, total loss: 9.707942\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 350, total loss: 9.710490\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.9433333333333334\n",
      "batch_idx: 3\n",
      "0.9325\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 351, total loss: 9.670590\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 352, total loss: 9.696857\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 353, total loss: 9.677825\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 354, total loss: 9.699492\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 355, total loss: 9.707788\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 356, total loss: 9.671467\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 357, total loss: 9.677713\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 358, total loss: 9.712514\n",
      "batch_idx: 0\n",
      "0.96\n",
      "batch_idx: 1\n",
      "0.955\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.9325\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 359, total loss: 9.673372\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 360, total loss: 9.690986\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.885\n",
      "batch_idx: 4\n",
      "0.894\n",
      "batch_idx: 5\n",
      "0.895\n",
      "Training Epoch: 361, total loss: 9.811915\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9\n",
      "Training Epoch: 362, total loss: 9.782354\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 363, total loss: 9.707373\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 364, total loss: 9.733701\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9333333333333333\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 365, total loss: 9.672231\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 366, total loss: 9.698813\n",
      "batch_idx: 0\n",
      "0.96\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 367, total loss: 9.695597\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 368, total loss: 9.688727\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.886\n",
      "batch_idx: 5\n",
      "0.8916666666666667\n",
      "Training Epoch: 369, total loss: 9.813554\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 370, total loss: 9.737010\n",
      "batch_idx: 0\n",
      "0.96\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 371, total loss: 9.718964\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 372, total loss: 9.714142\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 373, total loss: 9.716083\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 374, total loss: 9.733000\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 375, total loss: 9.718708\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 376, total loss: 9.707650\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 377, total loss: 9.697848\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 378, total loss: 9.709740\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 379, total loss: 9.709587\n",
      "batch_idx: 0\n",
      "0.81\n",
      "batch_idx: 1\n",
      "0.86\n",
      "batch_idx: 2\n",
      "0.8866666666666667\n",
      "batch_idx: 3\n",
      "0.885\n",
      "batch_idx: 4\n",
      "0.886\n",
      "batch_idx: 5\n",
      "0.8883333333333333\n",
      "Training Epoch: 380, total loss: 9.849640\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 381, total loss: 9.733380\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 382, total loss: 9.678074\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.905\n",
      "Training Epoch: 383, total loss: 9.780488\n",
      "batch_idx: 0\n",
      "0.96\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9333333333333333\n",
      "batch_idx: 3\n",
      "0.93\n",
      "batch_idx: 4\n",
      "0.932\n",
      "batch_idx: 5\n",
      "0.925\n",
      "Training Epoch: 384, total loss: 9.660152\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 385, total loss: 9.699702\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.95\n",
      "batch_idx: 2\n",
      "0.9466666666666667\n",
      "batch_idx: 3\n",
      "0.9375\n",
      "batch_idx: 4\n",
      "0.928\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 386, total loss: 9.675161\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9333333333333333\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 387, total loss: 9.674185\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.9325\n",
      "batch_idx: 4\n",
      "0.932\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 388, total loss: 9.685725\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 389, total loss: 9.658666\n",
      "batch_idx: 0\n",
      "0.98\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9366666666666666\n",
      "batch_idx: 3\n",
      "0.9325\n",
      "batch_idx: 4\n",
      "0.932\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 390, total loss: 9.661022\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 391, total loss: 9.699255\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 392, total loss: 9.657899\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 393, total loss: 9.706535\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9366666666666666\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.93\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 394, total loss: 9.681875\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 395, total loss: 9.672266\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 396, total loss: 9.680157\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.93\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 397, total loss: 9.686560\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 398, total loss: 9.669723\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 399, total loss: 9.689484\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.9333333333333333\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 400, total loss: 9.671223\n",
      "batch_idx: 0\n",
      "0.96\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.9325\n",
      "batch_idx: 4\n",
      "0.932\n",
      "batch_idx: 5\n",
      "0.9333333333333333\n",
      "Training Epoch: 401, total loss: 9.620186\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9283333333333333\n",
      "Training Epoch: 402, total loss: 9.652664\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9366666666666666\n",
      "batch_idx: 3\n",
      "0.9325\n",
      "batch_idx: 4\n",
      "0.928\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 403, total loss: 9.647615\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.96\n",
      "batch_idx: 2\n",
      "0.95\n",
      "batch_idx: 3\n",
      "0.9375\n",
      "batch_idx: 4\n",
      "0.93\n",
      "batch_idx: 5\n",
      "0.9283333333333333\n",
      "Training Epoch: 404, total loss: 9.642832\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 405, total loss: 9.681168\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 406, total loss: 9.682985\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 407, total loss: 9.681663\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9283333333333333\n",
      "Training Epoch: 408, total loss: 9.653990\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 409, total loss: 9.722572\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.898\n",
      "batch_idx: 5\n",
      "0.9\n",
      "Training Epoch: 410, total loss: 9.797476\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 411, total loss: 9.690534\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 412, total loss: 9.696763\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.945\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 413, total loss: 9.685982\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.945\n",
      "batch_idx: 2\n",
      "0.9366666666666666\n",
      "batch_idx: 3\n",
      "0.9375\n",
      "batch_idx: 4\n",
      "0.938\n",
      "batch_idx: 5\n",
      "0.93\n",
      "Training Epoch: 414, total loss: 9.638036\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 415, total loss: 9.677936\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.9333333333333333\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 416, total loss: 9.653465\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.94\n",
      "batch_idx: 3\n",
      "0.935\n",
      "batch_idx: 4\n",
      "0.934\n",
      "batch_idx: 5\n",
      "0.9333333333333333\n",
      "Training Epoch: 417, total loss: 9.622420\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9283333333333333\n",
      "Training Epoch: 418, total loss: 9.651895\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9066666666666666\n",
      "Training Epoch: 419, total loss: 9.758019\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.91\n",
      "Training Epoch: 420, total loss: 9.735233\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.9325\n",
      "batch_idx: 4\n",
      "0.928\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 421, total loss: 9.674063\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 422, total loss: 9.673307\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9366666666666666\n",
      "batch_idx: 3\n",
      "0.9325\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 423, total loss: 9.703315\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 424, total loss: 9.727593\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.955\n",
      "batch_idx: 2\n",
      "0.9366666666666666\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 425, total loss: 9.720308\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 426, total loss: 9.775773\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 427, total loss: 9.709020\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 428, total loss: 9.686497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.93\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 429, total loss: 9.688898\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.945\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 430, total loss: 9.692167\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 431, total loss: 9.701798\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 432, total loss: 9.687170\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.945\n",
      "batch_idx: 2\n",
      "0.9366666666666666\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 433, total loss: 9.675843\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 434, total loss: 9.685514\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 435, total loss: 9.689921\n",
      "batch_idx: 0\n",
      "0.96\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 436, total loss: 9.707052\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 437, total loss: 9.677453\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 438, total loss: 9.675859\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.95\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 439, total loss: 9.667004\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 440, total loss: 9.707245\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 441, total loss: 9.673419\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 442, total loss: 9.687898\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 443, total loss: 9.694558\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 444, total loss: 9.673006\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 445, total loss: 9.655557\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 446, total loss: 9.693791\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 447, total loss: 9.664248\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.93\n",
      "batch_idx: 4\n",
      "0.928\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 448, total loss: 9.683530\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.95\n",
      "batch_idx: 2\n",
      "0.9333333333333333\n",
      "batch_idx: 3\n",
      "0.9325\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9283333333333333\n",
      "Training Epoch: 449, total loss: 9.661548\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.865\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 450, total loss: 9.750072\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.91\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 451, total loss: 9.706603\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9366666666666666\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 452, total loss: 9.679830\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 453, total loss: 9.699475\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 454, total loss: 9.736094\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 455, total loss: 9.677996\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.925\n",
      "Training Epoch: 456, total loss: 9.656221\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 457, total loss: 9.696035\n",
      "batch_idx: 0\n",
      "0.96\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.925\n",
      "Training Epoch: 458, total loss: 9.659096\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.894\n",
      "batch_idx: 5\n",
      "0.895\n",
      "Training Epoch: 459, total loss: 9.807398\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 460, total loss: 9.678952\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 461, total loss: 9.714217\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.94\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.93\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 462, total loss: 9.705423\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 463, total loss: 9.660666\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.905\n",
      "Training Epoch: 464, total loss: 9.751352\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 465, total loss: 9.707358\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.925\n",
      "Training Epoch: 466, total loss: 9.662960\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 467, total loss: 9.701860\n",
      "batch_idx: 0\n",
      "0.96\n",
      "batch_idx: 1\n",
      "0.95\n",
      "batch_idx: 2\n",
      "0.9333333333333333\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 468, total loss: 9.692219\n",
      "batch_idx: 0\n",
      "0.97\n",
      "batch_idx: 1\n",
      "0.945\n",
      "batch_idx: 2\n",
      "0.9333333333333333\n",
      "batch_idx: 3\n",
      "0.93\n",
      "batch_idx: 4\n",
      "0.93\n",
      "batch_idx: 5\n",
      "0.925\n",
      "Training Epoch: 469, total loss: 9.669229\n",
      "batch_idx: 0\n",
      "0.96\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.93\n",
      "batch_idx: 5\n",
      "0.93\n",
      "Training Epoch: 470, total loss: 9.660926\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 471, total loss: 9.705299\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 472, total loss: 9.716667\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 473, total loss: 9.680582\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 474, total loss: 9.673836\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.95\n",
      "batch_idx: 2\n",
      "0.9466666666666667\n",
      "batch_idx: 3\n",
      "0.9375\n",
      "batch_idx: 4\n",
      "0.936\n",
      "batch_idx: 5\n",
      "0.9283333333333333\n",
      "Training Epoch: 475, total loss: 9.644510\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.932\n",
      "batch_idx: 5\n",
      "0.9283333333333333\n",
      "Training Epoch: 476, total loss: 9.655942\n",
      "batch_idx: 0\n",
      "0.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.93\n",
      "batch_idx: 5\n",
      "0.93\n",
      "Training Epoch: 477, total loss: 9.647132\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.9375\n",
      "batch_idx: 4\n",
      "0.938\n",
      "batch_idx: 5\n",
      "0.9283333333333333\n",
      "Training Epoch: 478, total loss: 9.646690\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 479, total loss: 9.706516\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 480, total loss: 9.723838\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.8983333333333333\n",
      "Training Epoch: 481, total loss: 9.775002\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 482, total loss: 9.717223\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 483, total loss: 9.730868\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.885\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.902\n",
      "batch_idx: 5\n",
      "0.9016666666666666\n",
      "Training Epoch: 484, total loss: 9.770386\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.93\n",
      "batch_idx: 5\n",
      "0.925\n",
      "Training Epoch: 485, total loss: 9.650384\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.93\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 486, total loss: 9.675981\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.935\n",
      "batch_idx: 4\n",
      "0.934\n",
      "batch_idx: 5\n",
      "0.925\n",
      "Training Epoch: 487, total loss: 9.670648\n",
      "batch_idx: 0\n",
      "0.96\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 488, total loss: 9.675910\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 489, total loss: 9.728629\n",
      "batch_idx: 0\n",
      "0.96\n",
      "batch_idx: 1\n",
      "0.945\n",
      "batch_idx: 2\n",
      "0.9533333333333334\n",
      "batch_idx: 3\n",
      "0.94\n",
      "batch_idx: 4\n",
      "0.932\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 490, total loss: 9.666208\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.925\n",
      "Training Epoch: 491, total loss: 9.664701\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.928\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 492, total loss: 9.667783\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 493, total loss: 9.686590\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.95\n",
      "batch_idx: 2\n",
      "0.94\n",
      "batch_idx: 3\n",
      "0.9425\n",
      "batch_idx: 4\n",
      "0.942\n",
      "batch_idx: 5\n",
      "0.9316666666666666\n",
      "Training Epoch: 494, total loss: 9.624675\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 495, total loss: 9.680841\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 496, total loss: 9.729258\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 497, total loss: 9.749925\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 498, total loss: 9.665996\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8833333333333333\n",
      "batch_idx: 3\n",
      "0.8925\n",
      "batch_idx: 4\n",
      "0.902\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 499, total loss: 9.749071\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 500, total loss: 9.683018\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9366666666666666\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.928\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 501, total loss: 9.649765\n",
      "batch_idx: 0\n",
      "0.96\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 502, total loss: 9.679714\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 503, total loss: 9.691546\n",
      "batch_idx: 0\n",
      "0.98\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 504, total loss: 9.702845\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 505, total loss: 9.674373\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9333333333333333\n",
      "batch_idx: 3\n",
      "0.935\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 506, total loss: 9.685254\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 507, total loss: 9.670159\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.9325\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9283333333333333\n",
      "Training Epoch: 508, total loss: 9.653766\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 509, total loss: 9.697907\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 510, total loss: 9.703538\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 511, total loss: 9.679433\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 512, total loss: 9.722354\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 513, total loss: 9.715841\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.895\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 514, total loss: 9.723281\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 515, total loss: 9.670466\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 516, total loss: 9.700610\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.8933333333333333\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.906\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 517, total loss: 9.752191\n",
      "batch_idx: 0\n",
      "0.82\n",
      "batch_idx: 1\n",
      "0.88\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.8925\n",
      "batch_idx: 4\n",
      "0.894\n",
      "batch_idx: 5\n",
      "0.9033333333333333\n",
      "Training Epoch: 518, total loss: 9.757355\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 519, total loss: 9.702003\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.89\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 520, total loss: 9.681282\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 521, total loss: 9.686041\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 522, total loss: 9.668024\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 523, total loss: 9.676915\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9333333333333333\n",
      "batch_idx: 3\n",
      "0.935\n",
      "batch_idx: 4\n",
      "0.934\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 524, total loss: 9.667396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9325\n",
      "batch_idx: 4\n",
      "0.932\n",
      "batch_idx: 5\n",
      "0.93\n",
      "Training Epoch: 525, total loss: 9.641413\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.925\n",
      "Training Epoch: 526, total loss: 9.672497\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.945\n",
      "batch_idx: 2\n",
      "0.9466666666666667\n",
      "batch_idx: 3\n",
      "0.94\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 527, total loss: 9.670444\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.8966666666666666\n",
      "batch_idx: 3\n",
      "0.905\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 528, total loss: 9.719910\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.93\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 529, total loss: 9.684868\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.9325\n",
      "batch_idx: 4\n",
      "0.936\n",
      "batch_idx: 5\n",
      "0.9283333333333333\n",
      "Training Epoch: 530, total loss: 9.648921\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 531, total loss: 9.699617\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9366666666666666\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "Training Epoch: 532, total loss: 9.692309\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.9083333333333333\n",
      "Training Epoch: 533, total loss: 9.725783\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 534, total loss: 9.708535\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 535, total loss: 9.708789\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 536, total loss: 9.687425\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 537, total loss: 9.683717\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 538, total loss: 9.674949\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.93\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.925\n",
      "Training Epoch: 539, total loss: 9.661199\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.9\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.914\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 540, total loss: 9.730791\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.912\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 541, total loss: 9.720964\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.93\n",
      "batch_idx: 4\n",
      "0.934\n",
      "batch_idx: 5\n",
      "0.925\n",
      "Training Epoch: 542, total loss: 9.658478\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.935\n",
      "batch_idx: 4\n",
      "0.936\n",
      "batch_idx: 5\n",
      "0.935\n",
      "Training Epoch: 543, total loss: 9.624925\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.9283333333333333\n",
      "Training Epoch: 544, total loss: 9.642300\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.93\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 545, total loss: 9.651332\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.925\n",
      "Training Epoch: 546, total loss: 9.664034\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 547, total loss: 9.646919\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.945\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 548, total loss: 9.651406\n",
      "batch_idx: 0\n",
      "0.96\n",
      "batch_idx: 1\n",
      "0.945\n",
      "batch_idx: 2\n",
      "0.9366666666666666\n",
      "batch_idx: 3\n",
      "0.9375\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.9283333333333333\n",
      "Training Epoch: 549, total loss: 9.653353\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9333333333333333\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 550, total loss: 9.660496\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9\n",
      "batch_idx: 3\n",
      "0.9075\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 551, total loss: 9.671958\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.905\n",
      "batch_idx: 2\n",
      "0.9033333333333333\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 552, total loss: 9.705405\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 553, total loss: 9.664006\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9066666666666666\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 554, total loss: 9.704897\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.9325\n",
      "batch_idx: 4\n",
      "0.924\n",
      "batch_idx: 5\n",
      "0.9216666666666666\n",
      "Training Epoch: 555, total loss: 9.679900\n",
      "batch_idx: 0\n",
      "0.97\n",
      "batch_idx: 1\n",
      "0.96\n",
      "batch_idx: 2\n",
      "0.9366666666666666\n",
      "batch_idx: 3\n",
      "0.9325\n",
      "batch_idx: 4\n",
      "0.928\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 556, total loss: 9.668944\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9133333333333333\n",
      "Training Epoch: 557, total loss: 9.717969\n",
      "batch_idx: 0\n",
      "0.91\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.925\n",
      "Training Epoch: 558, total loss: 9.660441\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9266666666666666\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.93\n",
      "Training Epoch: 559, total loss: 9.638633\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.93\n",
      "batch_idx: 4\n",
      "0.934\n",
      "batch_idx: 5\n",
      "0.9283333333333333\n",
      "Training Epoch: 560, total loss: 9.656406\n",
      "batch_idx: 0\n",
      "0.98\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.9366666666666666\n",
      "batch_idx: 3\n",
      "0.9275\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.9283333333333333\n",
      "Training Epoch: 561, total loss: 9.647837\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.91\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.93\n",
      "batch_idx: 4\n",
      "0.93\n",
      "batch_idx: 5\n",
      "0.93\n",
      "Training Epoch: 562, total loss: 9.638183\n",
      "batch_idx: 0\n",
      "0.96\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.9466666666666667\n",
      "batch_idx: 3\n",
      "0.94\n",
      "batch_idx: 4\n",
      "0.934\n",
      "batch_idx: 5\n",
      "0.93\n",
      "Training Epoch: 563, total loss: 9.640804\n",
      "batch_idx: 0\n",
      "0.93\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9025\n",
      "batch_idx: 4\n",
      "0.904\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 564, total loss: 9.714573\n",
      "batch_idx: 0\n",
      "0.89\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.91\n",
      "batch_idx: 3\n",
      "0.915\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 565, total loss: 9.663847\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9366666666666666\n",
      "batch_idx: 3\n",
      "0.93\n",
      "batch_idx: 4\n",
      "0.928\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 566, total loss: 9.654913\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.94\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.92\n",
      "batch_idx: 4\n",
      "0.918\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 567, total loss: 9.660631\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9116666666666666\n",
      "Training Epoch: 568, total loss: 9.719473\n",
      "batch_idx: 0\n",
      "0.95\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9366666666666666\n",
      "batch_idx: 3\n",
      "0.9225\n",
      "batch_idx: 4\n",
      "0.916\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 569, total loss: 9.681171\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.93\n",
      "batch_idx: 3\n",
      "0.93\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 570, total loss: 9.662105\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 571, total loss: 9.656476\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.93\n",
      "batch_idx: 2\n",
      "0.9333333333333333\n",
      "batch_idx: 3\n",
      "0.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 572, total loss: 9.668265\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.925\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.925\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9183333333333333\n",
      "Training Epoch: 573, total loss: 9.689038\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.92\n",
      "batch_idx: 5\n",
      "0.9266666666666666\n",
      "Training Epoch: 574, total loss: 9.656959\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.935\n",
      "batch_idx: 2\n",
      "0.9433333333333334\n",
      "batch_idx: 3\n",
      "0.93\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.925\n",
      "Training Epoch: 575, total loss: 9.661093\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.895\n",
      "batch_idx: 2\n",
      "0.9133333333333333\n",
      "batch_idx: 3\n",
      "0.91\n",
      "batch_idx: 4\n",
      "0.922\n",
      "batch_idx: 5\n",
      "0.9233333333333333\n",
      "Training Epoch: 576, total loss: 9.662170\n",
      "batch_idx: 0\n",
      "0.94\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9125\n",
      "batch_idx: 4\n",
      "0.908\n",
      "batch_idx: 5\n",
      "0.915\n",
      "Training Epoch: 577, total loss: 9.699267\n",
      "batch_idx: 0\n",
      "0.88\n",
      "batch_idx: 1\n",
      "0.92\n",
      "batch_idx: 2\n",
      "0.92\n",
      "batch_idx: 3\n",
      "0.9325\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.93\n",
      "Training Epoch: 578, total loss: 9.638466\n",
      "batch_idx: 0\n",
      "0.92\n",
      "batch_idx: 1\n",
      "0.915\n",
      "batch_idx: 2\n",
      "0.9233333333333333\n",
      "batch_idx: 3\n",
      "0.9175\n",
      "batch_idx: 4\n",
      "0.926\n",
      "batch_idx: 5\n",
      "0.92\n",
      "Training Epoch: 579, total loss: 9.688081\n"
     ]
    }
   ],
   "source": [
    "#前面训练了420，后边继续训练1000-420=580\n",
    "path='dataset/xiaoguan/RF/RF_for_train/train_class_9/model/gene_4000_NFM.pkl'\n",
    "nfm = NFM(nfm_config).cuda()\n",
    "net=nfm\n",
    "print(net)\n",
    "#net = nn.DataParallel(net)\n",
    "#net = net.to(device)\n",
    "net.load_state_dict(torch.load(path),strict=False)\n",
    "net.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "total = 0\n",
    "#loss_func = torch.nn.BCELoss()\n",
    "loss_func=torch.nn.CrossEntropyLoss()\n",
    "#loss_func=nn.MultiLabelSoftMarginLoss()\n",
    "#loss_func=torch.nn.LogSoftmax()\n",
    "    \n",
    "#model=nn.Softmax(nn.Linear(10149,16)).to(device)\n",
    "# 从DataLoader中获取小批量的id以及数据\n",
    "for epoch_id in range(580):\n",
    "    correct=0\n",
    "    total=0\n",
    "    for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "        x = Variable(x)\n",
    "        labels = Variable(labels)\n",
    "            \n",
    "            \n",
    "        #x = torch.tensor(x, dtype=torch.float)\n",
    "        #x=x.clone().detach().requires_grad_(True)\n",
    "        x=torch.tensor(x,dtype=torch.float)\n",
    "        labels=torch.tensor(labels,dtype=torch.float)\n",
    "        x, labels = x.cuda(), labels.cuda()\n",
    "            \n",
    "        #print(\"labels:\",labels)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        y_predict = net(x)\n",
    "        #print(\"y_predict:\",y_predict)\n",
    "        #loss = loss_func(y_predict.view(-1), labels)\n",
    "        loss = loss_func(y_predict, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        loss = loss.item()\n",
    "        #loss, predicted = self._train_single_batch(x, labels)\n",
    "\n",
    "        total += loss\n",
    "            \n",
    "            \n",
    "            \n",
    "        predicted = torch.max(y_predict.data,1)\n",
    "        #print(\"predicted:\",predicted)\n",
    "        predicted = torch.max(y_predict.data,1)[1]\n",
    "            \n",
    "            \n",
    "        labels=torch.max(labels,1)\n",
    "        #print(\"labels:\",labels)\n",
    "        labels=labels[1]\n",
    "            \n",
    "        correct += (predicted == labels).sum()\n",
    "        #print(\"correct:\",correct)\n",
    "        #correct=correct[0]\n",
    "        #print(\"new_correct:\",float(correct))\n",
    "        correct=float(correct)   \n",
    "        #if batch_idx % 10 == 0:\n",
    "        print(\"batch_idx:\",batch_idx)\n",
    "        print(correct/(BATCH_SIZE*(batch_idx+1)))\n",
    "            \n",
    "        \"\"\"\n",
    "            #y_predict.detach().numpy()\n",
    "            pred = y_predict\n",
    "            print(\"pred:\",pred.shape)\n",
    "            y=labels.clone().detach().requires_grad_(True)\n",
    "            print(\"y:\",y.shape)\n",
    "            #y=labels.data.cpu().numpy()\n",
    "            #y = labels.detach().numpy()\n",
    "            roc_auc_score(y, pred)\n",
    "        \"\"\"\n",
    "           \n",
    "        # print('[Training Epoch: {}] Batch: {}, Loss: {}'.format(epoch_id, batch_id, loss))\n",
    "    print(\"Training Epoch: %d, total loss: %f\" % (epoch_id, total))\n",
    "    #print(\"auc:\",roc_auc_score)\n",
    "#功能：保存训练完的网络的各层参数（即weights和bias)\n",
    "path='dataset/xiaoguan/RF/RF_for_train/train_class_9/model/gene_4000_NFM_1000.pkl'\n",
    "torch.save(nfm.state_dict(),path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afa35a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试集\n",
    "\n",
    "#加载模型\n",
    "path='dataset/xiaoguan/RF/RF_for_train/train_class_9/model/gene_4000_NFM.pkl'\n",
    "nfm = NFM(nfm_config).cuda()\n",
    "net=nfm\n",
    "print(net)\n",
    "#net = nn.DataParallel(net)\n",
    "#net = net.to(device)\n",
    "net.load_state_dict(torch.load(path),strict=False)\n",
    "net.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "total = 0\n",
    "#loss_func = torch.nn.BCELoss()\n",
    "loss_func=torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8b1ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d40f2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NFM(\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (linear_model1): Linear(in_features=5510, out_features=1000, bias=True)\n",
      "  (BN_linear1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear_model2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (BN_linear2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (embedding_layers): Embedding(1001, 100)\n",
      "  (bi_pooling): BiInteractionPooling()\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (BN_bi): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dnn_layers): ModuleList(\n",
      "    (0): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (1): Linear(in_features=100, out_features=9, bias=True)\n",
      "  )\n",
      "  (dnn_softmax): Softmax(dim=1)\n",
      ")\n",
      "NFM(\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (linear_model1): Linear(in_features=5510, out_features=1000, bias=True)\n",
      "  (BN_linear1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear_model2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (BN_linear2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (embedding_layers): Embedding(1001, 100)\n",
      "  (bi_pooling): BiInteractionPooling()\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (BN_bi): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dnn_layers): ModuleList(\n",
      "    (0): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (1): Linear(in_features=100, out_features=9, bias=True)\n",
      "  )\n",
      "  (dnn_softmax): Softmax(dim=1)\n",
      ")\n",
      "[('linear_model1.weight', Parameter containing:\n",
      "tensor([[ 4.1792e-03, -2.0012e-02,  9.8555e-03,  ...,  6.3034e-03,\n",
      "          1.9084e-03, -5.9339e-03],\n",
      "        [-7.2668e-03, -2.0609e-02,  5.7518e-04,  ...,  2.1113e-03,\n",
      "         -1.7399e-03, -1.0733e-03],\n",
      "        [ 1.1308e-02, -2.6177e-02,  3.5501e-03,  ...,  2.2635e-02,\n",
      "          1.7044e-02, -4.3090e-03],\n",
      "        ...,\n",
      "        [ 7.1105e-03, -5.6998e-03, -4.4347e-03,  ..., -5.0106e-03,\n",
      "          1.6435e-04, -8.8356e-03],\n",
      "        [ 1.2284e-02,  2.1440e-02,  2.2601e-04,  ..., -7.8331e-05,\n",
      "         -3.2474e-03,  5.5273e-04],\n",
      "        [ 7.9007e-03, -1.2099e-02,  5.4705e-04,  ...,  3.4282e-03,\n",
      "         -5.9106e-03, -3.3098e-03]], device='cuda:0', requires_grad=True)), ('linear_model1.bias', Parameter containing:\n",
      "tensor([ 4.3848e-08, -5.6438e-08, -7.1823e-09, -5.6667e-08, -1.1628e-08,\n",
      "         1.5024e-08,  3.7333e-09, -1.1327e-07, -5.3654e-08,  4.1108e-09,\n",
      "        -5.2567e-09, -1.4763e-07, -1.8665e-07, -3.4424e-08,  1.8804e-07,\n",
      "        -6.1235e-08,  4.1346e-08,  5.6652e-08,  5.2685e-08, -3.3446e-08,\n",
      "         8.8155e-09, -8.9571e-09, -8.9714e-11,  9.9724e-08,  1.3599e-08,\n",
      "        -8.9355e-09,  8.0802e-07,  9.5665e-10,  7.2274e-09, -5.2751e-09,\n",
      "        -2.3178e-07,  6.8839e-08,  2.4491e-08, -1.3064e-08,  2.8237e-09,\n",
      "        -8.2429e-08, -4.9668e-08, -3.2989e-08, -7.0069e-08,  2.2019e-08,\n",
      "        -1.5422e-09, -5.6434e-09,  9.6392e-09,  7.8476e-10,  6.8417e-09,\n",
      "        -2.6274e-08,  2.8393e-09,  1.5617e-08,  1.0064e-09, -5.4702e-10,\n",
      "        -1.6377e-08,  2.2441e-07, -4.5125e-08, -2.8669e-07, -9.5212e-08,\n",
      "        -1.6218e-07,  5.3668e-08, -1.2152e-08,  2.5797e-08,  4.6876e-08,\n",
      "        -2.8092e-09,  1.6416e-08, -8.4203e-08,  1.2175e-07,  8.4333e-10,\n",
      "        -5.2825e-08,  1.0406e-08, -1.5621e-07,  4.1611e-09, -3.0774e-08,\n",
      "        -4.0846e-08, -8.0099e-09,  6.0539e-09, -1.1639e-07,  5.0799e-08,\n",
      "        -7.1070e-09,  3.8663e-08, -9.5472e-08, -1.7492e-08, -1.3384e-07,\n",
      "         2.5999e-08,  5.2688e-08,  2.8970e-08, -1.6713e-08, -6.9491e-08,\n",
      "        -2.3207e-09, -1.2932e-08, -1.0236e-07,  2.3772e-09, -3.8188e-07,\n",
      "        -3.1920e-08, -1.1776e-08,  2.3366e-08, -4.5977e-09,  3.7073e-08,\n",
      "         1.1999e-08,  1.8215e-09, -4.5816e-08, -7.6533e-08, -1.5324e-07,\n",
      "         7.6647e-08,  2.5065e-08,  1.0415e-08, -3.5057e-09,  4.0556e-08,\n",
      "        -1.8088e-07,  2.6957e-08,  1.0095e-08, -7.0761e-09, -2.5410e-09,\n",
      "        -7.9138e-09,  1.4609e-08,  5.5010e-09, -4.7357e-08, -1.9609e-08,\n",
      "         1.5281e-08, -6.8227e-08, -9.6933e-09, -2.5343e-08,  9.3852e-09,\n",
      "         3.1578e-09,  2.3167e-08, -8.2904e-09, -1.5079e-08,  8.8243e-09,\n",
      "        -2.7071e-08,  1.1881e-07, -4.1448e-09, -1.0878e-08, -5.1559e-08,\n",
      "        -4.1941e-09, -8.0469e-08,  3.1930e-07, -1.6576e-08, -1.2465e-08,\n",
      "        -7.9815e-08, -7.0007e-08,  4.2236e-08, -2.4982e-08, -3.4990e-08,\n",
      "         5.4609e-08, -1.1004e-07,  9.1949e-09,  4.6515e-08, -7.1756e-08,\n",
      "         3.6593e-08,  2.9814e-08, -2.4276e-07,  6.8088e-08,  3.6424e-08,\n",
      "        -4.0718e-08, -2.3940e-08, -2.0879e-09, -1.4154e-09,  7.5952e-08,\n",
      "        -3.1355e-07,  2.2312e-08, -5.8224e-10,  4.6214e-08,  8.6226e-08,\n",
      "        -7.8474e-08, -5.2109e-09, -7.5711e-08, -1.1759e-08,  2.1974e-08,\n",
      "        -8.7680e-08,  1.9475e-09,  1.3056e-08,  1.7633e-08, -9.8920e-09,\n",
      "        -9.7531e-08, -1.1134e-08, -8.7057e-09, -1.0213e-08, -1.3883e-07,\n",
      "         3.2434e-08,  4.0531e-08, -2.0967e-08,  4.9239e-07, -1.7283e-08,\n",
      "        -2.5655e-08, -5.3511e-09,  8.3170e-07, -3.9982e-07,  5.9342e-08,\n",
      "         2.6122e-08, -6.3647e-08, -9.1577e-09,  3.6524e-09,  8.4885e-08,\n",
      "         1.0474e-08, -6.0808e-09,  3.4799e-08,  3.3230e-08,  5.6065e-09,\n",
      "         8.0714e-10,  3.0840e-08,  3.6149e-09,  1.1900e-07, -1.0032e-08,\n",
      "         5.6109e-09,  1.3375e-08, -2.3783e-07,  3.3928e-08,  3.6636e-07,\n",
      "         4.3789e-08, -1.5420e-08, -2.6253e-08,  3.5313e-08, -5.2050e-10,\n",
      "         1.6085e-08,  1.5857e-08,  4.7532e-08, -2.4274e-08, -1.4500e-08,\n",
      "        -1.4065e-08,  2.3768e-07,  6.5526e-08,  2.7241e-08,  1.4260e-07,\n",
      "        -7.2356e-08,  7.3187e-08, -1.7784e-07,  6.7165e-08, -1.0220e-07,\n",
      "         6.3184e-08, -2.2770e-08,  4.4085e-08,  1.0539e-08,  1.3577e-08,\n",
      "        -8.2711e-08,  7.4753e-09,  2.9657e-08, -5.4763e-08, -1.9007e-08,\n",
      "        -2.2381e-08,  6.5460e-08, -1.2421e-08,  3.8140e-08,  7.1151e-09,\n",
      "         7.7659e-09,  3.5373e-08,  1.1510e-07,  1.6055e-07,  4.5701e-08,\n",
      "         2.0588e-08,  6.1573e-08,  4.8004e-09, -1.8420e-08,  1.1875e-07,\n",
      "         1.1536e-09, -6.0474e-08,  1.8825e-08, -7.5862e-09,  8.6955e-08,\n",
      "         1.6367e-09,  1.6726e-08,  1.7074e-10,  8.5121e-09, -2.5001e-08,\n",
      "         5.2940e-08,  1.1620e-08, -2.9240e-08,  1.8761e-07, -6.9527e-08,\n",
      "         3.7632e-09,  1.1890e-08,  5.3613e-09, -3.6660e-08,  1.1829e-08,\n",
      "        -1.1031e-08, -1.2154e-08,  3.5805e-08, -3.4901e-09, -4.7359e-08,\n",
      "        -4.4148e-09, -2.8832e-08,  3.1346e-08,  7.7741e-08,  1.5455e-08,\n",
      "        -4.7774e-09, -1.5290e-09,  4.2392e-08, -2.8047e-09, -5.2224e-08,\n",
      "         6.4967e-08, -1.1730e-07,  1.0643e-07, -5.0200e-09,  1.0584e-08,\n",
      "        -4.7234e-09,  6.0134e-08,  6.6112e-08, -3.5271e-09, -1.0430e-08,\n",
      "         2.7920e-08,  1.2200e-11, -8.0966e-08, -5.0647e-08,  1.3021e-07,\n",
      "         6.0164e-08, -3.6944e-09,  3.9407e-08,  1.0934e-07,  9.3606e-08,\n",
      "        -3.4697e-08,  8.1255e-08, -3.5816e-08, -5.5525e-09, -1.4458e-07,\n",
      "        -2.6447e-09,  1.3025e-08,  1.6487e-08,  6.3865e-09, -1.2400e-07,\n",
      "         2.5968e-08, -4.1799e-07, -1.6383e-08, -1.4823e-08,  1.2452e-08,\n",
      "        -6.8563e-08,  5.1831e-09, -3.9770e-09, -1.0328e-07, -3.8199e-08,\n",
      "        -2.7047e-07, -2.9900e-08,  2.4856e-10, -6.9847e-09, -2.3442e-07,\n",
      "        -2.6422e-08,  1.9287e-08,  7.0323e-08,  6.1005e-09,  1.1958e-07,\n",
      "         9.3383e-10, -1.3144e-08, -1.8183e-07, -1.1525e-07,  1.9899e-08,\n",
      "        -4.5661e-08, -2.1414e-08, -2.8827e-08,  9.2029e-09,  5.2133e-10,\n",
      "        -6.9983e-09, -1.6661e-07, -1.6950e-07,  4.8780e-08, -1.8393e-08,\n",
      "         2.0929e-08,  1.1193e-07,  1.4744e-08,  3.1523e-09,  1.3417e-07,\n",
      "        -1.3244e-07, -1.7613e-08,  6.1297e-09,  3.8508e-08, -6.6219e-09,\n",
      "         1.9950e-07, -4.0301e-08,  3.4784e-07, -6.4159e-08,  5.4932e-08,\n",
      "         2.7328e-08,  2.0307e-08, -2.1830e-07, -1.0666e-07, -3.4823e-08,\n",
      "         9.5824e-09,  1.7207e-08, -4.5189e-08, -7.2735e-08,  2.1959e-08,\n",
      "        -1.9296e-08,  7.2895e-07, -1.7170e-07, -1.7995e-07, -5.3390e-08,\n",
      "         7.6367e-09, -6.6935e-09, -2.4919e-08, -4.0228e-07,  1.1919e-07,\n",
      "         6.2776e-09,  3.1937e-08, -4.3704e-09,  1.8790e-08, -3.3383e-08,\n",
      "         5.0413e-09, -3.5472e-08, -6.9820e-09, -4.2153e-09,  1.8239e-08,\n",
      "        -1.6536e-07, -1.2079e-07,  3.2334e-08, -2.3956e-09, -1.1274e-08,\n",
      "        -3.0565e-08,  3.5711e-08, -9.1160e-08,  1.5732e-07, -1.3680e-08,\n",
      "         1.1043e-08,  4.8098e-08,  1.5351e-07, -2.2367e-08,  1.6401e-07,\n",
      "         7.1740e-08,  1.7840e-08, -4.4361e-08, -1.8169e-08,  1.7619e-07,\n",
      "         3.6370e-08, -8.8790e-08, -3.3453e-08,  1.3403e-07,  1.8453e-08,\n",
      "         1.9120e-08,  3.1825e-07, -1.2370e-07,  2.1699e-08,  9.6957e-09,\n",
      "         2.4878e-07,  1.8962e-07, -2.2586e-07,  6.5819e-08, -1.5679e-08,\n",
      "         6.6098e-08,  6.6466e-08, -2.3626e-09,  3.1259e-08, -4.4766e-08,\n",
      "        -9.8644e-08, -1.4652e-07,  6.3022e-08, -8.3543e-09, -8.3388e-08,\n",
      "        -6.3064e-08,  1.8828e-08, -4.0859e-08,  1.4905e-08,  1.0963e-07,\n",
      "         2.7757e-08, -1.5050e-09,  6.3294e-08, -6.2649e-07,  3.4652e-08,\n",
      "        -4.2055e-08, -1.1199e-07,  3.9475e-09, -4.1580e-07, -4.1857e-10,\n",
      "        -2.6396e-08, -2.4030e-08,  4.2327e-08, -3.8333e-08, -3.6306e-08,\n",
      "         7.0586e-11, -1.2336e-07, -1.5098e-08,  3.9138e-08,  1.3239e-08,\n",
      "         2.3589e-09, -3.1072e-10,  1.8351e-08,  1.5695e-08, -5.3062e-09,\n",
      "         4.8362e-07, -7.8951e-08,  6.0497e-08,  1.6145e-08,  2.0088e-08,\n",
      "        -4.5495e-08,  4.9278e-09, -4.5295e-09,  1.5419e-08, -3.1287e-08,\n",
      "        -3.0625e-08, -7.7487e-08, -1.3934e-07, -1.9370e-08,  1.3912e-08,\n",
      "        -4.9092e-09, -1.7512e-08,  1.5532e-07, -4.9430e-08, -1.5960e-08,\n",
      "        -1.8479e-07,  1.1857e-08, -3.9433e-07,  2.2102e-07, -9.1547e-08,\n",
      "        -2.8302e-08, -2.5625e-08,  1.3873e-08, -3.7231e-08, -7.3708e-09,\n",
      "        -1.7066e-08, -8.4814e-09,  4.7089e-08, -4.9822e-10, -5.8560e-09,\n",
      "        -6.8206e-09,  8.0126e-08,  3.4989e-09, -5.9621e-09,  1.2439e-07,\n",
      "         2.4674e-08,  1.5379e-08,  1.3001e-08, -6.5638e-09, -4.2763e-08,\n",
      "        -6.6330e-08,  2.0768e-09,  8.0173e-08, -3.4733e-09,  1.3311e-08,\n",
      "         8.9692e-09, -3.2362e-08, -1.8261e-07,  2.1320e-07,  2.8041e-08,\n",
      "        -4.8666e-07, -5.4536e-09, -8.2288e-08,  7.9717e-07,  4.4077e-09,\n",
      "        -3.9470e-08, -3.7608e-08, -4.6779e-08, -1.4568e-07, -6.3848e-08,\n",
      "        -5.8709e-08,  4.7185e-09,  2.3224e-08, -1.1700e-08, -1.7418e-08,\n",
      "         1.1432e-08, -2.1776e-08, -3.1710e-08,  2.4384e-07,  7.8425e-07,\n",
      "        -3.3014e-09,  2.5163e-07, -7.4906e-08, -1.2220e-07,  3.8256e-08,\n",
      "         8.6612e-08, -5.9003e-09, -2.7484e-08,  2.6428e-08, -1.0304e-09,\n",
      "         3.5484e-08,  3.8064e-08, -4.3302e-09,  5.6011e-08,  5.5060e-08,\n",
      "         2.5044e-09, -2.5079e-07,  1.0937e-08, -2.8062e-09, -1.0315e-08,\n",
      "        -2.3099e-08,  3.7924e-09,  2.3000e-09,  1.1049e-08,  5.6415e-08,\n",
      "         1.2188e-08,  1.4939e-08,  4.4239e-08, -3.3514e-07,  6.4676e-08,\n",
      "        -1.5869e-09, -9.7802e-08, -1.1914e-07, -1.8699e-08,  1.0688e-08,\n",
      "         3.3059e-08, -5.6047e-08,  7.2818e-08, -5.2470e-09, -1.3686e-07,\n",
      "        -6.1227e-09,  6.1379e-08, -2.4692e-08,  4.4215e-09,  1.2759e-08,\n",
      "        -2.7610e-08, -3.7247e-08,  1.8861e-08,  2.8770e-08,  5.5965e-08,\n",
      "         5.2870e-08, -1.6733e-07,  6.2280e-07, -4.3912e-09,  1.2754e-08,\n",
      "        -2.6268e-08,  2.3690e-07, -6.9965e-08, -2.9375e-08,  8.7711e-09,\n",
      "        -1.0092e-08,  1.3506e-08,  1.1259e-08, -1.6421e-07,  6.2296e-09,\n",
      "        -9.8278e-08, -1.0947e-08, -1.7280e-08,  1.6960e-09,  6.0318e-09,\n",
      "         3.4524e-07, -7.7383e-09,  5.2962e-08,  3.8939e-09,  9.7390e-08,\n",
      "        -8.6637e-09, -2.6141e-08, -4.5104e-08, -3.4518e-09,  2.2179e-08,\n",
      "        -1.3965e-08,  1.9464e-08, -2.7216e-08,  5.1455e-08, -9.4902e-09,\n",
      "        -3.5508e-07, -9.2060e-09,  1.6083e-08, -1.6489e-08,  9.3177e-10,\n",
      "         2.5290e-08, -5.4153e-10, -3.6817e-08, -3.1707e-08, -1.5675e-08,\n",
      "        -2.1340e-08, -2.9777e-07,  1.9932e-08, -1.4936e-08, -8.0900e-09,\n",
      "         8.8038e-08, -6.6653e-07,  9.2276e-09,  5.5630e-08,  5.6259e-08,\n",
      "        -1.6725e-07, -2.5599e-07,  1.0456e-08, -1.9550e-08, -6.3192e-08,\n",
      "         8.5138e-08,  2.6346e-09, -2.0313e-08, -7.4833e-09,  2.2001e-08,\n",
      "        -2.1823e-09,  1.7879e-08,  1.0327e-07, -1.4527e-08,  2.7894e-08,\n",
      "         8.6927e-07, -6.9423e-08,  1.4302e-09,  7.6052e-08, -2.8007e-08,\n",
      "        -1.0717e-07,  1.0280e-08,  2.2790e-08,  2.2759e-08, -4.4018e-07,\n",
      "         1.1716e-08, -4.7766e-08,  1.9958e-08, -5.1608e-08,  1.8289e-08,\n",
      "         5.3998e-09, -2.2716e-08,  2.0398e-07,  3.1940e-09, -1.6257e-09,\n",
      "        -6.7838e-09,  1.2650e-08,  1.3637e-08,  4.9746e-08, -1.1027e-08,\n",
      "         1.7356e-09,  3.7668e-08, -3.0798e-08,  3.0039e-08, -3.1648e-07,\n",
      "         3.8608e-08, -1.8456e-07,  1.6248e-08, -9.1640e-08, -5.5022e-08,\n",
      "         4.8584e-08,  9.1610e-08, -3.2152e-08,  7.2586e-09, -1.8345e-08,\n",
      "         8.4026e-08, -2.3737e-08,  1.9941e-08, -2.7746e-08, -2.9205e-09,\n",
      "        -2.1644e-08, -7.1001e-08, -6.2629e-10,  1.0172e-07, -4.1646e-09,\n",
      "         6.0836e-09,  6.4650e-08, -1.2339e-08,  4.9909e-09,  4.4322e-09,\n",
      "        -5.7951e-08, -1.7666e-08, -4.6225e-07, -2.9032e-07,  1.0605e-08,\n",
      "        -2.7752e-08,  4.5021e-09,  4.0579e-08,  4.3699e-08,  2.5653e-08,\n",
      "        -8.0337e-08, -2.2464e-08,  3.3519e-08,  9.1292e-08, -5.6068e-08,\n",
      "        -3.2019e-08, -5.4400e-08,  2.9254e-08,  7.1995e-09, -2.3661e-07,\n",
      "        -1.1928e-07,  9.2759e-08,  4.3261e-09,  1.1086e-07,  1.8719e-08,\n",
      "         4.1283e-08, -5.3303e-08,  5.9693e-09,  7.6425e-08,  4.7878e-08,\n",
      "        -2.2883e-08, -1.5306e-08,  1.2193e-09, -6.1838e-08, -2.8462e-09,\n",
      "         8.6086e-08,  3.1820e-09, -1.7722e-07,  8.7448e-08, -2.9609e-08,\n",
      "        -3.4065e-08,  4.7640e-09, -1.3461e-08,  2.3862e-08,  5.9780e-08,\n",
      "         1.4918e-08,  3.3585e-08, -1.3013e-07,  1.0526e-08, -3.4152e-09,\n",
      "        -8.6813e-08,  2.1134e-08, -2.3063e-08, -1.5574e-09, -8.4834e-09,\n",
      "        -1.8568e-08, -1.1747e-08,  1.7147e-08, -6.1169e-08, -1.1034e-07,\n",
      "        -2.1928e-08,  7.5950e-09, -7.2011e-08, -1.6863e-07,  1.6359e-10,\n",
      "        -2.7398e-08, -9.1396e-08,  4.2316e-08,  6.0000e-09,  4.9157e-09,\n",
      "         2.0316e-08,  6.3701e-08, -1.9384e-08, -3.8638e-09,  7.0242e-08,\n",
      "         2.6866e-08,  1.0974e-07,  6.5897e-08, -2.9341e-10, -1.6017e-07,\n",
      "        -2.1927e-07,  2.7276e-08,  1.9122e-08,  6.4884e-10, -4.9727e-09,\n",
      "        -9.6755e-10, -1.2636e-09, -4.7014e-08, -3.9056e-08,  1.5554e-08,\n",
      "        -4.0094e-08, -2.6931e-08,  6.2502e-08, -3.3820e-08,  3.9723e-08,\n",
      "        -5.4243e-08, -1.3114e-08, -4.2549e-08, -3.5829e-08,  2.5603e-09,\n",
      "         8.9857e-08, -2.0489e-09, -1.4573e-08,  1.6787e-09, -2.5850e-09,\n",
      "        -1.5021e-08,  9.0563e-09, -4.2219e-08, -7.0780e-09, -1.8378e-07,\n",
      "        -2.8007e-08, -7.7823e-09,  1.2168e-09,  4.1414e-08, -1.6560e-09,\n",
      "        -3.0492e-09,  1.3620e-08,  4.5411e-08, -4.3885e-08, -1.4419e-08,\n",
      "        -1.3719e-08, -8.2341e-08, -4.0537e-08,  1.8596e-09, -3.9218e-08,\n",
      "         4.6068e-08, -1.5310e-08,  9.2534e-10,  1.1314e-07,  1.4023e-08,\n",
      "         3.4468e-08, -1.0966e-07, -8.6181e-10, -1.0756e-09,  1.3701e-09,\n",
      "         8.1982e-09, -6.3824e-09, -6.5542e-08,  3.7433e-09, -7.4975e-07,\n",
      "        -2.0400e-07,  1.8044e-08,  1.4238e-08, -4.5429e-08,  4.9946e-09,\n",
      "        -4.2269e-09, -1.4750e-08, -4.0041e-08, -1.7269e-08,  3.4119e-08,\n",
      "        -8.9796e-08,  3.9154e-08, -4.9913e-08, -3.3204e-08,  3.1766e-08,\n",
      "         3.5557e-08,  3.8765e-08,  4.5944e-09, -2.0717e-07, -1.0487e-08,\n",
      "         3.8613e-08,  2.1348e-08, -2.2519e-07, -3.5727e-08,  1.2532e-09,\n",
      "         1.8528e-08,  1.4903e-08, -4.1302e-08,  2.9804e-07,  4.9850e-08,\n",
      "        -2.0867e-08, -8.7905e-08,  3.9796e-08, -3.8879e-08, -1.9694e-07,\n",
      "         6.4360e-08,  1.4825e-09,  6.6492e-09, -3.7892e-08,  4.1171e-08,\n",
      "        -6.1278e-08,  6.3853e-10, -5.3935e-10, -5.1889e-08, -1.0714e-07,\n",
      "         1.7932e-08, -7.9999e-09, -2.2299e-08,  8.6500e-08,  3.2273e-07,\n",
      "        -1.1965e-07,  2.4421e-09, -3.2530e-08, -5.8613e-08,  7.3309e-08,\n",
      "         3.7829e-08,  2.1550e-08, -2.7032e-09,  3.2453e-08,  3.1900e-08,\n",
      "         6.7753e-09,  4.2803e-08, -1.5998e-08,  6.5015e-08, -1.9033e-09,\n",
      "         6.6158e-09, -2.0772e-07, -1.8231e-08,  2.8472e-08, -1.4803e-07,\n",
      "        -1.0320e-08,  5.3633e-08,  4.3029e-09,  3.7919e-08,  4.7181e-08,\n",
      "        -4.5123e-09, -5.1798e-09,  3.3633e-07,  2.3715e-08,  1.2023e-07,\n",
      "         1.9457e-08,  4.6336e-08, -3.8738e-08, -1.1823e-07,  1.3716e-08,\n",
      "        -2.4134e-07,  3.2492e-08,  1.0632e-07, -4.7861e-08, -1.6681e-08,\n",
      "         7.1590e-08,  3.2724e-08,  2.0337e-09,  3.1520e-08,  1.2524e-07,\n",
      "        -1.4892e-08, -7.1916e-09,  2.4776e-07,  3.9376e-08,  4.5618e-08,\n",
      "         1.6237e-08,  8.4436e-09, -4.3859e-08, -1.5252e-08,  2.9863e-08,\n",
      "         1.1440e-08, -3.6974e-09, -3.3102e-07,  1.8010e-09, -2.3797e-08,\n",
      "        -6.6152e-09, -9.0685e-09, -1.5749e-08, -1.4996e-09, -1.1549e-07,\n",
      "         4.2229e-08, -5.2440e-09,  7.8597e-11,  5.9592e-07,  1.3757e-08,\n",
      "         1.0472e-07, -1.5400e-08, -3.7680e-08, -5.0043e-08,  2.6323e-08,\n",
      "        -2.0412e-08, -1.2241e-07, -2.4579e-08,  7.9305e-09,  1.4331e-09,\n",
      "        -2.4277e-08,  3.6454e-08, -7.6628e-08, -3.5735e-08, -3.8415e-09,\n",
      "        -9.1778e-08,  1.8042e-09, -3.4761e-07,  1.4914e-09, -1.6463e-07],\n",
      "       device='cuda:0', requires_grad=True)), ('BN_linear1.weight', Parameter containing:\n",
      "tensor([0.5484, 0.5635, 0.6283, 0.5578, 0.6057, 0.4312, 0.6330, 0.6293, 0.5765,\n",
      "        0.5145, 0.4625, 0.4951, 0.6397, 0.4830, 0.5712, 0.5220, 0.6054, 0.6845,\n",
      "        0.5459, 0.5930, 0.5189, 0.6260, 0.4731, 0.6050, 0.5683, 0.4353, 0.6988,\n",
      "        0.5989, 0.3907, 0.4530, 0.5683, 0.7123, 0.4899, 0.5516, 0.5974, 0.5896,\n",
      "        0.4587, 0.5357, 0.5956, 0.6244, 0.5504, 0.5142, 0.4630, 0.6033, 0.4601,\n",
      "        0.5597, 0.4355, 0.7016, 0.5510, 0.6522, 0.4034, 0.6028, 0.4629, 0.5029,\n",
      "        0.5306, 0.5874, 0.5149, 0.5387, 0.4611, 0.5957, 0.5503, 0.4018, 0.5758,\n",
      "        0.4799, 0.6028, 0.6533, 0.5022, 0.5068, 0.5491, 0.5863, 0.4344, 0.5710,\n",
      "        0.4406, 0.6254, 0.6160, 0.6738, 0.6256, 0.5687, 0.6242, 0.5490, 0.4796,\n",
      "        0.6059, 0.6902, 0.5144, 0.5099, 0.4223, 0.5361, 0.6026, 0.6278, 0.5712,\n",
      "        0.4509, 0.5350, 0.5995, 0.4270, 0.5356, 0.6167, 0.5118, 0.6464, 0.5790,\n",
      "        0.5644, 0.5904, 0.5594, 0.4239, 0.5763, 0.4958, 0.5930, 0.5264, 0.6539,\n",
      "        0.4169, 0.6018, 0.5045, 0.4775, 0.4665, 0.4708, 0.6150, 0.5445, 0.4600,\n",
      "        0.5236, 0.5752, 0.6155, 0.5674, 0.5282, 0.5213, 0.5812, 0.5820, 0.5670,\n",
      "        0.6365, 0.4364, 0.7557, 0.6960, 0.4950, 0.5113, 0.6001, 0.6027, 0.5704,\n",
      "        0.5352, 0.5913, 0.4181, 0.5353, 0.5256, 0.6624, 0.6022, 0.6192, 0.5260,\n",
      "        0.5492, 0.4824, 0.5810, 0.6828, 0.5700, 0.6185, 0.6484, 0.4519, 0.6183,\n",
      "        0.5746, 0.4265, 0.6256, 0.5124, 0.6148, 0.5920, 0.6445, 0.5632, 0.6555,\n",
      "        0.4944, 0.4794, 0.5969, 0.6097, 0.5123, 0.5211, 0.6228, 0.4503, 0.5803,\n",
      "        0.6457, 0.3831, 0.6810, 0.5782, 0.5546, 0.5350, 0.6332, 0.6345, 0.4731,\n",
      "        0.5411, 0.5993, 0.7275, 0.6005, 0.5935, 0.6045, 0.6346, 0.5520, 0.5512,\n",
      "        0.6270, 0.5373, 0.4809, 0.4758, 0.5832, 0.5722, 0.5703, 0.5391, 0.5994,\n",
      "        0.6789, 0.6442, 0.6708, 0.4191, 0.5351, 0.4401, 0.5040, 0.6065, 0.4356,\n",
      "        0.4667, 0.4823, 0.4760, 0.5135, 0.4554, 0.6089, 0.4581, 0.4553, 0.5461,\n",
      "        0.6213, 0.5764, 0.5494, 0.6111, 0.5601, 0.5971, 0.4553, 0.6349, 0.5418,\n",
      "        0.4924, 0.5008, 0.4946, 0.5261, 0.5474, 0.4625, 0.3481, 0.5274, 0.5142,\n",
      "        0.5419, 0.4998, 0.6170, 0.5964, 0.6126, 0.5565, 0.5195, 0.3942, 0.5042,\n",
      "        0.6248, 0.5989, 0.5835, 0.5216, 0.5894, 0.5350, 0.4349, 0.4353, 0.5950,\n",
      "        0.4998, 0.5756, 0.5890, 0.5193, 0.5131, 0.3995, 0.4440, 0.6701, 0.4356,\n",
      "        0.4865, 0.4826, 0.6216, 0.4750, 0.5648, 0.4533, 0.4810, 0.5614, 0.4656,\n",
      "        0.5285, 0.5796, 0.6722, 0.5046, 0.5930, 0.4780, 0.4798, 0.5120, 0.5573,\n",
      "        0.5951, 0.6327, 0.7006, 0.5451, 0.5827, 0.5339, 0.5530, 0.5239, 0.4485,\n",
      "        0.5453, 0.6021, 0.4698, 0.4977, 0.5800, 0.4020, 0.5254, 0.4429, 0.5516,\n",
      "        0.4955, 0.7076, 0.6535, 0.4889, 0.6292, 0.5178, 0.6175, 0.5010, 0.6738,\n",
      "        0.6162, 0.5440, 0.5444, 0.6003, 0.6413, 0.5453, 0.4480, 0.3465, 0.7348,\n",
      "        0.4257, 0.5789, 0.6352, 0.5126, 0.5926, 0.5433, 0.5770, 0.5415, 0.6448,\n",
      "        0.5773, 0.6758, 0.5689, 0.4656, 0.4794, 0.6135, 0.4973, 0.5112, 0.6240,\n",
      "        0.5184, 0.6322, 0.4614, 0.6399, 0.5161, 0.5402, 0.6854, 0.5926, 0.5801,\n",
      "        0.4903, 0.5656, 0.4190, 0.4123, 0.5971, 0.5729, 0.4916, 0.4913, 0.5430,\n",
      "        0.5153, 0.4993, 0.6295, 0.6537, 0.7757, 0.6055, 0.6145, 0.7147, 0.6754,\n",
      "        0.5470, 0.5273, 0.6999, 0.5064, 0.4815, 0.5882, 0.6092, 0.5214, 0.5220,\n",
      "        0.6858, 0.4773, 0.7627, 0.4893, 0.6126, 0.5855, 0.7017, 0.6380, 0.7597,\n",
      "        0.6267, 0.3545, 0.4909, 0.5239, 0.5171, 0.5827, 0.5963, 0.5631, 0.4324,\n",
      "        0.4913, 0.5075, 0.5851, 0.4108, 0.6940, 0.5772, 0.6396, 0.4248, 0.6203,\n",
      "        0.4285, 0.4401, 0.6112, 0.6593, 0.6619, 0.6803, 0.5882, 0.4990, 0.5459,\n",
      "        0.3721, 0.6225, 0.5761, 0.5240, 0.5208, 0.6120, 0.4716, 0.5795, 0.5617,\n",
      "        0.7344, 0.4988, 0.5342, 0.4783, 0.6046, 0.6761, 0.3846, 0.5652, 0.4919,\n",
      "        0.4908, 0.4958, 0.6475, 0.5179, 0.4455, 0.6851, 0.4560, 0.3330, 0.4884,\n",
      "        0.5750, 0.5952, 0.6461, 0.6100, 0.4190, 0.5582, 0.4341, 0.4615, 0.5668,\n",
      "        0.4585, 0.5306, 0.5888, 0.6336, 0.6333, 0.6175, 0.6026, 0.6918, 0.5076,\n",
      "        0.6149, 0.6518, 0.6440, 0.7122, 0.6392, 0.4881, 0.4224, 0.4764, 0.5944,\n",
      "        0.7395, 0.5704, 0.5668, 0.5241, 0.6351, 0.5902, 0.5212, 0.4579, 0.6438,\n",
      "        0.5648, 0.5860, 0.6738, 0.4574, 0.5321, 0.5655, 0.7165, 0.5980, 0.5389,\n",
      "        0.4211, 0.5863, 0.3618, 0.5318, 0.4549, 0.6749, 0.5745, 0.5062, 0.4003,\n",
      "        0.5082, 0.5692, 0.5537, 0.4034, 0.6759, 0.5242, 0.6688, 0.6238, 0.4404,\n",
      "        0.5398, 0.5170, 0.4779, 0.4047, 0.6103, 0.6264, 0.5852, 0.5244, 0.5585,\n",
      "        0.6059, 0.4401, 0.6419, 0.4656, 0.5852, 0.4809, 0.4126, 0.4354, 0.6010,\n",
      "        0.5898, 0.4271, 0.5573, 0.4581, 0.5764, 0.4705, 0.4770, 0.5306, 0.7039,\n",
      "        0.6080, 0.5739, 0.5548, 0.6580, 0.5252, 0.6617, 0.5722, 0.4778, 0.6997,\n",
      "        0.4980, 0.5581, 0.4543, 0.5866, 0.7445, 0.5379, 0.5351, 0.4971, 0.6443,\n",
      "        0.4767, 0.5104, 0.6235, 0.6510, 0.5061, 0.5792, 0.6525, 0.5786, 0.5373,\n",
      "        0.6718, 0.5714, 0.6466, 0.4639, 0.4721, 0.5573, 0.6798, 0.5116, 0.6167,\n",
      "        0.6344, 0.5313, 0.5947, 0.5917, 0.6244, 0.5457, 0.5523, 0.5229, 0.5139,\n",
      "        0.4204, 0.4796, 0.7389, 0.4608, 0.4905, 0.5247, 0.6018, 0.4547, 0.5308,\n",
      "        0.5773, 0.5793, 0.4761, 0.6116, 0.6325, 0.5677, 0.4897, 0.6002, 0.5152,\n",
      "        0.5405, 0.6261, 0.5391, 0.4543, 0.5880, 0.5842, 0.5474, 0.6282, 0.5519,\n",
      "        0.7891, 0.5646, 0.6155, 0.6378, 0.5989, 0.5322, 0.6698, 0.4796, 0.6116,\n",
      "        0.6171, 0.6282, 0.5204, 0.5871, 0.4829, 0.6104, 0.5779, 0.5034, 0.3775,\n",
      "        0.4581, 0.5638, 0.4403, 0.5649, 0.5964, 0.4781, 0.4722, 0.6473, 0.5480,\n",
      "        0.6013, 0.6107, 0.5810, 0.5139, 0.5625, 0.3922, 0.4003, 0.6073, 0.4730,\n",
      "        0.7162, 0.6354, 0.5083, 0.5894, 0.6415, 0.7207, 0.5295, 0.4812, 0.6185,\n",
      "        0.3570, 0.6675, 0.7228, 0.5571, 0.5476, 0.6206, 0.4775, 0.6137, 0.4377,\n",
      "        0.3877, 0.4748, 0.5147, 0.5505, 0.6792, 0.3998, 0.4380, 0.4880, 0.4895,\n",
      "        0.5615, 0.4684, 0.6836, 0.5835, 0.5188, 0.4644, 0.6041, 0.5607, 0.5420,\n",
      "        0.4242, 0.4062, 0.5427, 0.6228, 0.6363, 0.6065, 0.5454, 0.5472, 0.6616,\n",
      "        0.3781, 0.6265, 0.5295, 0.5225, 0.5469, 0.4415, 0.5128, 0.5335, 0.5653,\n",
      "        0.5692, 0.4030, 0.5579, 0.6167, 0.4373, 0.5652, 0.4569, 0.6851, 0.4976,\n",
      "        0.4107, 0.7210, 0.4680, 0.7222, 0.4842, 0.6158, 0.5014, 0.5589, 0.6370,\n",
      "        0.4538, 0.6271, 0.3856, 0.6045, 0.6484, 0.6052, 0.7314, 0.5238, 0.4077,\n",
      "        0.5802, 0.6258, 0.4295, 0.5921, 0.6134, 0.6228, 0.6520, 0.4902, 0.5272,\n",
      "        0.5807, 0.5300, 0.6216, 0.6625, 0.5347, 0.5796, 0.3446, 0.6443, 0.5552,\n",
      "        0.5545, 0.5507, 0.6353, 0.6690, 0.4703, 0.6853, 0.6889, 0.5622, 0.5550,\n",
      "        0.6030, 0.6532, 0.6168, 0.4476, 0.5867, 0.4615, 0.4355, 0.5259, 0.5077,\n",
      "        0.4929, 0.5499, 0.5737, 0.5638, 0.4674, 0.5674, 0.4563, 0.4754, 0.5340,\n",
      "        0.6194, 0.6139, 0.6992, 0.5472, 0.5452, 0.6370, 0.4960, 0.6376, 0.4683,\n",
      "        0.6733, 0.7096, 0.6665, 0.3613, 0.5159, 0.3695, 0.4939, 0.4370, 0.3260,\n",
      "        0.6190, 0.5240, 0.4260, 0.5607, 0.4939, 0.4732, 0.4832, 0.5095, 0.5914,\n",
      "        0.6221, 0.5253, 0.6724, 0.5530, 0.6182, 0.6040, 0.5316, 0.7167, 0.4548,\n",
      "        0.5395, 0.5892, 0.4471, 0.5216, 0.4507, 0.5088, 0.5963, 0.7389, 0.4992,\n",
      "        0.3942, 0.4755, 0.5157, 0.5555, 0.5895, 0.3483, 0.4373, 0.4749, 0.4820,\n",
      "        0.4848, 0.5726, 0.4949, 0.7349, 0.4227, 0.6314, 0.5937, 0.6942, 0.6385,\n",
      "        0.5234, 0.5788, 0.5375, 0.6439, 0.4607, 0.4002, 0.4361, 0.5265, 0.4660,\n",
      "        0.4703, 0.4802, 0.6424, 0.6840, 0.5220, 0.4892, 0.6138, 0.4734, 0.3501,\n",
      "        0.7297, 0.4781, 0.4751, 0.5806, 0.4809, 0.6393, 0.7315, 0.5587, 0.4531,\n",
      "        0.6208, 0.4407, 0.6037, 0.6169, 0.4415, 0.5987, 0.5115, 0.4029, 0.5462,\n",
      "        0.4756, 0.3944, 0.5592, 0.4316, 0.5009, 0.5464, 0.5014, 0.5492, 0.4679,\n",
      "        0.5455, 0.6374, 0.4889, 0.7078, 0.4201, 0.6092, 0.5776, 0.4923, 0.5649,\n",
      "        0.6372, 0.6238, 0.6402, 0.6393, 0.3555, 0.5688, 0.4073, 0.4954, 0.6058,\n",
      "        0.6078, 0.3806, 0.4957, 0.4660, 0.5345, 0.4976, 0.5690, 0.3804, 0.7130,\n",
      "        0.5726, 0.5488, 0.5559, 0.6009, 0.5323, 0.4172, 0.5228, 0.5019, 0.5431,\n",
      "        0.5961, 0.5466, 0.5900, 0.6285, 0.7198, 0.5903, 0.4684, 0.5019, 0.6215,\n",
      "        0.7141, 0.4869, 0.4482, 0.4880, 0.5876, 0.6689, 0.5401, 0.5079, 0.5462,\n",
      "        0.5737, 0.4987, 0.4979, 0.5641, 0.6159, 0.5229, 0.5598, 0.5758, 0.5338,\n",
      "        0.6361, 0.5464, 0.5638, 0.4799, 0.5913, 0.4569, 0.5318, 0.6285, 0.4185,\n",
      "        0.5913, 0.5871, 0.5458, 0.4287, 0.3340, 0.5340, 0.5269, 0.5663, 0.5290,\n",
      "        0.5797, 0.5732, 0.5452, 0.6322, 0.4158, 0.6291, 0.5672, 0.6907, 0.4279,\n",
      "        0.4890, 0.5811, 0.5599, 0.4934, 0.5897, 0.5346, 0.5713, 0.4671, 0.5080,\n",
      "        0.5982, 0.5779, 0.4817, 0.5100, 0.6588, 0.5766, 0.5003, 0.5024, 0.5197,\n",
      "        0.7336, 0.4715, 0.5083, 0.5740, 0.5300, 0.4840, 0.6450, 0.4979, 0.6718,\n",
      "        0.4408, 0.5985, 0.5478, 0.5983, 0.7480, 0.5656, 0.4441, 0.5212, 0.4649,\n",
      "        0.5130, 0.4641, 0.5080, 0.5394, 0.5040, 0.4493, 0.4996, 0.4598, 0.4442,\n",
      "        0.6707], device='cuda:0', requires_grad=True)), ('BN_linear1.bias', Parameter containing:\n",
      "tensor([-0.0514, -0.0348,  0.0171, -0.0486, -0.0649, -0.0892, -0.0076, -0.0806,\n",
      "        -0.0267, -0.0611, -0.0554,  0.0126,  0.0373, -0.0510, -0.0702, -0.0200,\n",
      "        -0.1184, -0.0197, -0.0232, -0.0851, -0.0664, -0.1105, -0.0774, -0.0393,\n",
      "        -0.0570, -0.0693, -0.0340, -0.0205, -0.0896, -0.0709, -0.0328, -0.0696,\n",
      "        -0.0640, -0.0143, -0.0646, -0.0293, -0.0774, -0.1087, -0.0810, -0.0335,\n",
      "        -0.0629, -0.0946, -0.0711,  0.0025, -0.0883, -0.0196, -0.0544, -0.0842,\n",
      "        -0.0959, -0.0595, -0.0507, -0.0165, -0.0710, -0.0704, -0.0539, -0.0384,\n",
      "        -0.0353, -0.0760, -0.0437, -0.0370, -0.1066, -0.0675, -0.0599, -0.0960,\n",
      "         0.0004, -0.0154, -0.0436, -0.0935, -0.0316, -0.0746, -0.0719, -0.0569,\n",
      "        -0.0962, -0.0546,  0.0471, -0.0468, -0.0959, -0.0746, -0.0209, -0.0373,\n",
      "        -0.0611, -0.0008, -0.1106, -0.0887, -0.0846, -0.0244, -0.0820, -0.0410,\n",
      "        -0.0388, -0.0595, -0.0686, -0.0318, -0.1412, -0.0703, -0.0497, -0.0198,\n",
      "        -0.0041, -0.0623, -0.0556, -0.0600, -0.0802, -0.0779, -0.0734, -0.0785,\n",
      "        -0.0509, -0.1181, -0.0707,  0.0034, -0.0861, -0.0569, -0.0651, -0.0700,\n",
      "        -0.0463, -0.0263, -0.0094, -0.0356, -0.0422, -0.0884, -0.1136, -0.0534,\n",
      "        -0.0442, -0.0532, -0.1093, -0.0816, -0.1120, -0.0799, -0.1044, -0.1099,\n",
      "        -0.0523, -0.0588, -0.0775,  0.0050, -0.0751, -0.0532, -0.0024, -0.0415,\n",
      "        -0.1009, -0.0341, -0.0344, -0.0768, -0.0437, -0.0393, -0.0960,  0.0184,\n",
      "        -0.0679, -0.0671, -0.0930, -0.0740, -0.1000, -0.0230, -0.0730, -0.0765,\n",
      "        -0.0807, -0.0423, -0.0452,  0.0119, -0.0531, -0.0650, -0.0638, -0.0724,\n",
      "        -0.0994, -0.0467, -0.0572, -0.0354, -0.0681, -0.0424, -0.0455, -0.0497,\n",
      "        -0.0755, -0.0528, -0.0716, -0.0202, -0.0783, -0.0912, -0.0576, -0.0222,\n",
      "        -0.0445, -0.0359, -0.0682, -0.0531, -0.0991, -0.0406,  0.0026, -0.0398,\n",
      "        -0.0713, -0.1233, -0.0514,  0.0131, -0.0698, -0.0269, -0.0189, -0.0431,\n",
      "        -0.0587, -0.0123, -0.0284,  0.0107, -0.0539, -0.0604, -0.0478, -0.0171,\n",
      "        -0.1514, -0.0486, -0.0432, -0.0752, -0.0640, -0.0621, -0.0600, -0.0407,\n",
      "        -0.0701, -0.0265, -0.0549, -0.0962, -0.0299, -0.0519, -0.0347, -0.0535,\n",
      "        -0.0698, -0.0376,  0.0095, -0.0559, -0.0565, -0.0243, -0.0888, -0.0447,\n",
      "        -0.0566, -0.0741, -0.0591, -0.0482, -0.1060, -0.0900, -0.0830, -0.0277,\n",
      "        -0.0498, -0.0905, -0.0547, -0.0486,  0.0143, -0.0931, -0.0463, -0.0467,\n",
      "        -0.0947, -0.0367, -0.0669, -0.0582, -0.0796, -0.0083, -0.0055, -0.0615,\n",
      "        -0.1098, -0.0275, -0.0475, -0.0490, -0.0603, -0.1051, -0.1082, -0.0555,\n",
      "        -0.0818, -0.0734, -0.0560, -0.0638, -0.0206, -0.0548, -0.0120, -0.0789,\n",
      "        -0.0630, -0.0770, -0.0651, -0.0700, -0.0942, -0.0349, -0.1052, -0.0069,\n",
      "        -0.0781, -0.0768, -0.1005, -0.0784, -0.0796, -0.0320, -0.0955, -0.0317,\n",
      "        -0.0154, -0.0774, -0.0468, -0.0530, -0.0628, -0.0867, -0.0336, -0.0959,\n",
      "        -0.0365, -0.1049, -0.0669, -0.0872, -0.0242, -0.0630, -0.0659, -0.0806,\n",
      "        -0.0902, -0.0219, -0.0034, -0.0615, -0.0469, -0.0323, -0.0753, -0.0425,\n",
      "        -0.0609, -0.0580, -0.0232, -0.0412, -0.0861, -0.1262, -0.0613, -0.0475,\n",
      "        -0.0531, -0.0832, -0.0168, -0.0524, -0.0419, -0.0760, -0.0654, -0.0644,\n",
      "        -0.0477, -0.0872, -0.0756, -0.0239, -0.0974, -0.0790, -0.0620, -0.0755,\n",
      "        -0.0911, -0.0048, -0.0485, -0.0452, -0.0514, -0.0351, -0.0879, -0.0442,\n",
      "        -0.0650, -0.0635, -0.0684, -0.0764, -0.0449, -0.0794, -0.0789, -0.0596,\n",
      "        -0.0617, -0.0316, -0.0371, -0.0411, -0.0623, -0.0595, -0.0528, -0.0456,\n",
      "        -0.0703, -0.0544, -0.0647, -0.0311,  0.0068, -0.1248, -0.0323, -0.0494,\n",
      "        -0.0210, -0.0747, -0.0449, -0.0718, -0.0579, -0.1271, -0.1087, -0.0727,\n",
      "        -0.0563, -0.0538, -0.0525, -0.0135, -0.0405, -0.0337, -0.0859, -0.0538,\n",
      "        -0.0509, -0.0521, -0.0638, -0.0927, -0.0767, -0.0764, -0.0721, -0.0557,\n",
      "        -0.0381, -0.0816, -0.0408, -0.0175, -0.0482, -0.0878, -0.0713, -0.1371,\n",
      "        -0.0912,  0.0014, -0.0462, -0.0538, -0.0504, -0.0443, -0.0774, -0.0576,\n",
      "        -0.0520, -0.0856, -0.0535, -0.0875, -0.0702, -0.0724, -0.0613, -0.0878,\n",
      "        -0.0448, -0.0573, -0.0232, -0.0498, -0.0509, -0.0539, -0.0309, -0.0966,\n",
      "        -0.0304, -0.0493, -0.0184, -0.0736, -0.0727, -0.0439, -0.0628, -0.0749,\n",
      "        -0.0415, -0.0985, -0.0623, -0.0698, -0.0557, -0.0412, -0.0926, -0.0712,\n",
      "        -0.0247, -0.0480, -0.0890, -0.0486, -0.0638,  0.0305, -0.0546, -0.0847,\n",
      "        -0.0650, -0.0736, -0.0163, -0.0557, -0.0404, -0.0559, -0.0545, -0.0086,\n",
      "        -0.0887, -0.0999, -0.0238, -0.0652, -0.0829, -0.0604, -0.0480, -0.0769,\n",
      "        -0.0414, -0.0093, -0.0693, -0.1028, -0.0351, -0.0752, -0.0713, -0.0113,\n",
      "        -0.0648, -0.0352, -0.0041, -0.1081, -0.0129, -0.0308, -0.0217, -0.0386,\n",
      "        -0.0453, -0.1245, -0.0855, -0.0899, -0.0851, -0.0518, -0.1041, -0.0369,\n",
      "        -0.0468, -0.0436, -0.0848, -0.0445, -0.0735, -0.0434, -0.0297, -0.0753,\n",
      "        -0.0620, -0.0728, -0.0706, -0.0769, -0.0692, -0.0271, -0.0744, -0.0661,\n",
      "        -0.0925, -0.1022, -0.0895, -0.0187, -0.0424, -0.0507, -0.0515, -0.0638,\n",
      "        -0.0537, -0.0581, -0.0709, -0.0658, -0.0688, -0.0453, -0.0463, -0.0373,\n",
      "        -0.0444, -0.0630, -0.0613, -0.0632, -0.0235, -0.0492, -0.0645, -0.0366,\n",
      "        -0.0725, -0.0529, -0.1415, -0.0455, -0.0064, -0.0876, -0.0231, -0.0435,\n",
      "        -0.0659, -0.0867, -0.0341, -0.0619, -0.0543, -0.0405, -0.0926, -0.0533,\n",
      "        -0.0888, -0.0367, -0.0701, -0.0646, -0.0321, -0.0297, -0.0891, -0.0570,\n",
      "        -0.0605, -0.1063, -0.0617, -0.0587, -0.0478, -0.0973, -0.0895, -0.1238,\n",
      "        -0.0523, -0.0909,  0.0066, -0.0301, -0.0465, -0.1019, -0.0955, -0.0442,\n",
      "        -0.0467, -0.0550, -0.0628, -0.0287, -0.1135, -0.0439, -0.0391, -0.0649,\n",
      "        -0.0267, -0.0897, -0.0873, -0.0386, -0.0377, -0.0391, -0.0534, -0.0629,\n",
      "        -0.0672, -0.0470, -0.0533, -0.0200, -0.0673, -0.0023, -0.0732, -0.0031,\n",
      "        -0.0423, -0.0586, -0.0793, -0.0591, -0.0649, -0.0573, -0.0933, -0.0903,\n",
      "        -0.0737, -0.0818, -0.0708, -0.0024, -0.0461, -0.0564, -0.0013, -0.0389,\n",
      "        -0.0837, -0.0491, -0.0415, -0.0267, -0.0401, -0.0181, -0.0600, -0.0062,\n",
      "        -0.0296, -0.0386, -0.0821, -0.0559, -0.0909, -0.0508, -0.0760, -0.1069,\n",
      "        -0.0511, -0.0574, -0.0187, -0.1076, -0.0485, -0.0151, -0.0069, -0.0410,\n",
      "        -0.0363, -0.0543, -0.0974, -0.0703, -0.0748, -0.0412, -0.0415, -0.0291,\n",
      "        -0.0590, -0.0370, -0.1004, -0.0960, -0.0359, -0.0529,  0.0058, -0.0657,\n",
      "         0.0049,  0.0208, -0.0970, -0.0370, -0.0631, -0.0514, -0.0787, -0.0480,\n",
      "        -0.0485, -0.0212, -0.0217, -0.0583, -0.0957, -0.0646, -0.0430, -0.0465,\n",
      "        -0.0741, -0.0878, -0.0553,  0.0413, -0.0407, -0.0504, -0.0695, -0.0399,\n",
      "        -0.1191, -0.0748, -0.0485, -0.0594, -0.0810, -0.0507, -0.0485, -0.0264,\n",
      "        -0.0890, -0.0854, -0.0390, -0.0484, -0.0654, -0.0698, -0.0290, -0.0481,\n",
      "        -0.0778, -0.0542, -0.0564, -0.0615, -0.0899, -0.0817, -0.0358, -0.0851,\n",
      "        -0.0856, -0.0514, -0.0700,  0.0118, -0.0748, -0.0356, -0.0978, -0.0439,\n",
      "        -0.0598, -0.0368, -0.0871, -0.0204, -0.0572, -0.0618, -0.0674, -0.0097,\n",
      "        -0.0730, -0.0554, -0.0306, -0.0857, -0.1227, -0.0750, -0.0752, -0.0476,\n",
      "        -0.0597, -0.0535, -0.0686, -0.0519, -0.0491, -0.0988, -0.0805, -0.0336,\n",
      "        -0.0760, -0.0271, -0.0593, -0.0526, -0.0988, -0.0604, -0.0597, -0.0682,\n",
      "        -0.0250, -0.0423, -0.0698, -0.0404, -0.0874, -0.0586, -0.0990, -0.0341,\n",
      "        -0.0496, -0.0703, -0.0466, -0.0161, -0.0328, -0.0789, -0.0543, -0.0339,\n",
      "        -0.0416, -0.0487, -0.0462, -0.0591, -0.0390, -0.0024, -0.0091, -0.0572,\n",
      "        -0.0762, -0.0460, -0.0943, -0.0591, -0.0840, -0.0745, -0.0706, -0.0426,\n",
      "        -0.1058, -0.0266, -0.0989, -0.0898, -0.0654, -0.0462, -0.0826, -0.0920,\n",
      "        -0.0612, -0.0657, -0.0389, -0.0511, -0.0457, -0.0894, -0.0692, -0.0473,\n",
      "        -0.0462, -0.0748, -0.0253, -0.0335, -0.0619, -0.0653, -0.0328, -0.1014,\n",
      "        -0.0589, -0.0353, -0.0157, -0.1598, -0.0615, -0.0805, -0.0661, -0.0989,\n",
      "        -0.0539,  0.0020, -0.0225, -0.0779, -0.0353, -0.0417, -0.0889, -0.0512,\n",
      "        -0.0442, -0.0727, -0.0861, -0.0647, -0.0743, -0.0606, -0.1211, -0.0830,\n",
      "        -0.0408, -0.1164, -0.0889, -0.0469, -0.0884, -0.0689, -0.0111, -0.0664,\n",
      "        -0.0722, -0.1081, -0.0780, -0.0625, -0.0047, -0.0555, -0.0496, -0.1140,\n",
      "        -0.0840, -0.0270, -0.0783, -0.0296, -0.0401, -0.0560, -0.0569, -0.1130,\n",
      "        -0.0440, -0.0606, -0.0689, -0.0728, -0.0521, -0.0863, -0.0889, -0.0997,\n",
      "        -0.0239, -0.0634, -0.0495, -0.0523, -0.0392, -0.0580, -0.0440, -0.0774,\n",
      "        -0.1368, -0.1346, -0.0463, -0.0502, -0.0795, -0.0633, -0.0649, -0.0291,\n",
      "        -0.0906, -0.0865, -0.0275, -0.0567, -0.0437, -0.0684, -0.0410, -0.0678,\n",
      "        -0.0972, -0.0905, -0.0402, -0.0890, -0.0651, -0.1016, -0.0676, -0.0533,\n",
      "        -0.0199, -0.0162, -0.0381, -0.0213, -0.0850, -0.0604, -0.0597, -0.0717,\n",
      "        -0.0450, -0.1049, -0.0344, -0.0751, -0.0603, -0.0760, -0.0821, -0.0685,\n",
      "        -0.0039, -0.0370, -0.1264, -0.0790, -0.0716, -0.0570, -0.0254, -0.0097,\n",
      "        -0.0278, -0.0456, -0.0353, -0.0750, -0.0279, -0.0686, -0.0434, -0.0635,\n",
      "        -0.1239, -0.0134, -0.0929, -0.0584, -0.0535, -0.0596, -0.0486, -0.0280,\n",
      "        -0.0811, -0.0477, -0.0589, -0.0559, -0.0325, -0.0482, -0.0948, -0.0397,\n",
      "        -0.0331, -0.0825, -0.0225, -0.0782, -0.0119, -0.0655, -0.0392, -0.0186,\n",
      "        -0.0530, -0.0514, -0.0778, -0.0079, -0.0585, -0.0218, -0.0510, -0.0831,\n",
      "        -0.0566, -0.0173, -0.0611, -0.0502, -0.0609, -0.0645,  0.0074, -0.0605,\n",
      "        -0.0269, -0.0566, -0.0578, -0.0263, -0.0269, -0.0668, -0.0614, -0.0205,\n",
      "        -0.0387, -0.0704, -0.0348, -0.0196, -0.0222, -0.0446, -0.0964, -0.0373,\n",
      "        -0.0711, -0.0808, -0.0733, -0.0624, -0.0498, -0.0740, -0.0740, -0.0597,\n",
      "        -0.0785, -0.0707, -0.0724, -0.0893, -0.0920, -0.0463, -0.0592, -0.0607,\n",
      "        -0.0002, -0.0090, -0.0728, -0.0627, -0.0253, -0.0524,  0.0109, -0.0802,\n",
      "        -0.0622, -0.0212, -0.0130, -0.0708, -0.0324, -0.0676, -0.1050, -0.0300,\n",
      "        -0.0689, -0.0293, -0.0622, -0.0457, -0.0671, -0.1044, -0.0419, -0.0900],\n",
      "       device='cuda:0', requires_grad=True)), ('linear_model2.weight', Parameter containing:\n",
      "tensor([[-0.0244,  0.0044, -0.0670,  ..., -0.0163, -0.0495,  0.0680],\n",
      "        [-0.0057, -0.0112,  0.0329,  ..., -0.0322, -0.0229,  0.0453],\n",
      "        [ 0.0346, -0.0156,  0.0353,  ...,  0.0192,  0.0699,  0.0194],\n",
      "        ...,\n",
      "        [-0.0058,  0.0064, -0.0157,  ...,  0.0274, -0.0120,  0.0075],\n",
      "        [ 0.0113,  0.0078, -0.0293,  ...,  0.0287,  0.0023, -0.0044],\n",
      "        [ 0.0551, -0.0376,  0.0396,  ...,  0.0073,  0.0112, -0.0401]],\n",
      "       device='cuda:0', requires_grad=True)), ('linear_model2.bias', Parameter containing:\n",
      "tensor([-4.0378e-07, -1.9483e-06, -7.9927e-07,  8.0709e-07, -6.1532e-07,\n",
      "        -7.5352e-07,  7.4033e-07,  2.4875e-08,  7.8721e-08, -1.8723e-07,\n",
      "        -1.9509e-07, -2.0067e-07,  5.1587e-07, -1.1268e-06,  4.0813e-07,\n",
      "        -6.2177e-07,  1.0885e-06,  9.6041e-07, -8.2269e-08, -4.9902e-07,\n",
      "        -1.4694e-07,  1.2233e-07,  1.5641e-06,  2.8032e-07,  5.1821e-08,\n",
      "        -3.8186e-07, -8.1342e-09,  6.4068e-07, -1.8462e-07, -5.2754e-08,\n",
      "        -7.9653e-07,  4.8222e-07, -6.7220e-07,  1.1719e-07,  1.1482e-06,\n",
      "         7.2526e-07,  8.5812e-08,  7.9428e-07, -5.0141e-07, -4.5022e-07,\n",
      "         4.3422e-07, -9.6249e-07,  2.6901e-07, -4.9284e-07,  1.0173e-07,\n",
      "        -9.9410e-08,  6.5265e-07, -6.2114e-07,  1.4243e-06,  5.4230e-07,\n",
      "         6.7819e-08,  1.8803e-07,  1.9569e-06, -4.2714e-08, -1.8659e-06,\n",
      "         3.8263e-07,  5.9548e-08, -1.6190e-07, -1.9550e-07,  2.2009e-07,\n",
      "        -6.8140e-07,  1.6341e-06,  1.4520e-07, -2.8907e-07,  1.4693e-07,\n",
      "        -2.8034e-07,  1.8993e-08,  4.8390e-07, -2.5374e-07,  1.4442e-07,\n",
      "         2.7581e-07, -6.7045e-10,  1.8946e-07,  7.8287e-07, -1.0858e-06,\n",
      "        -8.0554e-07, -3.1034e-07,  2.5636e-07,  5.1157e-08,  2.6422e-07,\n",
      "        -1.1468e-07, -1.2448e-06,  3.8522e-07,  2.0948e-06, -2.5810e-07,\n",
      "         4.1963e-07,  7.8000e-07, -6.3390e-07, -2.1182e-07,  1.1709e-07,\n",
      "        -2.5814e-07,  2.4813e-08, -3.4453e-07, -1.2444e-06, -4.7430e-07,\n",
      "        -1.0426e-06, -7.4862e-07,  3.1120e-07, -7.8564e-07,  6.8396e-09],\n",
      "       device='cuda:0', requires_grad=True)), ('BN_linear2.weight', Parameter containing:\n",
      "tensor([1.0114, 1.0331, 1.0093, 0.9703, 0.9173, 0.8519, 1.0570, 0.9488, 0.8975,\n",
      "        1.0546, 0.8493, 0.8650, 0.9094, 0.8557, 0.9590, 0.9603, 0.9064, 0.9212,\n",
      "        0.9722, 0.9151, 0.9843, 0.8871, 0.9609, 0.9782, 0.9250, 0.9577, 0.9235,\n",
      "        0.8248, 0.9590, 0.9097, 0.9396, 0.8949, 0.9664, 0.9041, 0.9354, 0.9213,\n",
      "        0.8406, 0.9397, 1.0580, 0.9382, 0.9855, 0.9372, 0.9759, 1.0739, 0.9650,\n",
      "        0.8996, 1.0147, 0.9454, 0.9417, 1.0112, 0.9330, 1.0780, 0.9328, 0.9272,\n",
      "        1.1033, 0.8867, 0.9762, 0.9273, 0.9157, 0.7731, 0.8981, 1.0445, 0.9822,\n",
      "        0.8906, 0.9263, 1.0058, 0.9870, 1.0099, 1.0322, 1.0294, 0.9852, 0.8358,\n",
      "        0.9578, 0.8791, 1.0001, 0.9010, 1.0242, 0.9912, 0.9660, 1.0755, 0.8659,\n",
      "        0.9397, 0.9276, 1.0453, 0.9612, 0.9368, 0.9122, 0.9702, 0.8736, 0.9441,\n",
      "        0.9402, 0.9491, 1.0713, 0.9644, 0.8877, 0.8360, 1.0290, 0.9246, 1.0396,\n",
      "        0.9224], device='cuda:0', requires_grad=True)), ('BN_linear2.bias', Parameter containing:\n",
      "tensor([0.1715, 0.1388, 0.0960, 0.1677, 0.1224, 0.1328, 0.1313, 0.1086, 0.1404,\n",
      "        0.1574, 0.1936, 0.1577, 0.1680, 0.1224, 0.1462, 0.1136, 0.1556, 0.1503,\n",
      "        0.0678, 0.1155, 0.1642, 0.1601, 0.1026, 0.1524, 0.1294, 0.1614, 0.2192,\n",
      "        0.1134, 0.1535, 0.0790, 0.1605, 0.1479, 0.0827, 0.1676, 0.0976, 0.1139,\n",
      "        0.1661, 0.1065, 0.1216, 0.1241, 0.0879, 0.0998, 0.1179, 0.1246, 0.0683,\n",
      "        0.0736, 0.1497, 0.0819, 0.1286, 0.1540, 0.0593, 0.1064, 0.1818, 0.0664,\n",
      "        0.1141, 0.1677, 0.1399, 0.1626, 0.0768, 0.1866, 0.1446, 0.1235, 0.1266,\n",
      "        0.1364, 0.1187, 0.0635, 0.0913, 0.0939, 0.0478, 0.1022, 0.1344, 0.1842,\n",
      "        0.1754, 0.1339, 0.1291, 0.1459, 0.1388, 0.1521, 0.1964, 0.1394, 0.1405,\n",
      "        0.0942, 0.0952, 0.1522, 0.0671, 0.0732, 0.0773, 0.1717, 0.1733, 0.0763,\n",
      "        0.1636, 0.1679, 0.1494, 0.1610, 0.1471, 0.1080, 0.0908, 0.1481, 0.1360,\n",
      "        0.1328], device='cuda:0', requires_grad=True)), ('embedding_layers.weight', Parameter containing:\n",
      "tensor([[ 9.4405e-02,  4.5858e-02,  2.4007e-01,  ..., -2.6021e-05,\n",
      "          2.1055e-05, -1.1081e-01],\n",
      "        [ 1.1936e+00, -4.5160e-24,  3.9264e-02,  ..., -1.4509e-01,\n",
      "          3.3199e-01,  1.6833e-03],\n",
      "        [ 2.0348e-02, -4.9103e-02, -1.6417e-05,  ...,  5.2105e-02,\n",
      "         -6.5822e-02, -4.9641e-01],\n",
      "        ...,\n",
      "        [ 4.5052e-02, -1.9614e-01, -1.3809e-01,  ...,  9.3666e-05,\n",
      "         -6.1764e-01, -7.5040e-01],\n",
      "        [ 1.7533e-01, -4.4434e-10, -3.5739e-01,  ..., -5.5721e-01,\n",
      "          1.2746e-01,  1.2019e-01],\n",
      "        [ 1.0742e-03, -5.1132e-30,  3.5810e-01,  ...,  1.2480e-02,\n",
      "         -1.7718e-04,  5.9193e-02]], device='cuda:0', requires_grad=True)), ('BN_bi.weight', Parameter containing:\n",
      "tensor([0.6819, 0.4719, 0.4260, 0.6276, 0.5985, 0.4640, 0.3833, 0.5847, 0.4249,\n",
      "        0.3697, 0.6248, 0.4506, 0.6367, 0.5616, 0.4616, 0.5508, 0.7368, 0.5190,\n",
      "        0.4204, 0.7325, 0.7246, 0.7536, 0.4766, 0.4927, 0.5101, 0.4781, 0.3701,\n",
      "        0.6179, 0.3756, 0.5814, 0.4365, 0.4228, 0.6321, 0.5092, 0.4529, 0.4823,\n",
      "        0.4426, 0.3912, 0.5884, 0.4621, 0.5240, 0.6140, 0.4067, 0.4796, 0.6145,\n",
      "        0.8034, 0.6413, 0.3962, 0.7166, 0.4676, 0.6386, 0.5875, 0.6562, 0.4816,\n",
      "        0.6339, 0.4438, 0.3462, 0.5031, 0.5900, 0.4342, 0.4438, 0.4382, 0.4944,\n",
      "        0.6019, 0.5425, 0.8977, 0.5742, 0.4774, 0.4539, 0.7057, 0.4381, 0.6388,\n",
      "        0.4645, 0.5942, 0.4730, 0.3966, 0.4729, 0.4932, 0.4792, 0.4000, 0.5471,\n",
      "        0.4807, 0.6925, 0.4610, 0.4721, 0.5523, 0.3626, 0.3759, 0.3893, 0.4677,\n",
      "        0.6950, 0.4062, 0.4937, 0.4486, 0.4734, 0.5227, 0.4151, 0.5414, 0.4845,\n",
      "        0.8165], device='cuda:0', requires_grad=True)), ('BN_bi.bias', Parameter containing:\n",
      "tensor([-0.0403, -0.0970,  0.0285, -0.0613, -0.0536, -0.0321, -0.0232, -0.0148,\n",
      "         0.0367,  0.0434, -0.1094,  0.0793,  0.0654, -0.0041,  0.0641,  0.0054,\n",
      "        -0.0878,  0.0185, -0.1017,  0.0419,  0.0380, -0.0481, -0.0273,  0.1117,\n",
      "         0.0239,  0.0063, -0.0392,  0.0846, -0.0202, -0.0940, -0.0422, -0.0084,\n",
      "         0.0667, -0.1268, -0.0516, -0.0137, -0.0229, -0.0550,  0.0500,  0.0115,\n",
      "        -0.0737, -0.0117, -0.0107,  0.0346,  0.0014,  0.0820, -0.0875,  0.0003,\n",
      "        -0.0153,  0.0857,  0.0479, -0.0581,  0.0242,  0.0590, -0.0209, -0.0134,\n",
      "        -0.0557,  0.0532,  0.0163, -0.0290,  0.0130,  0.0851,  0.0500,  0.0955,\n",
      "        -0.0365,  0.0046,  0.0430,  0.0017,  0.0020, -0.0009, -0.0818, -0.0840,\n",
      "        -0.0291,  0.0027,  0.0349, -0.0195,  0.0528,  0.0059, -0.0053,  0.0694,\n",
      "         0.0836,  0.0399,  0.0045,  0.0394,  0.0843,  0.0423, -0.0705,  0.0453,\n",
      "         0.0789, -0.0915, -0.0567, -0.0062, -0.0537,  0.0252,  0.0035, -0.0054,\n",
      "        -0.0173,  0.0801,  0.0172, -0.0598], device='cuda:0',\n",
      "       requires_grad=True)), ('dnn_layers.0.weight', Parameter containing:\n",
      "tensor([[ 0.0635, -0.0138, -0.0038,  ...,  0.0391,  0.0858, -0.0583],\n",
      "        [ 0.0169, -0.0269,  0.0662,  ..., -0.0592, -0.0974,  0.0622],\n",
      "        [ 0.0275, -0.0023,  0.0091,  ...,  0.0784, -0.0309,  0.0612],\n",
      "        ...,\n",
      "        [-0.0458, -0.0221, -0.0180,  ..., -0.0567,  0.0017, -0.0481],\n",
      "        [ 0.0088, -0.0275,  0.0275,  ..., -0.0181, -0.0368,  0.0695],\n",
      "        [ 0.0438, -0.0167, -0.0489,  ...,  0.1253,  0.0860, -0.0006]],\n",
      "       device='cuda:0', requires_grad=True)), ('dnn_layers.0.bias', Parameter containing:\n",
      "tensor([ 0.0716,  0.0123,  0.0702,  0.0341,  0.1146,  0.1177,  0.0092,  0.1720,\n",
      "         0.0426,  0.0647,  0.0134, -0.0072,  0.1322,  0.0950,  0.0319, -0.0009,\n",
      "         0.0868,  0.0627,  0.0757,  0.1090,  0.0549,  0.0589,  0.0517,  0.0858,\n",
      "         0.0257,  0.1036,  0.1273,  0.1066,  0.0360,  0.0490,  0.0641,  0.1175,\n",
      "         0.0630,  0.1240,  0.0880,  0.0553,  0.1174,  0.0803,  0.1488,  0.0317,\n",
      "         0.0246,  0.0250,  0.0482,  0.0943,  0.0768,  0.1042,  0.1358,  0.1399,\n",
      "         0.0706,  0.0948,  0.0956,  0.0336,  0.0744,  0.0219,  0.0172,  0.0910,\n",
      "         0.0742,  0.0300,  0.0815,  0.0739,  0.0478,  0.0903,  0.0462,  0.0475,\n",
      "         0.1402,  0.0746,  0.1039,  0.0375,  0.0817, -0.0110,  0.0525,  0.0564,\n",
      "         0.0152,  0.1132,  0.0989,  0.0806,  0.0875,  0.1097,  0.0404,  0.1281,\n",
      "         0.0305, -0.0153,  0.0733,  0.1148,  0.1305,  0.1204,  0.1317,  0.0539,\n",
      "         0.0649,  0.0880,  0.0170,  0.0822,  0.0497,  0.0994,  0.0406,  0.0628,\n",
      "         0.0768,  0.0845,  0.0310,  0.1149], device='cuda:0',\n",
      "       requires_grad=True)), ('dnn_layers.1.weight', Parameter containing:\n",
      "tensor([[ 1.7068e-01,  1.9415e-01,  1.2658e-01, -1.3063e-01,  1.7064e-01,\n",
      "         -1.6787e-01, -1.6003e-01, -2.3589e-01, -7.2377e-02, -7.8750e-03,\n",
      "          9.0433e-02, -1.4943e-01, -2.0168e-01,  1.2635e-01, -1.6295e-01,\n",
      "          1.1766e-01,  1.7993e-01,  1.3120e-01,  1.4739e-01, -9.2848e-02,\n",
      "         -6.2393e-02,  2.0110e-01,  1.7488e-01, -1.4736e-01,  1.0497e-01,\n",
      "          1.4228e-01, -1.8133e-01, -1.3291e-01,  1.6580e-02,  2.3229e-01,\n",
      "          7.4918e-02, -6.7226e-02,  1.2578e-01, -3.1084e-02,  1.5107e-01,\n",
      "         -1.1579e-01, -3.9349e-02,  1.7371e-01, -1.2538e-01, -9.4629e-02,\n",
      "          1.0409e-03, -9.4665e-02, -3.7716e-02, -1.5299e-01,  1.0375e-01,\n",
      "          7.2166e-02, -1.7557e-01,  8.3713e-02, -1.7426e-01,  8.7350e-05,\n",
      "         -1.4760e-01, -1.8403e-01, -1.7330e-01, -1.1273e-01,  1.3372e-02,\n",
      "         -6.7062e-02,  1.7162e-01,  1.5074e-01, -1.4718e-01, -1.7946e-01,\n",
      "         -1.6132e-01,  1.7508e-02,  1.7028e-01,  2.0392e-01, -1.6006e-01,\n",
      "          2.9035e-02, -1.4306e-02, -7.4849e-02, -1.5917e-01,  1.5290e-01,\n",
      "         -1.3686e-01, -1.7989e-01,  1.6224e-01,  7.2301e-02, -1.3652e-01,\n",
      "          3.0773e-02, -2.4045e-01, -1.7843e-01,  1.3717e-01, -1.7386e-01,\n",
      "         -7.7369e-02,  2.1531e-01, -6.3364e-02, -2.3240e-01, -1.7298e-01,\n",
      "          1.3922e-01,  1.3292e-01,  1.2616e-01, -5.8179e-02, -9.3632e-02,\n",
      "          9.0299e-02, -1.5012e-01,  1.1059e-01, -1.9022e-01, -1.2257e-01,\n",
      "          8.6740e-02,  1.8432e-01, -1.8298e-01,  1.1332e-01,  9.8235e-02],\n",
      "        [-4.9523e-02, -2.0364e-01,  1.3156e-01, -1.9708e-01, -2.2012e-01,\n",
      "          9.8857e-02,  1.2226e-01,  4.0077e-02, -1.8262e-02, -1.1844e-01,\n",
      "          1.3443e-01,  1.7808e-01,  1.2149e-01, -2.2690e-01, -6.7149e-02,\n",
      "         -1.9141e-01,  1.0434e-01,  1.0233e-01, -5.2026e-02, -9.9042e-02,\n",
      "          1.8931e-01,  6.2473e-02,  1.3741e-01,  2.2113e-01, -1.1946e-01,\n",
      "          1.5081e-01,  1.7296e-01,  1.3214e-01, -1.3795e-01, -1.2698e-01,\n",
      "         -8.7573e-02,  1.0921e-01,  8.7606e-02, -2.1614e-01, -1.6358e-01,\n",
      "          9.6751e-02, -1.5734e-01,  4.6535e-02, -1.6848e-01,  2.2628e-01,\n",
      "          1.7531e-01,  5.3996e-02,  2.0179e-01,  1.5980e-01, -1.9471e-01,\n",
      "          9.8657e-02, -1.8034e-01, -1.3955e-01,  1.7561e-01,  1.2470e-01,\n",
      "         -1.3524e-01,  1.4769e-01, -7.0485e-02, -1.0873e-01,  1.4554e-01,\n",
      "          1.4823e-01, -9.2065e-02,  1.3866e-01, -1.5361e-01,  1.3983e-01,\n",
      "         -2.0780e-01,  1.7187e-01, -1.2220e-01, -1.3740e-01, -7.8714e-02,\n",
      "         -1.4905e-01, -1.5238e-01,  1.3715e-01, -5.9439e-02, -1.1388e-01,\n",
      "          2.2303e-01,  1.4594e-01,  1.0443e-01, -1.5350e-01,  9.0456e-02,\n",
      "         -2.4073e-01,  1.1535e-01, -2.6702e-01,  1.5146e-02,  6.2767e-02,\n",
      "          2.5666e-02,  1.3850e-01, -8.3306e-02, -5.4684e-02, -8.8358e-02,\n",
      "         -1.8996e-01, -1.9404e-01,  1.8193e-01, -1.8075e-01, -2.2153e-01,\n",
      "          1.2503e-01,  1.0588e-02, -1.1040e-01,  3.8110e-03, -2.6447e-02,\n",
      "         -2.0948e-01,  1.4687e-01, -1.8058e-01,  9.6429e-02,  1.5087e-01],\n",
      "        [ 1.3031e-01, -2.2685e-01, -6.5429e-02,  1.5233e-01,  1.5735e-01,\n",
      "          1.0391e-01, -8.2461e-02,  1.4377e-01, -1.9829e-01, -9.3105e-02,\n",
      "         -2.7376e-02,  1.2657e-01,  7.7057e-02,  1.8348e-01, -1.0372e-01,\n",
      "          1.7184e-01, -1.2230e-01,  9.4804e-02,  7.0386e-03, -1.1558e-01,\n",
      "         -9.6475e-02, -9.1037e-02,  1.1989e-02, -8.6892e-02, -1.4064e-01,\n",
      "          1.1615e-01,  1.4979e-01, -1.3431e-01,  2.8733e-01, -8.4808e-02,\n",
      "         -1.0070e-01, -1.1058e-01,  7.7538e-02,  9.4237e-02, -1.3145e-01,\n",
      "          1.7325e-01,  1.7907e-01, -1.8628e-01,  2.0202e-01, -1.6589e-01,\n",
      "         -1.6660e-01, -5.4872e-02, -1.4656e-01, -1.8384e-01, -1.5942e-01,\n",
      "         -7.0717e-02,  9.1578e-02,  1.1179e-01,  8.1248e-02,  2.1810e-01,\n",
      "         -1.3951e-01,  3.2280e-02,  2.0991e-01, -5.2192e-02, -1.1276e-01,\n",
      "          9.4626e-02, -1.4178e-01,  1.5476e-02,  5.6733e-02, -1.9752e-01,\n",
      "         -7.9080e-02,  1.6400e-01, -2.2360e-01,  9.5272e-03,  1.5870e-03,\n",
      "         -1.8171e-01,  1.0667e-01, -9.3563e-02,  8.1799e-02,  1.1256e-01,\n",
      "         -1.0089e-01,  1.1908e-01, -1.6995e-01, -4.8954e-02,  1.2702e-01,\n",
      "          1.6274e-01,  1.8875e-01, -5.5606e-02,  2.1532e-01,  1.9417e-01,\n",
      "         -2.5320e-01,  5.8259e-02, -2.0865e-01, -1.8946e-01,  1.2119e-01,\n",
      "         -2.0538e-01,  1.3915e-01, -9.9554e-02,  4.3564e-02, -1.6513e-01,\n",
      "          2.8837e-02,  6.9747e-02,  8.1986e-02,  2.3145e-01, -8.0000e-02,\n",
      "          1.4170e-01, -1.4220e-01, -1.2165e-02, -1.5234e-01,  4.5774e-02],\n",
      "        [-1.2300e-01, -1.1165e-01,  2.0868e-01, -1.6089e-01,  1.4192e-01,\n",
      "         -2.5463e-01,  1.0550e-01,  1.9564e-01,  2.1693e-01, -1.8098e-02,\n",
      "         -2.3459e-01,  3.4457e-02, -2.2430e-01, -7.6794e-02,  8.1662e-02,\n",
      "          9.6769e-02,  9.1946e-02,  1.1325e-01, -2.3023e-01, -1.2861e-01,\n",
      "         -1.5001e-02, -1.8516e-01,  6.8349e-02, -1.3764e-01,  2.0249e-01,\n",
      "         -1.4312e-02,  6.0419e-02,  4.9880e-02, -5.6596e-02, -4.2981e-02,\n",
      "          2.1512e-01, -1.7416e-01,  9.0924e-02, -1.4787e-01, -7.0006e-02,\n",
      "         -1.7849e-01,  8.2107e-02,  1.6814e-01, -1.6636e-01, -1.2999e-01,\n",
      "         -2.2126e-02,  7.1205e-02,  5.1810e-02, -2.4273e-01, -6.4719e-02,\n",
      "          2.4529e-01,  6.3275e-02,  9.2192e-03, -1.6406e-01,  2.0011e-01,\n",
      "          1.4413e-01, -2.2090e-01, -2.2148e-01,  1.7712e-01,  1.8319e-01,\n",
      "          1.0331e-01,  2.2922e-01,  1.1026e-01, -1.5584e-01, -9.6264e-02,\n",
      "          1.4749e-01, -1.4053e-02,  1.5521e-01, -1.5037e-01, -1.6351e-01,\n",
      "         -9.5284e-02, -1.0684e-01, -1.2442e-01, -1.9023e-01,  6.5675e-02,\n",
      "         -5.7326e-02, -7.1548e-02,  1.8937e-02,  2.2858e-01,  1.8039e-01,\n",
      "          1.9321e-01,  1.5186e-01, -7.4795e-02, -6.2862e-02,  1.2497e-01,\n",
      "          1.3434e-01,  6.7614e-02, -1.7123e-01,  4.9728e-03, -1.5569e-01,\n",
      "          3.5015e-02, -1.1726e-01,  1.2233e-01, -1.2982e-01,  7.3743e-02,\n",
      "          5.8018e-03,  1.5771e-01, -1.4452e-01,  7.0798e-02,  1.3238e-01,\n",
      "         -1.3304e-01,  4.0049e-02,  2.3857e-01, -7.4281e-02,  1.9847e-01],\n",
      "        [ 1.2798e-01,  3.1061e-03,  1.7879e-01, -2.3151e-01, -1.7949e-01,\n",
      "         -1.2969e-01, -1.2615e-01,  8.7239e-03,  1.4016e-01, -2.0883e-01,\n",
      "         -1.4430e-01, -1.1967e-01,  1.5802e-01,  6.1304e-02, -1.4399e-01,\n",
      "         -6.6712e-02,  1.4438e-01,  9.6091e-02,  1.7026e-01,  8.4515e-02,\n",
      "         -1.3846e-01,  1.0550e-01, -1.3237e-01,  1.2889e-01, -1.7806e-01,\n",
      "         -1.4276e-01, -1.7198e-01,  4.1103e-02, -1.3880e-01,  1.4403e-01,\n",
      "          1.3952e-01,  1.1669e-01, -1.9672e-01,  1.4526e-01, -2.0732e-01,\n",
      "          1.0234e-01,  9.1652e-02,  2.5128e-02,  8.6465e-02, -1.7347e-01,\n",
      "          1.7829e-01,  1.8391e-01,  1.7523e-01, -6.1418e-02,  1.2897e-01,\n",
      "         -1.3386e-01,  1.0151e-01,  9.4135e-02,  1.2072e-01, -1.8695e-01,\n",
      "         -7.8871e-02, -1.0913e-01, -1.5041e-01,  5.1093e-02, -6.7863e-02,\n",
      "          1.1993e-01, -1.8398e-01,  1.1170e-01, -1.2582e-01, -3.6292e-03,\n",
      "         -8.6397e-02,  1.2878e-01, -1.7303e-01,  1.1007e-01,  9.7983e-02,\n",
      "          1.7897e-01,  1.6472e-01,  1.6594e-01,  6.0297e-02, -1.2304e-01,\n",
      "         -1.7397e-01,  1.6623e-01, -1.8545e-01,  1.2864e-01, -1.3228e-01,\n",
      "         -1.7766e-01,  5.3742e-02,  5.2702e-02,  5.0993e-02,  9.4088e-02,\n",
      "          1.4159e-01, -9.0015e-02,  1.2410e-01,  1.2472e-01,  6.9492e-02,\n",
      "         -7.3738e-02,  9.8995e-02,  6.1400e-02, -1.5168e-01, -2.2574e-01,\n",
      "         -1.5882e-01,  1.2886e-01,  1.4145e-01, -8.4279e-02, -1.4806e-01,\n",
      "          1.6144e-01, -1.7653e-01, -1.9982e-01, -1.9492e-01,  1.7125e-01],\n",
      "        [-5.2664e-02, -1.2517e-01,  4.9233e-02, -4.4324e-02,  1.7940e-02,\n",
      "          7.8970e-02, -5.8679e-02,  1.9314e-01,  4.4681e-02,  9.2754e-02,\n",
      "          1.1190e-01, -3.8916e-02,  1.4443e-01, -2.8856e-02, -2.0654e-02,\n",
      "         -8.3792e-02,  1.4252e-01, -2.4312e-01, -1.8273e-01,  1.3933e-01,\n",
      "          1.9259e-01,  2.6369e-02, -5.6956e-02, -3.1005e-02,  4.8111e-03,\n",
      "          5.1534e-02,  1.3006e-01,  2.5836e-01,  4.2606e-03,  5.1573e-02,\n",
      "         -4.1226e-02,  2.0949e-01, -1.8459e-01,  6.5248e-02, -7.9359e-02,\n",
      "          7.9764e-02,  1.3424e-01,  3.2724e-02,  1.9297e-01, -1.4412e-01,\n",
      "          1.6629e-01, -1.6494e-01,  1.0043e-01,  1.5658e-01,  6.9291e-02,\n",
      "         -9.2241e-02,  1.7380e-01,  1.1325e-01, -8.9522e-02,  6.0997e-03,\n",
      "          1.1853e-01, -1.8357e-01,  1.8400e-01,  1.0293e-01,  9.7480e-03,\n",
      "          1.4511e-01,  1.1936e-01, -1.9366e-01,  1.7691e-01,  7.0474e-02,\n",
      "          9.0074e-02, -1.2885e-01, -1.3189e-01, -5.9107e-02,  1.6719e-01,\n",
      "          3.2164e-02, -6.6881e-02, -1.2185e-01, -1.0491e-01, -1.4714e-01,\n",
      "         -9.1506e-02,  1.5643e-01, -6.7732e-02, -3.7669e-03, -1.1576e-01,\n",
      "         -8.2294e-02, -1.7867e-01,  1.6178e-01, -1.5486e-01, -1.6323e-01,\n",
      "         -1.4425e-01,  1.1547e-01,  8.2708e-02, -7.2605e-03, -1.5051e-01,\n",
      "          8.9126e-02,  2.1433e-02, -1.8577e-01,  5.2491e-03,  2.5433e-01,\n",
      "         -1.6607e-01, -6.3656e-02, -1.6589e-01, -2.2550e-03,  2.3065e-01,\n",
      "         -8.9728e-02,  2.4413e-01,  1.4415e-01,  3.1728e-02,  1.3933e-02],\n",
      "        [ 5.4008e-02,  1.0686e-01, -1.6765e-01,  1.3552e-01,  3.3781e-02,\n",
      "          8.2201e-02, -5.6078e-02,  7.6189e-03, -1.2076e-01,  1.7325e-01,\n",
      "          4.1852e-02, -4.7447e-02,  9.0568e-02,  5.7373e-02,  1.5979e-01,\n",
      "         -5.4021e-02, -1.1622e-01,  4.9033e-03,  6.1330e-02,  6.9618e-02,\n",
      "          9.5380e-03,  3.5435e-02,  1.1770e-01,  1.3150e-01, -1.4947e-02,\n",
      "          1.0411e-01,  1.5077e-02, -8.7213e-02, -3.0814e-02, -1.9722e-02,\n",
      "         -3.8703e-03, -5.1800e-02,  6.1778e-02,  1.0881e-01,  1.5656e-01,\n",
      "         -1.0211e-01,  8.4070e-02,  3.4879e-02, -1.3358e-01,  1.7292e-01,\n",
      "         -4.6848e-02, -1.9679e-01,  4.4966e-02,  1.0611e-01,  5.6434e-03,\n",
      "          1.0791e-01,  2.4949e-03,  5.4870e-02,  4.0343e-02,  2.4547e-02,\n",
      "         -4.1262e-02,  5.8053e-02,  7.5746e-02,  1.3344e-02, -1.0453e-01,\n",
      "         -1.7373e-01,  3.6167e-02,  4.9990e-02,  6.3889e-02,  1.9419e-01,\n",
      "          1.7596e-01,  1.0798e-01,  1.4430e-01,  1.4332e-01,  1.0976e-01,\n",
      "         -1.1890e-01,  1.3191e-02, -1.3299e-01,  1.0453e-01,  4.2056e-03,\n",
      "          1.7576e-01, -1.8934e-01,  1.0358e-01, -1.7053e-01,  4.3405e-02,\n",
      "          9.5051e-02,  3.6052e-02,  1.4341e-01,  1.2814e-02,  5.6479e-02,\n",
      "         -1.6253e-01, -8.4615e-02, -8.7968e-02,  1.6349e-01,  9.1769e-02,\n",
      "          1.3446e-01,  4.9002e-02,  8.1430e-02,  8.9827e-03,  5.1066e-02,\n",
      "          4.0609e-02,  5.6744e-03,  1.2755e-01,  2.0545e-02, -2.9378e-02,\n",
      "          5.3122e-03,  2.2149e-03, -3.7235e-02,  1.2684e-01,  2.3422e-04],\n",
      "        [ 1.5984e-01,  1.2817e-01,  2.0062e-02,  8.7997e-02, -1.3414e-01,\n",
      "         -1.8762e-01,  1.8298e-01, -1.9985e-01,  1.5869e-01,  1.9730e-01,\n",
      "         -6.3465e-02,  1.1275e-01, -7.4290e-02, -1.5376e-01,  1.4087e-01,\n",
      "          9.3614e-02, -1.2173e-01, -1.8056e-01, -8.8350e-02,  1.5048e-01,\n",
      "         -1.7074e-01, -1.7041e-01, -1.5798e-01, -1.1483e-01,  1.1428e-01,\n",
      "         -2.1152e-01, -1.1444e-01, -1.5314e-01, -7.9918e-02, -9.1453e-02,\n",
      "          1.1469e-01, -6.3239e-02,  7.2205e-02, -1.5565e-01,  1.0683e-01,\n",
      "          1.0362e-01, -1.7392e-01,  1.5764e-01,  3.3393e-02,  1.6249e-01,\n",
      "         -1.6126e-01,  4.8812e-02, -1.1824e-01,  1.3650e-01, -1.6729e-01,\n",
      "         -2.0182e-01,  2.4612e-02,  6.7267e-02,  2.0442e-01, -1.7352e-01,\n",
      "          1.2089e-02,  1.0914e-01, -1.5267e-01,  1.7173e-01, -4.5100e-02,\n",
      "         -1.3910e-01, -1.7486e-01,  6.8863e-02, -4.4082e-02, -1.8105e-01,\n",
      "         -1.8171e-01, -9.9779e-02,  9.1739e-02, -1.8842e-01,  1.4844e-01,\n",
      "          1.5242e-01,  1.6423e-01,  1.0069e-01,  1.9551e-01,  5.4340e-02,\n",
      "          1.1007e-01,  8.1254e-02,  1.4533e-01,  1.5622e-01,  1.6861e-01,\n",
      "         -2.1133e-01,  1.5308e-01, -1.4806e-01, -1.0181e-01,  1.0134e-01,\n",
      "          1.9385e-01,  1.4209e-01,  1.4300e-01,  1.3202e-01, -1.8317e-01,\n",
      "         -1.5156e-01,  4.7189e-02,  1.3960e-01,  1.9468e-01,  8.7891e-02,\n",
      "          1.4744e-01,  5.8251e-02, -1.9062e-01,  1.5245e-01,  1.8628e-01,\n",
      "          1.2012e-01, -1.8827e-01,  1.1923e-01, -1.1476e-01, -1.5414e-01],\n",
      "        [-1.6200e-01,  7.7541e-02,  1.1173e-01,  1.4815e-01,  1.4018e-01,\n",
      "          1.4171e-01,  1.5767e-01, -2.5079e-01, -1.8534e-01, -6.4630e-02,\n",
      "          1.3295e-01,  1.4713e-01, -1.8517e-01,  1.5869e-01, -9.6076e-02,\n",
      "          1.4838e-01, -4.9269e-03,  7.7847e-02,  1.5486e-01, -1.1261e-01,\n",
      "          1.4422e-01,  1.5554e-01, -1.0610e-01, -1.0431e-01,  1.1513e-01,\n",
      "         -2.0797e-01,  1.2739e-01,  1.0602e-01,  1.7562e-01,  1.6913e-01,\n",
      "         -4.2891e-02,  2.8901e-02,  9.6862e-02,  6.2572e-02, -6.9774e-02,\n",
      "          6.5978e-02, -2.5735e-01,  7.6655e-02,  1.4420e-01, -5.5104e-02,\n",
      "         -1.4154e-01,  1.8014e-01, -1.9753e-01,  4.5944e-02,  1.4141e-01,\n",
      "          4.3502e-02, -1.4079e-01, -2.2501e-01, -9.5752e-02, -5.4598e-02,\n",
      "          1.6975e-01,  1.6008e-01,  1.7507e-01, -1.0216e-01,  1.3569e-01,\n",
      "          3.8846e-02, -1.3443e-02, -1.7588e-01,  2.2164e-01, -1.9255e-01,\n",
      "         -1.3694e-01, -1.6000e-01,  1.4537e-01, -1.7162e-01, -1.0459e-01,\n",
      "          1.2005e-01, -1.0985e-01,  1.7739e-01, -3.5507e-02,  1.4622e-01,\n",
      "         -1.3412e-01,  1.1136e-01, -1.0820e-01,  1.7876e-01, -1.2241e-01,\n",
      "         -2.3084e-01, -1.5342e-01, -1.4832e-01,  1.5102e-01, -1.0769e-01,\n",
      "          1.6522e-01,  1.6845e-04,  1.1492e-01, -1.1337e-01,  1.5047e-01,\n",
      "         -1.4343e-01, -1.6902e-01, -1.4440e-01,  1.6446e-01, -1.3184e-01,\n",
      "          9.6413e-02, -1.7135e-01, -8.1491e-02, -1.7413e-01,  1.0482e-01,\n",
      "          1.2717e-01,  1.2009e-01,  1.2245e-02,  1.2953e-01, -1.7506e-01]],\n",
      "       device='cuda:0', requires_grad=True)), ('dnn_layers.1.bias', Parameter containing:\n",
      "tensor([-0.0573,  0.0218,  0.0191,  0.0062,  0.1017,  0.1510,  0.0418, -0.0306,\n",
      "         0.0473], device='cuda:0', requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#功能：加载保存到path中的各层参数到神经网络\n",
    "from new_nfm_network import NFM\n",
    "path='data/xiaoqiu_gene_5000/model/new_model_param_gene_5000.pkl'\n",
    "nfm = NFM(nfm_config).cuda()\n",
    "net=nfm\n",
    "print(net)\n",
    "#net = nn.DataParallel(net)\n",
    "#net = net.to(device)\n",
    "net.load_state_dict(torch.load(path),strict=False)\n",
    "net.cuda()\n",
    "\n",
    "print(net)\n",
    "\n",
    "params = list(net.named_parameters())\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53f8db90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_model1.weight : torch.Size([1000, 5510])\n",
      "linear_model1.bias : torch.Size([1000])\n",
      "BN_linear1.weight : torch.Size([1000])\n",
      "BN_linear1.bias : torch.Size([1000])\n",
      "linear_model2.weight : torch.Size([100, 1000])\n",
      "linear_model2.bias : torch.Size([100])\n",
      "BN_linear2.weight : torch.Size([100])\n",
      "BN_linear2.bias : torch.Size([100])\n",
      "embedding_layers.weight : torch.Size([1001, 100])\n",
      "BN_bi.weight : torch.Size([100])\n",
      "BN_bi.bias : torch.Size([100])\n",
      "dnn_layers.0.weight : torch.Size([100, 200])\n",
      "dnn_layers.0.bias : torch.Size([100])\n",
      "dnn_layers.1.weight : torch.Size([9, 100])\n",
      "dnn_layers.1.bias : torch.Size([9])\n",
      "weight: {'linear_model1.weight': Parameter containing:\n",
      "tensor([[ 4.1792e-03, -2.0012e-02,  9.8555e-03,  ...,  6.3034e-03,\n",
      "          1.9084e-03, -5.9339e-03],\n",
      "        [-7.2668e-03, -2.0609e-02,  5.7518e-04,  ...,  2.1113e-03,\n",
      "         -1.7399e-03, -1.0733e-03],\n",
      "        [ 1.1308e-02, -2.6177e-02,  3.5501e-03,  ...,  2.2635e-02,\n",
      "          1.7044e-02, -4.3090e-03],\n",
      "        ...,\n",
      "        [ 7.1105e-03, -5.6998e-03, -4.4347e-03,  ..., -5.0106e-03,\n",
      "          1.6435e-04, -8.8356e-03],\n",
      "        [ 1.2284e-02,  2.1440e-02,  2.2601e-04,  ..., -7.8331e-05,\n",
      "         -3.2474e-03,  5.5273e-04],\n",
      "        [ 7.9007e-03, -1.2099e-02,  5.4705e-04,  ...,  3.4282e-03,\n",
      "         -5.9106e-03, -3.3098e-03]], device='cuda:0', requires_grad=True), 'linear_model1.bias': Parameter containing:\n",
      "tensor([ 4.3848e-08, -5.6438e-08, -7.1823e-09, -5.6667e-08, -1.1628e-08,\n",
      "         1.5024e-08,  3.7333e-09, -1.1327e-07, -5.3654e-08,  4.1108e-09,\n",
      "        -5.2567e-09, -1.4763e-07, -1.8665e-07, -3.4424e-08,  1.8804e-07,\n",
      "        -6.1235e-08,  4.1346e-08,  5.6652e-08,  5.2685e-08, -3.3446e-08,\n",
      "         8.8155e-09, -8.9571e-09, -8.9714e-11,  9.9724e-08,  1.3599e-08,\n",
      "        -8.9355e-09,  8.0802e-07,  9.5665e-10,  7.2274e-09, -5.2751e-09,\n",
      "        -2.3178e-07,  6.8839e-08,  2.4491e-08, -1.3064e-08,  2.8237e-09,\n",
      "        -8.2429e-08, -4.9668e-08, -3.2989e-08, -7.0069e-08,  2.2019e-08,\n",
      "        -1.5422e-09, -5.6434e-09,  9.6392e-09,  7.8476e-10,  6.8417e-09,\n",
      "        -2.6274e-08,  2.8393e-09,  1.5617e-08,  1.0064e-09, -5.4702e-10,\n",
      "        -1.6377e-08,  2.2441e-07, -4.5125e-08, -2.8669e-07, -9.5212e-08,\n",
      "        -1.6218e-07,  5.3668e-08, -1.2152e-08,  2.5797e-08,  4.6876e-08,\n",
      "        -2.8092e-09,  1.6416e-08, -8.4203e-08,  1.2175e-07,  8.4333e-10,\n",
      "        -5.2825e-08,  1.0406e-08, -1.5621e-07,  4.1611e-09, -3.0774e-08,\n",
      "        -4.0846e-08, -8.0099e-09,  6.0539e-09, -1.1639e-07,  5.0799e-08,\n",
      "        -7.1070e-09,  3.8663e-08, -9.5472e-08, -1.7492e-08, -1.3384e-07,\n",
      "         2.5999e-08,  5.2688e-08,  2.8970e-08, -1.6713e-08, -6.9491e-08,\n",
      "        -2.3207e-09, -1.2932e-08, -1.0236e-07,  2.3772e-09, -3.8188e-07,\n",
      "        -3.1920e-08, -1.1776e-08,  2.3366e-08, -4.5977e-09,  3.7073e-08,\n",
      "         1.1999e-08,  1.8215e-09, -4.5816e-08, -7.6533e-08, -1.5324e-07,\n",
      "         7.6647e-08,  2.5065e-08,  1.0415e-08, -3.5057e-09,  4.0556e-08,\n",
      "        -1.8088e-07,  2.6957e-08,  1.0095e-08, -7.0761e-09, -2.5410e-09,\n",
      "        -7.9138e-09,  1.4609e-08,  5.5010e-09, -4.7357e-08, -1.9609e-08,\n",
      "         1.5281e-08, -6.8227e-08, -9.6933e-09, -2.5343e-08,  9.3852e-09,\n",
      "         3.1578e-09,  2.3167e-08, -8.2904e-09, -1.5079e-08,  8.8243e-09,\n",
      "        -2.7071e-08,  1.1881e-07, -4.1448e-09, -1.0878e-08, -5.1559e-08,\n",
      "        -4.1941e-09, -8.0469e-08,  3.1930e-07, -1.6576e-08, -1.2465e-08,\n",
      "        -7.9815e-08, -7.0007e-08,  4.2236e-08, -2.4982e-08, -3.4990e-08,\n",
      "         5.4609e-08, -1.1004e-07,  9.1949e-09,  4.6515e-08, -7.1756e-08,\n",
      "         3.6593e-08,  2.9814e-08, -2.4276e-07,  6.8088e-08,  3.6424e-08,\n",
      "        -4.0718e-08, -2.3940e-08, -2.0879e-09, -1.4154e-09,  7.5952e-08,\n",
      "        -3.1355e-07,  2.2312e-08, -5.8224e-10,  4.6214e-08,  8.6226e-08,\n",
      "        -7.8474e-08, -5.2109e-09, -7.5711e-08, -1.1759e-08,  2.1974e-08,\n",
      "        -8.7680e-08,  1.9475e-09,  1.3056e-08,  1.7633e-08, -9.8920e-09,\n",
      "        -9.7531e-08, -1.1134e-08, -8.7057e-09, -1.0213e-08, -1.3883e-07,\n",
      "         3.2434e-08,  4.0531e-08, -2.0967e-08,  4.9239e-07, -1.7283e-08,\n",
      "        -2.5655e-08, -5.3511e-09,  8.3170e-07, -3.9982e-07,  5.9342e-08,\n",
      "         2.6122e-08, -6.3647e-08, -9.1577e-09,  3.6524e-09,  8.4885e-08,\n",
      "         1.0474e-08, -6.0808e-09,  3.4799e-08,  3.3230e-08,  5.6065e-09,\n",
      "         8.0714e-10,  3.0840e-08,  3.6149e-09,  1.1900e-07, -1.0032e-08,\n",
      "         5.6109e-09,  1.3375e-08, -2.3783e-07,  3.3928e-08,  3.6636e-07,\n",
      "         4.3789e-08, -1.5420e-08, -2.6253e-08,  3.5313e-08, -5.2050e-10,\n",
      "         1.6085e-08,  1.5857e-08,  4.7532e-08, -2.4274e-08, -1.4500e-08,\n",
      "        -1.4065e-08,  2.3768e-07,  6.5526e-08,  2.7241e-08,  1.4260e-07,\n",
      "        -7.2356e-08,  7.3187e-08, -1.7784e-07,  6.7165e-08, -1.0220e-07,\n",
      "         6.3184e-08, -2.2770e-08,  4.4085e-08,  1.0539e-08,  1.3577e-08,\n",
      "        -8.2711e-08,  7.4753e-09,  2.9657e-08, -5.4763e-08, -1.9007e-08,\n",
      "        -2.2381e-08,  6.5460e-08, -1.2421e-08,  3.8140e-08,  7.1151e-09,\n",
      "         7.7659e-09,  3.5373e-08,  1.1510e-07,  1.6055e-07,  4.5701e-08,\n",
      "         2.0588e-08,  6.1573e-08,  4.8004e-09, -1.8420e-08,  1.1875e-07,\n",
      "         1.1536e-09, -6.0474e-08,  1.8825e-08, -7.5862e-09,  8.6955e-08,\n",
      "         1.6367e-09,  1.6726e-08,  1.7074e-10,  8.5121e-09, -2.5001e-08,\n",
      "         5.2940e-08,  1.1620e-08, -2.9240e-08,  1.8761e-07, -6.9527e-08,\n",
      "         3.7632e-09,  1.1890e-08,  5.3613e-09, -3.6660e-08,  1.1829e-08,\n",
      "        -1.1031e-08, -1.2154e-08,  3.5805e-08, -3.4901e-09, -4.7359e-08,\n",
      "        -4.4148e-09, -2.8832e-08,  3.1346e-08,  7.7741e-08,  1.5455e-08,\n",
      "        -4.7774e-09, -1.5290e-09,  4.2392e-08, -2.8047e-09, -5.2224e-08,\n",
      "         6.4967e-08, -1.1730e-07,  1.0643e-07, -5.0200e-09,  1.0584e-08,\n",
      "        -4.7234e-09,  6.0134e-08,  6.6112e-08, -3.5271e-09, -1.0430e-08,\n",
      "         2.7920e-08,  1.2200e-11, -8.0966e-08, -5.0647e-08,  1.3021e-07,\n",
      "         6.0164e-08, -3.6944e-09,  3.9407e-08,  1.0934e-07,  9.3606e-08,\n",
      "        -3.4697e-08,  8.1255e-08, -3.5816e-08, -5.5525e-09, -1.4458e-07,\n",
      "        -2.6447e-09,  1.3025e-08,  1.6487e-08,  6.3865e-09, -1.2400e-07,\n",
      "         2.5968e-08, -4.1799e-07, -1.6383e-08, -1.4823e-08,  1.2452e-08,\n",
      "        -6.8563e-08,  5.1831e-09, -3.9770e-09, -1.0328e-07, -3.8199e-08,\n",
      "        -2.7047e-07, -2.9900e-08,  2.4856e-10, -6.9847e-09, -2.3442e-07,\n",
      "        -2.6422e-08,  1.9287e-08,  7.0323e-08,  6.1005e-09,  1.1958e-07,\n",
      "         9.3383e-10, -1.3144e-08, -1.8183e-07, -1.1525e-07,  1.9899e-08,\n",
      "        -4.5661e-08, -2.1414e-08, -2.8827e-08,  9.2029e-09,  5.2133e-10,\n",
      "        -6.9983e-09, -1.6661e-07, -1.6950e-07,  4.8780e-08, -1.8393e-08,\n",
      "         2.0929e-08,  1.1193e-07,  1.4744e-08,  3.1523e-09,  1.3417e-07,\n",
      "        -1.3244e-07, -1.7613e-08,  6.1297e-09,  3.8508e-08, -6.6219e-09,\n",
      "         1.9950e-07, -4.0301e-08,  3.4784e-07, -6.4159e-08,  5.4932e-08,\n",
      "         2.7328e-08,  2.0307e-08, -2.1830e-07, -1.0666e-07, -3.4823e-08,\n",
      "         9.5824e-09,  1.7207e-08, -4.5189e-08, -7.2735e-08,  2.1959e-08,\n",
      "        -1.9296e-08,  7.2895e-07, -1.7170e-07, -1.7995e-07, -5.3390e-08,\n",
      "         7.6367e-09, -6.6935e-09, -2.4919e-08, -4.0228e-07,  1.1919e-07,\n",
      "         6.2776e-09,  3.1937e-08, -4.3704e-09,  1.8790e-08, -3.3383e-08,\n",
      "         5.0413e-09, -3.5472e-08, -6.9820e-09, -4.2153e-09,  1.8239e-08,\n",
      "        -1.6536e-07, -1.2079e-07,  3.2334e-08, -2.3956e-09, -1.1274e-08,\n",
      "        -3.0565e-08,  3.5711e-08, -9.1160e-08,  1.5732e-07, -1.3680e-08,\n",
      "         1.1043e-08,  4.8098e-08,  1.5351e-07, -2.2367e-08,  1.6401e-07,\n",
      "         7.1740e-08,  1.7840e-08, -4.4361e-08, -1.8169e-08,  1.7619e-07,\n",
      "         3.6370e-08, -8.8790e-08, -3.3453e-08,  1.3403e-07,  1.8453e-08,\n",
      "         1.9120e-08,  3.1825e-07, -1.2370e-07,  2.1699e-08,  9.6957e-09,\n",
      "         2.4878e-07,  1.8962e-07, -2.2586e-07,  6.5819e-08, -1.5679e-08,\n",
      "         6.6098e-08,  6.6466e-08, -2.3626e-09,  3.1259e-08, -4.4766e-08,\n",
      "        -9.8644e-08, -1.4652e-07,  6.3022e-08, -8.3543e-09, -8.3388e-08,\n",
      "        -6.3064e-08,  1.8828e-08, -4.0859e-08,  1.4905e-08,  1.0963e-07,\n",
      "         2.7757e-08, -1.5050e-09,  6.3294e-08, -6.2649e-07,  3.4652e-08,\n",
      "        -4.2055e-08, -1.1199e-07,  3.9475e-09, -4.1580e-07, -4.1857e-10,\n",
      "        -2.6396e-08, -2.4030e-08,  4.2327e-08, -3.8333e-08, -3.6306e-08,\n",
      "         7.0586e-11, -1.2336e-07, -1.5098e-08,  3.9138e-08,  1.3239e-08,\n",
      "         2.3589e-09, -3.1072e-10,  1.8351e-08,  1.5695e-08, -5.3062e-09,\n",
      "         4.8362e-07, -7.8951e-08,  6.0497e-08,  1.6145e-08,  2.0088e-08,\n",
      "        -4.5495e-08,  4.9278e-09, -4.5295e-09,  1.5419e-08, -3.1287e-08,\n",
      "        -3.0625e-08, -7.7487e-08, -1.3934e-07, -1.9370e-08,  1.3912e-08,\n",
      "        -4.9092e-09, -1.7512e-08,  1.5532e-07, -4.9430e-08, -1.5960e-08,\n",
      "        -1.8479e-07,  1.1857e-08, -3.9433e-07,  2.2102e-07, -9.1547e-08,\n",
      "        -2.8302e-08, -2.5625e-08,  1.3873e-08, -3.7231e-08, -7.3708e-09,\n",
      "        -1.7066e-08, -8.4814e-09,  4.7089e-08, -4.9822e-10, -5.8560e-09,\n",
      "        -6.8206e-09,  8.0126e-08,  3.4989e-09, -5.9621e-09,  1.2439e-07,\n",
      "         2.4674e-08,  1.5379e-08,  1.3001e-08, -6.5638e-09, -4.2763e-08,\n",
      "        -6.6330e-08,  2.0768e-09,  8.0173e-08, -3.4733e-09,  1.3311e-08,\n",
      "         8.9692e-09, -3.2362e-08, -1.8261e-07,  2.1320e-07,  2.8041e-08,\n",
      "        -4.8666e-07, -5.4536e-09, -8.2288e-08,  7.9717e-07,  4.4077e-09,\n",
      "        -3.9470e-08, -3.7608e-08, -4.6779e-08, -1.4568e-07, -6.3848e-08,\n",
      "        -5.8709e-08,  4.7185e-09,  2.3224e-08, -1.1700e-08, -1.7418e-08,\n",
      "         1.1432e-08, -2.1776e-08, -3.1710e-08,  2.4384e-07,  7.8425e-07,\n",
      "        -3.3014e-09,  2.5163e-07, -7.4906e-08, -1.2220e-07,  3.8256e-08,\n",
      "         8.6612e-08, -5.9003e-09, -2.7484e-08,  2.6428e-08, -1.0304e-09,\n",
      "         3.5484e-08,  3.8064e-08, -4.3302e-09,  5.6011e-08,  5.5060e-08,\n",
      "         2.5044e-09, -2.5079e-07,  1.0937e-08, -2.8062e-09, -1.0315e-08,\n",
      "        -2.3099e-08,  3.7924e-09,  2.3000e-09,  1.1049e-08,  5.6415e-08,\n",
      "         1.2188e-08,  1.4939e-08,  4.4239e-08, -3.3514e-07,  6.4676e-08,\n",
      "        -1.5869e-09, -9.7802e-08, -1.1914e-07, -1.8699e-08,  1.0688e-08,\n",
      "         3.3059e-08, -5.6047e-08,  7.2818e-08, -5.2470e-09, -1.3686e-07,\n",
      "        -6.1227e-09,  6.1379e-08, -2.4692e-08,  4.4215e-09,  1.2759e-08,\n",
      "        -2.7610e-08, -3.7247e-08,  1.8861e-08,  2.8770e-08,  5.5965e-08,\n",
      "         5.2870e-08, -1.6733e-07,  6.2280e-07, -4.3912e-09,  1.2754e-08,\n",
      "        -2.6268e-08,  2.3690e-07, -6.9965e-08, -2.9375e-08,  8.7711e-09,\n",
      "        -1.0092e-08,  1.3506e-08,  1.1259e-08, -1.6421e-07,  6.2296e-09,\n",
      "        -9.8278e-08, -1.0947e-08, -1.7280e-08,  1.6960e-09,  6.0318e-09,\n",
      "         3.4524e-07, -7.7383e-09,  5.2962e-08,  3.8939e-09,  9.7390e-08,\n",
      "        -8.6637e-09, -2.6141e-08, -4.5104e-08, -3.4518e-09,  2.2179e-08,\n",
      "        -1.3965e-08,  1.9464e-08, -2.7216e-08,  5.1455e-08, -9.4902e-09,\n",
      "        -3.5508e-07, -9.2060e-09,  1.6083e-08, -1.6489e-08,  9.3177e-10,\n",
      "         2.5290e-08, -5.4153e-10, -3.6817e-08, -3.1707e-08, -1.5675e-08,\n",
      "        -2.1340e-08, -2.9777e-07,  1.9932e-08, -1.4936e-08, -8.0900e-09,\n",
      "         8.8038e-08, -6.6653e-07,  9.2276e-09,  5.5630e-08,  5.6259e-08,\n",
      "        -1.6725e-07, -2.5599e-07,  1.0456e-08, -1.9550e-08, -6.3192e-08,\n",
      "         8.5138e-08,  2.6346e-09, -2.0313e-08, -7.4833e-09,  2.2001e-08,\n",
      "        -2.1823e-09,  1.7879e-08,  1.0327e-07, -1.4527e-08,  2.7894e-08,\n",
      "         8.6927e-07, -6.9423e-08,  1.4302e-09,  7.6052e-08, -2.8007e-08,\n",
      "        -1.0717e-07,  1.0280e-08,  2.2790e-08,  2.2759e-08, -4.4018e-07,\n",
      "         1.1716e-08, -4.7766e-08,  1.9958e-08, -5.1608e-08,  1.8289e-08,\n",
      "         5.3998e-09, -2.2716e-08,  2.0398e-07,  3.1940e-09, -1.6257e-09,\n",
      "        -6.7838e-09,  1.2650e-08,  1.3637e-08,  4.9746e-08, -1.1027e-08,\n",
      "         1.7356e-09,  3.7668e-08, -3.0798e-08,  3.0039e-08, -3.1648e-07,\n",
      "         3.8608e-08, -1.8456e-07,  1.6248e-08, -9.1640e-08, -5.5022e-08,\n",
      "         4.8584e-08,  9.1610e-08, -3.2152e-08,  7.2586e-09, -1.8345e-08,\n",
      "         8.4026e-08, -2.3737e-08,  1.9941e-08, -2.7746e-08, -2.9205e-09,\n",
      "        -2.1644e-08, -7.1001e-08, -6.2629e-10,  1.0172e-07, -4.1646e-09,\n",
      "         6.0836e-09,  6.4650e-08, -1.2339e-08,  4.9909e-09,  4.4322e-09,\n",
      "        -5.7951e-08, -1.7666e-08, -4.6225e-07, -2.9032e-07,  1.0605e-08,\n",
      "        -2.7752e-08,  4.5021e-09,  4.0579e-08,  4.3699e-08,  2.5653e-08,\n",
      "        -8.0337e-08, -2.2464e-08,  3.3519e-08,  9.1292e-08, -5.6068e-08,\n",
      "        -3.2019e-08, -5.4400e-08,  2.9254e-08,  7.1995e-09, -2.3661e-07,\n",
      "        -1.1928e-07,  9.2759e-08,  4.3261e-09,  1.1086e-07,  1.8719e-08,\n",
      "         4.1283e-08, -5.3303e-08,  5.9693e-09,  7.6425e-08,  4.7878e-08,\n",
      "        -2.2883e-08, -1.5306e-08,  1.2193e-09, -6.1838e-08, -2.8462e-09,\n",
      "         8.6086e-08,  3.1820e-09, -1.7722e-07,  8.7448e-08, -2.9609e-08,\n",
      "        -3.4065e-08,  4.7640e-09, -1.3461e-08,  2.3862e-08,  5.9780e-08,\n",
      "         1.4918e-08,  3.3585e-08, -1.3013e-07,  1.0526e-08, -3.4152e-09,\n",
      "        -8.6813e-08,  2.1134e-08, -2.3063e-08, -1.5574e-09, -8.4834e-09,\n",
      "        -1.8568e-08, -1.1747e-08,  1.7147e-08, -6.1169e-08, -1.1034e-07,\n",
      "        -2.1928e-08,  7.5950e-09, -7.2011e-08, -1.6863e-07,  1.6359e-10,\n",
      "        -2.7398e-08, -9.1396e-08,  4.2316e-08,  6.0000e-09,  4.9157e-09,\n",
      "         2.0316e-08,  6.3701e-08, -1.9384e-08, -3.8638e-09,  7.0242e-08,\n",
      "         2.6866e-08,  1.0974e-07,  6.5897e-08, -2.9341e-10, -1.6017e-07,\n",
      "        -2.1927e-07,  2.7276e-08,  1.9122e-08,  6.4884e-10, -4.9727e-09,\n",
      "        -9.6755e-10, -1.2636e-09, -4.7014e-08, -3.9056e-08,  1.5554e-08,\n",
      "        -4.0094e-08, -2.6931e-08,  6.2502e-08, -3.3820e-08,  3.9723e-08,\n",
      "        -5.4243e-08, -1.3114e-08, -4.2549e-08, -3.5829e-08,  2.5603e-09,\n",
      "         8.9857e-08, -2.0489e-09, -1.4573e-08,  1.6787e-09, -2.5850e-09,\n",
      "        -1.5021e-08,  9.0563e-09, -4.2219e-08, -7.0780e-09, -1.8378e-07,\n",
      "        -2.8007e-08, -7.7823e-09,  1.2168e-09,  4.1414e-08, -1.6560e-09,\n",
      "        -3.0492e-09,  1.3620e-08,  4.5411e-08, -4.3885e-08, -1.4419e-08,\n",
      "        -1.3719e-08, -8.2341e-08, -4.0537e-08,  1.8596e-09, -3.9218e-08,\n",
      "         4.6068e-08, -1.5310e-08,  9.2534e-10,  1.1314e-07,  1.4023e-08,\n",
      "         3.4468e-08, -1.0966e-07, -8.6181e-10, -1.0756e-09,  1.3701e-09,\n",
      "         8.1982e-09, -6.3824e-09, -6.5542e-08,  3.7433e-09, -7.4975e-07,\n",
      "        -2.0400e-07,  1.8044e-08,  1.4238e-08, -4.5429e-08,  4.9946e-09,\n",
      "        -4.2269e-09, -1.4750e-08, -4.0041e-08, -1.7269e-08,  3.4119e-08,\n",
      "        -8.9796e-08,  3.9154e-08, -4.9913e-08, -3.3204e-08,  3.1766e-08,\n",
      "         3.5557e-08,  3.8765e-08,  4.5944e-09, -2.0717e-07, -1.0487e-08,\n",
      "         3.8613e-08,  2.1348e-08, -2.2519e-07, -3.5727e-08,  1.2532e-09,\n",
      "         1.8528e-08,  1.4903e-08, -4.1302e-08,  2.9804e-07,  4.9850e-08,\n",
      "        -2.0867e-08, -8.7905e-08,  3.9796e-08, -3.8879e-08, -1.9694e-07,\n",
      "         6.4360e-08,  1.4825e-09,  6.6492e-09, -3.7892e-08,  4.1171e-08,\n",
      "        -6.1278e-08,  6.3853e-10, -5.3935e-10, -5.1889e-08, -1.0714e-07,\n",
      "         1.7932e-08, -7.9999e-09, -2.2299e-08,  8.6500e-08,  3.2273e-07,\n",
      "        -1.1965e-07,  2.4421e-09, -3.2530e-08, -5.8613e-08,  7.3309e-08,\n",
      "         3.7829e-08,  2.1550e-08, -2.7032e-09,  3.2453e-08,  3.1900e-08,\n",
      "         6.7753e-09,  4.2803e-08, -1.5998e-08,  6.5015e-08, -1.9033e-09,\n",
      "         6.6158e-09, -2.0772e-07, -1.8231e-08,  2.8472e-08, -1.4803e-07,\n",
      "        -1.0320e-08,  5.3633e-08,  4.3029e-09,  3.7919e-08,  4.7181e-08,\n",
      "        -4.5123e-09, -5.1798e-09,  3.3633e-07,  2.3715e-08,  1.2023e-07,\n",
      "         1.9457e-08,  4.6336e-08, -3.8738e-08, -1.1823e-07,  1.3716e-08,\n",
      "        -2.4134e-07,  3.2492e-08,  1.0632e-07, -4.7861e-08, -1.6681e-08,\n",
      "         7.1590e-08,  3.2724e-08,  2.0337e-09,  3.1520e-08,  1.2524e-07,\n",
      "        -1.4892e-08, -7.1916e-09,  2.4776e-07,  3.9376e-08,  4.5618e-08,\n",
      "         1.6237e-08,  8.4436e-09, -4.3859e-08, -1.5252e-08,  2.9863e-08,\n",
      "         1.1440e-08, -3.6974e-09, -3.3102e-07,  1.8010e-09, -2.3797e-08,\n",
      "        -6.6152e-09, -9.0685e-09, -1.5749e-08, -1.4996e-09, -1.1549e-07,\n",
      "         4.2229e-08, -5.2440e-09,  7.8597e-11,  5.9592e-07,  1.3757e-08,\n",
      "         1.0472e-07, -1.5400e-08, -3.7680e-08, -5.0043e-08,  2.6323e-08,\n",
      "        -2.0412e-08, -1.2241e-07, -2.4579e-08,  7.9305e-09,  1.4331e-09,\n",
      "        -2.4277e-08,  3.6454e-08, -7.6628e-08, -3.5735e-08, -3.8415e-09,\n",
      "        -9.1778e-08,  1.8042e-09, -3.4761e-07,  1.4914e-09, -1.6463e-07],\n",
      "       device='cuda:0', requires_grad=True), 'BN_linear1.weight': Parameter containing:\n",
      "tensor([0.5484, 0.5635, 0.6283, 0.5578, 0.6057, 0.4312, 0.6330, 0.6293, 0.5765,\n",
      "        0.5145, 0.4625, 0.4951, 0.6397, 0.4830, 0.5712, 0.5220, 0.6054, 0.6845,\n",
      "        0.5459, 0.5930, 0.5189, 0.6260, 0.4731, 0.6050, 0.5683, 0.4353, 0.6988,\n",
      "        0.5989, 0.3907, 0.4530, 0.5683, 0.7123, 0.4899, 0.5516, 0.5974, 0.5896,\n",
      "        0.4587, 0.5357, 0.5956, 0.6244, 0.5504, 0.5142, 0.4630, 0.6033, 0.4601,\n",
      "        0.5597, 0.4355, 0.7016, 0.5510, 0.6522, 0.4034, 0.6028, 0.4629, 0.5029,\n",
      "        0.5306, 0.5874, 0.5149, 0.5387, 0.4611, 0.5957, 0.5503, 0.4018, 0.5758,\n",
      "        0.4799, 0.6028, 0.6533, 0.5022, 0.5068, 0.5491, 0.5863, 0.4344, 0.5710,\n",
      "        0.4406, 0.6254, 0.6160, 0.6738, 0.6256, 0.5687, 0.6242, 0.5490, 0.4796,\n",
      "        0.6059, 0.6902, 0.5144, 0.5099, 0.4223, 0.5361, 0.6026, 0.6278, 0.5712,\n",
      "        0.4509, 0.5350, 0.5995, 0.4270, 0.5356, 0.6167, 0.5118, 0.6464, 0.5790,\n",
      "        0.5644, 0.5904, 0.5594, 0.4239, 0.5763, 0.4958, 0.5930, 0.5264, 0.6539,\n",
      "        0.4169, 0.6018, 0.5045, 0.4775, 0.4665, 0.4708, 0.6150, 0.5445, 0.4600,\n",
      "        0.5236, 0.5752, 0.6155, 0.5674, 0.5282, 0.5213, 0.5812, 0.5820, 0.5670,\n",
      "        0.6365, 0.4364, 0.7557, 0.6960, 0.4950, 0.5113, 0.6001, 0.6027, 0.5704,\n",
      "        0.5352, 0.5913, 0.4181, 0.5353, 0.5256, 0.6624, 0.6022, 0.6192, 0.5260,\n",
      "        0.5492, 0.4824, 0.5810, 0.6828, 0.5700, 0.6185, 0.6484, 0.4519, 0.6183,\n",
      "        0.5746, 0.4265, 0.6256, 0.5124, 0.6148, 0.5920, 0.6445, 0.5632, 0.6555,\n",
      "        0.4944, 0.4794, 0.5969, 0.6097, 0.5123, 0.5211, 0.6228, 0.4503, 0.5803,\n",
      "        0.6457, 0.3831, 0.6810, 0.5782, 0.5546, 0.5350, 0.6332, 0.6345, 0.4731,\n",
      "        0.5411, 0.5993, 0.7275, 0.6005, 0.5935, 0.6045, 0.6346, 0.5520, 0.5512,\n",
      "        0.6270, 0.5373, 0.4809, 0.4758, 0.5832, 0.5722, 0.5703, 0.5391, 0.5994,\n",
      "        0.6789, 0.6442, 0.6708, 0.4191, 0.5351, 0.4401, 0.5040, 0.6065, 0.4356,\n",
      "        0.4667, 0.4823, 0.4760, 0.5135, 0.4554, 0.6089, 0.4581, 0.4553, 0.5461,\n",
      "        0.6213, 0.5764, 0.5494, 0.6111, 0.5601, 0.5971, 0.4553, 0.6349, 0.5418,\n",
      "        0.4924, 0.5008, 0.4946, 0.5261, 0.5474, 0.4625, 0.3481, 0.5274, 0.5142,\n",
      "        0.5419, 0.4998, 0.6170, 0.5964, 0.6126, 0.5565, 0.5195, 0.3942, 0.5042,\n",
      "        0.6248, 0.5989, 0.5835, 0.5216, 0.5894, 0.5350, 0.4349, 0.4353, 0.5950,\n",
      "        0.4998, 0.5756, 0.5890, 0.5193, 0.5131, 0.3995, 0.4440, 0.6701, 0.4356,\n",
      "        0.4865, 0.4826, 0.6216, 0.4750, 0.5648, 0.4533, 0.4810, 0.5614, 0.4656,\n",
      "        0.5285, 0.5796, 0.6722, 0.5046, 0.5930, 0.4780, 0.4798, 0.5120, 0.5573,\n",
      "        0.5951, 0.6327, 0.7006, 0.5451, 0.5827, 0.5339, 0.5530, 0.5239, 0.4485,\n",
      "        0.5453, 0.6021, 0.4698, 0.4977, 0.5800, 0.4020, 0.5254, 0.4429, 0.5516,\n",
      "        0.4955, 0.7076, 0.6535, 0.4889, 0.6292, 0.5178, 0.6175, 0.5010, 0.6738,\n",
      "        0.6162, 0.5440, 0.5444, 0.6003, 0.6413, 0.5453, 0.4480, 0.3465, 0.7348,\n",
      "        0.4257, 0.5789, 0.6352, 0.5126, 0.5926, 0.5433, 0.5770, 0.5415, 0.6448,\n",
      "        0.5773, 0.6758, 0.5689, 0.4656, 0.4794, 0.6135, 0.4973, 0.5112, 0.6240,\n",
      "        0.5184, 0.6322, 0.4614, 0.6399, 0.5161, 0.5402, 0.6854, 0.5926, 0.5801,\n",
      "        0.4903, 0.5656, 0.4190, 0.4123, 0.5971, 0.5729, 0.4916, 0.4913, 0.5430,\n",
      "        0.5153, 0.4993, 0.6295, 0.6537, 0.7757, 0.6055, 0.6145, 0.7147, 0.6754,\n",
      "        0.5470, 0.5273, 0.6999, 0.5064, 0.4815, 0.5882, 0.6092, 0.5214, 0.5220,\n",
      "        0.6858, 0.4773, 0.7627, 0.4893, 0.6126, 0.5855, 0.7017, 0.6380, 0.7597,\n",
      "        0.6267, 0.3545, 0.4909, 0.5239, 0.5171, 0.5827, 0.5963, 0.5631, 0.4324,\n",
      "        0.4913, 0.5075, 0.5851, 0.4108, 0.6940, 0.5772, 0.6396, 0.4248, 0.6203,\n",
      "        0.4285, 0.4401, 0.6112, 0.6593, 0.6619, 0.6803, 0.5882, 0.4990, 0.5459,\n",
      "        0.3721, 0.6225, 0.5761, 0.5240, 0.5208, 0.6120, 0.4716, 0.5795, 0.5617,\n",
      "        0.7344, 0.4988, 0.5342, 0.4783, 0.6046, 0.6761, 0.3846, 0.5652, 0.4919,\n",
      "        0.4908, 0.4958, 0.6475, 0.5179, 0.4455, 0.6851, 0.4560, 0.3330, 0.4884,\n",
      "        0.5750, 0.5952, 0.6461, 0.6100, 0.4190, 0.5582, 0.4341, 0.4615, 0.5668,\n",
      "        0.4585, 0.5306, 0.5888, 0.6336, 0.6333, 0.6175, 0.6026, 0.6918, 0.5076,\n",
      "        0.6149, 0.6518, 0.6440, 0.7122, 0.6392, 0.4881, 0.4224, 0.4764, 0.5944,\n",
      "        0.7395, 0.5704, 0.5668, 0.5241, 0.6351, 0.5902, 0.5212, 0.4579, 0.6438,\n",
      "        0.5648, 0.5860, 0.6738, 0.4574, 0.5321, 0.5655, 0.7165, 0.5980, 0.5389,\n",
      "        0.4211, 0.5863, 0.3618, 0.5318, 0.4549, 0.6749, 0.5745, 0.5062, 0.4003,\n",
      "        0.5082, 0.5692, 0.5537, 0.4034, 0.6759, 0.5242, 0.6688, 0.6238, 0.4404,\n",
      "        0.5398, 0.5170, 0.4779, 0.4047, 0.6103, 0.6264, 0.5852, 0.5244, 0.5585,\n",
      "        0.6059, 0.4401, 0.6419, 0.4656, 0.5852, 0.4809, 0.4126, 0.4354, 0.6010,\n",
      "        0.5898, 0.4271, 0.5573, 0.4581, 0.5764, 0.4705, 0.4770, 0.5306, 0.7039,\n",
      "        0.6080, 0.5739, 0.5548, 0.6580, 0.5252, 0.6617, 0.5722, 0.4778, 0.6997,\n",
      "        0.4980, 0.5581, 0.4543, 0.5866, 0.7445, 0.5379, 0.5351, 0.4971, 0.6443,\n",
      "        0.4767, 0.5104, 0.6235, 0.6510, 0.5061, 0.5792, 0.6525, 0.5786, 0.5373,\n",
      "        0.6718, 0.5714, 0.6466, 0.4639, 0.4721, 0.5573, 0.6798, 0.5116, 0.6167,\n",
      "        0.6344, 0.5313, 0.5947, 0.5917, 0.6244, 0.5457, 0.5523, 0.5229, 0.5139,\n",
      "        0.4204, 0.4796, 0.7389, 0.4608, 0.4905, 0.5247, 0.6018, 0.4547, 0.5308,\n",
      "        0.5773, 0.5793, 0.4761, 0.6116, 0.6325, 0.5677, 0.4897, 0.6002, 0.5152,\n",
      "        0.5405, 0.6261, 0.5391, 0.4543, 0.5880, 0.5842, 0.5474, 0.6282, 0.5519,\n",
      "        0.7891, 0.5646, 0.6155, 0.6378, 0.5989, 0.5322, 0.6698, 0.4796, 0.6116,\n",
      "        0.6171, 0.6282, 0.5204, 0.5871, 0.4829, 0.6104, 0.5779, 0.5034, 0.3775,\n",
      "        0.4581, 0.5638, 0.4403, 0.5649, 0.5964, 0.4781, 0.4722, 0.6473, 0.5480,\n",
      "        0.6013, 0.6107, 0.5810, 0.5139, 0.5625, 0.3922, 0.4003, 0.6073, 0.4730,\n",
      "        0.7162, 0.6354, 0.5083, 0.5894, 0.6415, 0.7207, 0.5295, 0.4812, 0.6185,\n",
      "        0.3570, 0.6675, 0.7228, 0.5571, 0.5476, 0.6206, 0.4775, 0.6137, 0.4377,\n",
      "        0.3877, 0.4748, 0.5147, 0.5505, 0.6792, 0.3998, 0.4380, 0.4880, 0.4895,\n",
      "        0.5615, 0.4684, 0.6836, 0.5835, 0.5188, 0.4644, 0.6041, 0.5607, 0.5420,\n",
      "        0.4242, 0.4062, 0.5427, 0.6228, 0.6363, 0.6065, 0.5454, 0.5472, 0.6616,\n",
      "        0.3781, 0.6265, 0.5295, 0.5225, 0.5469, 0.4415, 0.5128, 0.5335, 0.5653,\n",
      "        0.5692, 0.4030, 0.5579, 0.6167, 0.4373, 0.5652, 0.4569, 0.6851, 0.4976,\n",
      "        0.4107, 0.7210, 0.4680, 0.7222, 0.4842, 0.6158, 0.5014, 0.5589, 0.6370,\n",
      "        0.4538, 0.6271, 0.3856, 0.6045, 0.6484, 0.6052, 0.7314, 0.5238, 0.4077,\n",
      "        0.5802, 0.6258, 0.4295, 0.5921, 0.6134, 0.6228, 0.6520, 0.4902, 0.5272,\n",
      "        0.5807, 0.5300, 0.6216, 0.6625, 0.5347, 0.5796, 0.3446, 0.6443, 0.5552,\n",
      "        0.5545, 0.5507, 0.6353, 0.6690, 0.4703, 0.6853, 0.6889, 0.5622, 0.5550,\n",
      "        0.6030, 0.6532, 0.6168, 0.4476, 0.5867, 0.4615, 0.4355, 0.5259, 0.5077,\n",
      "        0.4929, 0.5499, 0.5737, 0.5638, 0.4674, 0.5674, 0.4563, 0.4754, 0.5340,\n",
      "        0.6194, 0.6139, 0.6992, 0.5472, 0.5452, 0.6370, 0.4960, 0.6376, 0.4683,\n",
      "        0.6733, 0.7096, 0.6665, 0.3613, 0.5159, 0.3695, 0.4939, 0.4370, 0.3260,\n",
      "        0.6190, 0.5240, 0.4260, 0.5607, 0.4939, 0.4732, 0.4832, 0.5095, 0.5914,\n",
      "        0.6221, 0.5253, 0.6724, 0.5530, 0.6182, 0.6040, 0.5316, 0.7167, 0.4548,\n",
      "        0.5395, 0.5892, 0.4471, 0.5216, 0.4507, 0.5088, 0.5963, 0.7389, 0.4992,\n",
      "        0.3942, 0.4755, 0.5157, 0.5555, 0.5895, 0.3483, 0.4373, 0.4749, 0.4820,\n",
      "        0.4848, 0.5726, 0.4949, 0.7349, 0.4227, 0.6314, 0.5937, 0.6942, 0.6385,\n",
      "        0.5234, 0.5788, 0.5375, 0.6439, 0.4607, 0.4002, 0.4361, 0.5265, 0.4660,\n",
      "        0.4703, 0.4802, 0.6424, 0.6840, 0.5220, 0.4892, 0.6138, 0.4734, 0.3501,\n",
      "        0.7297, 0.4781, 0.4751, 0.5806, 0.4809, 0.6393, 0.7315, 0.5587, 0.4531,\n",
      "        0.6208, 0.4407, 0.6037, 0.6169, 0.4415, 0.5987, 0.5115, 0.4029, 0.5462,\n",
      "        0.4756, 0.3944, 0.5592, 0.4316, 0.5009, 0.5464, 0.5014, 0.5492, 0.4679,\n",
      "        0.5455, 0.6374, 0.4889, 0.7078, 0.4201, 0.6092, 0.5776, 0.4923, 0.5649,\n",
      "        0.6372, 0.6238, 0.6402, 0.6393, 0.3555, 0.5688, 0.4073, 0.4954, 0.6058,\n",
      "        0.6078, 0.3806, 0.4957, 0.4660, 0.5345, 0.4976, 0.5690, 0.3804, 0.7130,\n",
      "        0.5726, 0.5488, 0.5559, 0.6009, 0.5323, 0.4172, 0.5228, 0.5019, 0.5431,\n",
      "        0.5961, 0.5466, 0.5900, 0.6285, 0.7198, 0.5903, 0.4684, 0.5019, 0.6215,\n",
      "        0.7141, 0.4869, 0.4482, 0.4880, 0.5876, 0.6689, 0.5401, 0.5079, 0.5462,\n",
      "        0.5737, 0.4987, 0.4979, 0.5641, 0.6159, 0.5229, 0.5598, 0.5758, 0.5338,\n",
      "        0.6361, 0.5464, 0.5638, 0.4799, 0.5913, 0.4569, 0.5318, 0.6285, 0.4185,\n",
      "        0.5913, 0.5871, 0.5458, 0.4287, 0.3340, 0.5340, 0.5269, 0.5663, 0.5290,\n",
      "        0.5797, 0.5732, 0.5452, 0.6322, 0.4158, 0.6291, 0.5672, 0.6907, 0.4279,\n",
      "        0.4890, 0.5811, 0.5599, 0.4934, 0.5897, 0.5346, 0.5713, 0.4671, 0.5080,\n",
      "        0.5982, 0.5779, 0.4817, 0.5100, 0.6588, 0.5766, 0.5003, 0.5024, 0.5197,\n",
      "        0.7336, 0.4715, 0.5083, 0.5740, 0.5300, 0.4840, 0.6450, 0.4979, 0.6718,\n",
      "        0.4408, 0.5985, 0.5478, 0.5983, 0.7480, 0.5656, 0.4441, 0.5212, 0.4649,\n",
      "        0.5130, 0.4641, 0.5080, 0.5394, 0.5040, 0.4493, 0.4996, 0.4598, 0.4442,\n",
      "        0.6707], device='cuda:0', requires_grad=True), 'BN_linear1.bias': Parameter containing:\n",
      "tensor([-0.0514, -0.0348,  0.0171, -0.0486, -0.0649, -0.0892, -0.0076, -0.0806,\n",
      "        -0.0267, -0.0611, -0.0554,  0.0126,  0.0373, -0.0510, -0.0702, -0.0200,\n",
      "        -0.1184, -0.0197, -0.0232, -0.0851, -0.0664, -0.1105, -0.0774, -0.0393,\n",
      "        -0.0570, -0.0693, -0.0340, -0.0205, -0.0896, -0.0709, -0.0328, -0.0696,\n",
      "        -0.0640, -0.0143, -0.0646, -0.0293, -0.0774, -0.1087, -0.0810, -0.0335,\n",
      "        -0.0629, -0.0946, -0.0711,  0.0025, -0.0883, -0.0196, -0.0544, -0.0842,\n",
      "        -0.0959, -0.0595, -0.0507, -0.0165, -0.0710, -0.0704, -0.0539, -0.0384,\n",
      "        -0.0353, -0.0760, -0.0437, -0.0370, -0.1066, -0.0675, -0.0599, -0.0960,\n",
      "         0.0004, -0.0154, -0.0436, -0.0935, -0.0316, -0.0746, -0.0719, -0.0569,\n",
      "        -0.0962, -0.0546,  0.0471, -0.0468, -0.0959, -0.0746, -0.0209, -0.0373,\n",
      "        -0.0611, -0.0008, -0.1106, -0.0887, -0.0846, -0.0244, -0.0820, -0.0410,\n",
      "        -0.0388, -0.0595, -0.0686, -0.0318, -0.1412, -0.0703, -0.0497, -0.0198,\n",
      "        -0.0041, -0.0623, -0.0556, -0.0600, -0.0802, -0.0779, -0.0734, -0.0785,\n",
      "        -0.0509, -0.1181, -0.0707,  0.0034, -0.0861, -0.0569, -0.0651, -0.0700,\n",
      "        -0.0463, -0.0263, -0.0094, -0.0356, -0.0422, -0.0884, -0.1136, -0.0534,\n",
      "        -0.0442, -0.0532, -0.1093, -0.0816, -0.1120, -0.0799, -0.1044, -0.1099,\n",
      "        -0.0523, -0.0588, -0.0775,  0.0050, -0.0751, -0.0532, -0.0024, -0.0415,\n",
      "        -0.1009, -0.0341, -0.0344, -0.0768, -0.0437, -0.0393, -0.0960,  0.0184,\n",
      "        -0.0679, -0.0671, -0.0930, -0.0740, -0.1000, -0.0230, -0.0730, -0.0765,\n",
      "        -0.0807, -0.0423, -0.0452,  0.0119, -0.0531, -0.0650, -0.0638, -0.0724,\n",
      "        -0.0994, -0.0467, -0.0572, -0.0354, -0.0681, -0.0424, -0.0455, -0.0497,\n",
      "        -0.0755, -0.0528, -0.0716, -0.0202, -0.0783, -0.0912, -0.0576, -0.0222,\n",
      "        -0.0445, -0.0359, -0.0682, -0.0531, -0.0991, -0.0406,  0.0026, -0.0398,\n",
      "        -0.0713, -0.1233, -0.0514,  0.0131, -0.0698, -0.0269, -0.0189, -0.0431,\n",
      "        -0.0587, -0.0123, -0.0284,  0.0107, -0.0539, -0.0604, -0.0478, -0.0171,\n",
      "        -0.1514, -0.0486, -0.0432, -0.0752, -0.0640, -0.0621, -0.0600, -0.0407,\n",
      "        -0.0701, -0.0265, -0.0549, -0.0962, -0.0299, -0.0519, -0.0347, -0.0535,\n",
      "        -0.0698, -0.0376,  0.0095, -0.0559, -0.0565, -0.0243, -0.0888, -0.0447,\n",
      "        -0.0566, -0.0741, -0.0591, -0.0482, -0.1060, -0.0900, -0.0830, -0.0277,\n",
      "        -0.0498, -0.0905, -0.0547, -0.0486,  0.0143, -0.0931, -0.0463, -0.0467,\n",
      "        -0.0947, -0.0367, -0.0669, -0.0582, -0.0796, -0.0083, -0.0055, -0.0615,\n",
      "        -0.1098, -0.0275, -0.0475, -0.0490, -0.0603, -0.1051, -0.1082, -0.0555,\n",
      "        -0.0818, -0.0734, -0.0560, -0.0638, -0.0206, -0.0548, -0.0120, -0.0789,\n",
      "        -0.0630, -0.0770, -0.0651, -0.0700, -0.0942, -0.0349, -0.1052, -0.0069,\n",
      "        -0.0781, -0.0768, -0.1005, -0.0784, -0.0796, -0.0320, -0.0955, -0.0317,\n",
      "        -0.0154, -0.0774, -0.0468, -0.0530, -0.0628, -0.0867, -0.0336, -0.0959,\n",
      "        -0.0365, -0.1049, -0.0669, -0.0872, -0.0242, -0.0630, -0.0659, -0.0806,\n",
      "        -0.0902, -0.0219, -0.0034, -0.0615, -0.0469, -0.0323, -0.0753, -0.0425,\n",
      "        -0.0609, -0.0580, -0.0232, -0.0412, -0.0861, -0.1262, -0.0613, -0.0475,\n",
      "        -0.0531, -0.0832, -0.0168, -0.0524, -0.0419, -0.0760, -0.0654, -0.0644,\n",
      "        -0.0477, -0.0872, -0.0756, -0.0239, -0.0974, -0.0790, -0.0620, -0.0755,\n",
      "        -0.0911, -0.0048, -0.0485, -0.0452, -0.0514, -0.0351, -0.0879, -0.0442,\n",
      "        -0.0650, -0.0635, -0.0684, -0.0764, -0.0449, -0.0794, -0.0789, -0.0596,\n",
      "        -0.0617, -0.0316, -0.0371, -0.0411, -0.0623, -0.0595, -0.0528, -0.0456,\n",
      "        -0.0703, -0.0544, -0.0647, -0.0311,  0.0068, -0.1248, -0.0323, -0.0494,\n",
      "        -0.0210, -0.0747, -0.0449, -0.0718, -0.0579, -0.1271, -0.1087, -0.0727,\n",
      "        -0.0563, -0.0538, -0.0525, -0.0135, -0.0405, -0.0337, -0.0859, -0.0538,\n",
      "        -0.0509, -0.0521, -0.0638, -0.0927, -0.0767, -0.0764, -0.0721, -0.0557,\n",
      "        -0.0381, -0.0816, -0.0408, -0.0175, -0.0482, -0.0878, -0.0713, -0.1371,\n",
      "        -0.0912,  0.0014, -0.0462, -0.0538, -0.0504, -0.0443, -0.0774, -0.0576,\n",
      "        -0.0520, -0.0856, -0.0535, -0.0875, -0.0702, -0.0724, -0.0613, -0.0878,\n",
      "        -0.0448, -0.0573, -0.0232, -0.0498, -0.0509, -0.0539, -0.0309, -0.0966,\n",
      "        -0.0304, -0.0493, -0.0184, -0.0736, -0.0727, -0.0439, -0.0628, -0.0749,\n",
      "        -0.0415, -0.0985, -0.0623, -0.0698, -0.0557, -0.0412, -0.0926, -0.0712,\n",
      "        -0.0247, -0.0480, -0.0890, -0.0486, -0.0638,  0.0305, -0.0546, -0.0847,\n",
      "        -0.0650, -0.0736, -0.0163, -0.0557, -0.0404, -0.0559, -0.0545, -0.0086,\n",
      "        -0.0887, -0.0999, -0.0238, -0.0652, -0.0829, -0.0604, -0.0480, -0.0769,\n",
      "        -0.0414, -0.0093, -0.0693, -0.1028, -0.0351, -0.0752, -0.0713, -0.0113,\n",
      "        -0.0648, -0.0352, -0.0041, -0.1081, -0.0129, -0.0308, -0.0217, -0.0386,\n",
      "        -0.0453, -0.1245, -0.0855, -0.0899, -0.0851, -0.0518, -0.1041, -0.0369,\n",
      "        -0.0468, -0.0436, -0.0848, -0.0445, -0.0735, -0.0434, -0.0297, -0.0753,\n",
      "        -0.0620, -0.0728, -0.0706, -0.0769, -0.0692, -0.0271, -0.0744, -0.0661,\n",
      "        -0.0925, -0.1022, -0.0895, -0.0187, -0.0424, -0.0507, -0.0515, -0.0638,\n",
      "        -0.0537, -0.0581, -0.0709, -0.0658, -0.0688, -0.0453, -0.0463, -0.0373,\n",
      "        -0.0444, -0.0630, -0.0613, -0.0632, -0.0235, -0.0492, -0.0645, -0.0366,\n",
      "        -0.0725, -0.0529, -0.1415, -0.0455, -0.0064, -0.0876, -0.0231, -0.0435,\n",
      "        -0.0659, -0.0867, -0.0341, -0.0619, -0.0543, -0.0405, -0.0926, -0.0533,\n",
      "        -0.0888, -0.0367, -0.0701, -0.0646, -0.0321, -0.0297, -0.0891, -0.0570,\n",
      "        -0.0605, -0.1063, -0.0617, -0.0587, -0.0478, -0.0973, -0.0895, -0.1238,\n",
      "        -0.0523, -0.0909,  0.0066, -0.0301, -0.0465, -0.1019, -0.0955, -0.0442,\n",
      "        -0.0467, -0.0550, -0.0628, -0.0287, -0.1135, -0.0439, -0.0391, -0.0649,\n",
      "        -0.0267, -0.0897, -0.0873, -0.0386, -0.0377, -0.0391, -0.0534, -0.0629,\n",
      "        -0.0672, -0.0470, -0.0533, -0.0200, -0.0673, -0.0023, -0.0732, -0.0031,\n",
      "        -0.0423, -0.0586, -0.0793, -0.0591, -0.0649, -0.0573, -0.0933, -0.0903,\n",
      "        -0.0737, -0.0818, -0.0708, -0.0024, -0.0461, -0.0564, -0.0013, -0.0389,\n",
      "        -0.0837, -0.0491, -0.0415, -0.0267, -0.0401, -0.0181, -0.0600, -0.0062,\n",
      "        -0.0296, -0.0386, -0.0821, -0.0559, -0.0909, -0.0508, -0.0760, -0.1069,\n",
      "        -0.0511, -0.0574, -0.0187, -0.1076, -0.0485, -0.0151, -0.0069, -0.0410,\n",
      "        -0.0363, -0.0543, -0.0974, -0.0703, -0.0748, -0.0412, -0.0415, -0.0291,\n",
      "        -0.0590, -0.0370, -0.1004, -0.0960, -0.0359, -0.0529,  0.0058, -0.0657,\n",
      "         0.0049,  0.0208, -0.0970, -0.0370, -0.0631, -0.0514, -0.0787, -0.0480,\n",
      "        -0.0485, -0.0212, -0.0217, -0.0583, -0.0957, -0.0646, -0.0430, -0.0465,\n",
      "        -0.0741, -0.0878, -0.0553,  0.0413, -0.0407, -0.0504, -0.0695, -0.0399,\n",
      "        -0.1191, -0.0748, -0.0485, -0.0594, -0.0810, -0.0507, -0.0485, -0.0264,\n",
      "        -0.0890, -0.0854, -0.0390, -0.0484, -0.0654, -0.0698, -0.0290, -0.0481,\n",
      "        -0.0778, -0.0542, -0.0564, -0.0615, -0.0899, -0.0817, -0.0358, -0.0851,\n",
      "        -0.0856, -0.0514, -0.0700,  0.0118, -0.0748, -0.0356, -0.0978, -0.0439,\n",
      "        -0.0598, -0.0368, -0.0871, -0.0204, -0.0572, -0.0618, -0.0674, -0.0097,\n",
      "        -0.0730, -0.0554, -0.0306, -0.0857, -0.1227, -0.0750, -0.0752, -0.0476,\n",
      "        -0.0597, -0.0535, -0.0686, -0.0519, -0.0491, -0.0988, -0.0805, -0.0336,\n",
      "        -0.0760, -0.0271, -0.0593, -0.0526, -0.0988, -0.0604, -0.0597, -0.0682,\n",
      "        -0.0250, -0.0423, -0.0698, -0.0404, -0.0874, -0.0586, -0.0990, -0.0341,\n",
      "        -0.0496, -0.0703, -0.0466, -0.0161, -0.0328, -0.0789, -0.0543, -0.0339,\n",
      "        -0.0416, -0.0487, -0.0462, -0.0591, -0.0390, -0.0024, -0.0091, -0.0572,\n",
      "        -0.0762, -0.0460, -0.0943, -0.0591, -0.0840, -0.0745, -0.0706, -0.0426,\n",
      "        -0.1058, -0.0266, -0.0989, -0.0898, -0.0654, -0.0462, -0.0826, -0.0920,\n",
      "        -0.0612, -0.0657, -0.0389, -0.0511, -0.0457, -0.0894, -0.0692, -0.0473,\n",
      "        -0.0462, -0.0748, -0.0253, -0.0335, -0.0619, -0.0653, -0.0328, -0.1014,\n",
      "        -0.0589, -0.0353, -0.0157, -0.1598, -0.0615, -0.0805, -0.0661, -0.0989,\n",
      "        -0.0539,  0.0020, -0.0225, -0.0779, -0.0353, -0.0417, -0.0889, -0.0512,\n",
      "        -0.0442, -0.0727, -0.0861, -0.0647, -0.0743, -0.0606, -0.1211, -0.0830,\n",
      "        -0.0408, -0.1164, -0.0889, -0.0469, -0.0884, -0.0689, -0.0111, -0.0664,\n",
      "        -0.0722, -0.1081, -0.0780, -0.0625, -0.0047, -0.0555, -0.0496, -0.1140,\n",
      "        -0.0840, -0.0270, -0.0783, -0.0296, -0.0401, -0.0560, -0.0569, -0.1130,\n",
      "        -0.0440, -0.0606, -0.0689, -0.0728, -0.0521, -0.0863, -0.0889, -0.0997,\n",
      "        -0.0239, -0.0634, -0.0495, -0.0523, -0.0392, -0.0580, -0.0440, -0.0774,\n",
      "        -0.1368, -0.1346, -0.0463, -0.0502, -0.0795, -0.0633, -0.0649, -0.0291,\n",
      "        -0.0906, -0.0865, -0.0275, -0.0567, -0.0437, -0.0684, -0.0410, -0.0678,\n",
      "        -0.0972, -0.0905, -0.0402, -0.0890, -0.0651, -0.1016, -0.0676, -0.0533,\n",
      "        -0.0199, -0.0162, -0.0381, -0.0213, -0.0850, -0.0604, -0.0597, -0.0717,\n",
      "        -0.0450, -0.1049, -0.0344, -0.0751, -0.0603, -0.0760, -0.0821, -0.0685,\n",
      "        -0.0039, -0.0370, -0.1264, -0.0790, -0.0716, -0.0570, -0.0254, -0.0097,\n",
      "        -0.0278, -0.0456, -0.0353, -0.0750, -0.0279, -0.0686, -0.0434, -0.0635,\n",
      "        -0.1239, -0.0134, -0.0929, -0.0584, -0.0535, -0.0596, -0.0486, -0.0280,\n",
      "        -0.0811, -0.0477, -0.0589, -0.0559, -0.0325, -0.0482, -0.0948, -0.0397,\n",
      "        -0.0331, -0.0825, -0.0225, -0.0782, -0.0119, -0.0655, -0.0392, -0.0186,\n",
      "        -0.0530, -0.0514, -0.0778, -0.0079, -0.0585, -0.0218, -0.0510, -0.0831,\n",
      "        -0.0566, -0.0173, -0.0611, -0.0502, -0.0609, -0.0645,  0.0074, -0.0605,\n",
      "        -0.0269, -0.0566, -0.0578, -0.0263, -0.0269, -0.0668, -0.0614, -0.0205,\n",
      "        -0.0387, -0.0704, -0.0348, -0.0196, -0.0222, -0.0446, -0.0964, -0.0373,\n",
      "        -0.0711, -0.0808, -0.0733, -0.0624, -0.0498, -0.0740, -0.0740, -0.0597,\n",
      "        -0.0785, -0.0707, -0.0724, -0.0893, -0.0920, -0.0463, -0.0592, -0.0607,\n",
      "        -0.0002, -0.0090, -0.0728, -0.0627, -0.0253, -0.0524,  0.0109, -0.0802,\n",
      "        -0.0622, -0.0212, -0.0130, -0.0708, -0.0324, -0.0676, -0.1050, -0.0300,\n",
      "        -0.0689, -0.0293, -0.0622, -0.0457, -0.0671, -0.1044, -0.0419, -0.0900],\n",
      "       device='cuda:0', requires_grad=True), 'linear_model2.weight': Parameter containing:\n",
      "tensor([[-0.0244,  0.0044, -0.0670,  ..., -0.0163, -0.0495,  0.0680],\n",
      "        [-0.0057, -0.0112,  0.0329,  ..., -0.0322, -0.0229,  0.0453],\n",
      "        [ 0.0346, -0.0156,  0.0353,  ...,  0.0192,  0.0699,  0.0194],\n",
      "        ...,\n",
      "        [-0.0058,  0.0064, -0.0157,  ...,  0.0274, -0.0120,  0.0075],\n",
      "        [ 0.0113,  0.0078, -0.0293,  ...,  0.0287,  0.0023, -0.0044],\n",
      "        [ 0.0551, -0.0376,  0.0396,  ...,  0.0073,  0.0112, -0.0401]],\n",
      "       device='cuda:0', requires_grad=True), 'linear_model2.bias': Parameter containing:\n",
      "tensor([-4.0378e-07, -1.9483e-06, -7.9927e-07,  8.0709e-07, -6.1532e-07,\n",
      "        -7.5352e-07,  7.4033e-07,  2.4875e-08,  7.8721e-08, -1.8723e-07,\n",
      "        -1.9509e-07, -2.0067e-07,  5.1587e-07, -1.1268e-06,  4.0813e-07,\n",
      "        -6.2177e-07,  1.0885e-06,  9.6041e-07, -8.2269e-08, -4.9902e-07,\n",
      "        -1.4694e-07,  1.2233e-07,  1.5641e-06,  2.8032e-07,  5.1821e-08,\n",
      "        -3.8186e-07, -8.1342e-09,  6.4068e-07, -1.8462e-07, -5.2754e-08,\n",
      "        -7.9653e-07,  4.8222e-07, -6.7220e-07,  1.1719e-07,  1.1482e-06,\n",
      "         7.2526e-07,  8.5812e-08,  7.9428e-07, -5.0141e-07, -4.5022e-07,\n",
      "         4.3422e-07, -9.6249e-07,  2.6901e-07, -4.9284e-07,  1.0173e-07,\n",
      "        -9.9410e-08,  6.5265e-07, -6.2114e-07,  1.4243e-06,  5.4230e-07,\n",
      "         6.7819e-08,  1.8803e-07,  1.9569e-06, -4.2714e-08, -1.8659e-06,\n",
      "         3.8263e-07,  5.9548e-08, -1.6190e-07, -1.9550e-07,  2.2009e-07,\n",
      "        -6.8140e-07,  1.6341e-06,  1.4520e-07, -2.8907e-07,  1.4693e-07,\n",
      "        -2.8034e-07,  1.8993e-08,  4.8390e-07, -2.5374e-07,  1.4442e-07,\n",
      "         2.7581e-07, -6.7045e-10,  1.8946e-07,  7.8287e-07, -1.0858e-06,\n",
      "        -8.0554e-07, -3.1034e-07,  2.5636e-07,  5.1157e-08,  2.6422e-07,\n",
      "        -1.1468e-07, -1.2448e-06,  3.8522e-07,  2.0948e-06, -2.5810e-07,\n",
      "         4.1963e-07,  7.8000e-07, -6.3390e-07, -2.1182e-07,  1.1709e-07,\n",
      "        -2.5814e-07,  2.4813e-08, -3.4453e-07, -1.2444e-06, -4.7430e-07,\n",
      "        -1.0426e-06, -7.4862e-07,  3.1120e-07, -7.8564e-07,  6.8396e-09],\n",
      "       device='cuda:0', requires_grad=True), 'BN_linear2.weight': Parameter containing:\n",
      "tensor([1.0114, 1.0331, 1.0093, 0.9703, 0.9173, 0.8519, 1.0570, 0.9488, 0.8975,\n",
      "        1.0546, 0.8493, 0.8650, 0.9094, 0.8557, 0.9590, 0.9603, 0.9064, 0.9212,\n",
      "        0.9722, 0.9151, 0.9843, 0.8871, 0.9609, 0.9782, 0.9250, 0.9577, 0.9235,\n",
      "        0.8248, 0.9590, 0.9097, 0.9396, 0.8949, 0.9664, 0.9041, 0.9354, 0.9213,\n",
      "        0.8406, 0.9397, 1.0580, 0.9382, 0.9855, 0.9372, 0.9759, 1.0739, 0.9650,\n",
      "        0.8996, 1.0147, 0.9454, 0.9417, 1.0112, 0.9330, 1.0780, 0.9328, 0.9272,\n",
      "        1.1033, 0.8867, 0.9762, 0.9273, 0.9157, 0.7731, 0.8981, 1.0445, 0.9822,\n",
      "        0.8906, 0.9263, 1.0058, 0.9870, 1.0099, 1.0322, 1.0294, 0.9852, 0.8358,\n",
      "        0.9578, 0.8791, 1.0001, 0.9010, 1.0242, 0.9912, 0.9660, 1.0755, 0.8659,\n",
      "        0.9397, 0.9276, 1.0453, 0.9612, 0.9368, 0.9122, 0.9702, 0.8736, 0.9441,\n",
      "        0.9402, 0.9491, 1.0713, 0.9644, 0.8877, 0.8360, 1.0290, 0.9246, 1.0396,\n",
      "        0.9224], device='cuda:0', requires_grad=True), 'BN_linear2.bias': Parameter containing:\n",
      "tensor([0.1715, 0.1388, 0.0960, 0.1677, 0.1224, 0.1328, 0.1313, 0.1086, 0.1404,\n",
      "        0.1574, 0.1936, 0.1577, 0.1680, 0.1224, 0.1462, 0.1136, 0.1556, 0.1503,\n",
      "        0.0678, 0.1155, 0.1642, 0.1601, 0.1026, 0.1524, 0.1294, 0.1614, 0.2192,\n",
      "        0.1134, 0.1535, 0.0790, 0.1605, 0.1479, 0.0827, 0.1676, 0.0976, 0.1139,\n",
      "        0.1661, 0.1065, 0.1216, 0.1241, 0.0879, 0.0998, 0.1179, 0.1246, 0.0683,\n",
      "        0.0736, 0.1497, 0.0819, 0.1286, 0.1540, 0.0593, 0.1064, 0.1818, 0.0664,\n",
      "        0.1141, 0.1677, 0.1399, 0.1626, 0.0768, 0.1866, 0.1446, 0.1235, 0.1266,\n",
      "        0.1364, 0.1187, 0.0635, 0.0913, 0.0939, 0.0478, 0.1022, 0.1344, 0.1842,\n",
      "        0.1754, 0.1339, 0.1291, 0.1459, 0.1388, 0.1521, 0.1964, 0.1394, 0.1405,\n",
      "        0.0942, 0.0952, 0.1522, 0.0671, 0.0732, 0.0773, 0.1717, 0.1733, 0.0763,\n",
      "        0.1636, 0.1679, 0.1494, 0.1610, 0.1471, 0.1080, 0.0908, 0.1481, 0.1360,\n",
      "        0.1328], device='cuda:0', requires_grad=True), 'embedding_layers.weight': Parameter containing:\n",
      "tensor([[ 9.4405e-02,  4.5858e-02,  2.4007e-01,  ..., -2.6021e-05,\n",
      "          2.1055e-05, -1.1081e-01],\n",
      "        [ 1.1936e+00, -4.5160e-24,  3.9264e-02,  ..., -1.4509e-01,\n",
      "          3.3199e-01,  1.6833e-03],\n",
      "        [ 2.0348e-02, -4.9103e-02, -1.6417e-05,  ...,  5.2105e-02,\n",
      "         -6.5822e-02, -4.9641e-01],\n",
      "        ...,\n",
      "        [ 4.5052e-02, -1.9614e-01, -1.3809e-01,  ...,  9.3666e-05,\n",
      "         -6.1764e-01, -7.5040e-01],\n",
      "        [ 1.7533e-01, -4.4434e-10, -3.5739e-01,  ..., -5.5721e-01,\n",
      "          1.2746e-01,  1.2019e-01],\n",
      "        [ 1.0742e-03, -5.1132e-30,  3.5810e-01,  ...,  1.2480e-02,\n",
      "         -1.7718e-04,  5.9193e-02]], device='cuda:0', requires_grad=True), 'BN_bi.weight': Parameter containing:\n",
      "tensor([0.6819, 0.4719, 0.4260, 0.6276, 0.5985, 0.4640, 0.3833, 0.5847, 0.4249,\n",
      "        0.3697, 0.6248, 0.4506, 0.6367, 0.5616, 0.4616, 0.5508, 0.7368, 0.5190,\n",
      "        0.4204, 0.7325, 0.7246, 0.7536, 0.4766, 0.4927, 0.5101, 0.4781, 0.3701,\n",
      "        0.6179, 0.3756, 0.5814, 0.4365, 0.4228, 0.6321, 0.5092, 0.4529, 0.4823,\n",
      "        0.4426, 0.3912, 0.5884, 0.4621, 0.5240, 0.6140, 0.4067, 0.4796, 0.6145,\n",
      "        0.8034, 0.6413, 0.3962, 0.7166, 0.4676, 0.6386, 0.5875, 0.6562, 0.4816,\n",
      "        0.6339, 0.4438, 0.3462, 0.5031, 0.5900, 0.4342, 0.4438, 0.4382, 0.4944,\n",
      "        0.6019, 0.5425, 0.8977, 0.5742, 0.4774, 0.4539, 0.7057, 0.4381, 0.6388,\n",
      "        0.4645, 0.5942, 0.4730, 0.3966, 0.4729, 0.4932, 0.4792, 0.4000, 0.5471,\n",
      "        0.4807, 0.6925, 0.4610, 0.4721, 0.5523, 0.3626, 0.3759, 0.3893, 0.4677,\n",
      "        0.6950, 0.4062, 0.4937, 0.4486, 0.4734, 0.5227, 0.4151, 0.5414, 0.4845,\n",
      "        0.8165], device='cuda:0', requires_grad=True), 'BN_bi.bias': Parameter containing:\n",
      "tensor([-0.0403, -0.0970,  0.0285, -0.0613, -0.0536, -0.0321, -0.0232, -0.0148,\n",
      "         0.0367,  0.0434, -0.1094,  0.0793,  0.0654, -0.0041,  0.0641,  0.0054,\n",
      "        -0.0878,  0.0185, -0.1017,  0.0419,  0.0380, -0.0481, -0.0273,  0.1117,\n",
      "         0.0239,  0.0063, -0.0392,  0.0846, -0.0202, -0.0940, -0.0422, -0.0084,\n",
      "         0.0667, -0.1268, -0.0516, -0.0137, -0.0229, -0.0550,  0.0500,  0.0115,\n",
      "        -0.0737, -0.0117, -0.0107,  0.0346,  0.0014,  0.0820, -0.0875,  0.0003,\n",
      "        -0.0153,  0.0857,  0.0479, -0.0581,  0.0242,  0.0590, -0.0209, -0.0134,\n",
      "        -0.0557,  0.0532,  0.0163, -0.0290,  0.0130,  0.0851,  0.0500,  0.0955,\n",
      "        -0.0365,  0.0046,  0.0430,  0.0017,  0.0020, -0.0009, -0.0818, -0.0840,\n",
      "        -0.0291,  0.0027,  0.0349, -0.0195,  0.0528,  0.0059, -0.0053,  0.0694,\n",
      "         0.0836,  0.0399,  0.0045,  0.0394,  0.0843,  0.0423, -0.0705,  0.0453,\n",
      "         0.0789, -0.0915, -0.0567, -0.0062, -0.0537,  0.0252,  0.0035, -0.0054,\n",
      "        -0.0173,  0.0801,  0.0172, -0.0598], device='cuda:0',\n",
      "       requires_grad=True), 'dnn_layers.0.weight': Parameter containing:\n",
      "tensor([[ 0.0635, -0.0138, -0.0038,  ...,  0.0391,  0.0858, -0.0583],\n",
      "        [ 0.0169, -0.0269,  0.0662,  ..., -0.0592, -0.0974,  0.0622],\n",
      "        [ 0.0275, -0.0023,  0.0091,  ...,  0.0784, -0.0309,  0.0612],\n",
      "        ...,\n",
      "        [-0.0458, -0.0221, -0.0180,  ..., -0.0567,  0.0017, -0.0481],\n",
      "        [ 0.0088, -0.0275,  0.0275,  ..., -0.0181, -0.0368,  0.0695],\n",
      "        [ 0.0438, -0.0167, -0.0489,  ...,  0.1253,  0.0860, -0.0006]],\n",
      "       device='cuda:0', requires_grad=True), 'dnn_layers.0.bias': Parameter containing:\n",
      "tensor([ 0.0716,  0.0123,  0.0702,  0.0341,  0.1146,  0.1177,  0.0092,  0.1720,\n",
      "         0.0426,  0.0647,  0.0134, -0.0072,  0.1322,  0.0950,  0.0319, -0.0009,\n",
      "         0.0868,  0.0627,  0.0757,  0.1090,  0.0549,  0.0589,  0.0517,  0.0858,\n",
      "         0.0257,  0.1036,  0.1273,  0.1066,  0.0360,  0.0490,  0.0641,  0.1175,\n",
      "         0.0630,  0.1240,  0.0880,  0.0553,  0.1174,  0.0803,  0.1488,  0.0317,\n",
      "         0.0246,  0.0250,  0.0482,  0.0943,  0.0768,  0.1042,  0.1358,  0.1399,\n",
      "         0.0706,  0.0948,  0.0956,  0.0336,  0.0744,  0.0219,  0.0172,  0.0910,\n",
      "         0.0742,  0.0300,  0.0815,  0.0739,  0.0478,  0.0903,  0.0462,  0.0475,\n",
      "         0.1402,  0.0746,  0.1039,  0.0375,  0.0817, -0.0110,  0.0525,  0.0564,\n",
      "         0.0152,  0.1132,  0.0989,  0.0806,  0.0875,  0.1097,  0.0404,  0.1281,\n",
      "         0.0305, -0.0153,  0.0733,  0.1148,  0.1305,  0.1204,  0.1317,  0.0539,\n",
      "         0.0649,  0.0880,  0.0170,  0.0822,  0.0497,  0.0994,  0.0406,  0.0628,\n",
      "         0.0768,  0.0845,  0.0310,  0.1149], device='cuda:0',\n",
      "       requires_grad=True), 'dnn_layers.1.weight': Parameter containing:\n",
      "tensor([[ 1.7068e-01,  1.9415e-01,  1.2658e-01, -1.3063e-01,  1.7064e-01,\n",
      "         -1.6787e-01, -1.6003e-01, -2.3589e-01, -7.2377e-02, -7.8750e-03,\n",
      "          9.0433e-02, -1.4943e-01, -2.0168e-01,  1.2635e-01, -1.6295e-01,\n",
      "          1.1766e-01,  1.7993e-01,  1.3120e-01,  1.4739e-01, -9.2848e-02,\n",
      "         -6.2393e-02,  2.0110e-01,  1.7488e-01, -1.4736e-01,  1.0497e-01,\n",
      "          1.4228e-01, -1.8133e-01, -1.3291e-01,  1.6580e-02,  2.3229e-01,\n",
      "          7.4918e-02, -6.7226e-02,  1.2578e-01, -3.1084e-02,  1.5107e-01,\n",
      "         -1.1579e-01, -3.9349e-02,  1.7371e-01, -1.2538e-01, -9.4629e-02,\n",
      "          1.0409e-03, -9.4665e-02, -3.7716e-02, -1.5299e-01,  1.0375e-01,\n",
      "          7.2166e-02, -1.7557e-01,  8.3713e-02, -1.7426e-01,  8.7350e-05,\n",
      "         -1.4760e-01, -1.8403e-01, -1.7330e-01, -1.1273e-01,  1.3372e-02,\n",
      "         -6.7062e-02,  1.7162e-01,  1.5074e-01, -1.4718e-01, -1.7946e-01,\n",
      "         -1.6132e-01,  1.7508e-02,  1.7028e-01,  2.0392e-01, -1.6006e-01,\n",
      "          2.9035e-02, -1.4306e-02, -7.4849e-02, -1.5917e-01,  1.5290e-01,\n",
      "         -1.3686e-01, -1.7989e-01,  1.6224e-01,  7.2301e-02, -1.3652e-01,\n",
      "          3.0773e-02, -2.4045e-01, -1.7843e-01,  1.3717e-01, -1.7386e-01,\n",
      "         -7.7369e-02,  2.1531e-01, -6.3364e-02, -2.3240e-01, -1.7298e-01,\n",
      "          1.3922e-01,  1.3292e-01,  1.2616e-01, -5.8179e-02, -9.3632e-02,\n",
      "          9.0299e-02, -1.5012e-01,  1.1059e-01, -1.9022e-01, -1.2257e-01,\n",
      "          8.6740e-02,  1.8432e-01, -1.8298e-01,  1.1332e-01,  9.8235e-02],\n",
      "        [-4.9523e-02, -2.0364e-01,  1.3156e-01, -1.9708e-01, -2.2012e-01,\n",
      "          9.8857e-02,  1.2226e-01,  4.0077e-02, -1.8262e-02, -1.1844e-01,\n",
      "          1.3443e-01,  1.7808e-01,  1.2149e-01, -2.2690e-01, -6.7149e-02,\n",
      "         -1.9141e-01,  1.0434e-01,  1.0233e-01, -5.2026e-02, -9.9042e-02,\n",
      "          1.8931e-01,  6.2473e-02,  1.3741e-01,  2.2113e-01, -1.1946e-01,\n",
      "          1.5081e-01,  1.7296e-01,  1.3214e-01, -1.3795e-01, -1.2698e-01,\n",
      "         -8.7573e-02,  1.0921e-01,  8.7606e-02, -2.1614e-01, -1.6358e-01,\n",
      "          9.6751e-02, -1.5734e-01,  4.6535e-02, -1.6848e-01,  2.2628e-01,\n",
      "          1.7531e-01,  5.3996e-02,  2.0179e-01,  1.5980e-01, -1.9471e-01,\n",
      "          9.8657e-02, -1.8034e-01, -1.3955e-01,  1.7561e-01,  1.2470e-01,\n",
      "         -1.3524e-01,  1.4769e-01, -7.0485e-02, -1.0873e-01,  1.4554e-01,\n",
      "          1.4823e-01, -9.2065e-02,  1.3866e-01, -1.5361e-01,  1.3983e-01,\n",
      "         -2.0780e-01,  1.7187e-01, -1.2220e-01, -1.3740e-01, -7.8714e-02,\n",
      "         -1.4905e-01, -1.5238e-01,  1.3715e-01, -5.9439e-02, -1.1388e-01,\n",
      "          2.2303e-01,  1.4594e-01,  1.0443e-01, -1.5350e-01,  9.0456e-02,\n",
      "         -2.4073e-01,  1.1535e-01, -2.6702e-01,  1.5146e-02,  6.2767e-02,\n",
      "          2.5666e-02,  1.3850e-01, -8.3306e-02, -5.4684e-02, -8.8358e-02,\n",
      "         -1.8996e-01, -1.9404e-01,  1.8193e-01, -1.8075e-01, -2.2153e-01,\n",
      "          1.2503e-01,  1.0588e-02, -1.1040e-01,  3.8110e-03, -2.6447e-02,\n",
      "         -2.0948e-01,  1.4687e-01, -1.8058e-01,  9.6429e-02,  1.5087e-01],\n",
      "        [ 1.3031e-01, -2.2685e-01, -6.5429e-02,  1.5233e-01,  1.5735e-01,\n",
      "          1.0391e-01, -8.2461e-02,  1.4377e-01, -1.9829e-01, -9.3105e-02,\n",
      "         -2.7376e-02,  1.2657e-01,  7.7057e-02,  1.8348e-01, -1.0372e-01,\n",
      "          1.7184e-01, -1.2230e-01,  9.4804e-02,  7.0386e-03, -1.1558e-01,\n",
      "         -9.6475e-02, -9.1037e-02,  1.1989e-02, -8.6892e-02, -1.4064e-01,\n",
      "          1.1615e-01,  1.4979e-01, -1.3431e-01,  2.8733e-01, -8.4808e-02,\n",
      "         -1.0070e-01, -1.1058e-01,  7.7538e-02,  9.4237e-02, -1.3145e-01,\n",
      "          1.7325e-01,  1.7907e-01, -1.8628e-01,  2.0202e-01, -1.6589e-01,\n",
      "         -1.6660e-01, -5.4872e-02, -1.4656e-01, -1.8384e-01, -1.5942e-01,\n",
      "         -7.0717e-02,  9.1578e-02,  1.1179e-01,  8.1248e-02,  2.1810e-01,\n",
      "         -1.3951e-01,  3.2280e-02,  2.0991e-01, -5.2192e-02, -1.1276e-01,\n",
      "          9.4626e-02, -1.4178e-01,  1.5476e-02,  5.6733e-02, -1.9752e-01,\n",
      "         -7.9080e-02,  1.6400e-01, -2.2360e-01,  9.5272e-03,  1.5870e-03,\n",
      "         -1.8171e-01,  1.0667e-01, -9.3563e-02,  8.1799e-02,  1.1256e-01,\n",
      "         -1.0089e-01,  1.1908e-01, -1.6995e-01, -4.8954e-02,  1.2702e-01,\n",
      "          1.6274e-01,  1.8875e-01, -5.5606e-02,  2.1532e-01,  1.9417e-01,\n",
      "         -2.5320e-01,  5.8259e-02, -2.0865e-01, -1.8946e-01,  1.2119e-01,\n",
      "         -2.0538e-01,  1.3915e-01, -9.9554e-02,  4.3564e-02, -1.6513e-01,\n",
      "          2.8837e-02,  6.9747e-02,  8.1986e-02,  2.3145e-01, -8.0000e-02,\n",
      "          1.4170e-01, -1.4220e-01, -1.2165e-02, -1.5234e-01,  4.5774e-02],\n",
      "        [-1.2300e-01, -1.1165e-01,  2.0868e-01, -1.6089e-01,  1.4192e-01,\n",
      "         -2.5463e-01,  1.0550e-01,  1.9564e-01,  2.1693e-01, -1.8098e-02,\n",
      "         -2.3459e-01,  3.4457e-02, -2.2430e-01, -7.6794e-02,  8.1662e-02,\n",
      "          9.6769e-02,  9.1946e-02,  1.1325e-01, -2.3023e-01, -1.2861e-01,\n",
      "         -1.5001e-02, -1.8516e-01,  6.8349e-02, -1.3764e-01,  2.0249e-01,\n",
      "         -1.4312e-02,  6.0419e-02,  4.9880e-02, -5.6596e-02, -4.2981e-02,\n",
      "          2.1512e-01, -1.7416e-01,  9.0924e-02, -1.4787e-01, -7.0006e-02,\n",
      "         -1.7849e-01,  8.2107e-02,  1.6814e-01, -1.6636e-01, -1.2999e-01,\n",
      "         -2.2126e-02,  7.1205e-02,  5.1810e-02, -2.4273e-01, -6.4719e-02,\n",
      "          2.4529e-01,  6.3275e-02,  9.2192e-03, -1.6406e-01,  2.0011e-01,\n",
      "          1.4413e-01, -2.2090e-01, -2.2148e-01,  1.7712e-01,  1.8319e-01,\n",
      "          1.0331e-01,  2.2922e-01,  1.1026e-01, -1.5584e-01, -9.6264e-02,\n",
      "          1.4749e-01, -1.4053e-02,  1.5521e-01, -1.5037e-01, -1.6351e-01,\n",
      "         -9.5284e-02, -1.0684e-01, -1.2442e-01, -1.9023e-01,  6.5675e-02,\n",
      "         -5.7326e-02, -7.1548e-02,  1.8937e-02,  2.2858e-01,  1.8039e-01,\n",
      "          1.9321e-01,  1.5186e-01, -7.4795e-02, -6.2862e-02,  1.2497e-01,\n",
      "          1.3434e-01,  6.7614e-02, -1.7123e-01,  4.9728e-03, -1.5569e-01,\n",
      "          3.5015e-02, -1.1726e-01,  1.2233e-01, -1.2982e-01,  7.3743e-02,\n",
      "          5.8018e-03,  1.5771e-01, -1.4452e-01,  7.0798e-02,  1.3238e-01,\n",
      "         -1.3304e-01,  4.0049e-02,  2.3857e-01, -7.4281e-02,  1.9847e-01],\n",
      "        [ 1.2798e-01,  3.1061e-03,  1.7879e-01, -2.3151e-01, -1.7949e-01,\n",
      "         -1.2969e-01, -1.2615e-01,  8.7239e-03,  1.4016e-01, -2.0883e-01,\n",
      "         -1.4430e-01, -1.1967e-01,  1.5802e-01,  6.1304e-02, -1.4399e-01,\n",
      "         -6.6712e-02,  1.4438e-01,  9.6091e-02,  1.7026e-01,  8.4515e-02,\n",
      "         -1.3846e-01,  1.0550e-01, -1.3237e-01,  1.2889e-01, -1.7806e-01,\n",
      "         -1.4276e-01, -1.7198e-01,  4.1103e-02, -1.3880e-01,  1.4403e-01,\n",
      "          1.3952e-01,  1.1669e-01, -1.9672e-01,  1.4526e-01, -2.0732e-01,\n",
      "          1.0234e-01,  9.1652e-02,  2.5128e-02,  8.6465e-02, -1.7347e-01,\n",
      "          1.7829e-01,  1.8391e-01,  1.7523e-01, -6.1418e-02,  1.2897e-01,\n",
      "         -1.3386e-01,  1.0151e-01,  9.4135e-02,  1.2072e-01, -1.8695e-01,\n",
      "         -7.8871e-02, -1.0913e-01, -1.5041e-01,  5.1093e-02, -6.7863e-02,\n",
      "          1.1993e-01, -1.8398e-01,  1.1170e-01, -1.2582e-01, -3.6292e-03,\n",
      "         -8.6397e-02,  1.2878e-01, -1.7303e-01,  1.1007e-01,  9.7983e-02,\n",
      "          1.7897e-01,  1.6472e-01,  1.6594e-01,  6.0297e-02, -1.2304e-01,\n",
      "         -1.7397e-01,  1.6623e-01, -1.8545e-01,  1.2864e-01, -1.3228e-01,\n",
      "         -1.7766e-01,  5.3742e-02,  5.2702e-02,  5.0993e-02,  9.4088e-02,\n",
      "          1.4159e-01, -9.0015e-02,  1.2410e-01,  1.2472e-01,  6.9492e-02,\n",
      "         -7.3738e-02,  9.8995e-02,  6.1400e-02, -1.5168e-01, -2.2574e-01,\n",
      "         -1.5882e-01,  1.2886e-01,  1.4145e-01, -8.4279e-02, -1.4806e-01,\n",
      "          1.6144e-01, -1.7653e-01, -1.9982e-01, -1.9492e-01,  1.7125e-01],\n",
      "        [-5.2664e-02, -1.2517e-01,  4.9233e-02, -4.4324e-02,  1.7940e-02,\n",
      "          7.8970e-02, -5.8679e-02,  1.9314e-01,  4.4681e-02,  9.2754e-02,\n",
      "          1.1190e-01, -3.8916e-02,  1.4443e-01, -2.8856e-02, -2.0654e-02,\n",
      "         -8.3792e-02,  1.4252e-01, -2.4312e-01, -1.8273e-01,  1.3933e-01,\n",
      "          1.9259e-01,  2.6369e-02, -5.6956e-02, -3.1005e-02,  4.8111e-03,\n",
      "          5.1534e-02,  1.3006e-01,  2.5836e-01,  4.2606e-03,  5.1573e-02,\n",
      "         -4.1226e-02,  2.0949e-01, -1.8459e-01,  6.5248e-02, -7.9359e-02,\n",
      "          7.9764e-02,  1.3424e-01,  3.2724e-02,  1.9297e-01, -1.4412e-01,\n",
      "          1.6629e-01, -1.6494e-01,  1.0043e-01,  1.5658e-01,  6.9291e-02,\n",
      "         -9.2241e-02,  1.7380e-01,  1.1325e-01, -8.9522e-02,  6.0997e-03,\n",
      "          1.1853e-01, -1.8357e-01,  1.8400e-01,  1.0293e-01,  9.7480e-03,\n",
      "          1.4511e-01,  1.1936e-01, -1.9366e-01,  1.7691e-01,  7.0474e-02,\n",
      "          9.0074e-02, -1.2885e-01, -1.3189e-01, -5.9107e-02,  1.6719e-01,\n",
      "          3.2164e-02, -6.6881e-02, -1.2185e-01, -1.0491e-01, -1.4714e-01,\n",
      "         -9.1506e-02,  1.5643e-01, -6.7732e-02, -3.7669e-03, -1.1576e-01,\n",
      "         -8.2294e-02, -1.7867e-01,  1.6178e-01, -1.5486e-01, -1.6323e-01,\n",
      "         -1.4425e-01,  1.1547e-01,  8.2708e-02, -7.2605e-03, -1.5051e-01,\n",
      "          8.9126e-02,  2.1433e-02, -1.8577e-01,  5.2491e-03,  2.5433e-01,\n",
      "         -1.6607e-01, -6.3656e-02, -1.6589e-01, -2.2550e-03,  2.3065e-01,\n",
      "         -8.9728e-02,  2.4413e-01,  1.4415e-01,  3.1728e-02,  1.3933e-02],\n",
      "        [ 5.4008e-02,  1.0686e-01, -1.6765e-01,  1.3552e-01,  3.3781e-02,\n",
      "          8.2201e-02, -5.6078e-02,  7.6189e-03, -1.2076e-01,  1.7325e-01,\n",
      "          4.1852e-02, -4.7447e-02,  9.0568e-02,  5.7373e-02,  1.5979e-01,\n",
      "         -5.4021e-02, -1.1622e-01,  4.9033e-03,  6.1330e-02,  6.9618e-02,\n",
      "          9.5380e-03,  3.5435e-02,  1.1770e-01,  1.3150e-01, -1.4947e-02,\n",
      "          1.0411e-01,  1.5077e-02, -8.7213e-02, -3.0814e-02, -1.9722e-02,\n",
      "         -3.8703e-03, -5.1800e-02,  6.1778e-02,  1.0881e-01,  1.5656e-01,\n",
      "         -1.0211e-01,  8.4070e-02,  3.4879e-02, -1.3358e-01,  1.7292e-01,\n",
      "         -4.6848e-02, -1.9679e-01,  4.4966e-02,  1.0611e-01,  5.6434e-03,\n",
      "          1.0791e-01,  2.4949e-03,  5.4870e-02,  4.0343e-02,  2.4547e-02,\n",
      "         -4.1262e-02,  5.8053e-02,  7.5746e-02,  1.3344e-02, -1.0453e-01,\n",
      "         -1.7373e-01,  3.6167e-02,  4.9990e-02,  6.3889e-02,  1.9419e-01,\n",
      "          1.7596e-01,  1.0798e-01,  1.4430e-01,  1.4332e-01,  1.0976e-01,\n",
      "         -1.1890e-01,  1.3191e-02, -1.3299e-01,  1.0453e-01,  4.2056e-03,\n",
      "          1.7576e-01, -1.8934e-01,  1.0358e-01, -1.7053e-01,  4.3405e-02,\n",
      "          9.5051e-02,  3.6052e-02,  1.4341e-01,  1.2814e-02,  5.6479e-02,\n",
      "         -1.6253e-01, -8.4615e-02, -8.7968e-02,  1.6349e-01,  9.1769e-02,\n",
      "          1.3446e-01,  4.9002e-02,  8.1430e-02,  8.9827e-03,  5.1066e-02,\n",
      "          4.0609e-02,  5.6744e-03,  1.2755e-01,  2.0545e-02, -2.9378e-02,\n",
      "          5.3122e-03,  2.2149e-03, -3.7235e-02,  1.2684e-01,  2.3422e-04],\n",
      "        [ 1.5984e-01,  1.2817e-01,  2.0062e-02,  8.7997e-02, -1.3414e-01,\n",
      "         -1.8762e-01,  1.8298e-01, -1.9985e-01,  1.5869e-01,  1.9730e-01,\n",
      "         -6.3465e-02,  1.1275e-01, -7.4290e-02, -1.5376e-01,  1.4087e-01,\n",
      "          9.3614e-02, -1.2173e-01, -1.8056e-01, -8.8350e-02,  1.5048e-01,\n",
      "         -1.7074e-01, -1.7041e-01, -1.5798e-01, -1.1483e-01,  1.1428e-01,\n",
      "         -2.1152e-01, -1.1444e-01, -1.5314e-01, -7.9918e-02, -9.1453e-02,\n",
      "          1.1469e-01, -6.3239e-02,  7.2205e-02, -1.5565e-01,  1.0683e-01,\n",
      "          1.0362e-01, -1.7392e-01,  1.5764e-01,  3.3393e-02,  1.6249e-01,\n",
      "         -1.6126e-01,  4.8812e-02, -1.1824e-01,  1.3650e-01, -1.6729e-01,\n",
      "         -2.0182e-01,  2.4612e-02,  6.7267e-02,  2.0442e-01, -1.7352e-01,\n",
      "          1.2089e-02,  1.0914e-01, -1.5267e-01,  1.7173e-01, -4.5100e-02,\n",
      "         -1.3910e-01, -1.7486e-01,  6.8863e-02, -4.4082e-02, -1.8105e-01,\n",
      "         -1.8171e-01, -9.9779e-02,  9.1739e-02, -1.8842e-01,  1.4844e-01,\n",
      "          1.5242e-01,  1.6423e-01,  1.0069e-01,  1.9551e-01,  5.4340e-02,\n",
      "          1.1007e-01,  8.1254e-02,  1.4533e-01,  1.5622e-01,  1.6861e-01,\n",
      "         -2.1133e-01,  1.5308e-01, -1.4806e-01, -1.0181e-01,  1.0134e-01,\n",
      "          1.9385e-01,  1.4209e-01,  1.4300e-01,  1.3202e-01, -1.8317e-01,\n",
      "         -1.5156e-01,  4.7189e-02,  1.3960e-01,  1.9468e-01,  8.7891e-02,\n",
      "          1.4744e-01,  5.8251e-02, -1.9062e-01,  1.5245e-01,  1.8628e-01,\n",
      "          1.2012e-01, -1.8827e-01,  1.1923e-01, -1.1476e-01, -1.5414e-01],\n",
      "        [-1.6200e-01,  7.7541e-02,  1.1173e-01,  1.4815e-01,  1.4018e-01,\n",
      "          1.4171e-01,  1.5767e-01, -2.5079e-01, -1.8534e-01, -6.4630e-02,\n",
      "          1.3295e-01,  1.4713e-01, -1.8517e-01,  1.5869e-01, -9.6076e-02,\n",
      "          1.4838e-01, -4.9269e-03,  7.7847e-02,  1.5486e-01, -1.1261e-01,\n",
      "          1.4422e-01,  1.5554e-01, -1.0610e-01, -1.0431e-01,  1.1513e-01,\n",
      "         -2.0797e-01,  1.2739e-01,  1.0602e-01,  1.7562e-01,  1.6913e-01,\n",
      "         -4.2891e-02,  2.8901e-02,  9.6862e-02,  6.2572e-02, -6.9774e-02,\n",
      "          6.5978e-02, -2.5735e-01,  7.6655e-02,  1.4420e-01, -5.5104e-02,\n",
      "         -1.4154e-01,  1.8014e-01, -1.9753e-01,  4.5944e-02,  1.4141e-01,\n",
      "          4.3502e-02, -1.4079e-01, -2.2501e-01, -9.5752e-02, -5.4598e-02,\n",
      "          1.6975e-01,  1.6008e-01,  1.7507e-01, -1.0216e-01,  1.3569e-01,\n",
      "          3.8846e-02, -1.3443e-02, -1.7588e-01,  2.2164e-01, -1.9255e-01,\n",
      "         -1.3694e-01, -1.6000e-01,  1.4537e-01, -1.7162e-01, -1.0459e-01,\n",
      "          1.2005e-01, -1.0985e-01,  1.7739e-01, -3.5507e-02,  1.4622e-01,\n",
      "         -1.3412e-01,  1.1136e-01, -1.0820e-01,  1.7876e-01, -1.2241e-01,\n",
      "         -2.3084e-01, -1.5342e-01, -1.4832e-01,  1.5102e-01, -1.0769e-01,\n",
      "          1.6522e-01,  1.6845e-04,  1.1492e-01, -1.1337e-01,  1.5047e-01,\n",
      "         -1.4343e-01, -1.6902e-01, -1.4440e-01,  1.6446e-01, -1.3184e-01,\n",
      "          9.6413e-02, -1.7135e-01, -8.1491e-02, -1.7413e-01,  1.0482e-01,\n",
      "          1.2717e-01,  1.2009e-01,  1.2245e-02,  1.2953e-01, -1.7506e-01]],\n",
      "       device='cuda:0', requires_grad=True), 'dnn_layers.1.bias': Parameter containing:\n",
      "tensor([-0.0573,  0.0218,  0.0191,  0.0062,  0.1017,  0.1510,  0.0418, -0.0306,\n",
      "         0.0473], device='cuda:0', requires_grad=True)}\n"
     ]
    }
   ],
   "source": [
    "weight={}\n",
    "for name,parameters in net.named_parameters():\n",
    "    print(name,':',parameters.size())\n",
    "    #names.append(name)\n",
    "    weight[name]=parameters\n",
    "print(\"weight:\",weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9e5cc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Con', 'DN', 'FSGS', 'HT', 'IgAN', 'MCD', 'MGN', 'RPGN', 'SLE']\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "{'Con': 0, 'DN': 1, 'FSGS': 2, 'HT': 3, 'IgAN': 4, 'MCD': 5, 'MGN': 6, 'RPGN': 7, 'SLE': 8}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "disease=pd.read_csv(nfm_config['all'],sep=',',header=None)\n",
    "disease=np.array(disease)\n",
    "disease=disease[1:,-1]\n",
    "disease=disease.tolist()\n",
    "#print(disease)\n",
    "dis_names=[]\n",
    "#dis_names.append()\n",
    "\n",
    "for id in disease:\n",
    "    if id not in dis_names:\n",
    "        dis_names.append(id)\n",
    "\n",
    "print(dis_names)\n",
    "val=[ i for i in range(nfm_config['n_class'])]\n",
    "print(val)\n",
    "\n",
    "disease_names={}\n",
    "#disease_names.keys=dis_names\n",
    "#disease_names.values=val\n",
    "#disease_names[dis_names]=val\n",
    "#print(disease_names)\n",
    "disease_names=dict(zip(dis_names,val))\n",
    "print(disease_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeb035ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "genes=[]\n",
    "gene_res={}\n",
    "title=pd.read_csv(nfm_config['title'],sep=',',header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d2d656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, classes, label_smoothing=0.2):\n",
    "    n = len(labels)\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        print(\"row:\",row,\"label:\",label)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d01fdc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row: 0 label: 0\n",
      "row: 1 label: 1\n",
      "row: 2 label: 2\n",
      "row: 3 label: 3\n",
      "row: 4 label: 4\n",
      "row: 5 label: 5\n",
      "row: 6 label: 6\n",
      "row: 7 label: 7\n",
      "row: 8 label: 8\n",
      "[[0.82222223 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.82222223 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.82222223 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.82222223 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.82222223\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.82222223 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]]\n"
     ]
    }
   ],
   "source": [
    "new_labels=[i for i in range(nfm_config['n_class'])]\n",
    "new_targets=one_hot(new_labels,nfm_config['n_class'])\n",
    "print(new_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff64bd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Con': ['FOSB', 'EGR1', 'MT1M', 'APOLD1', 'ALB', 'NR4A2', 'GDF15', 'PTGS1', 'PCK1', 'CEBPB'], 'DN': ['HSD17B2', 'CSF2RB', 'UGT2A3', 'CCN3', 'CD36', 'CXCL6', 'ECM1', 'INSL4', 'COMP', 'MS4A4A'], 'FSGS': ['ZBTB16', 'MEST', 'EGFL6', 'FKBP5', 'BTN3A2', 'MMP7', 'EYA1', 'MUC13', 'FCGR3B', 'NMU'], 'HT': ['BCAT1', 'ESM1', 'SLC16A6', 'SRPX', 'FOSB', 'NFIL3', 'PCK1', 'CYP27B1', 'GDF15', 'NUPR1'], 'IgAN': ['C6', 'PLG', 'ID1', 'AQP9', 'TMEM100', 'PCK1', 'VSIG4', 'CTSV', 'RNASE6', 'PLCG2'], 'MCD': ['LPL', 'ISG15', 'CYP27B1', 'COL15A1', 'MICA', 'SRPX', 'IFI44', 'HTR2B', 'HERC6', 'CD36'], 'MGN': ['TGFBI', 'ESM1', 'DDX11', 'APOC3', 'CXCL10', 'SRPX2', 'SPINK1', 'GPM6B', 'HBEGF', 'GATA6'], 'RPGN': ['ESM1', 'C3', 'ALB', 'CD163', 'SRPX', 'NOX4', 'BCAT1', 'SERPINA3', 'C6', 'FAP'], 'SLE': ['MX1', 'HERC6', 'ISG15', 'ISG20', 'CD163', 'IFI44', 'LPL', 'OAS2', 'OAS3', 'MS4A4A']}\n",
      "{'Con': ['FOSB', 'EGR1', 'MT1M', 'APOLD1', 'ALB', 'NR4A2', 'GDF15', 'PTGS1', 'PCK1', 'CEBPB'], 'DN': ['HSD17B2', 'CSF2RB', 'UGT2A3', 'CCN3', 'CD36', 'CXCL6', 'ECM1', 'INSL4', 'COMP', 'MS4A4A'], 'FSGS': ['ZBTB16', 'MEST', 'EGFL6', 'FKBP5', 'BTN3A2', 'MMP7', 'EYA1', 'MUC13', 'FCGR3B', 'NMU'], 'HT': ['BCAT1', 'ESM1', 'SLC16A6', 'SRPX', 'FOSB', 'NFIL3', 'PCK1', 'CYP27B1', 'GDF15', 'NUPR1'], 'IgAN': ['C6', 'PLG', 'ID1', 'AQP9', 'TMEM100', 'PCK1', 'VSIG4', 'CTSV', 'RNASE6', 'PLCG2'], 'MCD': ['LPL', 'ISG15', 'CYP27B1', 'COL15A1', 'MICA', 'SRPX', 'IFI44', 'HTR2B', 'HERC6', 'CD36'], 'MGN': ['TGFBI', 'ESM1', 'DDX11', 'APOC3', 'CXCL10', 'SRPX2', 'SPINK1', 'GPM6B', 'HBEGF', 'GATA6'], 'RPGN': ['ESM1', 'C3', 'ALB', 'CD163', 'SRPX', 'NOX4', 'BCAT1', 'SERPINA3', 'C6', 'FAP'], 'SLE': ['MX1', 'HERC6', 'ISG15', 'ISG20', 'CD163', 'IFI44', 'LPL', 'OAS2', 'OAS3', 'MS4A4A']}\n",
      "{'Con': ['FOSB', 'EGR1', 'MT1M', 'APOLD1', 'ALB', 'NR4A2', 'GDF15', 'PTGS1', 'PCK1', 'CEBPB'], 'DN': ['HSD17B2', 'CSF2RB', 'UGT2A3', 'CCN3', 'CD36', 'CXCL6', 'ECM1', 'INSL4', 'COMP', 'MS4A4A'], 'FSGS': ['ZBTB16', 'MEST', 'EGFL6', 'FKBP5', 'BTN3A2', 'MMP7', 'EYA1', 'MUC13', 'FCGR3B', 'NMU'], 'HT': ['BCAT1', 'ESM1', 'SLC16A6', 'SRPX', 'FOSB', 'NFIL3', 'PCK1', 'CYP27B1', 'GDF15', 'NUPR1'], 'IgAN': ['C6', 'PLG', 'ID1', 'AQP9', 'TMEM100', 'PCK1', 'VSIG4', 'CTSV', 'RNASE6', 'PLCG2'], 'MCD': ['LPL', 'ISG15', 'CYP27B1', 'COL15A1', 'MICA', 'SRPX', 'IFI44', 'HTR2B', 'HERC6', 'CD36'], 'MGN': ['TGFBI', 'ESM1', 'DDX11', 'APOC3', 'CXCL10', 'SRPX2', 'SPINK1', 'GPM6B', 'HBEGF', 'GATA6'], 'RPGN': ['ESM1', 'C3', 'ALB', 'CD163', 'SRPX', 'NOX4', 'BCAT1', 'SERPINA3', 'C6', 'FAP'], 'SLE': ['MX1', 'HERC6', 'ISG15', 'ISG20', 'CD163', 'IFI44', 'LPL', 'OAS2', 'OAS3', 'MS4A4A']}\n",
      "{'Con': ['FOSB', 'EGR1', 'MT1M', 'APOLD1', 'ALB', 'NR4A2', 'GDF15', 'PTGS1', 'PCK1', 'CEBPB'], 'DN': ['HSD17B2', 'CSF2RB', 'UGT2A3', 'CCN3', 'CD36', 'CXCL6', 'ECM1', 'INSL4', 'COMP', 'MS4A4A'], 'FSGS': ['ZBTB16', 'MEST', 'EGFL6', 'FKBP5', 'BTN3A2', 'MMP7', 'EYA1', 'MUC13', 'FCGR3B', 'NMU'], 'HT': ['BCAT1', 'ESM1', 'SLC16A6', 'SRPX', 'FOSB', 'NFIL3', 'PCK1', 'CYP27B1', 'GDF15', 'NUPR1'], 'IgAN': ['C6', 'PLG', 'ID1', 'AQP9', 'TMEM100', 'PCK1', 'VSIG4', 'CTSV', 'RNASE6', 'PLCG2'], 'MCD': ['LPL', 'ISG15', 'CYP27B1', 'COL15A1', 'MICA', 'SRPX', 'IFI44', 'HTR2B', 'HERC6', 'CD36'], 'MGN': ['TGFBI', 'ESM1', 'DDX11', 'APOC3', 'CXCL10', 'SRPX2', 'SPINK1', 'GPM6B', 'HBEGF', 'GATA6'], 'RPGN': ['ESM1', 'C3', 'ALB', 'CD163', 'SRPX', 'NOX4', 'BCAT1', 'SERPINA3', 'C6', 'FAP'], 'SLE': ['MX1', 'HERC6', 'ISG15', 'ISG20', 'CD163', 'IFI44', 'LPL', 'OAS2', 'OAS3', 'MS4A4A']}\n",
      "{'Con': ['FOSB', 'EGR1', 'MT1M', 'APOLD1', 'ALB', 'NR4A2', 'GDF15', 'PTGS1', 'PCK1', 'CEBPB'], 'DN': ['HSD17B2', 'CSF2RB', 'UGT2A3', 'CCN3', 'CD36', 'CXCL6', 'ECM1', 'INSL4', 'COMP', 'MS4A4A'], 'FSGS': ['ZBTB16', 'MEST', 'EGFL6', 'FKBP5', 'BTN3A2', 'MMP7', 'EYA1', 'MUC13', 'FCGR3B', 'NMU'], 'HT': ['BCAT1', 'ESM1', 'SLC16A6', 'SRPX', 'FOSB', 'NFIL3', 'PCK1', 'CYP27B1', 'GDF15', 'NUPR1'], 'IgAN': ['C6', 'PLG', 'ID1', 'AQP9', 'TMEM100', 'PCK1', 'VSIG4', 'CTSV', 'RNASE6', 'PLCG2'], 'MCD': ['LPL', 'ISG15', 'CYP27B1', 'COL15A1', 'MICA', 'SRPX', 'IFI44', 'HTR2B', 'HERC6', 'CD36'], 'MGN': ['TGFBI', 'ESM1', 'DDX11', 'APOC3', 'CXCL10', 'SRPX2', 'SPINK1', 'GPM6B', 'HBEGF', 'GATA6'], 'RPGN': ['ESM1', 'C3', 'ALB', 'CD163', 'SRPX', 'NOX4', 'BCAT1', 'SERPINA3', 'C6', 'FAP'], 'SLE': ['MX1', 'HERC6', 'ISG15', 'ISG20', 'CD163', 'IFI44', 'LPL', 'OAS2', 'OAS3', 'MS4A4A']}\n",
      "{'Con': ['FOSB', 'EGR1', 'MT1M', 'APOLD1', 'ALB', 'NR4A2', 'GDF15', 'PTGS1', 'PCK1', 'CEBPB'], 'DN': ['HSD17B2', 'CSF2RB', 'UGT2A3', 'CCN3', 'CD36', 'CXCL6', 'ECM1', 'INSL4', 'COMP', 'MS4A4A'], 'FSGS': ['ZBTB16', 'MEST', 'EGFL6', 'FKBP5', 'BTN3A2', 'MMP7', 'EYA1', 'MUC13', 'FCGR3B', 'NMU'], 'HT': ['BCAT1', 'ESM1', 'SLC16A6', 'SRPX', 'FOSB', 'NFIL3', 'PCK1', 'CYP27B1', 'GDF15', 'NUPR1'], 'IgAN': ['C6', 'PLG', 'ID1', 'AQP9', 'TMEM100', 'PCK1', 'VSIG4', 'CTSV', 'RNASE6', 'PLCG2'], 'MCD': ['LPL', 'ISG15', 'CYP27B1', 'COL15A1', 'MICA', 'SRPX', 'IFI44', 'HTR2B', 'HERC6', 'CD36'], 'MGN': ['TGFBI', 'ESM1', 'DDX11', 'APOC3', 'CXCL10', 'SRPX2', 'SPINK1', 'GPM6B', 'HBEGF', 'GATA6'], 'RPGN': ['ESM1', 'C3', 'ALB', 'CD163', 'SRPX', 'NOX4', 'BCAT1', 'SERPINA3', 'C6', 'FAP'], 'SLE': ['MX1', 'HERC6', 'ISG15', 'ISG20', 'CD163', 'IFI44', 'LPL', 'OAS2', 'OAS3', 'MS4A4A']}\n",
      "{'Con': ['FOSB', 'EGR1', 'MT1M', 'APOLD1', 'ALB', 'NR4A2', 'GDF15', 'PTGS1', 'PCK1', 'CEBPB'], 'DN': ['HSD17B2', 'CSF2RB', 'UGT2A3', 'CCN3', 'CD36', 'CXCL6', 'ECM1', 'INSL4', 'COMP', 'MS4A4A'], 'FSGS': ['ZBTB16', 'MEST', 'EGFL6', 'FKBP5', 'BTN3A2', 'MMP7', 'EYA1', 'MUC13', 'FCGR3B', 'NMU'], 'HT': ['BCAT1', 'ESM1', 'SLC16A6', 'SRPX', 'FOSB', 'NFIL3', 'PCK1', 'CYP27B1', 'GDF15', 'NUPR1'], 'IgAN': ['C6', 'PLG', 'ID1', 'AQP9', 'TMEM100', 'PCK1', 'VSIG4', 'CTSV', 'RNASE6', 'PLCG2'], 'MCD': ['LPL', 'ISG15', 'CYP27B1', 'COL15A1', 'MICA', 'SRPX', 'IFI44', 'HTR2B', 'HERC6', 'CD36'], 'MGN': ['TGFBI', 'ESM1', 'DDX11', 'APOC3', 'CXCL10', 'SRPX2', 'SPINK1', 'GPM6B', 'HBEGF', 'GATA6'], 'RPGN': ['ESM1', 'C3', 'ALB', 'CD163', 'SRPX', 'NOX4', 'BCAT1', 'SERPINA3', 'C6', 'FAP'], 'SLE': ['MX1', 'HERC6', 'ISG15', 'ISG20', 'CD163', 'IFI44', 'LPL', 'OAS2', 'OAS3', 'MS4A4A']}\n",
      "{'Con': ['FOSB', 'EGR1', 'MT1M', 'APOLD1', 'ALB', 'NR4A2', 'GDF15', 'PTGS1', 'PCK1', 'CEBPB'], 'DN': ['HSD17B2', 'CSF2RB', 'UGT2A3', 'CCN3', 'CD36', 'CXCL6', 'ECM1', 'INSL4', 'COMP', 'MS4A4A'], 'FSGS': ['ZBTB16', 'MEST', 'EGFL6', 'FKBP5', 'BTN3A2', 'MMP7', 'EYA1', 'MUC13', 'FCGR3B', 'NMU'], 'HT': ['BCAT1', 'ESM1', 'SLC16A6', 'SRPX', 'FOSB', 'NFIL3', 'PCK1', 'CYP27B1', 'GDF15', 'NUPR1'], 'IgAN': ['C6', 'PLG', 'ID1', 'AQP9', 'TMEM100', 'PCK1', 'VSIG4', 'CTSV', 'RNASE6', 'PLCG2'], 'MCD': ['LPL', 'ISG15', 'CYP27B1', 'COL15A1', 'MICA', 'SRPX', 'IFI44', 'HTR2B', 'HERC6', 'CD36'], 'MGN': ['TGFBI', 'ESM1', 'DDX11', 'APOC3', 'CXCL10', 'SRPX2', 'SPINK1', 'GPM6B', 'HBEGF', 'GATA6'], 'RPGN': ['ESM1', 'C3', 'ALB', 'CD163', 'SRPX', 'NOX4', 'BCAT1', 'SERPINA3', 'C6', 'FAP'], 'SLE': ['MX1', 'HERC6', 'ISG15', 'ISG20', 'CD163', 'IFI44', 'LPL', 'OAS2', 'OAS3', 'MS4A4A']}\n",
      "{'Con': ['FOSB', 'EGR1', 'MT1M', 'APOLD1', 'ALB', 'NR4A2', 'GDF15', 'PTGS1', 'PCK1', 'CEBPB'], 'DN': ['HSD17B2', 'CSF2RB', 'UGT2A3', 'CCN3', 'CD36', 'CXCL6', 'ECM1', 'INSL4', 'COMP', 'MS4A4A'], 'FSGS': ['ZBTB16', 'MEST', 'EGFL6', 'FKBP5', 'BTN3A2', 'MMP7', 'EYA1', 'MUC13', 'FCGR3B', 'NMU'], 'HT': ['BCAT1', 'ESM1', 'SLC16A6', 'SRPX', 'FOSB', 'NFIL3', 'PCK1', 'CYP27B1', 'GDF15', 'NUPR1'], 'IgAN': ['C6', 'PLG', 'ID1', 'AQP9', 'TMEM100', 'PCK1', 'VSIG4', 'CTSV', 'RNASE6', 'PLCG2'], 'MCD': ['LPL', 'ISG15', 'CYP27B1', 'COL15A1', 'MICA', 'SRPX', 'IFI44', 'HTR2B', 'HERC6', 'CD36'], 'MGN': ['TGFBI', 'ESM1', 'DDX11', 'APOC3', 'CXCL10', 'SRPX2', 'SPINK1', 'GPM6B', 'HBEGF', 'GATA6'], 'RPGN': ['ESM1', 'C3', 'ALB', 'CD163', 'SRPX', 'NOX4', 'BCAT1', 'SERPINA3', 'C6', 'FAP'], 'SLE': ['MX1', 'HERC6', 'ISG15', 'ISG20', 'CD163', 'IFI44', 'LPL', 'OAS2', 'OAS3', 'MS4A4A']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "for i in range(nfm_config['n_class']):\n",
    "    \n",
    "    #new_targets=torch.tensor([0, 0, 0, 0, 0, 0, 0, 1])\n",
    "    new_targets=torch.tensor(new_targets).cuda()\n",
    "    l1=torch.mv(weight['dnn_layers.1.weight'].T,(new_targets[i]-weight['dnn_layers.1.bias']))\n",
    "    l2=torch.mv(weight['dnn_layers.0.weight'].T,(l1-weight['dnn_layers.0.bias']))\n",
    "    l2=l2[100:]\n",
    "    l3=torch.mv(weight['linear_model2.weight'].T,(l2-weight['linear_model2.bias']))\n",
    "    l4=torch.mv(weight['linear_model1.weight'].T,(l3-weight['linear_model1.bias']))\n",
    "    top_k=torch.topk(l4,10,largest=True)\n",
    "    index=top_k.indices\n",
    "    index=index.tolist()\n",
    "    d1=pd.DataFrame(title,columns=index)#\n",
    "    gene=d1.iloc[1,:]\n",
    "    gene=gene.tolist()\n",
    "    genes.append(gene)\n",
    "    #print(genes)\n",
    "    \n",
    "    gene_res=dict(zip(dis_names,genes))\n",
    "print(gene_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e679b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'Con': ['GDF15', 'FOSB', 'GATA6', 'APOLD1', 'EGR1', 'NR4A2', 'CEBPB', 'PDK4', 'DEPP1', 'IGF1'], 'DN': ['HSD17B2', 'CD36', 'INSL4', 'ECM1', 'FOLH1', 'CSF2RB', 'MS4A4A', 'PID1', 'KRT17', 'CCL8'], 'FSGS': ['ZBTB16', 'EGFL6', 'MEST', 'CYP17A1', 'NMU', 'KRT19', 'MMP7', 'PF4', 'HBB', 'NFE2L3'], 'HT': ['FOSB', 'RHOB', 'CXCL3', 'UBE2C', 'BCAT1', 'STT3A', 'PDCD5', 'SLC16A6', 'KIF20A', 'ORC6'], 'IgAN': ['SOX17', 'PLCG2', 'HBB', 'BMP2', 'TMEM100', 'APLNR', 'AQP9', 'PTGS2', 'STAC', 'HLA-DQB1'], 'MCD': ['HIST1H1C', 'PEX6', 'UMOD', 'SLC12A1', 'CTNNA3', 'CYP24A1', 'IFI44', 'COL15A1', 'LPL', 'MICB'], 'MGN': ['SRPX2', 'INSL4', 'TGFBI', 'CXCL10', 'RBP4', 'NT5E', 'IGF1', 'CSGALNACT1', 'IER3', 'BMP2'], 'RPGN': ['C3', 'ESM1', 'SERPINA3', 'TGFBI', 'NOX4', 'SERPINE1', 'MMP9', 'PPBP', 'ZNF239', 'GZMH'], 'SLE': ['MX1', 'ISG15', 'HERC6', 'CR2', 'OAS1', 'OAS2', 'DDX58', 'OAS3', 'IFIT3', 'MS4A4A']}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
