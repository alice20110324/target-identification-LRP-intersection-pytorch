{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3faf1737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import Trainer\n",
    "from network import NFM\n",
    "import torch.utils.data as Data\n",
    "from Utils.criteo_loader import getTestData, getTrainData\n",
    "\n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':16,\n",
    "    'embed_dim': 100, # 用于控制稀疏特征经过Embedding层后的稠密特征大小\n",
    "    'dnn_hidden_units': [50, 16],\n",
    "    'num_sparse_features_cols':10149,#the number of the gene columns\n",
    "   # 'num_dense_features': 13,\n",
    "    'bi_dropout': 0.8,\n",
    "    'num_epoch': 500,\n",
    "    'batch_size': 128,\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    #'train_file': '../Data/criteo/processed_data/train_set.csv',\n",
    "    #'fea_file': '../Data/criteo/processed_data/fea_col.npy',\n",
    "    #'validate_file': '../Data/criteo/processed_data/val_set.csv',\n",
    "    #'test_file': '../Data/criteo/processed_data/test_set.csv',\n",
    "    #'model_name': '../TrainedModels/NFM.model'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c0ecac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "#from utils import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7c87ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, classes, label_smoothing=0.2):\n",
    "    n = len(labels)\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        print(\"row:\",row,\"label:\",label)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88127a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMData(data.Dataset):\n",
    "    \"\"\" Construct the FM pytorch dataset. \"\"\"\n",
    "    #def __init__(self, file,label_file, feature_map,n_class=16):\n",
    "    def __init__(self, file,label_file, n_class=16):\n",
    "        super(FMData, self).__init__()\n",
    "        self.label = []\n",
    "        self.features = []\n",
    "        #self.feature_values = []\n",
    "        \n",
    "        features=[]\n",
    "        #feature_map.keys()\n",
    "        #self.features=np.array(feature_map)\n",
    "        #feature_map\n",
    "        #file=\"data/frappe/c_df_v_fff2000.csv\"\n",
    "        #num = len(features)\n",
    "        fd=pd.read_csv(file,sep=',')\n",
    "        #nrow=fd.shape[0]\n",
    "        #ncol=fd.shape[1]\n",
    "        n_fd=np.array(fd)\n",
    "        #print(n_fd)\n",
    "        n_fd=n_fd[:,2:]\n",
    "        \n",
    "        \"\"\"\n",
    "        for i, item in enumerate(n_fd):\n",
    "            u=[feature_map[x] for x in item]\n",
    "            features.append(u)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.features=np.array(n_fd)\n",
    "        #self.features=features.tolist()\n",
    "        \n",
    "        \n",
    "        nrow,ncol=n_fd.shape\n",
    "        #ncol=10150\n",
    "        #feature_v=[]\n",
    "        \"\"\"\n",
    "            feature_v=[1 for i in range(ncol)]\n",
    "            #print(feature_v)\n",
    "            #feature_values=[feature_v for j in range(nrow)]\n",
    "\n",
    "            for item in range(nrow):\n",
    "            feature_values.append(feature_v)\n",
    "        \"\"\"\n",
    "\n",
    "        #feature_v=[[1 for i in range(ncol)] for i in range(nrow)]\n",
    "        #self.feature_values=np.array(feature_v)\n",
    "        #print(feature_value)\n",
    "        #self.feature_values=feature_value.tolist()\n",
    "        #feature_map,lenth=map_features()\n",
    "        #raw = [item for item in  enumerate(n_fd)]\n",
    "        #print(raw)\n",
    "        #raw=raw.tolist()\n",
    "        \n",
    "        #label_file=[]\n",
    "        label_fd=pd.read_csv(label_file,sep=',')\n",
    "        #print(features)\n",
    "        #print(label_fd)\n",
    "        label=np.array(label_fd)\n",
    "        label=label[:,1:]\n",
    "        label=one_hot(label,n_class)\n",
    "        self.label=label\n",
    "        print(\"label:\",label)\n",
    "        print(\"features:\",self.features)\n",
    "        #print(label)\n",
    "        # convert labels\n",
    "        \"\"\"if config.loss_type == 'square_loss':\n",
    "            self.label.append(np.float32(items[0]))\n",
    "        else: # log_loss\n",
    "            label = 1 if float(items[0]) > 0 else 0\n",
    "            self.label.append(label)\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        assert all(len(item) == len(self.features[0]\n",
    "            ) for item in self.features), 'features are of different length'\n",
    "        \"\"\"\n",
    "        #print(len(self.features))\n",
    "        #print(len(self.feature_values))\n",
    "        #print(len(self.label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.label[idx]\n",
    "        features = self.features[idx]\n",
    "        #feature_values = self.feature_values[idx]\n",
    "        #return features, feature_values, label\n",
    "        return features,  label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70ada5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row: 0 label: [0]\n",
      "row: 1 label: [0]\n",
      "row: 2 label: [0]\n",
      "row: 3 label: [0]\n",
      "row: 4 label: [0]\n",
      "row: 5 label: [0]\n",
      "row: 6 label: [0]\n",
      "row: 7 label: [0]\n",
      "row: 8 label: [0]\n",
      "row: 9 label: [0]\n",
      "row: 10 label: [0]\n",
      "row: 11 label: [0]\n",
      "row: 12 label: [0]\n",
      "row: 13 label: [0]\n",
      "row: 14 label: [0]\n",
      "row: 15 label: [0]\n",
      "row: 16 label: [0]\n",
      "row: 17 label: [0]\n",
      "row: 18 label: [0]\n",
      "row: 19 label: [0]\n",
      "row: 20 label: [0]\n",
      "row: 21 label: [0]\n",
      "row: 22 label: [0]\n",
      "row: 23 label: [0]\n",
      "row: 24 label: [0]\n",
      "row: 25 label: [0]\n",
      "row: 26 label: [0]\n",
      "row: 27 label: [0]\n",
      "row: 28 label: [0]\n",
      "row: 29 label: [0]\n",
      "row: 30 label: [0]\n",
      "row: 31 label: [0]\n",
      "row: 32 label: [0]\n",
      "row: 33 label: [0]\n",
      "row: 34 label: [0]\n",
      "row: 35 label: [0]\n",
      "row: 36 label: [0]\n",
      "row: 37 label: [0]\n",
      "row: 38 label: [0]\n",
      "row: 39 label: [0]\n",
      "row: 40 label: [0]\n",
      "row: 41 label: [0]\n",
      "row: 42 label: [0]\n",
      "row: 43 label: [0]\n",
      "row: 44 label: [0]\n",
      "row: 45 label: [0]\n",
      "row: 46 label: [0]\n",
      "row: 47 label: [0]\n",
      "row: 48 label: [0]\n",
      "row: 49 label: [0]\n",
      "row: 50 label: [0]\n",
      "row: 51 label: [0]\n",
      "row: 52 label: [0]\n",
      "row: 53 label: [0]\n",
      "row: 54 label: [0]\n",
      "row: 55 label: [0]\n",
      "row: 56 label: [0]\n",
      "row: 57 label: [1]\n",
      "row: 58 label: [1]\n",
      "row: 59 label: [1]\n",
      "row: 60 label: [1]\n",
      "row: 61 label: [2]\n",
      "row: 62 label: [2]\n",
      "row: 63 label: [2]\n",
      "row: 64 label: [2]\n",
      "row: 65 label: [2]\n",
      "row: 66 label: [2]\n",
      "row: 67 label: [2]\n",
      "row: 68 label: [2]\n",
      "row: 69 label: [2]\n",
      "row: 70 label: [2]\n",
      "row: 71 label: [2]\n",
      "row: 72 label: [2]\n",
      "row: 73 label: [2]\n",
      "row: 74 label: [2]\n",
      "row: 75 label: [2]\n",
      "row: 76 label: [2]\n",
      "row: 77 label: [2]\n",
      "row: 78 label: [2]\n",
      "row: 79 label: [2]\n",
      "row: 80 label: [2]\n",
      "row: 81 label: [2]\n",
      "row: 82 label: [2]\n",
      "row: 83 label: [2]\n",
      "row: 84 label: [2]\n",
      "row: 85 label: [2]\n",
      "row: 86 label: [2]\n",
      "row: 87 label: [2]\n",
      "row: 88 label: [2]\n",
      "row: 89 label: [2]\n",
      "row: 90 label: [2]\n",
      "row: 91 label: [2]\n",
      "row: 92 label: [2]\n",
      "row: 93 label: [2]\n",
      "row: 94 label: [2]\n",
      "row: 95 label: [2]\n",
      "row: 96 label: [3]\n",
      "row: 97 label: [3]\n",
      "row: 98 label: [3]\n",
      "row: 99 label: [3]\n",
      "row: 100 label: [3]\n",
      "row: 101 label: [3]\n",
      "row: 102 label: [3]\n",
      "row: 103 label: [3]\n",
      "row: 104 label: [3]\n",
      "row: 105 label: [3]\n",
      "row: 106 label: [3]\n",
      "row: 107 label: [3]\n",
      "row: 108 label: [3]\n",
      "row: 109 label: [3]\n",
      "row: 110 label: [3]\n",
      "row: 111 label: [3]\n",
      "row: 112 label: [3]\n",
      "row: 113 label: [3]\n",
      "row: 114 label: [3]\n",
      "row: 115 label: [3]\n",
      "row: 116 label: [3]\n",
      "row: 117 label: [3]\n",
      "row: 118 label: [3]\n",
      "row: 119 label: [3]\n",
      "row: 120 label: [3]\n",
      "row: 121 label: [3]\n",
      "row: 122 label: [3]\n",
      "row: 123 label: [3]\n",
      "row: 124 label: [3]\n",
      "row: 125 label: [3]\n",
      "row: 126 label: [3]\n",
      "row: 127 label: [3]\n",
      "row: 128 label: [3]\n",
      "row: 129 label: [3]\n",
      "row: 130 label: [3]\n",
      "row: 131 label: [3]\n",
      "row: 132 label: [3]\n",
      "row: 133 label: [3]\n",
      "row: 134 label: [3]\n",
      "row: 135 label: [3]\n",
      "row: 136 label: [3]\n",
      "row: 137 label: [3]\n",
      "row: 138 label: [3]\n",
      "row: 139 label: [3]\n",
      "row: 140 label: [3]\n",
      "row: 141 label: [3]\n",
      "row: 142 label: [3]\n",
      "row: 143 label: [3]\n",
      "row: 144 label: [3]\n",
      "row: 145 label: [3]\n",
      "row: 146 label: [3]\n",
      "row: 147 label: [3]\n",
      "row: 148 label: [3]\n",
      "row: 149 label: [3]\n",
      "row: 150 label: [3]\n",
      "row: 151 label: [3]\n",
      "row: 152 label: [3]\n",
      "row: 153 label: [3]\n",
      "row: 154 label: [3]\n",
      "row: 155 label: [3]\n",
      "row: 156 label: [3]\n",
      "row: 157 label: [3]\n",
      "row: 158 label: [3]\n",
      "row: 159 label: [3]\n",
      "row: 160 label: [3]\n",
      "row: 161 label: [3]\n",
      "row: 162 label: [3]\n",
      "row: 163 label: [3]\n",
      "row: 164 label: [3]\n",
      "row: 165 label: [3]\n",
      "row: 166 label: [3]\n",
      "row: 167 label: [3]\n",
      "row: 168 label: [3]\n",
      "row: 169 label: [3]\n",
      "row: 170 label: [3]\n",
      "row: 171 label: [3]\n",
      "row: 172 label: [3]\n",
      "row: 173 label: [3]\n",
      "row: 174 label: [3]\n",
      "row: 175 label: [3]\n",
      "row: 176 label: [3]\n",
      "row: 177 label: [3]\n",
      "row: 178 label: [3]\n",
      "row: 179 label: [3]\n",
      "row: 180 label: [3]\n",
      "row: 181 label: [3]\n",
      "row: 182 label: [3]\n",
      "row: 183 label: [3]\n",
      "row: 184 label: [3]\n",
      "row: 185 label: [3]\n",
      "row: 186 label: [3]\n",
      "row: 187 label: [3]\n",
      "row: 188 label: [3]\n",
      "row: 189 label: [3]\n",
      "row: 190 label: [3]\n",
      "row: 191 label: [3]\n",
      "row: 192 label: [3]\n",
      "row: 193 label: [3]\n",
      "row: 194 label: [3]\n",
      "row: 195 label: [3]\n",
      "row: 196 label: [3]\n",
      "row: 197 label: [3]\n",
      "row: 198 label: [3]\n",
      "row: 199 label: [3]\n",
      "row: 200 label: [3]\n",
      "row: 201 label: [3]\n",
      "row: 202 label: [3]\n",
      "row: 203 label: [3]\n",
      "row: 204 label: [3]\n",
      "row: 205 label: [3]\n",
      "row: 206 label: [3]\n",
      "row: 207 label: [3]\n",
      "row: 208 label: [3]\n",
      "row: 209 label: [3]\n",
      "row: 210 label: [3]\n",
      "row: 211 label: [3]\n",
      "row: 212 label: [3]\n",
      "row: 213 label: [3]\n",
      "row: 214 label: [3]\n",
      "row: 215 label: [3]\n",
      "row: 216 label: [3]\n",
      "row: 217 label: [3]\n",
      "row: 218 label: [3]\n",
      "row: 219 label: [3]\n",
      "row: 220 label: [3]\n",
      "row: 221 label: [3]\n",
      "row: 222 label: [3]\n",
      "row: 223 label: [3]\n",
      "row: 224 label: [3]\n",
      "row: 225 label: [3]\n",
      "row: 226 label: [3]\n",
      "row: 227 label: [3]\n",
      "row: 228 label: [3]\n",
      "row: 229 label: [3]\n",
      "row: 230 label: [3]\n",
      "row: 231 label: [3]\n",
      "row: 232 label: [3]\n",
      "row: 233 label: [3]\n",
      "row: 234 label: [3]\n",
      "row: 235 label: [3]\n",
      "row: 236 label: [3]\n",
      "row: 237 label: [3]\n",
      "row: 238 label: [3]\n",
      "row: 239 label: [3]\n",
      "row: 240 label: [3]\n",
      "row: 241 label: [3]\n",
      "row: 242 label: [3]\n",
      "row: 243 label: [3]\n",
      "row: 244 label: [3]\n",
      "row: 245 label: [3]\n",
      "row: 246 label: [3]\n",
      "row: 247 label: [3]\n",
      "row: 248 label: [3]\n",
      "row: 249 label: [3]\n",
      "row: 250 label: [3]\n",
      "row: 251 label: [3]\n",
      "row: 252 label: [3]\n",
      "row: 253 label: [3]\n",
      "row: 254 label: [3]\n",
      "row: 255 label: [3]\n",
      "row: 256 label: [3]\n",
      "row: 257 label: [3]\n",
      "row: 258 label: [3]\n",
      "row: 259 label: [3]\n",
      "row: 260 label: [3]\n",
      "row: 261 label: [3]\n",
      "row: 262 label: [3]\n",
      "row: 263 label: [3]\n",
      "row: 264 label: [3]\n",
      "row: 265 label: [3]\n",
      "row: 266 label: [3]\n",
      "row: 267 label: [3]\n",
      "row: 268 label: [3]\n",
      "row: 269 label: [3]\n",
      "row: 270 label: [3]\n",
      "row: 271 label: [3]\n",
      "row: 272 label: [3]\n",
      "row: 273 label: [3]\n",
      "row: 274 label: [4]\n",
      "row: 275 label: [4]\n",
      "row: 276 label: [4]\n",
      "row: 277 label: [4]\n",
      "row: 278 label: [4]\n",
      "row: 279 label: [4]\n",
      "row: 280 label: [4]\n",
      "row: 281 label: [4]\n",
      "row: 282 label: [5]\n",
      "row: 283 label: [5]\n",
      "row: 284 label: [5]\n",
      "row: 285 label: [5]\n",
      "row: 286 label: [5]\n",
      "row: 287 label: [5]\n",
      "row: 288 label: [5]\n",
      "row: 289 label: [5]\n",
      "row: 290 label: [5]\n",
      "row: 291 label: [5]\n",
      "row: 292 label: [5]\n",
      "row: 293 label: [5]\n",
      "row: 294 label: [5]\n",
      "row: 295 label: [5]\n",
      "row: 296 label: [5]\n",
      "row: 297 label: [5]\n",
      "row: 298 label: [5]\n",
      "row: 299 label: [5]\n",
      "row: 300 label: [5]\n",
      "row: 301 label: [5]\n",
      "row: 302 label: [5]\n",
      "row: 303 label: [5]\n",
      "row: 304 label: [5]\n",
      "row: 305 label: [5]\n",
      "row: 306 label: [5]\n",
      "row: 307 label: [5]\n",
      "row: 308 label: [5]\n",
      "row: 309 label: [5]\n",
      "row: 310 label: [5]\n",
      "row: 311 label: [5]\n",
      "row: 312 label: [5]\n",
      "row: 313 label: [5]\n",
      "row: 314 label: [5]\n",
      "row: 315 label: [5]\n",
      "row: 316 label: [5]\n",
      "row: 317 label: [5]\n",
      "row: 318 label: [5]\n",
      "row: 319 label: [5]\n",
      "row: 320 label: [5]\n",
      "row: 321 label: [5]\n",
      "row: 322 label: [5]\n",
      "row: 323 label: [6]\n",
      "row: 324 label: [6]\n",
      "row: 325 label: [6]\n",
      "row: 326 label: [6]\n",
      "row: 327 label: [6]\n",
      "row: 328 label: [6]\n",
      "row: 329 label: [6]\n",
      "row: 330 label: [6]\n",
      "row: 331 label: [6]\n",
      "row: 332 label: [6]\n",
      "row: 333 label: [6]\n",
      "row: 334 label: [6]\n",
      "row: 335 label: [6]\n",
      "row: 336 label: [6]\n",
      "row: 337 label: [6]\n",
      "row: 338 label: [6]\n",
      "row: 339 label: [6]\n",
      "row: 340 label: [6]\n",
      "row: 341 label: [6]\n",
      "row: 342 label: [6]\n",
      "row: 343 label: [6]\n",
      "row: 344 label: [6]\n",
      "row: 345 label: [6]\n",
      "row: 346 label: [6]\n",
      "row: 347 label: [6]\n",
      "row: 348 label: [6]\n",
      "row: 349 label: [6]\n",
      "row: 350 label: [6]\n",
      "row: 351 label: [6]\n",
      "row: 352 label: [6]\n",
      "row: 353 label: [6]\n",
      "row: 354 label: [6]\n",
      "row: 355 label: [6]\n",
      "row: 356 label: [6]\n",
      "row: 357 label: [6]\n",
      "row: 358 label: [6]\n",
      "row: 359 label: [6]\n",
      "row: 360 label: [6]\n",
      "row: 361 label: [6]\n",
      "row: 362 label: [6]\n",
      "row: 363 label: [6]\n",
      "row: 364 label: [6]\n",
      "row: 365 label: [6]\n",
      "row: 366 label: [6]\n",
      "row: 367 label: [6]\n",
      "row: 368 label: [6]\n",
      "row: 369 label: [6]\n",
      "row: 370 label: [6]\n",
      "row: 371 label: [6]\n",
      "row: 372 label: [6]\n",
      "row: 373 label: [6]\n",
      "row: 374 label: [6]\n",
      "row: 375 label: [6]\n",
      "row: 376 label: [6]\n",
      "row: 377 label: [6]\n",
      "row: 378 label: [6]\n",
      "row: 379 label: [6]\n",
      "row: 380 label: [6]\n",
      "row: 381 label: [6]\n",
      "row: 382 label: [6]\n",
      "row: 383 label: [6]\n",
      "row: 384 label: [6]\n",
      "row: 385 label: [6]\n",
      "row: 386 label: [6]\n",
      "row: 387 label: [6]\n",
      "row: 388 label: [6]\n",
      "row: 389 label: [6]\n",
      "row: 390 label: [6]\n",
      "row: 391 label: [6]\n",
      "row: 392 label: [6]\n",
      "row: 393 label: [6]\n",
      "row: 394 label: [6]\n",
      "row: 395 label: [6]\n",
      "row: 396 label: [6]\n",
      "row: 397 label: [6]\n",
      "row: 398 label: [6]\n",
      "row: 399 label: [7]\n",
      "row: 400 label: [7]\n",
      "row: 401 label: [7]\n",
      "row: 402 label: [7]\n",
      "row: 403 label: [7]\n",
      "row: 404 label: [7]\n",
      "row: 405 label: [7]\n",
      "row: 406 label: [7]\n",
      "row: 407 label: [7]\n",
      "row: 408 label: [7]\n",
      "row: 409 label: [7]\n",
      "row: 410 label: [7]\n",
      "row: 411 label: [7]\n",
      "row: 412 label: [7]\n",
      "row: 413 label: [7]\n",
      "row: 414 label: [7]\n",
      "row: 415 label: [7]\n",
      "row: 416 label: [7]\n",
      "row: 417 label: [7]\n",
      "row: 418 label: [7]\n",
      "row: 419 label: [7]\n",
      "row: 420 label: [8]\n",
      "row: 421 label: [8]\n",
      "row: 422 label: [8]\n",
      "row: 423 label: [8]\n",
      "row: 424 label: [8]\n",
      "row: 425 label: [8]\n",
      "row: 426 label: [8]\n",
      "row: 427 label: [8]\n",
      "row: 428 label: [8]\n",
      "row: 429 label: [8]\n",
      "row: 430 label: [8]\n",
      "row: 431 label: [8]\n",
      "row: 432 label: [8]\n",
      "row: 433 label: [8]\n",
      "row: 434 label: [8]\n",
      "row: 435 label: [8]\n",
      "row: 436 label: [8]\n",
      "row: 437 label: [8]\n",
      "row: 438 label: [8]\n",
      "row: 439 label: [8]\n",
      "row: 440 label: [8]\n",
      "row: 441 label: [8]\n",
      "row: 442 label: [8]\n",
      "row: 443 label: [8]\n",
      "row: 444 label: [8]\n",
      "row: 445 label: [8]\n",
      "row: 446 label: [8]\n",
      "row: 447 label: [8]\n",
      "row: 448 label: [8]\n",
      "row: 449 label: [8]\n",
      "row: 450 label: [8]\n",
      "row: 451 label: [8]\n",
      "row: 452 label: [8]\n",
      "row: 453 label: [8]\n",
      "row: 454 label: [8]\n",
      "row: 455 label: [8]\n",
      "row: 456 label: [8]\n",
      "row: 457 label: [8]\n",
      "row: 458 label: [8]\n",
      "row: 459 label: [8]\n",
      "row: 460 label: [8]\n",
      "row: 461 label: [8]\n",
      "row: 462 label: [8]\n",
      "row: 463 label: [8]\n",
      "row: 464 label: [8]\n",
      "row: 465 label: [8]\n",
      "row: 466 label: [8]\n",
      "row: 467 label: [8]\n",
      "row: 468 label: [8]\n",
      "row: 469 label: [8]\n",
      "row: 470 label: [8]\n",
      "row: 471 label: [8]\n",
      "row: 472 label: [8]\n",
      "row: 473 label: [8]\n",
      "row: 474 label: [8]\n",
      "row: 475 label: [8]\n",
      "row: 476 label: [8]\n",
      "row: 477 label: [8]\n",
      "row: 478 label: [8]\n",
      "row: 479 label: [8]\n",
      "row: 480 label: [8]\n",
      "row: 481 label: [8]\n",
      "row: 482 label: [8]\n",
      "row: 483 label: [8]\n",
      "row: 484 label: [8]\n",
      "row: 485 label: [8]\n",
      "row: 486 label: [8]\n",
      "row: 487 label: [8]\n",
      "row: 488 label: [8]\n",
      "row: 489 label: [8]\n",
      "row: 490 label: [8]\n",
      "row: 491 label: [8]\n",
      "row: 492 label: [8]\n",
      "row: 493 label: [8]\n",
      "row: 494 label: [8]\n",
      "row: 495 label: [8]\n",
      "row: 496 label: [8]\n",
      "row: 497 label: [8]\n",
      "row: 498 label: [8]\n",
      "row: 499 label: [8]\n",
      "row: 500 label: [8]\n",
      "row: 501 label: [8]\n",
      "row: 502 label: [8]\n",
      "row: 503 label: [8]\n",
      "row: 504 label: [8]\n",
      "row: 505 label: [8]\n",
      "row: 506 label: [8]\n",
      "row: 507 label: [8]\n",
      "row: 508 label: [8]\n",
      "row: 509 label: [8]\n",
      "row: 510 label: [8]\n",
      "row: 511 label: [8]\n",
      "row: 512 label: [8]\n",
      "row: 513 label: [8]\n",
      "row: 514 label: [8]\n",
      "row: 515 label: [8]\n",
      "row: 516 label: [8]\n",
      "row: 517 label: [8]\n",
      "row: 518 label: [8]\n",
      "row: 519 label: [8]\n",
      "row: 520 label: [8]\n",
      "row: 521 label: [8]\n",
      "row: 522 label: [8]\n",
      "row: 523 label: [8]\n",
      "row: 524 label: [8]\n",
      "row: 525 label: [8]\n",
      "row: 526 label: [8]\n",
      "row: 527 label: [8]\n",
      "row: 528 label: [8]\n",
      "row: 529 label: [8]\n",
      "row: 530 label: [8]\n",
      "row: 531 label: [8]\n",
      "row: 532 label: [8]\n",
      "row: 533 label: [8]\n",
      "row: 534 label: [8]\n",
      "row: 535 label: [8]\n",
      "row: 536 label: [9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row: 537 label: [9]\n",
      "row: 538 label: [9]\n",
      "row: 539 label: [9]\n",
      "row: 540 label: [9]\n",
      "row: 541 label: [9]\n",
      "row: 542 label: [9]\n",
      "row: 543 label: [9]\n",
      "row: 544 label: [9]\n",
      "row: 545 label: [9]\n",
      "row: 546 label: [9]\n",
      "row: 547 label: [9]\n",
      "row: 548 label: [9]\n",
      "row: 549 label: [9]\n",
      "row: 550 label: [9]\n",
      "row: 551 label: [9]\n",
      "row: 552 label: [9]\n",
      "row: 553 label: [9]\n",
      "row: 554 label: [9]\n",
      "row: 555 label: [9]\n",
      "row: 556 label: [9]\n",
      "row: 557 label: [9]\n",
      "row: 558 label: [9]\n",
      "row: 559 label: [9]\n",
      "row: 560 label: [9]\n",
      "row: 561 label: [9]\n",
      "row: 562 label: [9]\n",
      "row: 563 label: [9]\n",
      "row: 564 label: [9]\n",
      "row: 565 label: [9]\n",
      "row: 566 label: [9]\n",
      "row: 567 label: [9]\n",
      "row: 568 label: [9]\n",
      "row: 569 label: [9]\n",
      "row: 570 label: [9]\n",
      "row: 571 label: [9]\n",
      "row: 572 label: [9]\n",
      "row: 573 label: [9]\n",
      "row: 574 label: [9]\n",
      "row: 575 label: [9]\n",
      "row: 576 label: [9]\n",
      "row: 577 label: [9]\n",
      "row: 578 label: [9]\n",
      "row: 579 label: [9]\n",
      "row: 580 label: [9]\n",
      "row: 581 label: [9]\n",
      "row: 582 label: [9]\n",
      "row: 583 label: [9]\n",
      "row: 584 label: [9]\n",
      "row: 585 label: [9]\n",
      "row: 586 label: [9]\n",
      "row: 587 label: [9]\n",
      "row: 588 label: [9]\n",
      "row: 589 label: [9]\n",
      "row: 590 label: [9]\n",
      "row: 591 label: [9]\n",
      "row: 592 label: [9]\n",
      "row: 593 label: [10]\n",
      "row: 594 label: [10]\n",
      "row: 595 label: [10]\n",
      "row: 596 label: [10]\n",
      "row: 597 label: [10]\n",
      "row: 598 label: [10]\n",
      "row: 599 label: [10]\n",
      "row: 600 label: [10]\n",
      "row: 601 label: [10]\n",
      "row: 602 label: [10]\n",
      "row: 603 label: [10]\n",
      "row: 604 label: [10]\n",
      "row: 605 label: [10]\n",
      "row: 606 label: [10]\n",
      "row: 607 label: [10]\n",
      "row: 608 label: [10]\n",
      "row: 609 label: [10]\n",
      "row: 610 label: [10]\n",
      "row: 611 label: [10]\n",
      "row: 612 label: [10]\n",
      "row: 613 label: [10]\n",
      "row: 614 label: [10]\n",
      "row: 615 label: [10]\n",
      "row: 616 label: [10]\n",
      "row: 617 label: [10]\n",
      "row: 618 label: [10]\n",
      "row: 619 label: [10]\n",
      "row: 620 label: [10]\n",
      "row: 621 label: [10]\n",
      "row: 622 label: [10]\n",
      "row: 623 label: [10]\n",
      "row: 624 label: [10]\n",
      "row: 625 label: [10]\n",
      "row: 626 label: [10]\n",
      "row: 627 label: [10]\n",
      "row: 628 label: [10]\n",
      "row: 629 label: [10]\n",
      "row: 630 label: [10]\n",
      "row: 631 label: [10]\n",
      "row: 632 label: [10]\n",
      "row: 633 label: [10]\n",
      "row: 634 label: [10]\n",
      "row: 635 label: [10]\n",
      "row: 636 label: [10]\n",
      "row: 637 label: [10]\n",
      "row: 638 label: [10]\n",
      "row: 639 label: [10]\n",
      "row: 640 label: [10]\n",
      "row: 641 label: [10]\n",
      "row: 642 label: [10]\n",
      "row: 643 label: [10]\n",
      "row: 644 label: [10]\n",
      "row: 645 label: [10]\n",
      "row: 646 label: [10]\n",
      "row: 647 label: [10]\n",
      "row: 648 label: [10]\n",
      "row: 649 label: [10]\n",
      "row: 650 label: [10]\n",
      "row: 651 label: [10]\n",
      "row: 652 label: [10]\n",
      "row: 653 label: [10]\n",
      "row: 654 label: [10]\n",
      "row: 655 label: [10]\n",
      "row: 656 label: [10]\n",
      "row: 657 label: [10]\n",
      "row: 658 label: [10]\n",
      "row: 659 label: [10]\n",
      "row: 660 label: [10]\n",
      "row: 661 label: [10]\n",
      "row: 662 label: [10]\n",
      "row: 663 label: [10]\n",
      "row: 664 label: [10]\n",
      "row: 665 label: [10]\n",
      "row: 666 label: [10]\n",
      "row: 667 label: [10]\n",
      "row: 668 label: [10]\n",
      "row: 669 label: [10]\n",
      "row: 670 label: [10]\n",
      "row: 671 label: [10]\n",
      "row: 672 label: [10]\n",
      "row: 673 label: [10]\n",
      "row: 674 label: [10]\n",
      "row: 675 label: [10]\n",
      "row: 676 label: [10]\n",
      "row: 677 label: [10]\n",
      "row: 678 label: [10]\n",
      "row: 679 label: [10]\n",
      "row: 680 label: [10]\n",
      "row: 681 label: [10]\n",
      "row: 682 label: [10]\n",
      "row: 683 label: [10]\n",
      "row: 684 label: [10]\n",
      "row: 685 label: [10]\n",
      "row: 686 label: [10]\n",
      "row: 687 label: [10]\n",
      "row: 688 label: [10]\n",
      "row: 689 label: [10]\n",
      "row: 690 label: [10]\n",
      "row: 691 label: [10]\n",
      "row: 692 label: [10]\n",
      "row: 693 label: [11]\n",
      "row: 694 label: [11]\n",
      "row: 695 label: [11]\n",
      "row: 696 label: [11]\n",
      "row: 697 label: [11]\n",
      "row: 698 label: [11]\n",
      "row: 699 label: [11]\n",
      "row: 700 label: [11]\n",
      "row: 701 label: [11]\n",
      "row: 702 label: [11]\n",
      "row: 703 label: [11]\n",
      "row: 704 label: [11]\n",
      "row: 705 label: [11]\n",
      "row: 706 label: [11]\n",
      "row: 707 label: [11]\n",
      "row: 708 label: [11]\n",
      "row: 709 label: [11]\n",
      "row: 710 label: [11]\n",
      "row: 711 label: [11]\n",
      "row: 712 label: [11]\n",
      "row: 713 label: [11]\n",
      "row: 714 label: [11]\n",
      "row: 715 label: [11]\n",
      "row: 716 label: [11]\n",
      "row: 717 label: [11]\n",
      "row: 718 label: [11]\n",
      "row: 719 label: [11]\n",
      "row: 720 label: [11]\n",
      "row: 721 label: [11]\n",
      "row: 722 label: [11]\n",
      "row: 723 label: [11]\n",
      "row: 724 label: [11]\n",
      "row: 725 label: [11]\n",
      "row: 726 label: [11]\n",
      "row: 727 label: [11]\n",
      "row: 728 label: [11]\n",
      "row: 729 label: [11]\n",
      "row: 730 label: [11]\n",
      "row: 731 label: [11]\n",
      "row: 732 label: [11]\n",
      "row: 733 label: [11]\n",
      "row: 734 label: [11]\n",
      "row: 735 label: [11]\n",
      "row: 736 label: [11]\n",
      "row: 737 label: [11]\n",
      "row: 738 label: [11]\n",
      "row: 739 label: [11]\n",
      "row: 740 label: [11]\n",
      "row: 741 label: [11]\n",
      "row: 742 label: [11]\n",
      "row: 743 label: [11]\n",
      "label: [[0.8125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " [0.8125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " [0.8125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " ...\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]]\n",
      "features: [[220 490 450 ... 360 370 420]\n",
      " [220 500 460 ... 340 260 420]\n",
      " [300 500 490 ... 330 350 390]\n",
      " ...\n",
      " [200 490 500 ... 400 360 370]\n",
      " [190 490 480 ... 320 360 370]\n",
      " [240 510 490 ... 360 340 400]]\n",
      "row: 0 label: [11]\n",
      "row: 1 label: [11]\n",
      "row: 2 label: [11]\n",
      "row: 3 label: [11]\n",
      "row: 4 label: [11]\n",
      "row: 5 label: [11]\n",
      "row: 6 label: [11]\n",
      "row: 7 label: [11]\n",
      "row: 8 label: [11]\n",
      "row: 9 label: [11]\n",
      "row: 10 label: [11]\n",
      "row: 11 label: [11]\n",
      "row: 12 label: [11]\n",
      "row: 13 label: [11]\n",
      "row: 14 label: [11]\n",
      "row: 15 label: [11]\n",
      "row: 16 label: [11]\n",
      "row: 17 label: [11]\n",
      "row: 18 label: [11]\n",
      "row: 19 label: [11]\n",
      "row: 20 label: [11]\n",
      "row: 21 label: [11]\n",
      "row: 22 label: [11]\n",
      "row: 23 label: [11]\n",
      "row: 24 label: [11]\n",
      "row: 25 label: [11]\n",
      "row: 26 label: [11]\n",
      "row: 27 label: [11]\n",
      "row: 28 label: [11]\n",
      "row: 29 label: [11]\n",
      "row: 30 label: [11]\n",
      "row: 31 label: [11]\n",
      "row: 32 label: [11]\n",
      "row: 33 label: [11]\n",
      "row: 34 label: [11]\n",
      "row: 35 label: [11]\n",
      "row: 36 label: [11]\n",
      "row: 37 label: [11]\n",
      "row: 38 label: [11]\n",
      "row: 39 label: [11]\n",
      "row: 40 label: [11]\n",
      "row: 41 label: [11]\n",
      "row: 42 label: [11]\n",
      "row: 43 label: [11]\n",
      "row: 44 label: [11]\n",
      "row: 45 label: [11]\n",
      "row: 46 label: [11]\n",
      "row: 47 label: [11]\n",
      "row: 48 label: [11]\n",
      "row: 49 label: [11]\n",
      "row: 50 label: [11]\n",
      "row: 51 label: [11]\n",
      "row: 52 label: [11]\n",
      "row: 53 label: [11]\n",
      "row: 54 label: [11]\n",
      "row: 55 label: [11]\n",
      "row: 56 label: [11]\n",
      "row: 57 label: [11]\n",
      "row: 58 label: [11]\n",
      "row: 59 label: [11]\n",
      "row: 60 label: [11]\n",
      "row: 61 label: [11]\n",
      "row: 62 label: [11]\n",
      "row: 63 label: [11]\n",
      "row: 64 label: [11]\n",
      "row: 65 label: [11]\n",
      "row: 66 label: [11]\n",
      "row: 67 label: [11]\n",
      "row: 68 label: [11]\n",
      "row: 69 label: [11]\n",
      "row: 70 label: [11]\n",
      "row: 71 label: [12]\n",
      "row: 72 label: [12]\n",
      "row: 73 label: [12]\n",
      "row: 74 label: [12]\n",
      "row: 75 label: [12]\n",
      "row: 76 label: [12]\n",
      "row: 77 label: [12]\n",
      "row: 78 label: [12]\n",
      "row: 79 label: [12]\n",
      "row: 80 label: [12]\n",
      "row: 81 label: [12]\n",
      "row: 82 label: [12]\n",
      "row: 83 label: [12]\n",
      "row: 84 label: [12]\n",
      "row: 85 label: [12]\n",
      "row: 86 label: [12]\n",
      "row: 87 label: [12]\n",
      "row: 88 label: [12]\n",
      "row: 89 label: [12]\n",
      "row: 90 label: [12]\n",
      "row: 91 label: [12]\n",
      "row: 92 label: [12]\n",
      "row: 93 label: [12]\n",
      "row: 94 label: [12]\n",
      "row: 95 label: [12]\n",
      "row: 96 label: [12]\n",
      "row: 97 label: [12]\n",
      "row: 98 label: [12]\n",
      "row: 99 label: [12]\n",
      "label: [[0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " ...\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]]\n",
      "features: [[290 480 480 ... 420 300 390]\n",
      " [160 470 490 ... 340 310 380]\n",
      " [190 450 490 ... 360 390 410]\n",
      " ...\n",
      " [370 540 310 ... 450 380 510]\n",
      " [340 530 310 ... 450 370 480]\n",
      " [360 530 310 ... 450 360 510]]\n",
      "row: 0 label: [12]\n",
      "row: 1 label: [12]\n",
      "row: 2 label: [12]\n",
      "row: 3 label: [12]\n",
      "row: 4 label: [12]\n",
      "row: 5 label: [12]\n",
      "row: 6 label: [12]\n",
      "row: 7 label: [12]\n",
      "row: 8 label: [12]\n",
      "row: 9 label: [12]\n",
      "row: 10 label: [12]\n",
      "row: 11 label: [12]\n",
      "row: 12 label: [12]\n",
      "row: 13 label: [12]\n",
      "row: 14 label: [12]\n",
      "row: 15 label: [12]\n",
      "row: 16 label: [12]\n",
      "row: 17 label: [12]\n",
      "row: 18 label: [12]\n",
      "row: 19 label: [12]\n",
      "row: 20 label: [12]\n",
      "row: 21 label: [12]\n",
      "row: 22 label: [12]\n",
      "row: 23 label: [12]\n",
      "row: 24 label: [12]\n",
      "row: 25 label: [12]\n",
      "row: 26 label: [12]\n",
      "row: 27 label: [12]\n",
      "row: 28 label: [13]\n",
      "row: 29 label: [13]\n",
      "row: 30 label: [13]\n",
      "row: 31 label: [13]\n",
      "row: 32 label: [13]\n",
      "row: 33 label: [13]\n",
      "row: 34 label: [13]\n",
      "row: 35 label: [13]\n",
      "row: 36 label: [13]\n",
      "row: 37 label: [13]\n",
      "row: 38 label: [13]\n",
      "row: 39 label: [13]\n",
      "row: 40 label: [13]\n",
      "row: 41 label: [13]\n",
      "row: 42 label: [13]\n",
      "row: 43 label: [13]\n",
      "row: 44 label: [13]\n",
      "row: 45 label: [13]\n",
      "row: 46 label: [13]\n",
      "row: 47 label: [13]\n",
      "row: 48 label: [13]\n",
      "row: 49 label: [13]\n",
      "row: 50 label: [13]\n",
      "row: 51 label: [13]\n",
      "row: 52 label: [13]\n",
      "row: 53 label: [13]\n",
      "row: 54 label: [13]\n",
      "row: 55 label: [13]\n",
      "row: 56 label: [13]\n",
      "row: 57 label: [13]\n",
      "row: 58 label: [13]\n",
      "row: 59 label: [13]\n",
      "row: 60 label: [14]\n",
      "row: 61 label: [14]\n",
      "row: 62 label: [14]\n",
      "row: 63 label: [14]\n",
      "row: 64 label: [14]\n",
      "row: 65 label: [14]\n",
      "row: 66 label: [14]\n",
      "row: 67 label: [14]\n",
      "row: 68 label: [14]\n",
      "row: 69 label: [14]\n",
      "row: 70 label: [14]\n",
      "row: 71 label: [14]\n",
      "row: 72 label: [14]\n",
      "row: 73 label: [14]\n",
      "row: 74 label: [14]\n",
      "row: 75 label: [15]\n",
      "row: 76 label: [15]\n",
      "row: 77 label: [15]\n",
      "row: 78 label: [15]\n",
      "row: 79 label: [15]\n",
      "row: 80 label: [15]\n",
      "row: 81 label: [15]\n",
      "row: 82 label: [15]\n",
      "row: 83 label: [15]\n",
      "row: 84 label: [15]\n",
      "row: 85 label: [15]\n",
      "row: 86 label: [15]\n",
      "row: 87 label: [15]\n",
      "row: 88 label: [15]\n",
      "row: 89 label: [15]\n",
      "row: 90 label: [15]\n",
      "row: 91 label: [15]\n",
      "row: 92 label: [15]\n",
      "row: 93 label: [15]\n",
      "row: 94 label: [15]\n",
      "row: 95 label: [15]\n",
      "row: 96 label: [15]\n",
      "row: 97 label: [15]\n",
      "row: 98 label: [15]\n",
      "row: 99 label: [15]\n",
      "label: [[0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " ...\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.8125]\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.8125]\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.8125]]\n",
      "features: [[360 530 310 ... 450 380 500]\n",
      " [420 500 300 ... 440 400 490]\n",
      " [360 520 310 ... 440 340 470]\n",
      " ...\n",
      " [360 500 320 ... 430 360 500]\n",
      " [350 530 320 ... 440 370 500]\n",
      " [360 540 320 ... 430 370 450]]\n"
     ]
    }
   ],
   "source": [
    "#features_map,num_features=map_features()\n",
    "#print('num_features:',num_features)\n",
    "#n_class=16\n",
    "\"\"\"\n",
    "train_dataset = FMData(config.train_libfm,config.train_label,features_map)\n",
    "train_loader = data.DataLoader(train_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "validate_dataset = FMData(config.valid_libfm,config.valid_label,features_map)\n",
    "validate_loader = data.DataLoader(train_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "test_dataset = FMData(config.test_libfm,config.test_label,features_map)\n",
    "test_loader = data.DataLoader(test_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\"\"\"\n",
    "train_dataset = FMData(config.train_libfm,config.train_label)\n",
    "train_loader = data.DataLoader(train_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "validate_dataset = FMData(config.valid_libfm,config.valid_label)\n",
    "validate_loader = data.DataLoader(train_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "test_dataset = FMData(config.test_libfm,config.test_label)\n",
    "test_loader = data.DataLoader(test_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb655e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=10149, out_features=5000, bias=True)\n",
      "  (fc2): Linear(in_features=5000, out_features=1000, bias=True)\n",
      "  (fc3): Linear(in_features=1000, out_features=16, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F  # 激励函数的库\n",
    "#model definition\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(10149, 5000)\n",
    "        self.fc2 = nn.Linear(5000, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.softmax(self.fc3(x), dim=1)\n",
    "#model = MLP().cuda()\n",
    "model=MLP().cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2f970d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#from BaseModel.basemodel import BaseModel\n",
    "#import BaseModel\n",
    "from basemodel import BaseModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BiInteractionPooling(nn.Module):\n",
    "    \"\"\"Bi-Interaction Layer used in Neural FM,compress the\n",
    "      pairwise element-wise product of features into one single vector.\n",
    "      Input shape\n",
    "        - A 3D tensor with shape:``(batch_size,field_size,embedding_size)``.\n",
    "      Output shape\n",
    "    http://127.0.0.1:3000/notebooks/NFM-pyorch-master/NFM-pyorch-master/%E6%9C%AA%E5%91%BD%E5%90%8D5.ipynb?kernel_name=python3#    - 3D tensor with shape: ``(batch_size,1,embedding_size)``.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(BiInteractionPooling, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        concated_embeds_value = inputs\n",
    "        print(\"concated_embeds_value.shape:\",concated_embeds_value.shape)\n",
    "        square_of_sum = torch.pow(\n",
    "            torch.sum(concated_embeds_value, dim=1, keepdim=True), 2)\n",
    "        print(\"square_of_sum.shape:\",square_of_sum.shape)\n",
    "        sum_of_square = torch.sum(\n",
    "            concated_embeds_value * concated_embeds_value, dim=1, keepdim=True)\n",
    "        print(\"sum_of_square.shape:\",sum_of_square.shape)\n",
    "        \n",
    "        cross_term = 0.5 * (square_of_sum - sum_of_square)\n",
    "        print(\"cross_term.shape:\",cross_term.shape)\n",
    "        return cross_term\n",
    "\n",
    "class NFM(BaseModel):\n",
    "    #def __init__(self, config, dense_features_cols=[], sparse_features_cols,n_class):#=[]为新增\n",
    "    #def __init__(self, config, dense_features_cols, sparse_features_cols):\n",
    "    def __init__(self, config,dense_features_cols=[]):#=[]为新增\n",
    "        super(NFM, self).__init__(config)\n",
    "        # 稠密和稀疏特征的数量\n",
    "        #self.num_dense_feature = dense_features_cols.__len__()\n",
    "        \n",
    "        self.n_class=config['n_class']\n",
    "        self.num_dense_feature = 0#修改\n",
    "        #self.num_sparse_feature = sparse_features_cols.__len__()\n",
    "        #num_sparse_features_cols=config['num_sparse_features_cols']\n",
    "        self.num_sparse_feature=config['num_sparse_features_cols']\n",
    "        #self.num_sparse_feature = 0##修改\n",
    "        # NFM的线性部分，对应 ∑WiXi\n",
    "        #self.linear_model = nn.Linear(self.num_dense_feature + self.num_sparse_feature, 1)\n",
    "        self.linear_model = nn.Linear(self.num_dense_feature + self.num_sparse_feature, config['n_class'])##修改\n",
    "        \n",
    "        \n",
    "        # NFM的Embedding层\n",
    "        \"\"\"\n",
    "        self.embedding_layers = nn.ModuleList([\n",
    "            nn.Embedding(num_embeddings=feat_dim, embedding_dim=config['embed_dim'])\n",
    "                for feat_dim in sparse_features_cols#将每一行中的每一列中的数据进行嵌入化\n",
    "        ])\n",
    "        \"\"\"\n",
    "        self.embedding_layers=nn.Embedding(1001,config['embed_dim'])\n",
    "        \n",
    "        # B-Interaction 层\n",
    "        self.bi_pooling = BiInteractionPooling()\n",
    "        self.bi_dropout = config['bi_dropout']\n",
    "        if self.bi_dropout > 0:\n",
    "            self.dropout = nn.Dropout(self.bi_dropout)\n",
    "\n",
    "        # NFM的DNN部分\n",
    "       # self.hidden_layers = [self.num_dense_feature + config['embed_dim']] + config['dnn_hidden_units']#是加还是乘\n",
    "        self.hidden_layers = [self.num_dense_feature + self.num_sparse_feature] + config['dnn_hidden_units']#是加还是乘搁浅了好几天的问题居然出在这里，少了一个]\n",
    "        self.dnn_layers = nn.ModuleList([\n",
    "            nn.Linear(in_features=layer[0], out_features=layer[1])\\\n",
    "                for layer in list(zip(self.hidden_layers[:-1], self.hidden_layers[1:])) \n",
    "        ])\n",
    "        #self.dnn_linear = nn.Linear(self.hidden_layers[-1], 1, bias=False)\n",
    "        self.dnn_linear = nn.Linear(self.hidden_layers[-1],config['n_class'], bias=False)\n",
    "        \n",
    "        #增加\n",
    "        #self.dnn_softmax=nn.Softmax(input,dim=0) # 按列SoftMax,列和为1\n",
    "        #self.dnn_softmax=nn.Softmax(input) # 按列SoftMax,列和为1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.view(-1,self.num_sparse_feature)#新增，以适应batchsize\n",
    "        # 先区分出稀疏特征和稠密特征，这里是按照列来划分的，即所有的行都要进行筛选\n",
    "        dense_input, sparse_inputs = x[:, :self.num_dense_feature], x[:, self.num_dense_feature:]\n",
    "        print(\"spare_input:\",sparse_inputs.shape)\n",
    "        sparse_inputs = sparse_inputs.long()\n",
    "       # x=x.view(-1,self.num_sparse_feature)#新增，以适应batchsize\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 求出线性部分\n",
    "        #linear_output = self.linear_model(x)\n",
    "        #print('linear_output:',linear_output.is_cuda)\n",
    "        \n",
    "        \n",
    "        # 求出稀疏特征的embedding向量\n",
    "        \n",
    "        \"\"\"\n",
    "        sparse_embeds = [self.embedding_layers[i](sparse_inputs[:, i]) for i in range(sparse_inputs.shape[1])]\n",
    "        sparse_embeds = torch.cat(sparse_embeds, axis=-1)\n",
    "        \n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        sparse_inputs=[]\n",
    "        for i in rangege(sparse_inputs.shape[0]):\n",
    "            sparse_inputs.append(self.embedding_layers(sparse_inputs[i,:]))\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        #sparse_inputs_test=self.embedding_layers(sparse_inputs[0,:])\n",
    "        #print(\"sparse_inputs_test.shape:\",sparse_inputs_test.shape)\n",
    "        sparse_embeds = [self.embedding_layers(sparse_inputs[i, :]) for i in range(sparse_inputs.shape[0])]\n",
    "        print('sparse_embeds:',sparse_embeds)\n",
    "        #sparse_embeds=np.array(sparse_embeds)\n",
    "        #sparse_embeds=torch.tensor(sparse_embeds,dtype=float)\n",
    "        #print('sparse_embeds.shape:',sparse_embeds.shape)\n",
    "        sparse_embeds = torch.cat(sparse_embeds, axis=-1)\n",
    "        print('sparse_embeds:',sparse_embeds)\n",
    "        #print('spare_embeds:',len(sparse_embeds))\n",
    "        # 送入B-Interaction层\n",
    "        fm_input = sparse_embeds.view(-1, self.num_sparse_feature, self._config['embed_dim'])#整理成n行m列\n",
    "        #print('fm_input_bi_interaction:',fm_input)\n",
    "        #print('fm_input.shape:',fm_input.shape)\n",
    "        # print(fm_input.shape)\n",
    "        #fm_input=fm_input.view(-1,self.num_sparse_feature,self._config['embed_dim'])\n",
    "        \n",
    "        bi_out = self.bi_pooling(fm_input)\n",
    "        #print('bi_out_after_pooling:',bi_out)\n",
    "        print('bi_out.shape:',bi_out.shape)\n",
    "        if self.bi_dropout:\n",
    "            bi_out = self.dropout(bi_out)\n",
    "\n",
    "        #bi_out = bi_out.view(-1, self._config['embed_dim'])\n",
    "        #bi_out = bi_out.view(-1, self.num_sparse_feature*self._config['embed_dim'])\n",
    "        #bi_out = bi_out.view(-1, self.num_sparse_feature)\n",
    "        \n",
    "        bi_out = bi_out.view(-1, self._config['embed_dim'])\n",
    "        print(\"bi_out.shape:\",bi_out.shape)\n",
    "        # 将结果聚合起来\n",
    "        dnn_input = torch.cat((dense_input, bi_out), dim=-1)\n",
    "\n",
    "        # DNN 层\n",
    "        dnn_output = dnn_input\n",
    "        for dnn in self.dnn_layers:\n",
    "            dnn_output = dnn(dnn_output)#dnn_output为tensor\n",
    "            # dnn_output = nn.BatchNormalize(dnn_output)\n",
    "            dnn_output = torch.relu(dnn_output)\n",
    "        dnn_output = self.dnn_linear(dnn_output)\n",
    "        #dnn_pred=self.dnn_softmax(dnn_output,dim=0)#增加\n",
    "        # Final\n",
    "        #output = linear_output + dnn_output#修改\n",
    "        \n",
    "        output=dnn_output\n",
    "        #y_pred = self.dnn_softmax(output,dim=0)\n",
    "        #y_pred = self.dnn_softmax(output)\n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "19afee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#from BaseModel.basemodel import BaseModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BiInteractionPooling(nn.Module):\n",
    "    \"\"\"Bi-Interaction Layer used in Neural FM,compress the\n",
    "      pairwise element-wise product of features into one single vector.\n",
    "      Input shape\n",
    "        - A 3D tensor with shape:``(batch_size,field_size,embedding_size)``.\n",
    "      Output shape\n",
    "    http://127.0.0.1:3000/notebooks/NFM-pyorch-master/NFM-pyorch-master/%E6%9C%AA%E5%91%BD%E5%90%8D5.ipynb?kernel_name=python3#    - 3D tensor with shape: ``(batch_size,1,embedding_size)``.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(BiInteractionPooling, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        concated_embeds_value = inputs\n",
    "        square_of_sum = torch.pow(\n",
    "            torch.sum(concated_embeds_value, dim=1, keepdim=True), 2)\n",
    "        sum_of_square = torch.sum(\n",
    "            concated_embeds_value * concated_embeds_value, dim=1, keepdim=True)\n",
    "        cross_term = 0.5 * (square_of_sum - sum_of_square)\n",
    "        return cross_term\n",
    "\n",
    "class NFM(BaseModel):\n",
    "    def __init__(self, config, dense_features_cols=[]):#=[]为新增\n",
    "    #def __init__(self, config, dense_features_cols, sparse_features_cols):\n",
    "        super(NFM, self).__init__(config)\n",
    "        # 稠密和稀疏特征的数量\n",
    "        #self.num_dense_feature = dense_features_cols.__len__()\n",
    "        self.num_dense_feature = 0#修改\n",
    "        self.num_sparse_feature = 10149\n",
    "        #self.num_sparse_feature = 0##修改\n",
    "        # NFM的线性部分，对应 ∑WiXi\n",
    "        #self.linear_model = nn.Linear(self.num_dense_feature + self.num_sparse_feature, 1)\n",
    "        #self.linear_model = nn.Linear(self.num_dense_feature + self.num_sparse_feature, n_class)##修改\n",
    "        # NFM的Embedding层\n",
    "        \n",
    "        \"\"\"\n",
    "        self.dnn_layers = nn.ModuleList([\n",
    "            nn.Linear(in_features=layer[0], out_features=layer[1])\\\n",
    "                for layer in list(zip(self.hidden_layers[:-1], self.hidden_layers[1:])) \n",
    "        ])\n",
    "        \"\"\"\n",
    "        self.embedding_layers=nn.Embedding(1001,config['embed_dim'])\n",
    "        # B-Interaction 层\n",
    "        self.bi_pooling = BiInteractionPooling()\n",
    "        self.bi_dropout = config['bi_dropout']\n",
    "        if self.bi_dropout > 0:\n",
    "            self.dropout = nn.Dropout(self.bi_dropout)\n",
    "\n",
    "        # NFM的DNN部分\n",
    "        self.hidden_layers = [self.num_dense_feature + config['embed_dim']] + config['dnn_hidden_units']#是加还是乘\n",
    "        self.dnn_layers = nn.ModuleList([\n",
    "            nn.Linear(in_features=layer[0], out_features=layer[1])\\\n",
    "                for layer in list(zip(self.hidden_layers[:-1], self.hidden_layers[1:])) \n",
    "        ])\n",
    "        #self.dnn_linear = nn.Linear(self.hidden_layers[-1], 1, bias=False)\n",
    "        #self.dnn_linear = nn.Linear(self.hidden_layers[-1], n_class, bias=False)\n",
    "        \n",
    "        #增加\n",
    "        self.dnn_softmax=nn.Softmax(dim=1) # 按列SoftMax,列和为1  #注意nn.softmax的定义和调用\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 先区分出稀疏特征和稠密特征，这里是按照列来划分的，即所有的行都要进行筛选\n",
    "        dense_input, sparse_inputs = x[:, :self.num_dense_feature], x[:, self.num_dense_feature:]\n",
    "        sparse_inputs = sparse_inputs.long()\n",
    "\n",
    "        # 求出线性部分\n",
    "        #linear_output = self.linear_model(x)\n",
    "\n",
    "        # 求出稀疏特征的embedding向量\n",
    "        sparse_embeds = [self.embedding_layers(sparse_inputs[:, i]) for i in range(sparse_inputs.shape[1])]\n",
    "        sparse_embeds = torch.cat(sparse_embeds, axis=-1)\n",
    "\n",
    "        # 送入B-Interaction层\n",
    "        fm_input = sparse_embeds.view(-1, self.num_sparse_feature, self._config['embed_dim'])#整理成n行m列\n",
    "        # print(fm_input)\n",
    "        # print(fm_input.shape)\n",
    "\n",
    "        bi_out = self.bi_pooling(fm_input)\n",
    "        if self.bi_dropout:\n",
    "            bi_out = self.dropout(bi_out)\n",
    "\n",
    "        bi_out = bi_out.view(-1, self._config['embed_dim'])\n",
    "        # 将结果聚合起来\n",
    "        dnn_input = torch.cat((dense_input, bi_out), dim=-1)\n",
    "\n",
    "        # DNN 层\n",
    "        dnn_output = dnn_input\n",
    "        for dnn in self.dnn_layers:\n",
    "            dnn_output = dnn(dnn_output)#dnn_output为tensor\n",
    "            # dnn_output = nn.BatchNormalize(dnn_output)\n",
    "            dnn_output = torch.relu(dnn_output)\n",
    "        #dnn_output = self.dnn_linear(dnn_output)\n",
    "        \n",
    "        print(\"dnn_softmax_output:\",dnn_output.shape)\n",
    "        y_pred=self.dnn_softmax(dnn_output)#增加\n",
    "        \n",
    "        # Final\n",
    "        #output = linear_output + dnn_output#修改\n",
    "        #y_pred = self.dnn_softmax(output,dim=0)\n",
    "\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c239d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([100, 10149])\n",
      "labels.shape: torch.Size([100, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concated_embeds_value.shape: torch.Size([100, 10149])\n",
      "square_of_sum.shape: torch.Size([100, 1])\n",
      "sum_of_square.shape: torch.Size([100, 1])\n",
      "cross_term.shape: torch.Size([100, 1])\n",
      "y: torch.Size([100, 1])\n",
      "x.shape: torch.Size([100, 10149])\n",
      "labels.shape: torch.Size([100, 16])\n",
      "concated_embeds_value.shape: torch.Size([100, 10149])\n",
      "square_of_sum.shape: torch.Size([100, 1])\n",
      "sum_of_square.shape: torch.Size([100, 1])\n",
      "cross_term.shape: torch.Size([100, 1])\n",
      "y: torch.Size([100, 1])\n",
      "x.shape: torch.Size([100, 10149])\n",
      "labels.shape: torch.Size([100, 16])\n",
      "concated_embeds_value.shape: torch.Size([100, 10149])\n",
      "square_of_sum.shape: torch.Size([100, 1])\n",
      "sum_of_square.shape: torch.Size([100, 1])\n",
      "cross_term.shape: torch.Size([100, 1])\n",
      "y: torch.Size([100, 1])\n",
      "x.shape: torch.Size([100, 10149])\n",
      "labels.shape: torch.Size([100, 16])\n",
      "concated_embeds_value.shape: torch.Size([100, 10149])\n",
      "square_of_sum.shape: torch.Size([100, 1])\n",
      "sum_of_square.shape: torch.Size([100, 1])\n",
      "cross_term.shape: torch.Size([100, 1])\n",
      "y: torch.Size([100, 1])\n",
      "x.shape: torch.Size([100, 10149])\n",
      "labels.shape: torch.Size([100, 16])\n",
      "concated_embeds_value.shape: torch.Size([100, 10149])\n",
      "square_of_sum.shape: torch.Size([100, 1])\n",
      "sum_of_square.shape: torch.Size([100, 1])\n",
      "cross_term.shape: torch.Size([100, 1])\n",
      "y: torch.Size([100, 1])\n",
      "x.shape: torch.Size([100, 10149])\n",
      "labels.shape: torch.Size([100, 16])\n",
      "concated_embeds_value.shape: torch.Size([100, 10149])\n",
      "square_of_sum.shape: torch.Size([100, 1])\n",
      "sum_of_square.shape: torch.Size([100, 1])\n",
      "cross_term.shape: torch.Size([100, 1])\n",
      "y: torch.Size([100, 1])\n",
      "x.shape: torch.Size([100, 10149])\n",
      "labels.shape: torch.Size([100, 16])\n",
      "concated_embeds_value.shape: torch.Size([100, 10149])\n",
      "square_of_sum.shape: torch.Size([100, 1])\n",
      "sum_of_square.shape: torch.Size([100, 1])\n",
      "cross_term.shape: torch.Size([100, 1])\n",
      "y: torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "#import network\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    model= BiInteractionPooling()\n",
    "    for batch_id, (x, labels) in enumerate(train_loader):\n",
    "            #x=torch.from_numpy(x,d)\n",
    "            #x = Variable(x,requires_grad=True)\n",
    "            #x=Variable(torch.FloatTensor(x), requires_grad=True)\n",
    "            #x=torch.FloatTensor(x)\n",
    "            x=x.view(-1,10149)\n",
    "            print('x.shape:',x.shape)\n",
    "            labels=labels.view(-1,16)\n",
    "            print('labels.shape:',labels.shape)\n",
    "            #labels=labels.squeeze(0)\n",
    "            x=torch.tensor(x,dtype=torch.float)#NameError: name 'dtype' is not defined:错误摘录，torch.Tensor()和torch.tensor()的区别\n",
    "\n",
    "            #labels=Variable(labels)\n",
    "            labels=torch.tensor(labels)\n",
    "            #labels = Variable(torch.from_numpy().float(), requires_grad=True)\n",
    "            #if self._config['use_cuda'] is True:\n",
    "            x, labels = x.cuda(), labels.cuda()\n",
    "            \n",
    "            #model.to(device)\n",
    "            #optimizer.zero_grad()\n",
    "            #y_predict = nfm(x)\n",
    "            #y_predict=model(x)\n",
    "            y=model(x)\n",
    "            print(\"y:\",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca052a76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nfm: NFM(\n",
      "  (linear_model): Linear(in_features=10149, out_features=16, bias=True)\n",
      "  (embedding_layers): Embedding(1001, 100)\n",
      "  (bi_pooling): BiInteractionPooling()\n",
      "  (dropout): Dropout(p=0.8, inplace=False)\n",
      "  (dnn_layers): ModuleList(\n",
      "    (0): Linear(in_features=10149, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=16, bias=True)\n",
      "  )\n",
      "  (dnn_linear): Linear(in_features=16, out_features=16, bias=False)\n",
      ")\n",
      "x.shape: torch.Size([100, 10149])\n",
      "labels.shape: torch.Size([100, 16])\n",
      "spare_input: torch.Size([100, 10149])\n",
      "sparse_embeds: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-1.3831,  0.2942,  0.2118,  ..., -1.8926, -0.8853, -0.2953],\n",
      "        [-2.1675,  0.8389, -1.1488,  ...,  1.0069, -0.9605,  1.7246],\n",
      "        [ 0.9878,  0.1677,  0.0621,  ...,  0.2801, -1.0537, -0.9801],\n",
      "        ...,\n",
      "        [-0.6216,  1.5653, -1.2259,  ...,  0.2656,  0.8910,  0.5925],\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-0.3450, -0.4505,  1.1307,  ..., -1.6200,  0.9929, -1.0829],\n",
      "        [-0.6216,  1.5653, -1.2259,  ...,  0.2656,  0.8910,  0.5925],\n",
      "        [-0.6363,  1.0086, -0.3931,  ..., -1.2917, -0.5492, -1.3642],\n",
      "        ...,\n",
      "        [-1.0294,  1.6148,  1.2561,  ...,  0.4634, -0.5319,  0.9742],\n",
      "        [ 1.0439,  0.3007,  0.2878,  ..., -0.2392,  0.0075,  0.2264],\n",
      "        [-1.0294,  1.6148,  1.2561,  ...,  0.4634, -0.5319,  0.9742]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.8073,  1.0816, -0.0726,  ...,  1.3689, -0.8928,  1.3709],\n",
      "        [-0.6216,  1.5653, -1.2259,  ...,  0.2656,  0.8910,  0.5925],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387],\n",
      "        ...,\n",
      "        [ 0.6055,  1.2497,  0.7557,  ...,  0.3586,  0.2028,  0.0275],\n",
      "        [ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-0.3583,  1.2817, -0.7967,  ..., -1.4272,  1.6632,  0.2032],\n",
      "        [-0.0695, -0.3896, -0.4230,  ...,  1.3443, -2.0588, -0.1447],\n",
      "        [ 1.4377,  1.1952,  0.3310,  ...,  0.2546, -0.0350, -1.7429],\n",
      "        ...,\n",
      "        [ 0.5311,  1.1490,  1.3763,  ..., -1.8212,  0.9219, -0.2400],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058],\n",
      "        [ 0.7640, -0.4149,  0.3832,  ...,  0.7362, -0.7043, -1.6631],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        ...,\n",
      "        [-0.4297, -1.0602, -0.6045,  ..., -1.2894, -1.8477,  0.7277],\n",
      "        [-0.0695, -0.3896, -0.4230,  ...,  1.3443, -2.0588, -0.1447],\n",
      "        [ 0.9878,  0.1677,  0.0621,  ...,  0.2801, -1.0537, -0.9801]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.6055,  1.2497,  0.7557,  ...,  0.3586,  0.2028,  0.0275],\n",
      "        [-1.5713, -0.6621, -0.6136,  ...,  0.4324, -0.2257, -0.8327],\n",
      "        [-0.3583,  1.2817, -0.7967,  ..., -1.4272,  1.6632,  0.2032],\n",
      "        ...,\n",
      "        [-1.3831,  0.2942,  0.2118,  ..., -1.8926, -0.8853, -0.2953],\n",
      "        [ 0.6055,  1.2497,  0.7557,  ...,  0.3586,  0.2028,  0.0275],\n",
      "        [ 0.9878,  0.1677,  0.0621,  ...,  0.2801, -1.0537, -0.9801]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.7640, -0.4149,  0.3832,  ...,  0.7362, -0.7043, -1.6631],\n",
      "        [-2.1675,  0.8389, -1.1488,  ...,  1.0069, -0.9605,  1.7246],\n",
      "        [ 0.9878,  0.1677,  0.0621,  ...,  0.2801, -1.0537, -0.9801],\n",
      "        ...,\n",
      "        [-1.3831,  0.2942,  0.2118,  ..., -1.8926, -0.8853, -0.2953],\n",
      "        [-0.3244,  0.0231,  0.4095,  ...,  0.8166,  0.2658,  0.3560],\n",
      "        [-0.0695, -0.3896, -0.4230,  ...,  1.3443, -2.0588, -0.1447]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 1.0439,  0.3007,  0.2878,  ..., -0.2392,  0.0075,  0.2264],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387],\n",
      "        [-1.3831,  0.2942,  0.2118,  ..., -1.8926, -0.8853, -0.2953],\n",
      "        ...,\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        [-1.0294,  1.6148,  1.2561,  ...,  0.4634, -0.5319,  0.9742]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.1622,  1.6779, -0.9465,  ..., -0.6479,  0.1470,  0.9163],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        ...,\n",
      "        [-1.0294,  1.6148,  1.2561,  ...,  0.4634, -0.5319,  0.9742],\n",
      "        [ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        [ 0.6055,  1.2497,  0.7557,  ...,  0.3586,  0.2028,  0.0275]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-1.3831,  0.2942,  0.2118,  ..., -1.8926, -0.8853, -0.2953],\n",
      "        [ 0.6755,  0.2317, -0.8647,  ...,  0.2791, -1.6866,  0.1567],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        ...,\n",
      "        [-1.5372,  0.5324,  1.7181,  ..., -1.4738, -2.6210, -0.7430],\n",
      "        [ 0.6755,  0.2317, -0.8647,  ...,  0.2791, -1.6866,  0.1567],\n",
      "        [-0.0695, -0.3896, -0.4230,  ...,  1.3443, -2.0588, -0.1447]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.8073,  1.0816, -0.0726,  ...,  1.3689, -0.8928,  1.3709],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        ...,\n",
      "        [ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        [ 0.3882, -0.2834, -2.3430,  ..., -0.4396,  0.3833,  0.7997]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.1622,  1.6779, -0.9465,  ..., -0.6479,  0.1470,  0.9163],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387],\n",
      "        [ 0.0778,  1.9595, -0.5862,  ...,  0.8202,  0.9166, -1.4922],\n",
      "        ...,\n",
      "        [ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [ 0.3882, -0.2834, -2.3430,  ..., -0.4396,  0.3833,  0.7997],\n",
      "        [-0.6363,  1.0086, -0.3931,  ..., -1.2917, -0.5492, -1.3642]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.9878,  0.1677,  0.0621,  ...,  0.2801, -1.0537, -0.9801],\n",
      "        [-1.2857, -1.4086,  0.9322,  ...,  1.3655, -1.1220, -1.4872],\n",
      "        [ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        ...,\n",
      "        [-0.9113,  0.0314, -0.0132,  ..., -0.8909,  0.1527,  0.9571],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387],\n",
      "        [-1.2857, -1.4086,  0.9322,  ...,  1.3655, -1.1220, -1.4872]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.1622,  1.6779, -0.9465,  ..., -0.6479,  0.1470,  0.9163],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        ...,\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        [-0.8760, -1.1808,  0.9993,  ...,  1.0108, -0.4633, -0.8207],\n",
      "        [-1.0294,  1.6148,  1.2561,  ...,  0.4634, -0.5319,  0.9742]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-1.0294,  1.6148,  1.2561,  ...,  0.4634, -0.5319,  0.9742],\n",
      "        [-0.9113,  0.0314, -0.0132,  ..., -0.8909,  0.1527,  0.9571],\n",
      "        [-0.3583,  1.2817, -0.7967,  ..., -1.4272,  1.6632,  0.2032],\n",
      "        ...,\n",
      "        [ 0.5311,  1.1490,  1.3763,  ..., -1.8212,  0.9219, -0.2400],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        [ 0.0778,  1.9595, -0.5862,  ...,  0.8202,  0.9166, -1.4922]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 1.0439,  0.3007,  0.2878,  ..., -0.2392,  0.0075,  0.2264],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        ...,\n",
      "        [ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        [ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [-0.3244,  0.0231,  0.4095,  ...,  0.8166,  0.2658,  0.3560]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-1.3744,  0.2777,  0.4107,  ..., -1.4458,  2.2524,  0.2371],\n",
      "        [ 0.0778,  1.9595, -0.5862,  ...,  0.8202,  0.9166, -1.4922],\n",
      "        [-1.3831,  0.2942,  0.2118,  ..., -1.8926, -0.8853, -0.2953],\n",
      "        ...,\n",
      "        [-0.8760, -1.1808,  0.9993,  ...,  1.0108, -0.4633, -0.8207],\n",
      "        [-0.7536,  0.2433,  0.9827,  ..., -1.0905, -0.0130, -1.1619],\n",
      "        [ 1.3449,  0.9549,  0.3177,  ...,  0.7293, -0.9414, -0.3726]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.1622,  1.6779, -0.9465,  ..., -0.6479,  0.1470,  0.9163],\n",
      "        [-0.8760, -1.1808,  0.9993,  ...,  1.0108, -0.4633, -0.8207],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        ...,\n",
      "        [ 0.3882, -0.2834, -2.3430,  ..., -0.4396,  0.3833,  0.7997],\n",
      "        [ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [-0.7536,  0.2433,  0.9827,  ..., -1.0905, -0.0130, -1.1619]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 1.2760, -0.3585,  1.5814,  ..., -0.8996,  0.1906, -0.2162],\n",
      "        [-0.6216,  1.5653, -1.2259,  ...,  0.2656,  0.8910,  0.5925],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387],\n",
      "        ...,\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        [ 0.3882, -0.2834, -2.3430,  ..., -0.4396,  0.3833,  0.7997],\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.3437,  2.3262,  1.0931,  ..., -1.1826,  1.3136,  0.6829],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387],\n",
      "        [ 0.5311,  1.1490,  1.3763,  ..., -1.8212,  0.9219, -0.2400],\n",
      "        ...,\n",
      "        [-0.8760, -1.1808,  0.9993,  ...,  1.0108, -0.4633, -0.8207],\n",
      "        [-0.7536,  0.2433,  0.9827,  ..., -1.0905, -0.0130, -1.1619],\n",
      "        [-1.0498, -0.4133, -0.4733,  ...,  1.1184, -2.3151, -1.0088]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.0778,  1.9595, -0.5862,  ...,  0.8202,  0.9166, -1.4922],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058],\n",
      "        [ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        ...,\n",
      "        [-1.2857, -1.4086,  0.9322,  ...,  1.3655, -1.1220, -1.4872],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387],\n",
      "        [-0.4297, -1.0602, -0.6045,  ..., -1.2894, -1.8477,  0.7277]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-1.1628,  0.3368,  0.1534,  ..., -1.0601, -0.6522,  0.0741],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        [ 0.0778,  1.9595, -0.5862,  ...,  0.8202,  0.9166, -1.4922],\n",
      "        ...,\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422],\n",
      "        [ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [-0.3244,  0.0231,  0.4095,  ...,  0.8166,  0.2658,  0.3560]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-0.8760, -1.1808,  0.9993,  ...,  1.0108, -0.4633, -0.8207],\n",
      "        [ 0.0778,  1.9595, -0.5862,  ...,  0.8202,  0.9166, -1.4922],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387],\n",
      "        ...,\n",
      "        [ 1.4377,  1.1952,  0.3310,  ...,  0.2546, -0.0350, -1.7429],\n",
      "        [ 0.3882, -0.2834, -2.3430,  ..., -0.4396,  0.3833,  0.7997],\n",
      "        [ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-0.3450, -0.4505,  1.1307,  ..., -1.6200,  0.9929, -1.0829],\n",
      "        [ 0.9878,  0.1677,  0.0621,  ...,  0.2801, -1.0537, -0.9801],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387],\n",
      "        ...,\n",
      "        [-0.3583,  1.2817, -0.7967,  ..., -1.4272,  1.6632,  0.2032],\n",
      "        [ 0.3882, -0.2834, -2.3430,  ..., -0.4396,  0.3833,  0.7997],\n",
      "        [ 0.3882, -0.2834, -2.3430,  ..., -0.4396,  0.3833,  0.7997]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 1.1228, -0.9397, -0.0948,  ...,  0.1395, -0.7568, -0.5425],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058],\n",
      "        [-0.6216,  1.5653, -1.2259,  ...,  0.2656,  0.8910,  0.5925],\n",
      "        ...,\n",
      "        [-0.8760, -1.1808,  0.9993,  ...,  1.0108, -0.4633, -0.8207],\n",
      "        [-0.3583,  1.2817, -0.7967,  ..., -1.4272,  1.6632,  0.2032],\n",
      "        [-0.7536,  0.2433,  0.9827,  ..., -1.0905, -0.0130, -1.1619]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-1.4999, -0.4894, -0.9999,  ..., -0.1023,  0.5037, -0.4422],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058],\n",
      "        [-0.6216,  1.5653, -1.2259,  ...,  0.2656,  0.8910,  0.5925],\n",
      "        ...,\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422],\n",
      "        [ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.6055,  1.2497,  0.7557,  ...,  0.3586,  0.2028,  0.0275],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        [ 0.6055,  1.2497,  0.7557,  ...,  0.3586,  0.2028,  0.0275],\n",
      "        ...,\n",
      "        [-1.2857, -1.4086,  0.9322,  ...,  1.3655, -1.1220, -1.4872],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-0.6216,  1.5653, -1.2259,  ...,  0.2656,  0.8910,  0.5925],\n",
      "        [-0.6363,  1.0086, -0.3931,  ..., -1.2917, -0.5492, -1.3642],\n",
      "        [-1.0294,  1.6148,  1.2561,  ...,  0.4634, -0.5319,  0.9742],\n",
      "        ...,\n",
      "        [-1.2857, -1.4086,  0.9322,  ...,  1.3655, -1.1220, -1.4872],\n",
      "        [-0.0695, -0.3896, -0.4230,  ...,  1.3443, -2.0588, -0.1447],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [-0.0695, -0.3896, -0.4230,  ...,  1.3443, -2.0588, -0.1447],\n",
      "        [-0.3244,  0.0231,  0.4095,  ...,  0.8166,  0.2658,  0.3560],\n",
      "        ...,\n",
      "        [-1.3831,  0.2942,  0.2118,  ..., -1.8926, -0.8853, -0.2953],\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422],\n",
      "        [ 0.0778,  1.9595, -0.5862,  ...,  0.8202,  0.9166, -1.4922]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.5311,  1.1490,  1.3763,  ..., -1.8212,  0.9219, -0.2400],\n",
      "        [ 0.6755,  0.2317, -0.8647,  ...,  0.2791, -1.6866,  0.1567],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        ...,\n",
      "        [-1.5372,  0.5324,  1.7181,  ..., -1.4738, -2.6210, -0.7430],\n",
      "        [ 0.6755,  0.2317, -0.8647,  ...,  0.2791, -1.6866,  0.1567],\n",
      "        [ 0.9878,  0.1677,  0.0621,  ...,  0.2801, -1.0537, -0.9801]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 1.4377,  1.1952,  0.3310,  ...,  0.2546, -0.0350, -1.7429],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        [-0.8760, -1.1808,  0.9993,  ...,  1.0108, -0.4633, -0.8207],\n",
      "        ...,\n",
      "        [-0.6363,  1.0086, -0.3931,  ..., -1.2917, -0.5492, -1.3642],\n",
      "        [ 0.6055,  1.2497,  0.7557,  ...,  0.3586,  0.2028,  0.0275],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        [ 0.6755,  0.2317, -0.8647,  ...,  0.2791, -1.6866,  0.1567],\n",
      "        [ 1.4377,  1.1952,  0.3310,  ...,  0.2546, -0.0350, -1.7429],\n",
      "        ...,\n",
      "        [ 0.5311,  1.1490,  1.3763,  ..., -1.8212,  0.9219, -0.2400],\n",
      "        [ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-1.4999, -0.4894, -0.9999,  ..., -0.1023,  0.5037, -0.4422],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058],\n",
      "        ...,\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        [ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [-0.3583,  1.2817, -0.7967,  ..., -1.4272,  1.6632,  0.2032]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [-0.0695, -0.3896, -0.4230,  ...,  1.3443, -2.0588, -0.1447],\n",
      "        [-0.3244,  0.0231,  0.4095,  ...,  0.8166,  0.2658,  0.3560],\n",
      "        ...,\n",
      "        [ 0.5311,  1.1490,  1.3763,  ..., -1.8212,  0.9219, -0.2400],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-1.1628,  0.3368,  0.1534,  ..., -1.0601, -0.6522,  0.0741],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        ...,\n",
      "        [-0.3583,  1.2817, -0.7967,  ..., -1.4272,  1.6632,  0.2032],\n",
      "        [-0.8760, -1.1808,  0.9993,  ...,  1.0108, -0.4633, -0.8207],\n",
      "        [-1.0294,  1.6148,  1.2561,  ...,  0.4634, -0.5319,  0.9742]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 1.2760, -0.3585,  1.5814,  ..., -0.8996,  0.1906, -0.2162],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        ...,\n",
      "        [ 0.6055,  1.2497,  0.7557,  ...,  0.3586,  0.2028,  0.0275],\n",
      "        [-0.8760, -1.1808,  0.9993,  ...,  1.0108, -0.4633, -0.8207],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-0.3583,  1.2817, -0.7967,  ..., -1.4272,  1.6632,  0.2032],\n",
      "        [-0.0695, -0.3896, -0.4230,  ...,  1.3443, -2.0588, -0.1447],\n",
      "        [-0.3244,  0.0231,  0.4095,  ...,  0.8166,  0.2658,  0.3560],\n",
      "        ...,\n",
      "        [ 0.5311,  1.1490,  1.3763,  ..., -1.8212,  0.9219, -0.2400],\n",
      "        [ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        [ 0.6755,  0.2317, -0.8647,  ...,  0.2791, -1.6866,  0.1567]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.1622,  1.6779, -0.9465,  ..., -0.6479,  0.1470,  0.9163],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        ...,\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422],\n",
      "        [-0.7536,  0.2433,  0.9827,  ..., -1.0905, -0.0130, -1.1619],\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.2159, -0.7143,  1.4130,  ..., -1.4968,  0.5686, -0.6451],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058],\n",
      "        ...,\n",
      "        [-1.0294,  1.6148,  1.2561,  ...,  0.4634, -0.5319,  0.9742],\n",
      "        [-0.3583,  1.2817, -0.7967,  ..., -1.4272,  1.6632,  0.2032],\n",
      "        [-1.0294,  1.6148,  1.2561,  ...,  0.4634, -0.5319,  0.9742]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-0.3583,  1.2817, -0.7967,  ..., -1.4272,  1.6632,  0.2032],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058],\n",
      "        [ 0.6055,  1.2497,  0.7557,  ...,  0.3586,  0.2028,  0.0275],\n",
      "        ...,\n",
      "        [-1.3831,  0.2942,  0.2118,  ..., -1.8926, -0.8853, -0.2953],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.5311,  1.1490,  1.3763,  ..., -1.8212,  0.9219, -0.2400],\n",
      "        [ 0.0778,  1.9595, -0.5862,  ...,  0.8202,  0.9166, -1.4922],\n",
      "        [ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        ...,\n",
      "        [-0.4297, -1.0602, -0.6045,  ..., -1.2894, -1.8477,  0.7277],\n",
      "        [-0.4297, -1.0602, -0.6045,  ..., -1.2894, -1.8477,  0.7277],\n",
      "        [-0.0695, -0.3896, -0.4230,  ...,  1.3443, -2.0588, -0.1447]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-0.5235, -1.5884, -0.5879,  ...,  1.2596, -1.9344, -0.1476],\n",
      "        [-0.8760, -1.1808,  0.9993,  ...,  1.0108, -0.4633, -0.8207],\n",
      "        [ 0.9878,  0.1677,  0.0621,  ...,  0.2801, -1.0537, -0.9801],\n",
      "        ...,\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        [-1.0294,  1.6148,  1.2561,  ...,  0.4634, -0.5319,  0.9742]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-1.0294,  1.6148,  1.2561,  ...,  0.4634, -0.5319,  0.9742],\n",
      "        [ 0.9878,  0.1677,  0.0621,  ...,  0.2801, -1.0537, -0.9801],\n",
      "        [-0.3244,  0.0231,  0.4095,  ...,  0.8166,  0.2658,  0.3560],\n",
      "        ...,\n",
      "        [ 0.5311,  1.1490,  1.3763,  ..., -1.8212,  0.9219, -0.2400],\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422],\n",
      "        [ 0.0778,  1.9595, -0.5862,  ...,  0.8202,  0.9166, -1.4922]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.3437,  2.3262,  1.0931,  ..., -1.1826,  1.3136,  0.6829],\n",
      "        [-0.6363,  1.0086, -0.3931,  ..., -1.2917, -0.5492, -1.3642],\n",
      "        [ 0.9878,  0.1677,  0.0621,  ...,  0.2801, -1.0537, -0.9801],\n",
      "        ...,\n",
      "        [ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422],\n",
      "        [ 0.3882, -0.2834, -2.3430,  ..., -0.4396,  0.3833,  0.7997]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.1622,  1.6779, -0.9465,  ..., -0.6479,  0.1470,  0.9163],\n",
      "        [-0.6216,  1.5653, -1.2259,  ...,  0.2656,  0.8910,  0.5925],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        ...,\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422],\n",
      "        [ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [-1.0294,  1.6148,  1.2561,  ...,  0.4634, -0.5319,  0.9742]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.5311,  1.1490,  1.3763,  ..., -1.8212,  0.9219, -0.2400],\n",
      "        [ 0.9878,  0.1677,  0.0621,  ...,  0.2801, -1.0537, -0.9801],\n",
      "        [-0.8760, -1.1808,  0.9993,  ...,  1.0108, -0.4633, -0.8207],\n",
      "        ...,\n",
      "        [-0.6363,  1.0086, -0.3931,  ..., -1.2917, -0.5492, -1.3642],\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.3437,  2.3262,  1.0931,  ..., -1.1826,  1.3136,  0.6829],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        ...,\n",
      "        [ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [-1.0498, -0.4133, -0.4733,  ...,  1.1184, -2.3151, -1.0088],\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-1.4999, -0.4894, -0.9999,  ..., -0.1023,  0.5037, -0.4422],\n",
      "        [-1.3831,  0.2942,  0.2118,  ..., -1.8926, -0.8853, -0.2953],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        ...,\n",
      "        [ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        [-1.0498, -0.4133, -0.4733,  ...,  1.1184, -2.3151, -1.0088],\n",
      "        [-0.3244,  0.0231,  0.4095,  ...,  0.8166,  0.2658,  0.3560]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-1.3744,  0.2777,  0.4107,  ..., -1.4458,  2.2524,  0.2371],\n",
      "        [ 0.0778,  1.9595, -0.5862,  ...,  0.8202,  0.9166, -1.4922],\n",
      "        [-1.3831,  0.2942,  0.2118,  ..., -1.8926, -0.8853, -0.2953],\n",
      "        ...,\n",
      "        [-0.8760, -1.1808,  0.9993,  ...,  1.0108, -0.4633, -0.8207],\n",
      "        [ 1.3449,  0.9549,  0.3177,  ...,  0.7293, -0.9414, -0.3726],\n",
      "        [ 0.8073,  1.0816, -0.0726,  ...,  1.3689, -0.8928,  1.3709]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.1622,  1.6779, -0.9465,  ..., -0.6479,  0.1470,  0.9163],\n",
      "        [ 0.6755,  0.2317, -0.8647,  ...,  0.2791, -1.6866,  0.1567],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387],\n",
      "        ...,\n",
      "        [ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [-0.3583,  1.2817, -0.7967,  ..., -1.4272,  1.6632,  0.2032]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-0.3450, -0.4505,  1.1307,  ..., -1.6200,  0.9929, -1.0829],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        ...,\n",
      "        [ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [-0.8760, -1.1808,  0.9993,  ...,  1.0108, -0.4633, -0.8207],\n",
      "        [-1.0294,  1.6148,  1.2561,  ...,  0.4634, -0.5319,  0.9742]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-0.3450, -0.4505,  1.1307,  ..., -1.6200,  0.9929, -1.0829],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        ...,\n",
      "        [-0.3583,  1.2817, -0.7967,  ..., -1.4272,  1.6632,  0.2032],\n",
      "        [ 0.3882, -0.2834, -2.3430,  ..., -0.4396,  0.3833,  0.7997],\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-1.4999, -0.4894, -0.9999,  ..., -0.1023,  0.5037, -0.4422],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        [ 0.0778,  1.9595, -0.5862,  ...,  0.8202,  0.9166, -1.4922],\n",
      "        ...,\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422],\n",
      "        [ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        [-1.0294,  1.6148,  1.2561,  ...,  0.4634, -0.5319,  0.9742]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 1.0439,  0.3007,  0.2878,  ..., -0.2392,  0.0075,  0.2264],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        [-0.6216,  1.5653, -1.2259,  ...,  0.2656,  0.8910,  0.5925],\n",
      "        ...,\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        [ 1.3449,  0.9549,  0.3177,  ...,  0.7293, -0.9414, -0.3726],\n",
      "        [-0.3244,  0.0231,  0.4095,  ...,  0.8166,  0.2658,  0.3560]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        [-0.0695, -0.3896, -0.4230,  ...,  1.3443, -2.0588, -0.1447],\n",
      "        [-0.3244,  0.0231,  0.4095,  ...,  0.8166,  0.2658,  0.3560],\n",
      "        ...,\n",
      "        [ 0.5311,  1.1490,  1.3763,  ..., -1.8212,  0.9219, -0.2400],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        [-0.6216,  1.5653, -1.2259,  ...,  0.2656,  0.8910,  0.5925]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.1622,  1.6779, -0.9465,  ..., -0.6479,  0.1470,  0.9163],\n",
      "        [ 0.9878,  0.1677,  0.0621,  ...,  0.2801, -1.0537, -0.9801],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387],\n",
      "        ...,\n",
      "        [ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        [ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [ 0.6055,  1.2497,  0.7557,  ...,  0.3586,  0.2028,  0.0275]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [-0.0695, -0.3896, -0.4230,  ...,  1.3443, -2.0588, -0.1447],\n",
      "        [-0.3244,  0.0231,  0.4095,  ...,  0.8166,  0.2658,  0.3560],\n",
      "        ...,\n",
      "        [-1.3831,  0.2942,  0.2118,  ..., -1.8926, -0.8853, -0.2953],\n",
      "        [ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.7640, -0.4149,  0.3832,  ...,  0.7362, -0.7043, -1.6631],\n",
      "        [ 0.6755,  0.2317, -0.8647,  ...,  0.2791, -1.6866,  0.1567],\n",
      "        [ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        ...,\n",
      "        [-0.9113,  0.0314, -0.0132,  ..., -0.8909,  0.1527,  0.9571],\n",
      "        [-0.6216,  1.5653, -1.2259,  ...,  0.2656,  0.8910,  0.5925],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        [ 0.6755,  0.2317, -0.8647,  ...,  0.2791, -1.6866,  0.1567],\n",
      "        [ 1.4377,  1.1952,  0.3310,  ...,  0.2546, -0.0350, -1.7429],\n",
      "        ...,\n",
      "        [ 0.5311,  1.1490,  1.3763,  ..., -1.8212,  0.9219, -0.2400],\n",
      "        [ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-1.4999, -0.4894, -0.9999,  ..., -0.1023,  0.5037, -0.4422],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        [ 0.6755,  0.2317, -0.8647,  ...,  0.2791, -1.6866,  0.1567],\n",
      "        ...,\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        [ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [-1.0294,  1.6148,  1.2561,  ...,  0.4634, -0.5319,  0.9742]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 1.4377,  1.1952,  0.3310,  ...,  0.2546, -0.0350, -1.7429],\n",
      "        [ 0.1977,  0.9364, -0.7765,  ...,  0.6419, -0.7179, -0.7007],\n",
      "        [ 0.6755,  0.2317, -0.8647,  ...,  0.2791, -1.6866,  0.1567],\n",
      "        ...,\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422],\n",
      "        [-0.6216,  1.5653, -1.2259,  ...,  0.2656,  0.8910,  0.5925]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.3437,  2.3262,  1.0931,  ..., -1.1826,  1.3136,  0.6829],\n",
      "        [ 0.6755,  0.2317, -0.8647,  ...,  0.2791, -1.6866,  0.1567],\n",
      "        [ 1.4377,  1.1952,  0.3310,  ...,  0.2546, -0.0350, -1.7429],\n",
      "        ...,\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422],\n",
      "        [-0.3583,  1.2817, -0.7967,  ..., -1.4272,  1.6632,  0.2032],\n",
      "        [ 0.7640, -0.4149,  0.3832,  ...,  0.7362, -0.7043, -1.6631]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-1.1628,  0.3368,  0.1534,  ..., -1.0601, -0.6522,  0.0741],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        [-0.6363,  1.0086, -0.3931,  ..., -1.2917, -0.5492, -1.3642],\n",
      "        ...,\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422],\n",
      "        [-0.7536,  0.2433,  0.9827,  ..., -1.0905, -0.0130, -1.1619],\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.1622,  1.6779, -0.9465,  ..., -0.6479,  0.1470,  0.9163],\n",
      "        [ 0.9878,  0.1677,  0.0621,  ...,  0.2801, -1.0537, -0.9801],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058],\n",
      "        ...,\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        [ 0.6055,  1.2497,  0.7557,  ...,  0.3586,  0.2028,  0.0275],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-1.4999, -0.4894, -0.9999,  ..., -0.1023,  0.5037, -0.4422],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058],\n",
      "        [-0.6363,  1.0086, -0.3931,  ..., -1.2917, -0.5492, -1.3642],\n",
      "        ...,\n",
      "        [-0.3583,  1.2817, -0.7967,  ..., -1.4272,  1.6632,  0.2032],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        [ 0.6055,  1.2497,  0.7557,  ...,  0.3586,  0.2028,  0.0275]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-0.3450, -0.4505,  1.1307,  ..., -1.6200,  0.9929, -1.0829],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        ...,\n",
      "        [ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        [ 0.6055,  1.2497,  0.7557,  ...,  0.3586,  0.2028,  0.0275]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-0.3244,  0.0231,  0.4095,  ...,  0.8166,  0.2658,  0.3560],\n",
      "        [ 0.6755,  0.2317, -0.8647,  ...,  0.2791, -1.6866,  0.1567],\n",
      "        [-1.0498, -0.4133, -0.4733,  ...,  1.1184, -2.3151, -1.0088],\n",
      "        ...,\n",
      "        [-0.6363,  1.0086, -0.3931,  ..., -1.2917, -0.5492, -1.3642],\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422],\n",
      "        [-0.6216,  1.5653, -1.2259,  ...,  0.2656,  0.8910,  0.5925]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058],\n",
      "        [ 1.4377,  1.1952,  0.3310,  ...,  0.2546, -0.0350, -1.7429],\n",
      "        ...,\n",
      "        [-0.6363,  1.0086, -0.3931,  ..., -1.2917, -0.5492, -1.3642],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.3882, -0.2834, -2.3430,  ..., -0.4396,  0.3833,  0.7997],\n",
      "        [-1.5713, -0.6621, -0.6136,  ...,  0.4324, -0.2257, -0.8327],\n",
      "        [-0.3244,  0.0231,  0.4095,  ...,  0.8166,  0.2658,  0.3560],\n",
      "        ...,\n",
      "        [-1.3831,  0.2942,  0.2118,  ..., -1.8926, -0.8853, -0.2953],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-1.4999, -0.4894, -0.9999,  ..., -0.1023,  0.5037, -0.4422],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387],\n",
      "        [ 0.0778,  1.9595, -0.5862,  ...,  0.8202,  0.9166, -1.4922],\n",
      "        ...,\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        [-1.0294,  1.6148,  1.2561,  ...,  0.4634, -0.5319,  0.9742]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 1.0439,  0.3007,  0.2878,  ..., -0.2392,  0.0075,  0.2264],\n",
      "        [ 0.6755,  0.2317, -0.8647,  ...,  0.2791, -1.6866,  0.1567],\n",
      "        [ 0.5311,  1.1490,  1.3763,  ..., -1.8212,  0.9219, -0.2400],\n",
      "        ...,\n",
      "        [-1.0294,  1.6148,  1.2561,  ...,  0.4634, -0.5319,  0.9742],\n",
      "        [ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [-1.0294,  1.6148,  1.2561,  ...,  0.4634, -0.5319,  0.9742]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-1.3744,  0.2777,  0.4107,  ..., -1.4458,  2.2524,  0.2371],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058],\n",
      "        [-0.6216,  1.5653, -1.2259,  ...,  0.2656,  0.8910,  0.5925],\n",
      "        ...,\n",
      "        [-0.3583,  1.2817, -0.7967,  ..., -1.4272,  1.6632,  0.2032],\n",
      "        [ 0.3882, -0.2834, -2.3430,  ..., -0.4396,  0.3833,  0.7997],\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.7640, -0.4149,  0.3832,  ...,  0.7362, -0.7043, -1.6631],\n",
      "        [ 0.5311,  1.1490,  1.3763,  ..., -1.8212,  0.9219, -0.2400],\n",
      "        [-1.0294,  1.6148,  1.2561,  ...,  0.4634, -0.5319,  0.9742],\n",
      "        ...,\n",
      "        [-1.2857, -1.4086,  0.9322,  ...,  1.3655, -1.1220, -1.4872],\n",
      "        [-1.5713, -0.6621, -0.6136,  ...,  0.4324, -0.2257, -0.8327],\n",
      "        [ 0.9878,  0.1677,  0.0621,  ...,  0.2801, -1.0537, -0.9801]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        [ 0.9878,  0.1677,  0.0621,  ...,  0.2801, -1.0537, -0.9801],\n",
      "        [ 0.6055,  1.2497,  0.7557,  ...,  0.3586,  0.2028,  0.0275],\n",
      "        ...,\n",
      "        [-0.6363,  1.0086, -0.3931,  ..., -1.2917, -0.5492, -1.3642],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        [-0.0695, -0.3896, -0.4230,  ...,  1.3443, -2.0588, -0.1447],\n",
      "        [ 1.4377,  1.1952,  0.3310,  ...,  0.2546, -0.0350, -1.7429],\n",
      "        ...,\n",
      "        [ 0.5311,  1.1490,  1.3763,  ..., -1.8212,  0.9219, -0.2400],\n",
      "        [ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [-0.6216,  1.5653, -1.2259,  ...,  0.2656,  0.8910,  0.5925]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.1622,  1.6779, -0.9465,  ..., -0.6479,  0.1470,  0.9163],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387],\n",
      "        [ 0.7640, -0.4149,  0.3832,  ...,  0.7362, -0.7043, -1.6631],\n",
      "        ...,\n",
      "        [ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        [-1.0498, -0.4133, -0.4733,  ...,  1.1184, -2.3151, -1.0088],\n",
      "        [-0.3244,  0.0231,  0.4095,  ...,  0.8166,  0.2658,  0.3560]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        [ 0.9878,  0.1677,  0.0621,  ...,  0.2801, -1.0537, -0.9801],\n",
      "        [ 1.4377,  1.1952,  0.3310,  ...,  0.2546, -0.0350, -1.7429],\n",
      "        ...,\n",
      "        [-1.3831,  0.2942,  0.2118,  ..., -1.8926, -0.8853, -0.2953],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 1.0439,  0.3007,  0.2878,  ..., -0.2392,  0.0075,  0.2264],\n",
      "        [-1.0294,  1.6148,  1.2561,  ...,  0.4634, -0.5319,  0.9742],\n",
      "        [ 0.9878,  0.1677,  0.0621,  ...,  0.2801, -1.0537, -0.9801],\n",
      "        ...,\n",
      "        [ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [-0.3583,  1.2817, -0.7967,  ..., -1.4272,  1.6632,  0.2032],\n",
      "        [ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.5311,  1.1490,  1.3763,  ..., -1.8212,  0.9219, -0.2400],\n",
      "        [-1.3831,  0.2942,  0.2118,  ..., -1.8926, -0.8853, -0.2953],\n",
      "        [-1.0294,  1.6148,  1.2561,  ...,  0.4634, -0.5319,  0.9742],\n",
      "        ...,\n",
      "        [-0.9113,  0.0314, -0.0132,  ..., -0.8909,  0.1527,  0.9571],\n",
      "        [ 0.0778,  1.9595, -0.5862,  ...,  0.8202,  0.9166, -1.4922],\n",
      "        [-0.0695, -0.3896, -0.4230,  ...,  1.3443, -2.0588, -0.1447]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.2650,  0.5007,  0.6334,  ...,  0.4092, -3.1097, -1.5192],\n",
      "        [ 0.5311,  1.1490,  1.3763,  ..., -1.8212,  0.9219, -0.2400],\n",
      "        [-0.0695, -0.3896, -0.4230,  ...,  1.3443, -2.0588, -0.1447],\n",
      "        ...,\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        [ 0.6055,  1.2497,  0.7557,  ...,  0.3586,  0.2028,  0.0275]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-1.1628,  0.3368,  0.1534,  ..., -1.0601, -0.6522,  0.0741],\n",
      "        [-0.6216,  1.5653, -1.2259,  ...,  0.2656,  0.8910,  0.5925],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        ...,\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        [-0.3244,  0.0231,  0.4095,  ...,  0.8166,  0.2658,  0.3560]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-0.3450, -0.4505,  1.1307,  ..., -1.6200,  0.9929, -1.0829],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        ...,\n",
      "        [ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        [-0.7536,  0.2433,  0.9827,  ..., -1.0905, -0.0130, -1.1619],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 1.1228, -0.9397, -0.0948,  ...,  0.1395, -0.7568, -0.5425],\n",
      "        [ 0.5311,  1.1490,  1.3763,  ..., -1.8212,  0.9219, -0.2400],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        ...,\n",
      "        [ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        [ 0.3882, -0.2834, -2.3430,  ..., -0.4396,  0.3833,  0.7997],\n",
      "        [ 0.6055,  1.2497,  0.7557,  ...,  0.3586,  0.2028,  0.0275]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-1.4999, -0.4894, -0.9999,  ..., -0.1023,  0.5037, -0.4422],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058],\n",
      "        [ 0.5311,  1.1490,  1.3763,  ..., -1.8212,  0.9219, -0.2400],\n",
      "        ...,\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422],\n",
      "        [-0.7536,  0.2433,  0.9827,  ..., -1.0905, -0.0130, -1.1619],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 1.0439,  0.3007,  0.2878,  ..., -0.2392,  0.0075,  0.2264],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133],\n",
      "        ...,\n",
      "        [ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        [-0.3583,  1.2817, -0.7967,  ..., -1.4272,  1.6632,  0.2032],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.5311,  1.1490,  1.3763,  ..., -1.8212,  0.9219, -0.2400],\n",
      "        [ 0.0778,  1.9595, -0.5862,  ...,  0.8202,  0.9166, -1.4922],\n",
      "        [-0.3583,  1.2817, -0.7967,  ..., -1.4272,  1.6632,  0.2032],\n",
      "        ...,\n",
      "        [-0.4297, -1.0602, -0.6045,  ..., -1.2894, -1.8477,  0.7277],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387],\n",
      "        [-1.5713, -0.6621, -0.6136,  ...,  0.4324, -0.2257, -0.8327]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.6055,  1.2497,  0.7557,  ...,  0.3586,  0.2028,  0.0275],\n",
      "        [-0.9113,  0.0314, -0.0132,  ..., -0.8909,  0.1527,  0.9571],\n",
      "        [-0.3583,  1.2817, -0.7967,  ..., -1.4272,  1.6632,  0.2032],\n",
      "        ...,\n",
      "        [-1.3831,  0.2942,  0.2118,  ..., -1.8926, -0.8853, -0.2953],\n",
      "        [-1.0294,  1.6148,  1.2561,  ...,  0.4634, -0.5319,  0.9742],\n",
      "        [ 0.0778,  1.9595, -0.5862,  ...,  0.8202,  0.9166, -1.4922]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387],\n",
      "        [-0.3244,  0.0231,  0.4095,  ...,  0.8166,  0.2658,  0.3560],\n",
      "        ...,\n",
      "        [-1.3831,  0.2942,  0.2118,  ..., -1.8926, -0.8853, -0.2953],\n",
      "        [ 1.4377,  1.1952,  0.3310,  ...,  0.2546, -0.0350, -1.7429],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [-0.9113,  0.0314, -0.0132,  ..., -0.8909,  0.1527,  0.9571],\n",
      "        [ 0.6055,  1.2497,  0.7557,  ...,  0.3586,  0.2028,  0.0275],\n",
      "        ...,\n",
      "        [ 0.5311,  1.1490,  1.3763,  ..., -1.8212,  0.9219, -0.2400],\n",
      "        [ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        [ 0.0778,  1.9595, -0.5862,  ...,  0.8202,  0.9166, -1.4922]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-0.3583,  1.2817, -0.7967,  ..., -1.4272,  1.6632,  0.2032],\n",
      "        [-0.0695, -0.3896, -0.4230,  ...,  1.3443, -2.0588, -0.1447],\n",
      "        [-0.3244,  0.0231,  0.4095,  ...,  0.8166,  0.2658,  0.3560],\n",
      "        ...,\n",
      "        [ 0.5311,  1.1490,  1.3763,  ..., -1.8212,  0.9219, -0.2400],\n",
      "        [ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-0.3244,  0.0231,  0.4095,  ...,  0.8166,  0.2658,  0.3560],\n",
      "        [-1.3831,  0.2942,  0.2118,  ..., -1.8926, -0.8853, -0.2953],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        ...,\n",
      "        [-1.2857, -1.4086,  0.9322,  ...,  1.3655, -1.1220, -1.4872],\n",
      "        [ 0.6755,  0.2317, -0.8647,  ...,  0.2791, -1.6866,  0.1567],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-1.4999, -0.4894, -0.9999,  ..., -0.1023,  0.5037, -0.4422],\n",
      "        [ 0.9878,  0.1677,  0.0621,  ...,  0.2801, -1.0537, -0.9801],\n",
      "        [ 0.7640, -0.4149,  0.3832,  ...,  0.7362, -0.7043, -1.6631],\n",
      "        ...,\n",
      "        [ 0.3882, -0.2834, -2.3430,  ..., -0.4396,  0.3833,  0.7997],\n",
      "        [-1.0498, -0.4133, -0.4733,  ...,  1.1184, -2.3151, -1.0088],\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.1622,  1.6779, -0.9465,  ..., -0.6479,  0.1470,  0.9163],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387],\n",
      "        [-0.6216,  1.5653, -1.2259,  ...,  0.2656,  0.8910,  0.5925],\n",
      "        ...,\n",
      "        [ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        [-0.8760, -1.1808,  0.9993,  ...,  1.0108, -0.4633, -0.8207],\n",
      "        [ 0.6055,  1.2497,  0.7557,  ...,  0.3586,  0.2028,  0.0275]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.1622,  1.6779, -0.9465,  ..., -0.6479,  0.1470,  0.9163],\n",
      "        [-0.1782, -0.8454, -0.1892,  ..., -1.0560, -1.3506,  0.1387],\n",
      "        [-1.3831,  0.2942,  0.2118,  ..., -1.8926, -0.8853, -0.2953],\n",
      "        ...,\n",
      "        [-0.3583,  1.2817, -0.7967,  ..., -1.4272,  1.6632,  0.2032],\n",
      "        [ 0.8073,  1.0816, -0.0726,  ...,  1.3689, -0.8928,  1.3709],\n",
      "        [ 1.4377,  1.1952,  0.3310,  ...,  0.2546, -0.0350, -1.7429]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        [-1.5713, -0.6621, -0.6136,  ...,  0.4324, -0.2257, -0.8327],\n",
      "        [-0.8760, -1.1808,  0.9993,  ...,  1.0108, -0.4633, -0.8207],\n",
      "        ...,\n",
      "        [-0.6363,  1.0086, -0.3931,  ..., -1.2917, -0.5492, -1.3642],\n",
      "        [ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [ 0.0778,  1.9595, -0.5862,  ...,  0.8202,  0.9166, -1.4922]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-0.3244,  0.0231,  0.4095,  ...,  0.8166,  0.2658,  0.3560],\n",
      "        [-0.0695, -0.3896, -0.4230,  ...,  1.3443, -2.0588, -0.1447],\n",
      "        [-0.3583,  1.2817, -0.7967,  ..., -1.4272,  1.6632,  0.2032],\n",
      "        ...,\n",
      "        [-0.6216,  1.5653, -1.2259,  ...,  0.2656,  0.8910,  0.5925],\n",
      "        [-1.0294,  1.6148,  1.2561,  ...,  0.4634, -0.5319,  0.9742],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  1.0500,  1.2519, -0.0133]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.6055,  1.2497,  0.7557,  ...,  0.3586,  0.2028,  0.0275],\n",
      "        [ 0.0778,  1.9595, -0.5862,  ...,  0.8202,  0.9166, -1.4922],\n",
      "        [-1.9134,  0.1295,  0.7004,  ..., -0.0916, -1.2212, -0.6995],\n",
      "        ...,\n",
      "        [-1.5372,  0.5324,  1.7181,  ..., -1.4738, -2.6210, -0.7430],\n",
      "        [ 0.9878,  0.1677,  0.0621,  ...,  0.2801, -1.0537, -0.9801],\n",
      "        [-0.4297, -1.0602, -0.6045,  ..., -1.2894, -1.8477,  0.7277]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[ 0.1622,  1.6779, -0.9465,  ..., -0.6479,  0.1470,  0.9163],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058],\n",
      "        [ 0.5311,  1.1490,  1.3763,  ..., -1.8212,  0.9219, -0.2400],\n",
      "        ...,\n",
      "        [ 0.6055,  1.2497,  0.7557,  ...,  0.3586,  0.2028,  0.0275],\n",
      "        [ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [-0.3244,  0.0231,  0.4095,  ...,  0.8166,  0.2658,  0.3560]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-0.3450, -0.4505,  1.1307,  ..., -1.6200,  0.9929, -1.0829],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058],\n",
      "        [ 1.4377,  1.1952,  0.3310,  ...,  0.2546, -0.0350, -1.7429],\n",
      "        ...,\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.5786, -0.5585, -2.1422],\n",
      "        [ 0.1554,  1.2613, -0.5996,  ...,  1.8762, -0.3426, -0.3800],\n",
      "        [-0.3583,  1.2817, -0.7967,  ..., -1.4272,  1.6632,  0.2032]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>), tensor([[-1.3831,  0.2942,  0.2118,  ..., -1.8926, -0.8853, -0.2953],\n",
      "        [ 0.2083,  0.6276, -0.3099,  ..., -0.6403, -0.6912,  1.5058],\n",
      "        [ 1.4926,  0.7462, -0.9013,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        ...,\n",
      "        [-1.5372,  0.5324,  1.7181,  ..., -1.4738, -2.6210, -0.7430],\n",
      "        [-1.5713, -0.6621, -0.6136,  ...,  0.4324, -0.2257, -0.8327],\n",
      "        [ 0.6755,  0.2317, -0.8647,  ...,  0.2791, -1.6866,  0.1567]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)]\n",
      "sparse_embeds: tensor([[-1.3831,  0.2942,  0.2118,  ..., -1.8926, -0.8853, -0.2953],\n",
      "        [-2.1675,  0.8389, -1.1488,  ..., -0.6403, -0.6912,  1.5058],\n",
      "        [ 0.9878,  0.1677,  0.0621,  ...,  0.2948,  0.0908, -0.8330],\n",
      "        ...,\n",
      "        [-0.6216,  1.5653, -1.2259,  ..., -1.4738, -2.6210, -0.7430],\n",
      "        [ 0.0482,  0.9825,  2.6207,  ...,  0.4324, -0.2257, -0.8327],\n",
      "        [-2.1953,  0.8099,  1.0203,  ...,  0.2791, -1.6866,  0.1567]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>)\n",
      "concated_embeds_value.shape: torch.Size([100, 10149, 100])\n",
      "square_of_sum.shape: torch.Size([100, 1, 100])\n",
      "sum_of_square.shape: torch.Size([100, 1, 100])\n",
      "cross_term.shape: torch.Size([100, 1, 100])\n",
      "bi_out.shape: torch.Size([100, 1, 100])\n",
      "bi_out.shape: torch.Size([100, 100])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (100x100 and 10149x50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e178fad77758>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m#y_predict = nfm(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;31m#y_predict=model(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0my_predict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnfm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0my_predict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#再增加\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-5475b2d6f8d3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mdnn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdnn_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdnn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mdnn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#dnn_output为tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0;31m# dnn_output = nn.BatchNormalize(dnn_output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mdnn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (100x100 and 10149x50)"
     ]
    }
   ],
   "source": [
    "#import network\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "if __name__ == \"__main__\":\n",
    "    ####################################################################################\n",
    "    # NFM 模型\n",
    "    ####################################################################################\n",
    "    \n",
    "    \"\"\"\n",
    "    training_data, training_label, dense_features_col, sparse_features_col = getTrainData(nfm_config['train_file'], nfm_config['fea_file'])\n",
    "    train_dataset = Data.TensorDataset(torch.tensor(training_data).float(), torch.tensor(training_label).float())\n",
    "\n",
    "    test_data = getTestData(nfm_config['test_file'])\n",
    "    test_dataset = Data.TensorDataset(torch.tensor(test_data).float())\n",
    "    \n",
    "    test_data = getTestData(nfm_config['test_file'])\n",
    "    test_dataset = Data.TensorDataset(torch.tensor(test_data).float())\n",
    "\n",
    "    \"\"\"\n",
    "    #device = torch.device('cuda:0')\n",
    "    \n",
    "    \n",
    "    #model=nn.Linear(10149,16).to(device)\n",
    "    #model=nn.Linear(10149,16)\n",
    "    #model=nn.ReLU(nn.Linear(10149,16))#RuntimeError: all elements of input should be between 0 and 1\n",
    "    #print('model:',model)\n",
    "    nfm = NFM(nfm_config).cuda()#加了device防止出现GPU CPU两种设备的错误提示\n",
    "    print(\"nfm:\",nfm)\n",
    "    #print(nfm)\n",
    "    #nfm.train()\n",
    "    #u=nfm.parameters()\n",
    "    #print(u)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    optimizer = torch.optim.Adam(nfm.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    total = 0\n",
    "    #loss_func = torch.nn.BCELoss()\n",
    "    loss_func=torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    #model=nn.Softmax(nn.Linear(10149,16)).to(device)\n",
    "    # 从DataLoader中获取小批量的id以及数据\n",
    "    for batch_id, (x, labels) in enumerate(train_loader):\n",
    "            #x=torch.from_numpy(x,d)\n",
    "            #x = Variable(x,requires_grad=True)\n",
    "            #x=Variable(torch.FloatTensor(x), requires_grad=True)\n",
    "            #x=torch.FloatTensor(x)\n",
    "            x=x.view(-1,10149)\n",
    "            print('x.shape:',x.shape)\n",
    "            labels=labels.view(-1,16)\n",
    "            print('labels.shape:',labels.shape)\n",
    "            #labels=labels.squeeze(0)\n",
    "            x=torch.tensor(x,dtype=torch.float)#NameError: name 'dtype' is not defined:错误摘录，torch.Tensor()和torch.tensor()的区别\n",
    "\n",
    "            #labels=Variable(labels)\n",
    "            labels=torch.tensor(labels)\n",
    "            #labels = Variable(torch.from_numpy().float(), requires_grad=True)\n",
    "            #if self._config['use_cuda'] is True:\n",
    "            x, labels = x.cuda(), labels.cuda()\n",
    "            \n",
    "            #model.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            #y_predict = nfm(x)\n",
    "            #y_predict=model(x)\n",
    "            y_predict=nfm(x)\n",
    "            \n",
    "            y_predict=y_predict.squeeze(-1)#再增加\n",
    "            print('y_pred:',y_predict)\n",
    "            #loss = loss_func(y_predict.view(-1), labels)\n",
    "            loss=loss_func(y_predict,labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss = loss.item()\n",
    "            print(\"loss:\",loss)\n",
    "    ####################################################################################\n",
    "    # 模型训练阶段\n",
    "    ####################################################################################\n",
    "    # # 实例化模型训练器\n",
    "    #nfm(9)\n",
    "    #trainer = Trainer(nfm, nfm_config)\n",
    "    \n",
    "    # 训练\n",
    "    #trainer._train_an_epoch(train_loader,1)\n",
    "    #trainer.train(train_dataset\n",
    "    # 保存模型\n",
    "    #trainer.save()\n",
    "\n",
    "    ####################################################################################\n",
    "    # 模型测试阶段\n",
    "    ####################################################################################\n",
    "    \"\"\"\n",
    "    nfm.eval()\n",
    "    if nfm_config['use_cuda']:\n",
    "        nfm.loadModel(map_location=lambda storage, loc: storage.cuda(nfm_config['device_id']))\n",
    "        nfm = nfm.cuda()\n",
    "    else:\n",
    "        nfm.loadModel(map_location=torch.device('cpu'))\n",
    "\n",
    "    y_pred_probs = nfm(torch.tensor(test_data).float())\n",
    "    y_pred = torch.where(y_pred_probs>0.5, torch.ones_like(y_pred_probs), torch.zeros_like(y_pred_probs))\n",
    "    print(\"Test Data CTR Predict...\\n \", y_pred.view(-1))\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eba52aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4, 6]])\n",
      "tensor([[1, 4, 6],\n",
      "        [2, 1, 3]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "l1=[1,4,6]\n",
    "l2=[2,1,3]\n",
    "l1=torch.tensor(l1)\n",
    "l2=torch.tensor(l2)\n",
    "l1=torch.unsqueeze(l1,0)\n",
    "print(l1)\n",
    "l2=torch.unsqueeze(l2,0)\n",
    "#l=l1=[l1,l2]\n",
    "l=torch.cat([l1,l2],dim=0)\n",
    "print(l)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36386ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[121],\n",
      "        [ 36]])\n",
      "tensor([[53],\n",
      "        [14]])\n"
     ]
    }
   ],
   "source": [
    "square_of_sum = torch.pow(\n",
    "            torch.sum(l, dim=1, keepdim=True), 2)\n",
    "\n",
    "print(square_of_sum)\n",
    "sum_of_square = torch.sum(\n",
    "            l * l, dim=1, keepdim=True)\n",
    "print(sum_of_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0acc2c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-ffa6d01c1ad8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10149\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import network\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model=nn.Linear(10149,16).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5257e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
