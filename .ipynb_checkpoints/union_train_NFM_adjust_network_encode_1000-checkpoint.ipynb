{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "126d1f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import Trainer\n",
    "from network import NFM\n",
    "import torch.utils.data as Data\n",
    "from Utils.criteo_loader import getTestData, getTrainData\n",
    "\n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':9,\n",
    "    'linear_hidden1':1000,\n",
    "    #'linear_hidden':100,#线性模型输出层（隐层个数）\n",
    "    'embed_input_dim':1001,#embed输入维度\n",
    "    'embed_dim': 100, # 用于控制稀疏特征经过Embedding层后的稠密特征大小，embed输出维度\n",
    "    #'dnn_hidden_units': [100,11],#MLP隐层和输出层\n",
    "    'linear1_drop':0.5,\n",
    "    'dnn_hidden_units':[100,9],#MLP隐层\n",
    "    'num_sparse_features_cols':10477,#the number of the gene columns\n",
    "    'num_dense_features': 0,#dense features number\n",
    "    'bi_dropout': 0.3,#Bi-Interaction 的dropout\n",
    "    'num_epoch': 500,#训练epoch次数\n",
    "    'batch_size': 24,#batch_size\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    'epoch':1000,\n",
    "    \n",
    "    #'train_file': '../Data/criteo/processed_data/train_set.csv',\n",
    "    #'fea_file': '../Data/criteo/processed_data/fea_col.npy',\n",
    "    #'validate_file': '../Data/criteo/processed_data/val_set.csv',\n",
    "    #'test_file': '../Data/criteo/processed_data/test_set.csv',\n",
    "    #'model_name': '../TrainedModels/NFM.model'\n",
    "    #'train_file':'data/xiaoqiu_gene_5000/train/final_5000_encode_100x.csv',\n",
    "    'train_data':'dataset/qiuguan/encode/encode_1000/train/train_encode_data_1000_new.csv',\n",
    "    'train_label':'dataset/qiuguan/non_code/train/train_label.csv',\n",
    "    #'test_data':'dataset/qiuguan/non_code/test/test_encode_data.csv',\n",
    "    'test_data':'dataset/qiuguan/encode/encode_1000/test/test_encode_data_1000_new.csv',\n",
    "    'test_label':'dataset/qiuguan/non_code/test/test_labels.csv'\n",
    "    #'title':'dataset/xiaoguan/RF/RF_for_train/train_class_9/test/test_data.csv',\n",
    "    \n",
    "    #'all':''\n",
    "    #'title':'data/xiaoqiu_gene_5000/train/gene_5000_gene_name.csv',\n",
    "    #'all':'data/xiaoqiu_gene_5000/train/gene_5000_label_name.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69498a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[190 220 490 ... 360 370 420]\n",
      " [110 220 500 ... 340 260 420]\n",
      " [100 300 500 ... 330 350 390]\n",
      " ...\n",
      " [450 360 500 ... 430 360 500]\n",
      " [420 350 530 ... 440 370 500]\n",
      " [420 360 540 ... 430 370 450]]\n"
     ]
    }
   ],
   "source": [
    "#准备训练集\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "#from utils import \n",
    "#准备训练集\n",
    "#from new_dataset_processed import FMData\n",
    "from dataset_process import FMData\n",
    "def prepare_dataset(m_data,m_label,batch_size,n_class):\n",
    "    m_dataset=FMData(m_data,m_label,n_class)\n",
    "    m_dataloader=data.DataLoader(m_dataset, drop_last=True,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "    \n",
    "    return m_dataset,m_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2acb065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import sys \n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "import torchmetrics\n",
    "def   train_data(model,train_loader,test_loader,batch_size,model_path):\n",
    "    #train_accuracy=torchmetrics.Accuracy()\n",
    "    #test_accuracy=torchmetrics.Accuracy()\n",
    "    BATCH_SIZE=batch_size\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    total = 0\n",
    "    \n",
    "    #loss_func = torch.nn.BCELoss()\n",
    "    loss_func=torch.nn.CrossEntropyLoss()\n",
    "    #loss_func=nn.MultiLabelSoftMarginLoss()\n",
    "    #loss_func=torch.nn.LogSoftmax()\n",
    "    num=2000\n",
    "    #model=nn.Softmax(nn.Linear(10149,16)).to(device)\n",
    "    # 从DataLoader中获取小批量的id以及数据\n",
    "    \n",
    "    batch_size=0\n",
    "    for epoch_id in range(1000):\n",
    "        correct=0\n",
    "        total=0\n",
    "        total_test_acc=0\n",
    "        total_train_accuracy=0\n",
    "        for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "            x = Variable(x)\n",
    "            labels = Variable(labels)\n",
    "            \n",
    "            \n",
    "            #x = torch.tensor(x, dtype=torch.float)\n",
    "            #x=x.clone().detach().requires_grad_(True)\n",
    "            x=torch.tensor(x,dtype=torch.float)\n",
    "            labels=torch.tensor(labels,dtype=torch.float)\n",
    "            x, labels = x.cuda(), labels.cuda()\n",
    "            labels_int=labels=torch.max(labels,1)[1]\n",
    "            #labels_int.cuda()\n",
    "            #print(\"labels:\",labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_predict = model(x)\n",
    "            #print(\"y_predict:\",y_predict)\n",
    "            #loss = loss_func(y_predict.view(-1), labels)\n",
    "            loss = loss_func(y_predict, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss = loss.item()\n",
    "            #loss, predicted = self._train_single_batch(x, labels)\n",
    "\n",
    "            total += loss\n",
    "            \n",
    "            \n",
    "            #predicted = torch.max(y_predict.data,1)\n",
    "            #print(\"predicted:\",predicted)\n",
    "            #predicted = torch.max(y_predict.data,1)[1]\n",
    "            #predicted=predicted.detach().cpu().numpy()\n",
    "            \n",
    "            #labels=torch.max(labels,1)\n",
    "            #print(\"labels:\",labels)\n",
    "            #labels=labels[1]\n",
    "            #y_predict=y_predict.cuda()\n",
    "            #labels=torch.max(labels,1)[1].cuda()\n",
    "            #labels=labels.detach().cpu().numpy()\n",
    "            #correct += (predicted == labels).sum()\n",
    "            #print(\"correct:\",correct)\n",
    "            #correct=correct[0]\n",
    "            #print(\"new_correct:\",float(correct))\n",
    "            #correct=float(correct)   \n",
    "            #if batch_idx % 10 == 0:\n",
    "            #print(\"batch_idx:\",batch_idx)\n",
    "            #print(correct/(BATCH_SIZE*(batch_idx+1)))\n",
    "            batch_train_acc=torchmetrics.functional.accuracy(y_predict,labels_int)\n",
    "            #print('batch_train_acc:',batch_train_acc)\n",
    "            total_train_accuracy+=batch_train_acc\n",
    "        #total_train_accuracy=torchmetrics.functional.compute_details()\n",
    "        total_train_accuracy/=(batch_idx+1)\n",
    "        print('total_train_accuracy:',total_train_accuracy)\n",
    "        #print('total_train_accuracy:',total_train_accuracy)\n",
    "        for i , (inputs , targets) in enumerate(test_loader):   \n",
    "            print(\"test\")\n",
    "            # evaluate the model on the test set   \n",
    "            #print(\\ inputs:\\  inputs)   \n",
    "            #print(\\ targets:\\  targets)   \n",
    "            inputs = Variable(inputs)   \n",
    "            targets = Variable(targets)     \n",
    "            #x = torch.tensor(x  dtype=torch.float)   \n",
    "            #x=x.clone().detach().requires_grad_(True)   \n",
    "            inputs=torch.tensor(inputs ,dtype=torch.float)   \n",
    "            targets=torch.tensor(targets ,dtype=torch.float)   \n",
    "            inputs , targets = inputs.cuda(),  targets.cuda()   \n",
    "            yhat = model(inputs)  \n",
    "            \n",
    "            #yhat = torch.max(yhat.data,1)[1]\n",
    "            #yhat=yhat.detach().cpu().numpy()\n",
    "            #print(\"predicted:\",predicted)\n",
    "            #predicted = torch.max(y_predict.data,1)[1]\n",
    "             #predicted = torch.max(y_predict.data,1)[1]\n",
    "            \n",
    "            \n",
    "            targets=torch.max(targets,1)[1]\n",
    "            #print(\"labels:\",labels)\n",
    "            #labels=labels[1]\n",
    "            #targets=targets.detach().cpu().numpy()\n",
    "            \n",
    "            \n",
    "            \n",
    "            batch_test_acc=torchmetrics.functional.accuracy(yhat,targets)\n",
    "            #print(\"batch_test_acc:\",batch_test_acc)\n",
    "            total_test_acc+=batch_test_acc\n",
    "            #total_test_accuracy=torchmetrics.functional.compute_details()\n",
    "            batch_size=i\n",
    "        print('total_test_accuracy:',total_test_acc/(batch_size+1))\n",
    "        \n",
    "                    \n",
    "                    \n",
    "            \n",
    "            #model.evaluate()\n",
    "            #model.eval()\n",
    "            #train_result = evaluate.metrics(model, train_loader)\n",
    "            #valid_result = evaluate.metrics(model, valid_loader)\n",
    "            #est_result = evaluate.metrics(model, test_loader)\n",
    "            #acturals,predictions,acc_test=evaluate_model(test_loader,model)\n",
    "            #print(\"acc_test:  %d  \" %(acc_test))\n",
    "            #print(\"Train_RMSE: {:.3f}, Valid_RMSE: {:.3f}, Test_RMSE: {:.3f}\".format(\n",
    "            #train_result, valid_result, test_result))\n",
    "            # print('[Training Epoch: {}] Batch: {}, Loss: {}'.format(epoch_id, batch_id, loss))\n",
    "        print(\"Training Epoch: %d, total loss: %f\" % (epoch_id, total))\n",
    "        #print(\"auc:\",roc_auc_score)\n",
    "    #功能：保存训练完的网络的各层参数（即weights和bias)\n",
    "    #path='dataset/xiaoguan/RF/RF_for_train/train_class_9/model/gene_4000_NFM.pkl'\n",
    "        if epoch_id %100==0:\n",
    "            num=num+1\n",
    "            path=os.path.join(model_path,'NMF'+str(num)+'.pkl')\n",
    "            torch.save(model.state_dict(),path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9ca3984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from basemodel import BaseModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BiInteractionPooling(nn.Module):\n",
    "    \"\"\"Bi-Interaction Layer used in Neural FM,compress the\n",
    "      pairwise element-wise product of features into one single vector.\n",
    "      Input shape\n",
    "        - A 3D tensor with shape:``(batch_size,field_size,embedding_size)``.\n",
    "      Output shape\n",
    "    http://127.0.0.1:3000/notebooks/NFM-pyorch-master/NFM-pyorch-master/%E6%9C%AA%E5%91%BD%E5%90%8D5.ipynb?kernel_name=python3#    - 3D tensor with shape: ``(batch_size,1,embedding_size)``.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(BiInteractionPooling, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        concated_embeds_value = inputs\n",
    "        square_of_sum = torch.pow(\n",
    "            torch.sum(concated_embeds_value, dim=1, keepdim=True), 2)\n",
    "        sum_of_square = torch.sum(\n",
    "            concated_embeds_value * concated_embeds_value, dim=1, keepdim=True)\n",
    "        cross_term = 0.5 * (square_of_sum - sum_of_square)\n",
    "        return cross_term\n",
    "\n",
    "class NFM(BaseModel):\n",
    "    def __init__(self, config, dense_features_cols=[]):#=[]为新增\n",
    "    #def __init__(self, config, dense_features_cols, sparse_features_cols):\n",
    "        super(NFM, self).__init__(config)\n",
    "        # 稠密和稀疏特征的数量\n",
    "        #self.num_dense_feature = dense_features_cols.__len__()\n",
    "        self.num_dense_feature = 0#修改\n",
    "        self.num_sparse_feature = config['num_sparse_features_cols']\n",
    "        #self.num_sparse_feature = 0##修改\n",
    "        self.__config=config\n",
    "        self.drop=nn.Dropout(config['linear1_drop'])\n",
    "        # NFM的线性部分，对应 ∑WiXi\n",
    "        self.linear_model1 =nn.Linear(self.num_dense_feature + self.num_sparse_feature, config['linear_hidden1'])\n",
    "        self.BN_num=nn.BatchNorm1d(self.num_sparse_feature)  \n",
    "        self.BN_linear1 = nn.BatchNorm1d(config['linear_hidden1'])\n",
    "        \n",
    "        \n",
    "        #self.linear_model2=nn.Linear(config['linear_hidden1'],config['linear_hidden'])\n",
    "        #self.BN_linear2=nn.BatchNorm1d(config['linear_hidden'])\n",
    "       \n",
    "        \n",
    "        \n",
    "        #self.linear_model = nn.Linear(self.num_dense_feature + self.num_sparse_feature, n_class)##修改\n",
    "        # NFM的Embedding层\n",
    "        \n",
    "        \"\"\"\n",
    "        self.dnn_layers = nn.ModuleList([\n",
    "            nn.Linear(in_features=layer[0], out_features=layer[1])\\\n",
    "                for layer in list(zip(self.hidden_layers[:-1], self.hidden_layers[1:])) \n",
    "        ])\n",
    "        \"\"\"\n",
    "        self.embedding_layers=nn.Embedding(config['embed_input_dim'],config['embed_dim'])\n",
    "        \n",
    "        # B-Interaction 层\n",
    "        self.bi_pooling = BiInteractionPooling()\n",
    "        self.bi_dropout = config['bi_dropout']\n",
    "        if self.bi_dropout > 0:\n",
    "            self.dropout = nn.Dropout(self.bi_dropout)\n",
    "            \n",
    "            \n",
    "        self.BN_bi = nn.BatchNorm1d(config['embed_dim'])\n",
    "        \n",
    "        \n",
    "        # NFM的DNN部分\n",
    "        self.hidden_layers = [self.num_dense_feature+config['linear_hidden1'] + config['embed_dim']] + config['dnn_hidden_units']#是加还是乘\n",
    "        self.dnn_layers = nn.ModuleList([\n",
    "            nn.Linear(in_features=layer[0], out_features=layer[1])\\\n",
    "                for layer in list(zip(self.hidden_layers[:-1], self.hidden_layers[1:])) \n",
    "        ])\n",
    "        #self.dnn_linear = nn.Linear(self.hidden_layers[-1], 1, bias=False)\n",
    "        #self.dnn_linear = nn.Linear(self.hidden_layers[-1], n_class, bias=False)\n",
    "        \n",
    "        #增加\n",
    "        self.dnn_softmax=nn.Softmax(dim=1) # 按列SoftMax,列和为1  #注意nn.softmax的定义和调用\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 先区分出稀疏特征和稠密特征，这里是按照列来划分的，即所有的行都要进行筛选\n",
    "        dense_input, sparse_inputs = x[:, :self.num_dense_feature], x[:, self.num_dense_feature:]\n",
    "        sparse_inputs = sparse_inputs.long()\n",
    "        \n",
    "        # 求出线性部分\n",
    "        #x=F.relu(self.drop(self.BN_linear1(self.linear_model1(self.BN_num(x))))\n",
    "        #x=F.relu(self.drop(self.BN_linear1(self.linear_model1(x))))\n",
    "        x=self.BN_num(x)\n",
    "        linear_output = x\n",
    "        \n",
    "        #print(\"linear_output:\",linear_output)\n",
    "        linear_output=linear_output.view(-1,self.__config['linear_hidden1'])\n",
    "        #linear_output=self.drop(linear_output)\n",
    "        #linear_output=self.BN_linear(linear_output)\n",
    "        # 求出稀疏特征的embedding向量\n",
    "        sparse_embeds = [self.embedding_layers(sparse_inputs[:, i]) for i in range(sparse_inputs.shape[1])]\n",
    "        sparse_embeds = torch.cat(sparse_embeds, axis=-1)\n",
    "\n",
    "        # 送入B-Interaction层\n",
    "        fm_input = sparse_embeds.view(-1, self.num_sparse_feature, self.__config['embed_dim'])#整理成n行m列\n",
    "        # print(fm_input)\n",
    "        # print(fm_input.shape)\n",
    "\n",
    "        bi_out = self.bi_pooling(fm_input)\n",
    "        if self.bi_dropout:\n",
    "            bi_out = self.dropout(bi_out)\n",
    "\n",
    "        bi_out = bi_out.view(-1, self.__config['embed_dim'])\n",
    "        \n",
    "        bi_out=self.BN_bi(bi_out)\n",
    "        \n",
    "        # 将结果聚合起来\n",
    "        dnn_input = torch.cat((dense_input, bi_out,linear_output), dim=-1)\n",
    "\n",
    "        # DNN 层\n",
    "        dnn_output = dnn_input\n",
    "        for dnn in self.dnn_layers:\n",
    "            dnn_output = dnn(dnn_output)#dnn_output为tensor\n",
    "            # dnn_output = nn.BatchNormalize(dnn_output)\n",
    "            dnn_output = torch.relu(dnn_output)\n",
    "        #dnn_output = self.dnn_linear(dnn_output)\n",
    "        \n",
    "        #print(\"dnn_softmax_output:\",dnn_output.shape)\n",
    "        y_pred=self.dnn_softmax(dnn_output)#增加\n",
    "        \n",
    "        # Final\n",
    "        #output = linear_output + y_pred#修改\n",
    "        #y_pred = self.dnn_softmax(output,dim=0)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe3907d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMData\n",
      "FMData\n",
      "NFM: NFM(\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (linear_model1): Linear(in_features=10477, out_features=1000, bias=True)\n",
      "  (BN_num): BatchNorm1d(10477, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (BN_linear1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (embedding_layers): Embedding(1001, 100)\n",
      "  (bi_pooling): BiInteractionPooling()\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (BN_bi): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dnn_layers): ModuleList(\n",
      "    (0): Linear(in_features=1100, out_features=100, bias=True)\n",
      "    (1): Linear(in_features=100, out_features=9, bias=True)\n",
      "  )\n",
      "  (dnn_softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_train_accuracy: tensor(0.1715, device='cuda:0')\n",
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:109: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_test_accuracy: tensor(0.2917, device='cuda:0')\n",
      "Training Epoch: 0, total loss: 56.393041\n",
      "total_train_accuracy: tensor(0.2196, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.2917, device='cuda:0')\n",
      "Training Epoch: 1, total loss: 55.580913\n",
      "total_train_accuracy: tensor(0.2452, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 2, total loss: 54.985201\n",
      "total_train_accuracy: tensor(0.2917, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.3750, device='cuda:0')\n",
      "Training Epoch: 3, total loss: 54.031256\n",
      "total_train_accuracy: tensor(0.3317, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 4, total loss: 52.996930\n",
      "total_train_accuracy: tensor(0.3606, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.4167, device='cuda:0')\n",
      "Training Epoch: 5, total loss: 52.448913\n",
      "total_train_accuracy: tensor(0.3782, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.4583, device='cuda:0')\n",
      "Training Epoch: 6, total loss: 51.955941\n",
      "total_train_accuracy: tensor(0.3926, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.4583, device='cuda:0')\n",
      "Training Epoch: 7, total loss: 51.668394\n",
      "total_train_accuracy: tensor(0.4151, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 8, total loss: 50.976652\n",
      "total_train_accuracy: tensor(0.3862, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 9, total loss: 51.175751\n",
      "total_train_accuracy: tensor(0.4071, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.4167, device='cuda:0')\n",
      "Training Epoch: 10, total loss: 50.947078\n",
      "total_train_accuracy: tensor(0.4183, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.4583, device='cuda:0')\n",
      "Training Epoch: 11, total loss: 50.892642\n",
      "total_train_accuracy: tensor(0.4167, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 12, total loss: 50.946245\n",
      "total_train_accuracy: tensor(0.4375, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 13, total loss: 50.342449\n",
      "total_train_accuracy: tensor(0.4455, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 14, total loss: 50.093447\n",
      "total_train_accuracy: tensor(0.4647, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 15, total loss: 49.809466\n",
      "total_train_accuracy: tensor(0.4295, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.4583, device='cuda:0')\n",
      "Training Epoch: 16, total loss: 50.478445\n",
      "total_train_accuracy: tensor(0.4776, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 17, total loss: 49.322534\n",
      "total_train_accuracy: tensor(0.4391, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 18, total loss: 50.315420\n",
      "total_train_accuracy: tensor(0.4647, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 19, total loss: 49.775145\n",
      "total_train_accuracy: tensor(0.4535, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.6667, device='cuda:0')\n",
      "Training Epoch: 20, total loss: 49.688450\n",
      "total_train_accuracy: tensor(0.4744, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 21, total loss: 49.525917\n",
      "total_train_accuracy: tensor(0.4760, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 22, total loss: 49.508958\n",
      "total_train_accuracy: tensor(0.4583, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 23, total loss: 49.474083\n",
      "total_train_accuracy: tensor(0.4824, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 24, total loss: 49.121865\n",
      "total_train_accuracy: tensor(0.4776, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 25, total loss: 49.351112\n",
      "total_train_accuracy: tensor(0.4888, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 26, total loss: 49.009350\n",
      "total_train_accuracy: tensor(0.4744, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.7083, device='cuda:0')\n",
      "Training Epoch: 27, total loss: 49.235909\n",
      "total_train_accuracy: tensor(0.4952, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 28, total loss: 48.906178\n",
      "total_train_accuracy: tensor(0.4696, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 29, total loss: 49.398034\n",
      "total_train_accuracy: tensor(0.4968, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.6667, device='cuda:0')\n",
      "Training Epoch: 30, total loss: 48.907704\n",
      "total_train_accuracy: tensor(0.5176, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.4583, device='cuda:0')\n",
      "Training Epoch: 31, total loss: 48.564614\n",
      "total_train_accuracy: tensor(0.5176, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 32, total loss: 48.373309\n",
      "total_train_accuracy: tensor(0.5016, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.6667, device='cuda:0')\n",
      "Training Epoch: 33, total loss: 48.634803\n",
      "total_train_accuracy: tensor(0.5369, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 34, total loss: 48.036700\n",
      "total_train_accuracy: tensor(0.5208, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.6667, device='cuda:0')\n",
      "Training Epoch: 35, total loss: 48.363717\n",
      "total_train_accuracy: tensor(0.5609, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 36, total loss: 47.457339\n",
      "total_train_accuracy: tensor(0.5497, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 37, total loss: 47.552546\n",
      "total_train_accuracy: tensor(0.5256, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 38, total loss: 47.982463\n",
      "total_train_accuracy: tensor(0.5240, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 39, total loss: 48.052194\n",
      "total_train_accuracy: tensor(0.5737, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 40, total loss: 46.938050\n",
      "total_train_accuracy: tensor(0.5529, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.7083, device='cuda:0')\n",
      "Training Epoch: 41, total loss: 47.370758\n",
      "total_train_accuracy: tensor(0.5401, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 42, total loss: 47.705264\n",
      "total_train_accuracy: tensor(0.5176, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 43, total loss: 48.176686\n",
      "total_train_accuracy: tensor(0.5593, device='cuda:0')\n",
      "test\n",
      "total_test_accuracy: tensor(0.6667, device='cuda:0')\n",
      "Training Epoch: 44, total loss: 47.313438\n"
     ]
    }
   ],
   "source": [
    "train_dataset,train_loader=prepare_dataset(nfm_config['train_data'],nfm_config['train_label'],nfm_config['batch_size'],nfm_config['n_class'])\n",
    "test_dataset,test_loader=prepare_dataset(nfm_config['test_data'],nfm_config['test_label'],nfm_config['batch_size'],nfm_config['n_class'])\n",
    "#from MLP import MLP\n",
    "#from nfm_network_adjust import NFM\n",
    "#model=MLP(4224,1000,100,9)\n",
    "model=NFM(nfm_config)\n",
    "model.cuda()\n",
    "print(\"NFM:\",model)\n",
    "train_data(model,train_loader,test_loader,nfm_config['batch_size'],'dataset/qiuguan/model/NFM_adjust_encode_1000/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33655298",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
