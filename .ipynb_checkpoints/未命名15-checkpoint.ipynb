{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e2e9a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import Trainer\n",
    "from network import NFM\n",
    "import torch.utils.data as Data\n",
    "from Utils.criteo_loader import getTestData, getTrainData\n",
    "\n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':9,\n",
    "    'linear_hidden1':1000,\n",
    "    'linear_hidden':100,#线性模型输出层（隐层个数）\n",
    "    'embed_input_dim':1001,#embed输入维度\n",
    "    'embed_dim': 100, # 用于控制稀疏特征经过Embedding层后的稠密特征大小，embed输出维度\n",
    "    #'dnn_hidden_units': [100,11],#MLP隐层和输出层\n",
    "    \n",
    "    'dnn_hidden_units':[100,9],#MLP隐层\n",
    "    'num_sparse_features_cols':4225,#the number of the gene columns\n",
    "    'num_dense_features': 0,#dense features number\n",
    "    'bi_dropout': 0.5,#Bi-Interaction 的dropout\n",
    "    'num_epoch': 500,#训练epoch次数\n",
    "    'batch_size': 24,#batch_size\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    'epoch':1000,\n",
    "    'USE_MIXUP':True,\n",
    "    'MIXUP_ALPHA': 0.5,#add mixup alpha ,用于beta分布\n",
    "    \n",
    "    #'train_file': '../Data/criteo/processed_data/train_set.csv',\n",
    "    #'fea_file': '../Data/criteo/processed_data/fea_col.npy',\n",
    "    #'validate_file': '../Data/criteo/processed_data/val_set.csv',\n",
    "    #'test_file': '../Data/criteo/processed_data/test_set.csv',\n",
    "    #'model_name': '../TrainedModels/NFM.model'\n",
    "    #'train_file':'data/xiaoqiu_gene_5000/train/final_5000_encode_100x.csv',\n",
    "    'train_data':'dataset/xiaoguan/RF/RF_for_train/train_class_9/train/train_encode_data.csv',\n",
    "    'train_label':'dataset/xiaoguan/RF/RF_for_train/train_class_9/train/train_label.csv',\n",
    "    'test_data':'dataset/xiaoguan/RF/RF_for_train/train_class_9/test/test_encode_data.csv',\n",
    "    'test_label':'dataset/xiaoguan/RF/RF_for_train/train_class_9/test/test_label.csv',\n",
    "    #'title':'dataset/xiaoguan/RF/RF_for_train/train_class_9/test/test_data.csv',\n",
    "    \n",
    "    #'all':''\n",
    "    #'title':'data/xiaoqiu_gene_5000/train/gene_5000_gene_name.csv',\n",
    "    #'all':'data/xiaoqiu_gene_5000/train/gene_5000_label_name.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30a6590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#准备训练集\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "#from utils import \n",
    "#准备训练集\n",
    "#from new_dataset_processed import FMData\n",
    "from dataset_process import FMData\n",
    "def prepare_dataset(m_data,m_label,batch_size,n_class):\n",
    "    m_dataset=FMData(m_data,m_label,n_class)\n",
    "    m_dataloader=data.DataLoader(m_dataset, drop_last=True,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "    \n",
    "    return m_dataset,m_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f98b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = FMData(config.train_libfm,config.train_label,nfm_config['n_class'])\n",
    "#train_dataset = FMData(nfm_config['train_file'],nfm_config['label_file'],nfm_config['n_class'])\n",
    "#train_loader = data.DataLoader(train_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c9e3159",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "\n",
    "\n",
    "def   train_data(model,data_loader,batch_size,model_path):\n",
    "    \n",
    "    BATCH_SIZE=batch_size\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    total = 0\n",
    "    #loss_func = torch.nn.BCELoss()\n",
    "    loss_func=torch.nn.CrossEntropyLoss()\n",
    "    #loss_func=nn.MultiLabelSoftMarginLoss()\n",
    "    #loss_func=torch.nn.LogSoftmax()\n",
    "    \n",
    "    #model=nn.Softmax(nn.Linear(10149,16)).to(device)\n",
    "    # 从DataLoader中获取小批量的id以及数据\n",
    "    for epoch_id in range(1000):\n",
    "        correct=0\n",
    "        total=0\n",
    "        for batch_idx, (x, labels) in enumerate(data_loader):\n",
    "            x = Variable(x)\n",
    "            labels = Variable(labels)\n",
    "            \n",
    "            \n",
    "            #x = torch.tensor(x, dtype=torch.float)\n",
    "            #x=x.clone().detach().requires_grad_(True)\n",
    "            x=torch.tensor(x,dtype=torch.float)\n",
    "            labels=torch.tensor(labels,dtype=torch.float)\n",
    "            x, labels = x.cuda(), labels.cuda()\n",
    "            \n",
    "            #print(\"labels:\",labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_predict = model(x)\n",
    "            #print(\"y_predict:\",y_predict)\n",
    "            #loss = loss_func(y_predict.view(-1), labels)\n",
    "            loss = loss_func(y_predict, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss = loss.item()\n",
    "            #loss, predicted = self._train_single_batch(x, labels)\n",
    "\n",
    "            total += loss\n",
    "            \n",
    "            \n",
    "            \n",
    "            predicted = torch.max(y_predict.data,1)\n",
    "            #print(\"predicted:\",predicted)\n",
    "            predicted = torch.max(y_predict.data,1)[1]\n",
    "            \n",
    "            \n",
    "            labels=torch.max(labels,1)\n",
    "            #print(\"labels:\",labels)\n",
    "            labels=labels[1]\n",
    "            \n",
    "            correct += (predicted == labels).sum()\n",
    "            #print(\"correct:\",correct)\n",
    "            #correct=correct[0]\n",
    "            #print(\"new_correct:\",float(correct))\n",
    "            correct=float(correct)   \n",
    "            #if batch_idx % 10 == 0:\n",
    "            print(\"batch_idx:\",batch_idx)\n",
    "            print(correct/(BATCH_SIZE*(batch_idx+1)))\n",
    "            \n",
    "            \"\"\"\n",
    "            #y_predict.detach().numpy()\n",
    "            pred = y_predict\n",
    "            print(\"pred:\",pred.shape)\n",
    "            y=labels.clone().detach().requires_grad_(True)\n",
    "            print(\"y:\",y.shape)\n",
    "            #y=labels.data.cpu().numpy()\n",
    "            #y = labels.detach().numpy()\n",
    "            roc_auc_score(y, pred)\n",
    "            \"\"\"\n",
    "           \n",
    "            # print('[Training Epoch: {}] Batch: {}, Loss: {}'.format(epoch_id, batch_id, loss))\n",
    "        print(\"Training Epoch: %d, total loss: %f\" % (epoch_id, total))\n",
    "        #print(\"auc:\",roc_auc_score)\n",
    "    #功能：保存训练完的网络的各层参数（即weights和bias)\n",
    "    #path='dataset/xiaoguan/RF/RF_for_train/train_class_9/model/gene_4000_NFM.pkl'\n",
    "    \n",
    "    torch.save(nfm.state_dict(),model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc96a673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row: 0 label: [0]\n",
      "row: 1 label: [0]\n",
      "row: 2 label: [0]\n",
      "row: 3 label: [0]\n",
      "row: 4 label: [0]\n",
      "row: 5 label: [0]\n",
      "row: 6 label: [0]\n",
      "row: 7 label: [0]\n",
      "row: 8 label: [0]\n",
      "row: 9 label: [0]\n",
      "row: 10 label: [0]\n",
      "row: 11 label: [0]\n",
      "row: 12 label: [0]\n",
      "row: 13 label: [0]\n",
      "row: 14 label: [0]\n",
      "row: 15 label: [0]\n",
      "row: 16 label: [0]\n",
      "row: 17 label: [0]\n",
      "row: 18 label: [0]\n",
      "row: 19 label: [0]\n",
      "row: 20 label: [0]\n",
      "row: 21 label: [0]\n",
      "row: 22 label: [0]\n",
      "row: 23 label: [0]\n",
      "row: 24 label: [0]\n",
      "row: 25 label: [0]\n",
      "row: 26 label: [0]\n",
      "row: 27 label: [0]\n",
      "row: 28 label: [0]\n",
      "row: 29 label: [0]\n",
      "row: 30 label: [0]\n",
      "row: 31 label: [0]\n",
      "row: 32 label: [0]\n",
      "row: 33 label: [0]\n",
      "row: 34 label: [0]\n",
      "row: 35 label: [0]\n",
      "row: 36 label: [0]\n",
      "row: 37 label: [0]\n",
      "row: 38 label: [0]\n",
      "row: 39 label: [0]\n",
      "row: 40 label: [0]\n",
      "row: 41 label: [0]\n",
      "row: 42 label: [0]\n",
      "row: 43 label: [1]\n",
      "row: 44 label: [1]\n",
      "row: 45 label: [1]\n",
      "row: 46 label: [1]\n",
      "row: 47 label: [1]\n",
      "row: 48 label: [1]\n",
      "row: 49 label: [1]\n",
      "row: 50 label: [1]\n",
      "row: 51 label: [1]\n",
      "row: 52 label: [1]\n",
      "row: 53 label: [1]\n",
      "row: 54 label: [1]\n",
      "row: 55 label: [1]\n",
      "row: 56 label: [1]\n",
      "row: 57 label: [1]\n",
      "row: 58 label: [1]\n",
      "row: 59 label: [1]\n",
      "row: 60 label: [1]\n",
      "row: 61 label: [1]\n",
      "row: 62 label: [1]\n",
      "row: 63 label: [1]\n",
      "row: 64 label: [1]\n",
      "row: 65 label: [1]\n",
      "row: 66 label: [1]\n",
      "row: 67 label: [1]\n",
      "row: 68 label: [1]\n",
      "row: 69 label: [1]\n",
      "row: 70 label: [1]\n",
      "row: 71 label: [1]\n",
      "row: 72 label: [1]\n",
      "row: 73 label: [1]\n",
      "row: 74 label: [1]\n",
      "row: 75 label: [1]\n",
      "row: 76 label: [1]\n",
      "row: 77 label: [1]\n",
      "row: 78 label: [1]\n",
      "row: 79 label: [1]\n",
      "row: 80 label: [1]\n",
      "row: 81 label: [1]\n",
      "row: 82 label: [1]\n",
      "row: 83 label: [1]\n",
      "row: 84 label: [1]\n",
      "row: 85 label: [1]\n",
      "row: 86 label: [1]\n",
      "row: 87 label: [1]\n",
      "row: 88 label: [1]\n",
      "row: 89 label: [1]\n",
      "row: 90 label: [1]\n",
      "row: 91 label: [1]\n",
      "row: 92 label: [1]\n",
      "row: 93 label: [1]\n",
      "row: 94 label: [1]\n",
      "row: 95 label: [1]\n",
      "row: 96 label: [1]\n",
      "row: 97 label: [1]\n",
      "row: 98 label: [1]\n",
      "row: 99 label: [1]\n",
      "row: 100 label: [1]\n",
      "row: 101 label: [1]\n",
      "row: 102 label: [1]\n",
      "row: 103 label: [1]\n",
      "row: 104 label: [1]\n",
      "row: 105 label: [1]\n",
      "row: 106 label: [1]\n",
      "row: 107 label: [1]\n",
      "row: 108 label: [1]\n",
      "row: 109 label: [1]\n",
      "row: 110 label: [1]\n",
      "row: 111 label: [1]\n",
      "row: 112 label: [1]\n",
      "row: 113 label: [1]\n",
      "row: 114 label: [1]\n",
      "row: 115 label: [1]\n",
      "row: 116 label: [1]\n",
      "row: 117 label: [1]\n",
      "row: 118 label: [1]\n",
      "row: 119 label: [1]\n",
      "row: 120 label: [1]\n",
      "row: 121 label: [1]\n",
      "row: 122 label: [1]\n",
      "row: 123 label: [1]\n",
      "row: 124 label: [1]\n",
      "row: 125 label: [1]\n",
      "row: 126 label: [1]\n",
      "row: 127 label: [1]\n",
      "row: 128 label: [1]\n",
      "row: 129 label: [1]\n",
      "row: 130 label: [1]\n",
      "row: 131 label: [1]\n",
      "row: 132 label: [1]\n",
      "row: 133 label: [1]\n",
      "row: 134 label: [1]\n",
      "row: 135 label: [1]\n",
      "row: 136 label: [1]\n",
      "row: 137 label: [1]\n",
      "row: 138 label: [1]\n",
      "row: 139 label: [1]\n",
      "row: 140 label: [1]\n",
      "row: 141 label: [1]\n",
      "row: 142 label: [1]\n",
      "row: 143 label: [1]\n",
      "row: 144 label: [1]\n",
      "row: 145 label: [1]\n",
      "row: 146 label: [2]\n",
      "row: 147 label: [2]\n",
      "row: 148 label: [2]\n",
      "row: 149 label: [2]\n",
      "row: 150 label: [2]\n",
      "row: 151 label: [2]\n",
      "row: 152 label: [2]\n",
      "row: 153 label: [2]\n",
      "row: 154 label: [2]\n",
      "row: 155 label: [2]\n",
      "row: 156 label: [2]\n",
      "row: 157 label: [2]\n",
      "row: 158 label: [2]\n",
      "row: 159 label: [2]\n",
      "row: 160 label: [2]\n",
      "row: 161 label: [2]\n",
      "row: 162 label: [2]\n",
      "row: 163 label: [2]\n",
      "row: 164 label: [2]\n",
      "row: 165 label: [2]\n",
      "row: 166 label: [2]\n",
      "row: 167 label: [2]\n",
      "row: 168 label: [2]\n",
      "row: 169 label: [2]\n",
      "row: 170 label: [2]\n",
      "row: 171 label: [2]\n",
      "row: 172 label: [2]\n",
      "row: 173 label: [2]\n",
      "row: 174 label: [2]\n",
      "row: 175 label: [2]\n",
      "row: 176 label: [2]\n",
      "row: 177 label: [2]\n",
      "row: 178 label: [2]\n",
      "row: 179 label: [2]\n",
      "row: 180 label: [2]\n",
      "row: 181 label: [2]\n",
      "row: 182 label: [2]\n",
      "row: 183 label: [2]\n",
      "row: 184 label: [2]\n",
      "row: 185 label: [2]\n",
      "row: 186 label: [2]\n",
      "row: 187 label: [2]\n",
      "row: 188 label: [2]\n",
      "row: 189 label: [2]\n",
      "row: 190 label: [2]\n",
      "row: 191 label: [2]\n",
      "row: 192 label: [2]\n",
      "row: 193 label: [2]\n",
      "row: 194 label: [2]\n",
      "row: 195 label: [2]\n",
      "row: 196 label: [2]\n",
      "row: 197 label: [2]\n",
      "row: 198 label: [2]\n",
      "row: 199 label: [2]\n",
      "row: 200 label: [3]\n",
      "row: 201 label: [3]\n",
      "row: 202 label: [3]\n",
      "row: 203 label: [3]\n",
      "row: 204 label: [3]\n",
      "row: 205 label: [3]\n",
      "row: 206 label: [3]\n",
      "row: 207 label: [3]\n",
      "row: 208 label: [3]\n",
      "row: 209 label: [3]\n",
      "row: 210 label: [3]\n",
      "row: 211 label: [3]\n",
      "row: 212 label: [3]\n",
      "row: 213 label: [3]\n",
      "row: 214 label: [3]\n",
      "row: 215 label: [3]\n",
      "row: 216 label: [3]\n",
      "row: 217 label: [3]\n",
      "row: 218 label: [3]\n",
      "row: 219 label: [3]\n",
      "row: 220 label: [3]\n",
      "row: 221 label: [3]\n",
      "row: 222 label: [3]\n",
      "row: 223 label: [3]\n",
      "row: 224 label: [3]\n",
      "row: 225 label: [3]\n",
      "row: 226 label: [3]\n",
      "row: 227 label: [3]\n",
      "row: 228 label: [3]\n",
      "row: 229 label: [3]\n",
      "row: 230 label: [3]\n",
      "row: 231 label: [3]\n",
      "row: 232 label: [3]\n",
      "row: 233 label: [3]\n",
      "row: 234 label: [3]\n",
      "row: 235 label: [3]\n",
      "row: 236 label: [3]\n",
      "row: 237 label: [3]\n",
      "row: 238 label: [3]\n",
      "row: 239 label: [3]\n",
      "row: 240 label: [3]\n",
      "row: 241 label: [3]\n",
      "row: 242 label: [3]\n",
      "row: 243 label: [3]\n",
      "row: 244 label: [3]\n",
      "row: 245 label: [3]\n",
      "row: 246 label: [3]\n",
      "row: 247 label: [3]\n",
      "row: 248 label: [3]\n",
      "row: 249 label: [3]\n",
      "row: 250 label: [3]\n",
      "row: 251 label: [3]\n",
      "row: 252 label: [3]\n",
      "row: 253 label: [3]\n",
      "row: 254 label: [3]\n",
      "row: 255 label: [3]\n",
      "row: 256 label: [3]\n",
      "row: 257 label: [3]\n",
      "row: 258 label: [3]\n",
      "row: 259 label: [3]\n",
      "row: 260 label: [3]\n",
      "row: 261 label: [3]\n",
      "row: 262 label: [3]\n",
      "row: 263 label: [3]\n",
      "row: 264 label: [3]\n",
      "row: 265 label: [3]\n",
      "row: 266 label: [3]\n",
      "row: 267 label: [3]\n",
      "row: 268 label: [3]\n",
      "row: 269 label: [3]\n",
      "row: 270 label: [3]\n",
      "row: 271 label: [3]\n",
      "row: 272 label: [3]\n",
      "row: 273 label: [4]\n",
      "row: 274 label: [4]\n",
      "row: 275 label: [4]\n",
      "row: 276 label: [4]\n",
      "row: 277 label: [4]\n",
      "row: 278 label: [4]\n",
      "row: 279 label: [4]\n",
      "row: 280 label: [4]\n",
      "row: 281 label: [4]\n",
      "row: 282 label: [4]\n",
      "row: 283 label: [4]\n",
      "row: 284 label: [4]\n",
      "row: 285 label: [4]\n",
      "row: 286 label: [4]\n",
      "row: 287 label: [4]\n",
      "row: 288 label: [4]\n",
      "row: 289 label: [4]\n",
      "row: 290 label: [4]\n",
      "row: 291 label: [4]\n",
      "row: 292 label: [4]\n",
      "row: 293 label: [4]\n",
      "row: 294 label: [4]\n",
      "row: 295 label: [4]\n",
      "row: 296 label: [5]\n",
      "row: 297 label: [5]\n",
      "row: 298 label: [5]\n",
      "row: 299 label: [5]\n",
      "row: 300 label: [5]\n",
      "row: 301 label: [5]\n",
      "row: 302 label: [5]\n",
      "row: 303 label: [5]\n",
      "row: 304 label: [5]\n",
      "row: 305 label: [5]\n",
      "row: 306 label: [5]\n",
      "row: 307 label: [5]\n",
      "row: 308 label: [5]\n",
      "row: 309 label: [5]\n",
      "row: 310 label: [5]\n",
      "row: 311 label: [5]\n",
      "row: 312 label: [5]\n",
      "row: 313 label: [5]\n",
      "row: 314 label: [5]\n",
      "row: 315 label: [5]\n",
      "row: 316 label: [5]\n",
      "row: 317 label: [5]\n",
      "row: 318 label: [5]\n",
      "row: 319 label: [5]\n",
      "row: 320 label: [5]\n",
      "row: 321 label: [5]\n",
      "row: 322 label: [5]\n",
      "row: 323 label: [5]\n",
      "row: 324 label: [5]\n",
      "row: 325 label: [5]\n",
      "row: 326 label: [6]\n",
      "row: 327 label: [6]\n",
      "row: 328 label: [6]\n",
      "row: 329 label: [6]\n",
      "row: 330 label: [6]\n",
      "row: 331 label: [6]\n",
      "row: 332 label: [6]\n",
      "row: 333 label: [6]\n",
      "row: 334 label: [6]\n",
      "row: 335 label: [6]\n",
      "row: 336 label: [6]\n",
      "row: 337 label: [6]\n",
      "row: 338 label: [6]\n",
      "row: 339 label: [6]\n",
      "row: 340 label: [6]\n",
      "row: 341 label: [6]\n",
      "row: 342 label: [6]\n",
      "row: 343 label: [6]\n",
      "row: 344 label: [6]\n",
      "row: 345 label: [6]\n",
      "row: 346 label: [6]\n",
      "row: 347 label: [6]\n",
      "row: 348 label: [6]\n",
      "row: 349 label: [6]\n",
      "row: 350 label: [6]\n",
      "row: 351 label: [6]\n",
      "row: 352 label: [6]\n",
      "row: 353 label: [6]\n",
      "row: 354 label: [6]\n",
      "row: 355 label: [6]\n",
      "row: 356 label: [6]\n",
      "row: 357 label: [6]\n",
      "row: 358 label: [6]\n",
      "row: 359 label: [6]\n",
      "row: 360 label: [6]\n",
      "row: 361 label: [6]\n",
      "row: 362 label: [6]\n",
      "row: 363 label: [6]\n",
      "row: 364 label: [6]\n",
      "row: 365 label: [6]\n",
      "row: 366 label: [6]\n",
      "row: 367 label: [6]\n",
      "row: 368 label: [6]\n",
      "row: 369 label: [6]\n",
      "row: 370 label: [6]\n",
      "row: 371 label: [6]\n",
      "row: 372 label: [6]\n",
      "row: 373 label: [6]\n",
      "row: 374 label: [6]\n",
      "row: 375 label: [6]\n",
      "row: 376 label: [6]\n",
      "row: 377 label: [6]\n",
      "row: 378 label: [6]\n",
      "row: 379 label: [6]\n",
      "row: 380 label: [6]\n",
      "row: 381 label: [6]\n",
      "row: 382 label: [6]\n",
      "row: 383 label: [6]\n",
      "row: 384 label: [6]\n",
      "row: 385 label: [6]\n",
      "row: 386 label: [6]\n",
      "row: 387 label: [6]\n",
      "row: 388 label: [6]\n",
      "row: 389 label: [6]\n",
      "row: 390 label: [6]\n",
      "row: 391 label: [6]\n",
      "row: 392 label: [6]\n",
      "row: 393 label: [6]\n",
      "row: 394 label: [6]\n",
      "row: 395 label: [6]\n",
      "row: 396 label: [6]\n",
      "row: 397 label: [6]\n",
      "row: 398 label: [6]\n",
      "row: 399 label: [6]\n",
      "row: 400 label: [6]\n",
      "row: 401 label: [6]\n",
      "row: 402 label: [6]\n",
      "row: 403 label: [6]\n",
      "row: 404 label: [6]\n",
      "row: 405 label: [6]\n",
      "row: 406 label: [6]\n",
      "row: 407 label: [6]\n",
      "row: 408 label: [6]\n",
      "row: 409 label: [6]\n",
      "row: 410 label: [6]\n",
      "row: 411 label: [6]\n",
      "row: 412 label: [6]\n",
      "row: 413 label: [6]\n",
      "row: 414 label: [6]\n",
      "row: 415 label: [6]\n",
      "row: 416 label: [6]\n",
      "row: 417 label: [6]\n",
      "row: 418 label: [6]\n",
      "row: 419 label: [6]\n",
      "row: 420 label: [6]\n",
      "row: 421 label: [6]\n",
      "row: 422 label: [6]\n",
      "row: 423 label: [6]\n",
      "row: 424 label: [6]\n",
      "row: 425 label: [6]\n",
      "row: 426 label: [6]\n",
      "row: 427 label: [6]\n",
      "row: 428 label: [6]\n",
      "row: 429 label: [6]\n",
      "row: 430 label: [6]\n",
      "row: 431 label: [6]\n",
      "row: 432 label: [6]\n",
      "row: 433 label: [6]\n",
      "row: 434 label: [6]\n",
      "row: 435 label: [6]\n",
      "row: 436 label: [6]\n",
      "row: 437 label: [6]\n",
      "row: 438 label: [6]\n",
      "row: 439 label: [6]\n",
      "row: 440 label: [6]\n",
      "row: 441 label: [6]\n",
      "row: 442 label: [6]\n",
      "row: 443 label: [6]\n",
      "row: 444 label: [6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row: 445 label: [6]\n",
      "row: 446 label: [6]\n",
      "row: 447 label: [6]\n",
      "row: 448 label: [6]\n",
      "row: 449 label: [6]\n",
      "row: 450 label: [6]\n",
      "row: 451 label: [6]\n",
      "row: 452 label: [6]\n",
      "row: 453 label: [6]\n",
      "row: 454 label: [6]\n",
      "row: 455 label: [6]\n",
      "row: 456 label: [6]\n",
      "row: 457 label: [6]\n",
      "row: 458 label: [6]\n",
      "row: 459 label: [6]\n",
      "row: 460 label: [6]\n",
      "row: 461 label: [6]\n",
      "row: 462 label: [6]\n",
      "row: 463 label: [6]\n",
      "row: 464 label: [6]\n",
      "row: 465 label: [6]\n",
      "row: 466 label: [6]\n",
      "row: 467 label: [6]\n",
      "row: 468 label: [6]\n",
      "row: 469 label: [6]\n",
      "row: 470 label: [6]\n",
      "row: 471 label: [6]\n",
      "row: 472 label: [6]\n",
      "row: 473 label: [6]\n",
      "row: 474 label: [6]\n",
      "row: 475 label: [6]\n",
      "row: 476 label: [6]\n",
      "row: 477 label: [6]\n",
      "row: 478 label: [6]\n",
      "row: 479 label: [7]\n",
      "row: 480 label: [7]\n",
      "row: 481 label: [7]\n",
      "row: 482 label: [7]\n",
      "row: 483 label: [7]\n",
      "row: 484 label: [7]\n",
      "row: 485 label: [7]\n",
      "row: 486 label: [7]\n",
      "row: 487 label: [7]\n",
      "row: 488 label: [7]\n",
      "row: 489 label: [7]\n",
      "row: 490 label: [7]\n",
      "row: 491 label: [7]\n",
      "row: 492 label: [7]\n",
      "row: 493 label: [7]\n",
      "row: 494 label: [7]\n",
      "row: 495 label: [7]\n",
      "row: 496 label: [7]\n",
      "row: 497 label: [7]\n",
      "row: 498 label: [7]\n",
      "row: 499 label: [7]\n",
      "row: 500 label: [7]\n",
      "row: 501 label: [7]\n",
      "row: 502 label: [7]\n",
      "row: 503 label: [7]\n",
      "row: 504 label: [7]\n",
      "row: 505 label: [7]\n",
      "row: 506 label: [7]\n",
      "row: 507 label: [7]\n",
      "row: 508 label: [7]\n",
      "row: 509 label: [7]\n",
      "row: 510 label: [7]\n",
      "row: 511 label: [7]\n",
      "row: 512 label: [7]\n",
      "row: 513 label: [7]\n",
      "row: 514 label: [7]\n",
      "row: 515 label: [7]\n",
      "row: 516 label: [8]\n",
      "row: 517 label: [8]\n",
      "row: 518 label: [8]\n",
      "row: 519 label: [8]\n",
      "row: 520 label: [8]\n",
      "row: 521 label: [8]\n",
      "row: 522 label: [8]\n",
      "row: 523 label: [8]\n",
      "row: 524 label: [8]\n",
      "row: 525 label: [8]\n",
      "row: 526 label: [8]\n",
      "row: 527 label: [8]\n",
      "row: 528 label: [8]\n",
      "row: 529 label: [8]\n",
      "row: 530 label: [8]\n",
      "row: 531 label: [8]\n",
      "row: 532 label: [8]\n",
      "row: 533 label: [8]\n",
      "row: 534 label: [8]\n",
      "row: 535 label: [8]\n",
      "row: 536 label: [8]\n",
      "row: 537 label: [8]\n",
      "row: 538 label: [8]\n",
      "row: 539 label: [8]\n",
      "row: 540 label: [8]\n",
      "row: 541 label: [8]\n",
      "row: 542 label: [8]\n",
      "row: 543 label: [8]\n",
      "row: 544 label: [8]\n",
      "row: 545 label: [8]\n",
      "row: 546 label: [8]\n",
      "row: 547 label: [8]\n",
      "row: 548 label: [8]\n",
      "row: 549 label: [8]\n",
      "row: 550 label: [8]\n",
      "row: 551 label: [8]\n",
      "row: 552 label: [8]\n",
      "row: 553 label: [8]\n",
      "row: 554 label: [8]\n",
      "row: 555 label: [8]\n",
      "row: 556 label: [8]\n",
      "row: 557 label: [8]\n",
      "row: 558 label: [8]\n",
      "row: 559 label: [8]\n",
      "row: 560 label: [8]\n",
      "row: 561 label: [8]\n",
      "row: 562 label: [8]\n",
      "row: 563 label: [8]\n",
      "row: 564 label: [8]\n",
      "row: 565 label: [8]\n",
      "row: 566 label: [8]\n",
      "row: 567 label: [8]\n",
      "row: 568 label: [8]\n",
      "row: 569 label: [8]\n",
      "row: 570 label: [8]\n",
      "row: 571 label: [8]\n",
      "row: 572 label: [8]\n",
      "row: 573 label: [8]\n",
      "row: 574 label: [8]\n",
      "row: 575 label: [8]\n",
      "row: 576 label: [8]\n",
      "row: 577 label: [8]\n",
      "row: 578 label: [8]\n",
      "row: 579 label: [8]\n",
      "row: 580 label: [8]\n",
      "row: 581 label: [8]\n",
      "row: 582 label: [8]\n",
      "row: 583 label: [8]\n",
      "row: 584 label: [8]\n",
      "row: 585 label: [8]\n",
      "row: 586 label: [8]\n",
      "row: 587 label: [8]\n",
      "row: 588 label: [8]\n",
      "row: 589 label: [8]\n",
      "row: 590 label: [8]\n",
      "row: 591 label: [8]\n",
      "row: 592 label: [8]\n",
      "row: 593 label: [8]\n",
      "row: 594 label: [8]\n",
      "row: 595 label: [8]\n",
      "row: 596 label: [8]\n",
      "row: 597 label: [8]\n",
      "row: 598 label: [8]\n",
      "row: 599 label: [8]\n",
      "row: 600 label: [8]\n",
      "row: 601 label: [8]\n",
      "row: 602 label: [8]\n",
      "label: [[0.82222223 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      " ...\n",
      " [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.82222223]]\n",
      "features: [[  0  35  45 ...  31  27  28]\n",
      " [  1  35  46 ...  30  27  28]\n",
      " [  2  35  41 ...  32  26  29]\n",
      " ...\n",
      " [601  36  38 ...  14  35  32]\n",
      " [602  32  42 ...  10  29  36]\n",
      " [603  37  46 ...  13  27  33]]\n",
      "nfm: NFM(\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (linear_model1): Linear(in_features=4225, out_features=1000, bias=True)\n",
      "  (BN_linear1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear_model2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (BN_linear2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (embedding_layers): Embedding(1001, 100)\n",
      "  (bi_pooling): BiInteractionPooling()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (BN_bi): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dnn_layers): ModuleList(\n",
      "    (0): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (1): Linear(in_features=100, out_features=9, bias=True)\n",
      "  )\n",
      "  (dnn_softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.041666666666666664\n",
      "batch_idx: 1\n",
      "0.10416666666666667\n",
      "batch_idx: 2\n",
      "0.1527777777777778\n",
      "batch_idx: 3\n",
      "0.17708333333333334\n",
      "batch_idx: 4\n",
      "0.21666666666666667\n",
      "batch_idx: 5\n",
      "0.2361111111111111\n",
      "batch_idx: 6\n",
      "0.22023809523809523\n",
      "batch_idx: 7\n",
      "0.23958333333333334\n",
      "batch_idx: 8\n",
      "0.2638888888888889\n",
      "batch_idx: 9\n",
      "0.26666666666666666\n",
      "batch_idx: 10\n",
      "0.2689393939393939\n",
      "batch_idx: 11\n",
      "0.28125\n",
      "batch_idx: 12\n",
      "0.266025641025641\n",
      "batch_idx: 13\n",
      "0.2767857142857143\n",
      "batch_idx: 14\n",
      "0.2777777777777778\n",
      "batch_idx: 15\n",
      "0.2786458333333333\n",
      "batch_idx: 16\n",
      "0.2769607843137255\n",
      "batch_idx: 17\n",
      "0.2824074074074074\n",
      "batch_idx: 18\n",
      "0.2807017543859649\n",
      "batch_idx: 19\n",
      "0.27291666666666664\n",
      "batch_idx: 20\n",
      "0.27976190476190477\n",
      "batch_idx: 21\n",
      "0.2840909090909091\n",
      "batch_idx: 22\n",
      "0.28442028985507245\n",
      "batch_idx: 23\n",
      "0.2899305555555556\n",
      "batch_idx: 24\n",
      "0.29333333333333333\n",
      "Training Epoch: 0, total loss: 53.730611\n",
      "batch_idx: 0\n",
      "0.2916666666666667\n",
      "batch_idx: 1\n",
      "0.2708333333333333\n",
      "batch_idx: 2\n",
      "0.3333333333333333\n",
      "batch_idx: 3\n",
      "0.28125\n",
      "batch_idx: 4\n",
      "0.30833333333333335\n",
      "batch_idx: 5\n",
      "0.3263888888888889\n",
      "batch_idx: 6\n",
      "0.3392857142857143\n",
      "batch_idx: 7\n",
      "0.328125\n",
      "batch_idx: 8\n",
      "0.3333333333333333\n",
      "batch_idx: 9\n",
      "0.3375\n",
      "batch_idx: 10\n",
      "0.3371212121212121\n",
      "batch_idx: 11\n",
      "0.3333333333333333\n",
      "batch_idx: 12\n",
      "0.33653846153846156\n",
      "batch_idx: 13\n",
      "0.3273809523809524\n",
      "batch_idx: 14\n",
      "0.33611111111111114\n",
      "batch_idx: 15\n",
      "0.3385416666666667\n",
      "batch_idx: 16\n",
      "0.3382352941176471\n",
      "batch_idx: 17\n",
      "0.3425925925925926\n",
      "batch_idx: 18\n",
      "0.34210526315789475\n",
      "batch_idx: 19\n",
      "0.35\n",
      "batch_idx: 20\n",
      "0.3531746031746032\n",
      "batch_idx: 21\n",
      "0.3484848484848485\n",
      "batch_idx: 22\n",
      "0.3532608695652174\n",
      "batch_idx: 23\n",
      "0.3576388888888889\n",
      "batch_idx: 24\n",
      "0.3466666666666667\n",
      "Training Epoch: 1, total loss: 51.666355\n",
      "batch_idx: 0\n",
      "0.4166666666666667\n",
      "batch_idx: 1\n",
      "0.3958333333333333\n",
      "batch_idx: 2\n",
      "0.4166666666666667\n",
      "batch_idx: 3\n",
      "0.3854166666666667\n",
      "batch_idx: 4\n",
      "0.36666666666666664\n",
      "batch_idx: 5\n",
      "0.375\n",
      "batch_idx: 6\n",
      "0.36904761904761907\n",
      "batch_idx: 7\n",
      "0.3541666666666667\n",
      "batch_idx: 8\n",
      "0.37962962962962965\n",
      "batch_idx: 9\n",
      "0.375\n",
      "batch_idx: 10\n",
      "0.36742424242424243\n",
      "batch_idx: 11\n",
      "0.3611111111111111\n",
      "batch_idx: 12\n",
      "0.3717948717948718\n",
      "batch_idx: 13\n",
      "0.375\n",
      "batch_idx: 14\n",
      "0.37777777777777777\n",
      "batch_idx: 15\n",
      "0.390625\n",
      "batch_idx: 16\n",
      "0.40441176470588236\n",
      "batch_idx: 17\n",
      "0.39814814814814814\n",
      "batch_idx: 18\n",
      "0.40789473684210525\n",
      "batch_idx: 19\n",
      "0.4083333333333333\n",
      "batch_idx: 20\n",
      "0.4126984126984127\n",
      "batch_idx: 21\n",
      "0.42234848484848486\n",
      "batch_idx: 22\n",
      "0.42391304347826086\n",
      "batch_idx: 23\n",
      "0.4201388888888889\n",
      "batch_idx: 24\n",
      "0.4166666666666667\n",
      "Training Epoch: 2, total loss: 50.537923\n",
      "batch_idx: 0\n",
      "0.3333333333333333\n",
      "batch_idx: 1\n",
      "0.3333333333333333\n",
      "batch_idx: 2\n",
      "0.3611111111111111\n",
      "batch_idx: 3\n",
      "0.3645833333333333\n",
      "batch_idx: 4\n",
      "0.375\n",
      "batch_idx: 5\n",
      "0.375\n",
      "batch_idx: 6\n",
      "0.4166666666666667\n",
      "batch_idx: 7\n",
      "0.4322916666666667\n",
      "batch_idx: 8\n",
      "0.4351851851851852\n",
      "batch_idx: 9\n",
      "0.44583333333333336\n",
      "batch_idx: 10\n",
      "0.4431818181818182\n",
      "batch_idx: 11\n",
      "0.4479166666666667\n",
      "batch_idx: 12\n",
      "0.44551282051282054\n",
      "batch_idx: 13\n",
      "0.4642857142857143\n",
      "batch_idx: 14\n",
      "0.4666666666666667\n",
      "batch_idx: 15\n",
      "0.4661458333333333\n",
      "batch_idx: 16\n",
      "0.46568627450980393\n",
      "batch_idx: 17\n",
      "0.4699074074074074\n",
      "batch_idx: 18\n",
      "0.4780701754385965\n",
      "batch_idx: 19\n",
      "0.4791666666666667\n",
      "batch_idx: 20\n",
      "0.48214285714285715\n",
      "batch_idx: 21\n",
      "0.48295454545454547\n",
      "batch_idx: 22\n",
      "0.4855072463768116\n",
      "batch_idx: 23\n",
      "0.4878472222222222\n",
      "batch_idx: 24\n",
      "0.49166666666666664\n",
      "Training Epoch: 3, total loss: 49.066293\n",
      "batch_idx: 0\n",
      "0.375\n",
      "batch_idx: 1\n",
      "0.3541666666666667\n",
      "batch_idx: 2\n",
      "0.4166666666666667\n",
      "batch_idx: 3\n",
      "0.4895833333333333\n",
      "batch_idx: 4\n",
      "0.5416666666666666\n",
      "batch_idx: 5\n",
      "0.5347222222222222\n",
      "batch_idx: 6\n",
      "0.5059523809523809\n",
      "batch_idx: 7\n",
      "0.5\n",
      "batch_idx: 8\n",
      "0.49537037037037035\n",
      "batch_idx: 9\n",
      "0.5\n",
      "batch_idx: 10\n",
      "0.5037878787878788\n",
      "batch_idx: 11\n",
      "0.5104166666666666\n",
      "batch_idx: 12\n",
      "0.5224358974358975\n",
      "batch_idx: 13\n",
      "0.5267857142857143\n",
      "batch_idx: 14\n",
      "0.5333333333333333\n",
      "batch_idx: 15\n",
      "0.5416666666666666\n",
      "batch_idx: 16\n",
      "0.553921568627451\n",
      "batch_idx: 17\n",
      "0.5486111111111112\n",
      "batch_idx: 18\n",
      "0.5657894736842105\n",
      "batch_idx: 19\n",
      "0.5666666666666667\n",
      "batch_idx: 20\n",
      "0.5634920634920635\n",
      "batch_idx: 21\n",
      "0.571969696969697\n",
      "batch_idx: 22\n",
      "0.5760869565217391\n",
      "batch_idx: 23\n",
      "0.5729166666666666\n",
      "batch_idx: 24\n",
      "0.58\n",
      "Training Epoch: 4, total loss: 47.650624\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.5416666666666666\n",
      "batch_idx: 2\n",
      "0.5694444444444444\n",
      "batch_idx: 3\n",
      "0.5416666666666666\n",
      "batch_idx: 4\n",
      "0.55\n",
      "batch_idx: 5\n",
      "0.5486111111111112\n",
      "batch_idx: 6\n",
      "0.5654761904761905\n",
      "batch_idx: 7\n",
      "0.5833333333333334\n",
      "batch_idx: 8\n",
      "0.5879629629629629\n",
      "batch_idx: 9\n",
      "0.6041666666666666\n",
      "batch_idx: 10\n",
      "0.6136363636363636\n",
      "batch_idx: 11\n",
      "0.6215277777777778\n",
      "batch_idx: 12\n",
      "0.6217948717948718\n",
      "batch_idx: 13\n",
      "0.6190476190476191\n",
      "batch_idx: 14\n",
      "0.6111111111111112\n",
      "batch_idx: 15\n",
      "0.609375\n",
      "batch_idx: 16\n",
      "0.6274509803921569\n",
      "batch_idx: 17\n",
      "0.6226851851851852\n",
      "batch_idx: 18\n",
      "0.6206140350877193\n",
      "batch_idx: 19\n",
      "0.6270833333333333\n",
      "batch_idx: 20\n",
      "0.628968253968254\n",
      "batch_idx: 21\n",
      "0.6363636363636364\n",
      "batch_idx: 22\n",
      "0.6376811594202898\n",
      "batch_idx: 23\n",
      "0.6371527777777778\n",
      "batch_idx: 24\n",
      "0.6316666666666667\n",
      "Training Epoch: 5, total loss: 46.694583\n",
      "batch_idx: 0\n",
      "0.4583333333333333\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.6111111111111112\n",
      "batch_idx: 3\n",
      "0.59375\n",
      "batch_idx: 4\n",
      "0.6083333333333333\n",
      "batch_idx: 5\n",
      "0.5972222222222222\n",
      "batch_idx: 6\n",
      "0.6011904761904762\n",
      "batch_idx: 7\n",
      "0.6197916666666666\n",
      "batch_idx: 8\n",
      "0.6296296296296297\n",
      "batch_idx: 9\n",
      "0.6375\n",
      "batch_idx: 10\n",
      "0.6477272727272727\n",
      "batch_idx: 11\n",
      "0.6493055555555556\n",
      "batch_idx: 12\n",
      "0.6442307692307693\n",
      "batch_idx: 13\n",
      "0.6547619047619048\n",
      "batch_idx: 14\n",
      "0.6472222222222223\n",
      "batch_idx: 15\n",
      "0.6536458333333334\n",
      "batch_idx: 16\n",
      "0.6372549019607843\n",
      "batch_idx: 17\n",
      "0.6527777777777778\n",
      "batch_idx: 18\n",
      "0.6491228070175439\n",
      "batch_idx: 19\n",
      "0.6520833333333333\n",
      "batch_idx: 20\n",
      "0.6468253968253969\n",
      "batch_idx: 21\n",
      "0.6515151515151515\n",
      "batch_idx: 22\n",
      "0.644927536231884\n",
      "batch_idx: 23\n",
      "0.640625\n",
      "batch_idx: 24\n",
      "0.6383333333333333\n",
      "Training Epoch: 6, total loss: 46.243730\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.5833333333333334\n",
      "batch_idx: 3\n",
      "0.6041666666666666\n",
      "batch_idx: 4\n",
      "0.6166666666666667\n",
      "batch_idx: 5\n",
      "0.625\n",
      "batch_idx: 6\n",
      "0.6309523809523809\n",
      "batch_idx: 7\n",
      "0.6302083333333334\n",
      "batch_idx: 8\n",
      "0.6342592592592593\n",
      "batch_idx: 9\n",
      "0.6416666666666667\n",
      "batch_idx: 10\n",
      "0.6363636363636364\n",
      "batch_idx: 11\n",
      "0.6493055555555556\n",
      "batch_idx: 12\n",
      "0.657051282051282\n",
      "batch_idx: 13\n",
      "0.6607142857142857\n",
      "batch_idx: 14\n",
      "0.6666666666666666\n",
      "batch_idx: 15\n",
      "0.6770833333333334\n",
      "batch_idx: 16\n",
      "0.6740196078431373\n",
      "batch_idx: 17\n",
      "0.6736111111111112\n",
      "batch_idx: 18\n",
      "0.6776315789473685\n",
      "batch_idx: 19\n",
      "0.6729166666666667\n",
      "batch_idx: 20\n",
      "0.6765873015873016\n",
      "batch_idx: 21\n",
      "0.6723484848484849\n",
      "batch_idx: 22\n",
      "0.6721014492753623\n",
      "batch_idx: 23\n",
      "0.6666666666666666\n",
      "batch_idx: 24\n",
      "0.6716666666666666\n",
      "Training Epoch: 7, total loss: 45.771832\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.7142857142857143\n",
      "batch_idx: 7\n",
      "0.7135416666666666\n",
      "batch_idx: 8\n",
      "0.7083333333333334\n",
      "batch_idx: 9\n",
      "0.6958333333333333\n",
      "batch_idx: 10\n",
      "0.7007575757575758\n",
      "batch_idx: 11\n",
      "0.6979166666666666\n",
      "batch_idx: 12\n",
      "0.6891025641025641\n",
      "batch_idx: 13\n",
      "0.6875\n",
      "batch_idx: 14\n",
      "0.6888888888888889\n",
      "batch_idx: 15\n",
      "0.6796875\n",
      "batch_idx: 16\n",
      "0.678921568627451\n",
      "batch_idx: 17\n",
      "0.6805555555555556\n",
      "batch_idx: 18\n",
      "0.6754385964912281\n",
      "batch_idx: 19\n",
      "0.675\n",
      "batch_idx: 20\n",
      "0.6765873015873016\n",
      "batch_idx: 21\n",
      "0.6761363636363636\n",
      "batch_idx: 22\n",
      "0.677536231884058\n",
      "batch_idx: 23\n",
      "0.6788194444444444\n",
      "batch_idx: 24\n",
      "0.68\n",
      "Training Epoch: 8, total loss: 45.515344\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.5\n",
      "batch_idx: 2\n",
      "0.5833333333333334\n",
      "batch_idx: 3\n",
      "0.5833333333333334\n",
      "batch_idx: 4\n",
      "0.6166666666666667\n",
      "batch_idx: 5\n",
      "0.6666666666666666\n",
      "batch_idx: 6\n",
      "0.6607142857142857\n",
      "batch_idx: 7\n",
      "0.6458333333333334\n",
      "batch_idx: 8\n",
      "0.6574074074074074\n",
      "batch_idx: 9\n",
      "0.6666666666666666\n",
      "batch_idx: 10\n",
      "0.6856060606060606\n",
      "batch_idx: 11\n",
      "0.6944444444444444\n",
      "batch_idx: 12\n",
      "0.7051282051282052\n",
      "batch_idx: 13\n",
      "0.7053571428571429\n",
      "batch_idx: 14\n",
      "0.6916666666666667\n",
      "batch_idx: 15\n",
      "0.6901041666666666\n",
      "batch_idx: 16\n",
      "0.6715686274509803\n",
      "batch_idx: 17\n",
      "0.6759259259259259\n",
      "batch_idx: 18\n",
      "0.6710526315789473\n",
      "batch_idx: 19\n",
      "0.675\n",
      "batch_idx: 20\n",
      "0.6785714285714286\n",
      "batch_idx: 21\n",
      "0.6818181818181818\n",
      "batch_idx: 22\n",
      "0.6757246376811594\n",
      "batch_idx: 23\n",
      "0.6753472222222222\n",
      "batch_idx: 24\n",
      "0.6733333333333333\n",
      "Training Epoch: 9, total loss: 45.435873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.6666666666666666\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6736111111111112\n",
      "batch_idx: 6\n",
      "0.6785714285714286\n",
      "batch_idx: 7\n",
      "0.6927083333333334\n",
      "batch_idx: 8\n",
      "0.7037037037037037\n",
      "batch_idx: 9\n",
      "0.7083333333333334\n",
      "batch_idx: 10\n",
      "0.7196969696969697\n",
      "batch_idx: 11\n",
      "0.7152777777777778\n",
      "batch_idx: 12\n",
      "0.7051282051282052\n",
      "batch_idx: 13\n",
      "0.6964285714285714\n",
      "batch_idx: 14\n",
      "0.7027777777777777\n",
      "batch_idx: 15\n",
      "0.6953125\n",
      "batch_idx: 16\n",
      "0.6911764705882353\n",
      "batch_idx: 17\n",
      "0.6990740740740741\n",
      "batch_idx: 18\n",
      "0.6907894736842105\n",
      "batch_idx: 19\n",
      "0.6895833333333333\n",
      "batch_idx: 20\n",
      "0.6884920634920635\n",
      "batch_idx: 21\n",
      "0.6912878787878788\n",
      "batch_idx: 22\n",
      "0.6865942028985508\n",
      "batch_idx: 23\n",
      "0.6892361111111112\n",
      "batch_idx: 24\n",
      "0.6933333333333334\n",
      "Training Epoch: 10, total loss: 45.201853\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7440476190476191\n",
      "batch_idx: 7\n",
      "0.7291666666666666\n",
      "batch_idx: 8\n",
      "0.7175925925925926\n",
      "batch_idx: 9\n",
      "0.7083333333333334\n",
      "batch_idx: 10\n",
      "0.7045454545454546\n",
      "batch_idx: 11\n",
      "0.7083333333333334\n",
      "batch_idx: 12\n",
      "0.7051282051282052\n",
      "batch_idx: 13\n",
      "0.7053571428571429\n",
      "batch_idx: 14\n",
      "0.6972222222222222\n",
      "batch_idx: 15\n",
      "0.7005208333333334\n",
      "batch_idx: 16\n",
      "0.7058823529411765\n",
      "batch_idx: 17\n",
      "0.7013888888888888\n",
      "batch_idx: 18\n",
      "0.6995614035087719\n",
      "batch_idx: 19\n",
      "0.6979166666666666\n",
      "batch_idx: 20\n",
      "0.7003968253968254\n",
      "batch_idx: 21\n",
      "0.696969696969697\n",
      "batch_idx: 22\n",
      "0.6938405797101449\n",
      "batch_idx: 23\n",
      "0.6944444444444444\n",
      "batch_idx: 24\n",
      "0.695\n",
      "Training Epoch: 11, total loss: 44.941817\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7152777777777778\n",
      "batch_idx: 6\n",
      "0.7142857142857143\n",
      "batch_idx: 7\n",
      "0.7239583333333334\n",
      "batch_idx: 8\n",
      "0.7175925925925926\n",
      "batch_idx: 9\n",
      "0.7333333333333333\n",
      "batch_idx: 10\n",
      "0.7234848484848485\n",
      "batch_idx: 11\n",
      "0.7361111111111112\n",
      "batch_idx: 12\n",
      "0.7307692307692307\n",
      "batch_idx: 13\n",
      "0.7291666666666666\n",
      "batch_idx: 14\n",
      "0.7305555555555555\n",
      "batch_idx: 15\n",
      "0.7317708333333334\n",
      "batch_idx: 16\n",
      "0.7279411764705882\n",
      "batch_idx: 17\n",
      "0.7314814814814815\n",
      "batch_idx: 18\n",
      "0.7324561403508771\n",
      "batch_idx: 19\n",
      "0.7375\n",
      "batch_idx: 20\n",
      "0.7281746031746031\n",
      "batch_idx: 21\n",
      "0.7253787878787878\n",
      "batch_idx: 22\n",
      "0.7264492753623188\n",
      "batch_idx: 23\n",
      "0.7309027777777778\n",
      "batch_idx: 24\n",
      "0.7316666666666667\n",
      "Training Epoch: 12, total loss: 44.357399\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.7447916666666666\n",
      "batch_idx: 8\n",
      "0.7407407407407407\n",
      "batch_idx: 9\n",
      "0.75\n",
      "batch_idx: 10\n",
      "0.7575757575757576\n",
      "batch_idx: 11\n",
      "0.7569444444444444\n",
      "batch_idx: 12\n",
      "0.7564102564102564\n",
      "batch_idx: 13\n",
      "0.7559523809523809\n",
      "batch_idx: 14\n",
      "0.7583333333333333\n",
      "batch_idx: 15\n",
      "0.7552083333333334\n",
      "batch_idx: 16\n",
      "0.7524509803921569\n",
      "batch_idx: 17\n",
      "0.75\n",
      "batch_idx: 18\n",
      "0.7521929824561403\n",
      "batch_idx: 19\n",
      "0.7583333333333333\n",
      "batch_idx: 20\n",
      "0.746031746031746\n",
      "batch_idx: 21\n",
      "0.740530303030303\n",
      "batch_idx: 22\n",
      "0.7318840579710145\n",
      "batch_idx: 23\n",
      "0.7378472222222222\n",
      "batch_idx: 24\n",
      "0.74\n",
      "Training Epoch: 13, total loss: 44.312508\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7731481481481481\n",
      "batch_idx: 9\n",
      "0.7625\n",
      "batch_idx: 10\n",
      "0.75\n",
      "batch_idx: 11\n",
      "0.75\n",
      "batch_idx: 12\n",
      "0.7275641025641025\n",
      "batch_idx: 13\n",
      "0.7380952380952381\n",
      "batch_idx: 14\n",
      "0.7333333333333333\n",
      "batch_idx: 15\n",
      "0.7317708333333334\n",
      "batch_idx: 16\n",
      "0.7352941176470589\n",
      "batch_idx: 17\n",
      "0.7291666666666666\n",
      "batch_idx: 18\n",
      "0.7236842105263158\n",
      "batch_idx: 19\n",
      "0.7166666666666667\n",
      "batch_idx: 20\n",
      "0.7242063492063492\n",
      "batch_idx: 21\n",
      "0.7253787878787878\n",
      "batch_idx: 22\n",
      "0.7264492753623188\n",
      "batch_idx: 23\n",
      "0.7309027777777778\n",
      "batch_idx: 24\n",
      "0.725\n",
      "Training Epoch: 14, total loss: 44.313203\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.725\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.7261904761904762\n",
      "batch_idx: 7\n",
      "0.7395833333333334\n",
      "batch_idx: 8\n",
      "0.7407407407407407\n",
      "batch_idx: 9\n",
      "0.7458333333333333\n",
      "batch_idx: 10\n",
      "0.7424242424242424\n",
      "batch_idx: 11\n",
      "0.7534722222222222\n",
      "batch_idx: 12\n",
      "0.75\n",
      "batch_idx: 13\n",
      "0.7470238095238095\n",
      "batch_idx: 14\n",
      "0.75\n",
      "batch_idx: 15\n",
      "0.7552083333333334\n",
      "batch_idx: 16\n",
      "0.7524509803921569\n",
      "batch_idx: 17\n",
      "0.75\n",
      "batch_idx: 18\n",
      "0.7543859649122807\n",
      "batch_idx: 19\n",
      "0.7520833333333333\n",
      "batch_idx: 20\n",
      "0.751984126984127\n",
      "batch_idx: 21\n",
      "0.7462121212121212\n",
      "batch_idx: 22\n",
      "0.7536231884057971\n",
      "batch_idx: 23\n",
      "0.75\n",
      "batch_idx: 24\n",
      "0.75\n",
      "Training Epoch: 15, total loss: 44.179812\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.6944444444444444\n",
      "batch_idx: 6\n",
      "0.6964285714285714\n",
      "batch_idx: 7\n",
      "0.7083333333333334\n",
      "batch_idx: 8\n",
      "0.7129629629629629\n",
      "batch_idx: 9\n",
      "0.7333333333333333\n",
      "batch_idx: 10\n",
      "0.7348484848484849\n",
      "batch_idx: 11\n",
      "0.7326388888888888\n",
      "batch_idx: 12\n",
      "0.7275641025641025\n",
      "batch_idx: 13\n",
      "0.7261904761904762\n",
      "batch_idx: 14\n",
      "0.7194444444444444\n",
      "batch_idx: 15\n",
      "0.7135416666666666\n",
      "batch_idx: 16\n",
      "0.7083333333333334\n",
      "batch_idx: 17\n",
      "0.7060185185185185\n",
      "batch_idx: 18\n",
      "0.7149122807017544\n",
      "batch_idx: 19\n",
      "0.7166666666666667\n",
      "batch_idx: 20\n",
      "0.7222222222222222\n",
      "batch_idx: 21\n",
      "0.7253787878787878\n",
      "batch_idx: 22\n",
      "0.7228260869565217\n",
      "batch_idx: 23\n",
      "0.7291666666666666\n",
      "batch_idx: 24\n",
      "0.7233333333333334\n",
      "Training Epoch: 16, total loss: 44.441707\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.7083333333333334\n",
      "batch_idx: 6\n",
      "0.7142857142857143\n",
      "batch_idx: 7\n",
      "0.7135416666666666\n",
      "batch_idx: 8\n",
      "0.7037037037037037\n",
      "batch_idx: 9\n",
      "0.7083333333333334\n",
      "batch_idx: 10\n",
      "0.7083333333333334\n",
      "batch_idx: 11\n",
      "0.7048611111111112\n",
      "batch_idx: 12\n",
      "0.7147435897435898\n",
      "batch_idx: 13\n",
      "0.7202380952380952\n",
      "batch_idx: 14\n",
      "0.7111111111111111\n",
      "batch_idx: 15\n",
      "0.7135416666666666\n",
      "batch_idx: 16\n",
      "0.7083333333333334\n",
      "batch_idx: 17\n",
      "0.7013888888888888\n",
      "batch_idx: 18\n",
      "0.7039473684210527\n",
      "batch_idx: 19\n",
      "0.7125\n",
      "batch_idx: 20\n",
      "0.7142857142857143\n",
      "batch_idx: 21\n",
      "0.7196969696969697\n",
      "batch_idx: 22\n",
      "0.7246376811594203\n",
      "batch_idx: 23\n",
      "0.7256944444444444\n",
      "batch_idx: 24\n",
      "0.7233333333333334\n",
      "Training Epoch: 17, total loss: 44.360811\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.7541666666666667\n",
      "batch_idx: 10\n",
      "0.7613636363636364\n",
      "batch_idx: 11\n",
      "0.7465277777777778\n",
      "batch_idx: 12\n",
      "0.7532051282051282\n",
      "batch_idx: 13\n",
      "0.7470238095238095\n",
      "batch_idx: 14\n",
      "0.7527777777777778\n",
      "batch_idx: 15\n",
      "0.7473958333333334\n",
      "batch_idx: 16\n",
      "0.7475490196078431\n",
      "batch_idx: 17\n",
      "0.7476851851851852\n",
      "batch_idx: 18\n",
      "0.7478070175438597\n",
      "batch_idx: 19\n",
      "0.75\n",
      "batch_idx: 20\n",
      "0.75\n",
      "batch_idx: 21\n",
      "0.7367424242424242\n",
      "batch_idx: 22\n",
      "0.7427536231884058\n",
      "batch_idx: 23\n",
      "0.7413194444444444\n",
      "batch_idx: 24\n",
      "0.7333333333333333\n",
      "Training Epoch: 18, total loss: 44.252057\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.7708333333333334\n",
      "batch_idx: 10\n",
      "0.7689393939393939\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.7628205128205128\n",
      "batch_idx: 13\n",
      "0.7648809523809523\n",
      "batch_idx: 14\n",
      "0.7611111111111111\n",
      "batch_idx: 15\n",
      "0.7604166666666666\n",
      "batch_idx: 16\n",
      "0.7647058823529411\n",
      "batch_idx: 17\n",
      "0.7708333333333334\n",
      "batch_idx: 18\n",
      "0.7763157894736842\n",
      "batch_idx: 19\n",
      "0.775\n",
      "batch_idx: 20\n",
      "0.7718253968253969\n",
      "batch_idx: 21\n",
      "0.7651515151515151\n",
      "batch_idx: 22\n",
      "0.7626811594202898\n",
      "batch_idx: 23\n",
      "0.7517361111111112\n",
      "batch_idx: 24\n",
      "0.755\n",
      "Training Epoch: 19, total loss: 43.979266\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.7660256410256411\n",
      "batch_idx: 13\n",
      "0.7708333333333334\n",
      "batch_idx: 14\n",
      "0.7638888888888888\n",
      "batch_idx: 15\n",
      "0.7578125\n",
      "batch_idx: 16\n",
      "0.7524509803921569\n",
      "batch_idx: 17\n",
      "0.75\n",
      "batch_idx: 18\n",
      "0.7521929824561403\n",
      "batch_idx: 19\n",
      "0.7479166666666667\n",
      "batch_idx: 20\n",
      "0.746031746031746\n",
      "batch_idx: 21\n",
      "0.7424242424242424\n",
      "batch_idx: 22\n",
      "0.7427536231884058\n",
      "batch_idx: 23\n",
      "0.7482638888888888\n",
      "batch_idx: 24\n",
      "0.7516666666666667\n",
      "Training Epoch: 20, total loss: 43.847061\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.7791666666666667\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7886904761904762\n",
      "batch_idx: 14\n",
      "0.7888888888888889\n",
      "batch_idx: 15\n",
      "0.78125\n",
      "batch_idx: 16\n",
      "0.7867647058823529\n",
      "batch_idx: 17\n",
      "0.7870370370370371\n",
      "batch_idx: 18\n",
      "0.7916666666666666\n",
      "batch_idx: 19\n",
      "0.7916666666666666\n",
      "batch_idx: 20\n",
      "0.7817460317460317\n",
      "batch_idx: 21\n",
      "0.7727272727272727\n",
      "batch_idx: 22\n",
      "0.7753623188405797\n",
      "batch_idx: 23\n",
      "0.7690972222222222\n",
      "batch_idx: 24\n",
      "0.7733333333333333\n",
      "Training Epoch: 21, total loss: 43.505010\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7870370370370371\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7797619047619048\n",
      "batch_idx: 14\n",
      "0.775\n",
      "batch_idx: 15\n",
      "0.78125\n",
      "batch_idx: 16\n",
      "0.7745098039215687\n",
      "batch_idx: 17\n",
      "0.7777777777777778\n",
      "batch_idx: 18\n",
      "0.7763157894736842\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7837301587301587\n",
      "batch_idx: 21\n",
      "0.7803030303030303\n",
      "batch_idx: 22\n",
      "0.7735507246376812\n",
      "batch_idx: 23\n",
      "0.7760416666666666\n",
      "batch_idx: 24\n",
      "0.7716666666666666\n",
      "Training Epoch: 22, total loss: 43.681193\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.7395833333333334\n",
      "batch_idx: 8\n",
      "0.7407407407407407\n",
      "batch_idx: 9\n",
      "0.7458333333333333\n",
      "batch_idx: 10\n",
      "0.7613636363636364\n",
      "batch_idx: 11\n",
      "0.7604166666666666\n",
      "batch_idx: 12\n",
      "0.7724358974358975\n",
      "batch_idx: 13\n",
      "0.7738095238095238\n",
      "batch_idx: 14\n",
      "0.7777777777777778\n",
      "batch_idx: 15\n",
      "0.78125\n",
      "batch_idx: 16\n",
      "0.7843137254901961\n",
      "batch_idx: 17\n",
      "0.7708333333333334\n",
      "batch_idx: 18\n",
      "0.7741228070175439\n",
      "batch_idx: 19\n",
      "0.7770833333333333\n",
      "batch_idx: 20\n",
      "0.7817460317460317\n",
      "batch_idx: 21\n",
      "0.7784090909090909\n",
      "batch_idx: 22\n",
      "0.7735507246376812\n",
      "batch_idx: 23\n",
      "0.7760416666666666\n",
      "batch_idx: 24\n",
      "0.775\n",
      "Training Epoch: 23, total loss: 43.400473\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8257575757575758\n",
      "batch_idx: 11\n",
      "0.8298611111111112\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.8063725490196079\n",
      "batch_idx: 17\n",
      "0.8055555555555556\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.7979166666666667\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.7916666666666666\n",
      "batch_idx: 22\n",
      "0.7971014492753623\n",
      "batch_idx: 23\n",
      "0.7934027777777778\n",
      "batch_idx: 24\n",
      "0.8\n",
      "Training Epoch: 24, total loss: 43.004478\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.7261904761904762\n",
      "batch_idx: 7\n",
      "0.7239583333333334\n",
      "batch_idx: 8\n",
      "0.7361111111111112\n",
      "batch_idx: 9\n",
      "0.75\n",
      "batch_idx: 10\n",
      "0.7613636363636364\n",
      "batch_idx: 11\n",
      "0.7638888888888888\n",
      "batch_idx: 12\n",
      "0.7564102564102564\n",
      "batch_idx: 13\n",
      "0.7619047619047619\n",
      "batch_idx: 14\n",
      "0.7611111111111111\n",
      "batch_idx: 15\n",
      "0.7630208333333334\n",
      "batch_idx: 16\n",
      "0.7720588235294118\n",
      "batch_idx: 17\n",
      "0.7754629629629629\n",
      "batch_idx: 18\n",
      "0.7741228070175439\n",
      "batch_idx: 19\n",
      "0.775\n",
      "batch_idx: 20\n",
      "0.7738095238095238\n",
      "batch_idx: 21\n",
      "0.7784090909090909\n",
      "batch_idx: 22\n",
      "0.7807971014492754\n",
      "batch_idx: 23\n",
      "0.7777777777777778\n",
      "batch_idx: 24\n",
      "0.775\n",
      "Training Epoch: 25, total loss: 43.419749\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.7946428571428571\n",
      "batch_idx: 14\n",
      "0.7944444444444444\n",
      "batch_idx: 15\n",
      "0.8020833333333334\n",
      "batch_idx: 16\n",
      "0.8063725490196079\n",
      "batch_idx: 17\n",
      "0.7939814814814815\n",
      "batch_idx: 18\n",
      "0.7960526315789473\n",
      "batch_idx: 19\n",
      "0.7875\n",
      "batch_idx: 20\n",
      "0.7916666666666666\n",
      "batch_idx: 21\n",
      "0.7992424242424242\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.7934027777777778\n",
      "batch_idx: 24\n",
      "0.7966666666666666\n",
      "Training Epoch: 26, total loss: 43.218852\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9375\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7870370370370371\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n",
      "batch_idx: 12\n",
      "0.7916666666666666\n",
      "batch_idx: 13\n",
      "0.7767857142857143\n",
      "batch_idx: 14\n",
      "0.7777777777777778\n",
      "batch_idx: 15\n",
      "0.7708333333333334\n",
      "batch_idx: 16\n",
      "0.7720588235294118\n",
      "batch_idx: 17\n",
      "0.7754629629629629\n",
      "batch_idx: 18\n",
      "0.7653508771929824\n",
      "batch_idx: 19\n",
      "0.76875\n",
      "batch_idx: 20\n",
      "0.7658730158730159\n",
      "batch_idx: 21\n",
      "0.7670454545454546\n",
      "batch_idx: 22\n",
      "0.769927536231884\n",
      "batch_idx: 23\n",
      "0.7743055555555556\n",
      "batch_idx: 24\n",
      "0.775\n",
      "Training Epoch: 27, total loss: 43.501219\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8154761904761905\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.8179824561403509\n",
      "batch_idx: 19\n",
      "0.8145833333333333\n",
      "batch_idx: 20\n",
      "0.8174603174603174\n",
      "batch_idx: 21\n",
      "0.8125\n",
      "batch_idx: 22\n",
      "0.8152173913043478\n",
      "batch_idx: 23\n",
      "0.8142361111111112\n",
      "batch_idx: 24\n",
      "0.8016666666666666\n",
      "Training Epoch: 28, total loss: 42.944712\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.7986111111111112\n",
      "batch_idx: 18\n",
      "0.8004385964912281\n",
      "batch_idx: 19\n",
      "0.8041666666666667\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.7916666666666666\n",
      "batch_idx: 24\n",
      "0.7883333333333333\n",
      "Training Epoch: 29, total loss: 43.063049\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.6944444444444444\n",
      "batch_idx: 6\n",
      "0.6845238095238095\n",
      "batch_idx: 7\n",
      "0.703125\n",
      "batch_idx: 8\n",
      "0.7129629629629629\n",
      "batch_idx: 9\n",
      "0.7291666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 10\n",
      "0.7424242424242424\n",
      "batch_idx: 11\n",
      "0.75\n",
      "batch_idx: 12\n",
      "0.7532051282051282\n",
      "batch_idx: 13\n",
      "0.7619047619047619\n",
      "batch_idx: 14\n",
      "0.7638888888888888\n",
      "batch_idx: 15\n",
      "0.765625\n",
      "batch_idx: 16\n",
      "0.7671568627450981\n",
      "batch_idx: 17\n",
      "0.7685185185185185\n",
      "batch_idx: 18\n",
      "0.7587719298245614\n",
      "batch_idx: 19\n",
      "0.7604166666666666\n",
      "batch_idx: 20\n",
      "0.7638888888888888\n",
      "batch_idx: 21\n",
      "0.7651515151515151\n",
      "batch_idx: 22\n",
      "0.7717391304347826\n",
      "batch_idx: 23\n",
      "0.7604166666666666\n",
      "batch_idx: 24\n",
      "0.7616666666666667\n",
      "Training Epoch: 30, total loss: 43.503893\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.8137254901960784\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.81875\n",
      "batch_idx: 20\n",
      "0.8154761904761905\n",
      "batch_idx: 21\n",
      "0.8162878787878788\n",
      "batch_idx: 22\n",
      "0.8188405797101449\n",
      "batch_idx: 23\n",
      "0.8194444444444444\n",
      "batch_idx: 24\n",
      "0.825\n",
      "Training Epoch: 31, total loss: 42.502735\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7878787878787878\n",
      "batch_idx: 11\n",
      "0.7743055555555556\n",
      "batch_idx: 12\n",
      "0.782051282051282\n",
      "batch_idx: 13\n",
      "0.7797619047619048\n",
      "batch_idx: 14\n",
      "0.775\n",
      "batch_idx: 15\n",
      "0.7760416666666666\n",
      "batch_idx: 16\n",
      "0.7794117647058824\n",
      "batch_idx: 17\n",
      "0.7754629629629629\n",
      "batch_idx: 18\n",
      "0.7719298245614035\n",
      "batch_idx: 19\n",
      "0.7604166666666666\n",
      "batch_idx: 20\n",
      "0.7579365079365079\n",
      "batch_idx: 21\n",
      "0.7443181818181818\n",
      "batch_idx: 22\n",
      "0.7427536231884058\n",
      "batch_idx: 23\n",
      "0.7482638888888888\n",
      "batch_idx: 24\n",
      "0.7516666666666667\n",
      "Training Epoch: 32, total loss: 43.902134\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7583333333333333\n",
      "batch_idx: 10\n",
      "0.7727272727272727\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.7660256410256411\n",
      "batch_idx: 13\n",
      "0.7678571428571429\n",
      "batch_idx: 14\n",
      "0.7694444444444445\n",
      "batch_idx: 15\n",
      "0.765625\n",
      "batch_idx: 16\n",
      "0.7573529411764706\n",
      "batch_idx: 17\n",
      "0.7592592592592593\n",
      "batch_idx: 18\n",
      "0.7653508771929824\n",
      "batch_idx: 19\n",
      "0.7645833333333333\n",
      "batch_idx: 20\n",
      "0.7718253968253969\n",
      "batch_idx: 21\n",
      "0.7613636363636364\n",
      "batch_idx: 22\n",
      "0.7554347826086957\n",
      "batch_idx: 23\n",
      "0.7552083333333334\n",
      "batch_idx: 24\n",
      "0.7616666666666667\n",
      "Training Epoch: 33, total loss: 43.530912\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7592592592592593\n",
      "batch_idx: 9\n",
      "0.7708333333333334\n",
      "batch_idx: 10\n",
      "0.7727272727272727\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.7724358974358975\n",
      "batch_idx: 13\n",
      "0.7797619047619048\n",
      "batch_idx: 14\n",
      "0.7777777777777778\n",
      "batch_idx: 15\n",
      "0.7786458333333334\n",
      "batch_idx: 16\n",
      "0.7794117647058824\n",
      "batch_idx: 17\n",
      "0.7777777777777778\n",
      "batch_idx: 18\n",
      "0.7785087719298246\n",
      "batch_idx: 19\n",
      "0.775\n",
      "batch_idx: 20\n",
      "0.7797619047619048\n",
      "batch_idx: 21\n",
      "0.7821969696969697\n",
      "batch_idx: 22\n",
      "0.7807971014492754\n",
      "batch_idx: 23\n",
      "0.7829861111111112\n",
      "batch_idx: 24\n",
      "0.7883333333333333\n",
      "Training Epoch: 34, total loss: 42.988401\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.8621794871794872\n",
      "batch_idx: 13\n",
      "0.8511904761904762\n",
      "batch_idx: 14\n",
      "0.8444444444444444\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8263888888888888\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.8208333333333333\n",
      "batch_idx: 20\n",
      "0.8174603174603174\n",
      "batch_idx: 21\n",
      "0.8162878787878788\n",
      "batch_idx: 22\n",
      "0.8152173913043478\n",
      "batch_idx: 23\n",
      "0.8107638888888888\n",
      "batch_idx: 24\n",
      "0.81\n",
      "Training Epoch: 35, total loss: 42.869393\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8298611111111112\n",
      "batch_idx: 12\n",
      "0.8301282051282052\n",
      "batch_idx: 13\n",
      "0.8333333333333334\n",
      "batch_idx: 14\n",
      "0.8222222222222222\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8186274509803921\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.8166666666666667\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.8043478260869565\n",
      "batch_idx: 23\n",
      "0.8055555555555556\n",
      "batch_idx: 24\n",
      "0.805\n",
      "Training Epoch: 36, total loss: 42.741901\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7440476190476191\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.7546296296296297\n",
      "batch_idx: 9\n",
      "0.7583333333333333\n",
      "batch_idx: 10\n",
      "0.7613636363636364\n",
      "batch_idx: 11\n",
      "0.7673611111111112\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7797619047619048\n",
      "batch_idx: 14\n",
      "0.7805555555555556\n",
      "batch_idx: 15\n",
      "0.78125\n",
      "batch_idx: 16\n",
      "0.7867647058823529\n",
      "batch_idx: 17\n",
      "0.7824074074074074\n",
      "batch_idx: 18\n",
      "0.7850877192982456\n",
      "batch_idx: 19\n",
      "0.7854166666666667\n",
      "batch_idx: 20\n",
      "0.7817460317460317\n",
      "batch_idx: 21\n",
      "0.7859848484848485\n",
      "batch_idx: 22\n",
      "0.7807971014492754\n",
      "batch_idx: 23\n",
      "0.7795138888888888\n",
      "batch_idx: 24\n",
      "0.78\n",
      "Training Epoch: 37, total loss: 43.201692\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7440476190476191\n",
      "batch_idx: 7\n",
      "0.7447916666666666\n",
      "batch_idx: 8\n",
      "0.7546296296296297\n",
      "batch_idx: 9\n",
      "0.7416666666666667\n",
      "batch_idx: 10\n",
      "0.7386363636363636\n",
      "batch_idx: 11\n",
      "0.7430555555555556\n",
      "batch_idx: 12\n",
      "0.7564102564102564\n",
      "batch_idx: 13\n",
      "0.7529761904761905\n",
      "batch_idx: 14\n",
      "0.7555555555555555\n",
      "batch_idx: 15\n",
      "0.7578125\n",
      "batch_idx: 16\n",
      "0.7622549019607843\n",
      "batch_idx: 17\n",
      "0.7638888888888888\n",
      "batch_idx: 18\n",
      "0.7675438596491229\n",
      "batch_idx: 19\n",
      "0.7708333333333334\n",
      "batch_idx: 20\n",
      "0.75\n",
      "batch_idx: 21\n",
      "0.7556818181818182\n",
      "batch_idx: 22\n",
      "0.7626811594202898\n",
      "batch_idx: 23\n",
      "0.7638888888888888\n",
      "batch_idx: 24\n",
      "0.7683333333333333\n",
      "Training Epoch: 38, total loss: 43.423249\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.7946428571428571\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.8145833333333333\n",
      "batch_idx: 20\n",
      "0.8174603174603174\n",
      "batch_idx: 21\n",
      "0.8143939393939394\n",
      "batch_idx: 22\n",
      "0.8097826086956522\n",
      "batch_idx: 23\n",
      "0.8142361111111112\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 39, total loss: 42.525432\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 12\n",
      "0.7916666666666666\n",
      "batch_idx: 13\n",
      "0.7886904761904762\n",
      "batch_idx: 14\n",
      "0.7861111111111111\n",
      "batch_idx: 15\n",
      "0.7916666666666666\n",
      "batch_idx: 16\n",
      "0.7892156862745098\n",
      "batch_idx: 17\n",
      "0.7847222222222222\n",
      "batch_idx: 18\n",
      "0.7828947368421053\n",
      "batch_idx: 19\n",
      "0.7791666666666667\n",
      "batch_idx: 20\n",
      "0.7757936507936508\n",
      "batch_idx: 21\n",
      "0.7765151515151515\n",
      "batch_idx: 22\n",
      "0.7789855072463768\n",
      "batch_idx: 23\n",
      "0.7829861111111112\n",
      "batch_idx: 24\n",
      "0.7866666666666666\n",
      "Training Epoch: 40, total loss: 43.125823\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8101851851851852\n",
      "batch_idx: 18\n",
      "0.7960526315789473\n",
      "batch_idx: 19\n",
      "0.7916666666666666\n",
      "batch_idx: 20\n",
      "0.7916666666666666\n",
      "batch_idx: 21\n",
      "0.7935606060606061\n",
      "batch_idx: 22\n",
      "0.7916666666666666\n",
      "batch_idx: 23\n",
      "0.7916666666666666\n",
      "batch_idx: 24\n",
      "0.7916666666666666\n",
      "Training Epoch: 41, total loss: 42.939203\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.7994791666666666\n",
      "batch_idx: 16\n",
      "0.7916666666666666\n",
      "batch_idx: 17\n",
      "0.7939814814814815\n",
      "batch_idx: 18\n",
      "0.8026315789473685\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.7956349206349206\n",
      "batch_idx: 21\n",
      "0.7992424242424242\n",
      "batch_idx: 22\n",
      "0.7952898550724637\n",
      "batch_idx: 23\n",
      "0.796875\n",
      "batch_idx: 24\n",
      "0.8016666666666666\n",
      "Training Epoch: 42, total loss: 42.838122\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.8137254901960784\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.8015873015873016\n",
      "batch_idx: 21\n",
      "0.803030303030303\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8090277777777778\n",
      "batch_idx: 24\n",
      "0.8066666666666666\n",
      "Training Epoch: 43, total loss: 42.641667\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8222222222222222\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8179824561403509\n",
      "batch_idx: 19\n",
      "0.8208333333333333\n",
      "batch_idx: 20\n",
      "0.8154761904761905\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.8152173913043478\n",
      "batch_idx: 23\n",
      "0.8107638888888888\n",
      "batch_idx: 24\n",
      "0.81\n",
      "Training Epoch: 44, total loss: 42.687235\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8674242424242424\n",
      "batch_idx: 11\n",
      "0.8680555555555556\n",
      "batch_idx: 12\n",
      "0.8557692307692307\n",
      "batch_idx: 13\n",
      "0.8511904761904762\n",
      "batch_idx: 14\n",
      "0.8416666666666667\n",
      "batch_idx: 15\n",
      "0.8489583333333334\n",
      "batch_idx: 16\n",
      "0.8431372549019608\n",
      "batch_idx: 17\n",
      "0.8402777777777778\n",
      "batch_idx: 18\n",
      "0.8377192982456141\n",
      "batch_idx: 19\n",
      "0.8395833333333333\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.8238636363636364\n",
      "batch_idx: 22\n",
      "0.8152173913043478\n",
      "batch_idx: 23\n",
      "0.8194444444444444\n",
      "batch_idx: 24\n",
      "0.8133333333333334\n",
      "Training Epoch: 45, total loss: 42.630647\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8298611111111112\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8222222222222222\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8287037037037037\n",
      "batch_idx: 18\n",
      "0.831140350877193\n",
      "batch_idx: 19\n",
      "0.8291666666666667\n",
      "batch_idx: 20\n",
      "0.8273809523809523\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.8188405797101449\n",
      "batch_idx: 23\n",
      "0.8177083333333334\n",
      "batch_idx: 24\n",
      "0.8233333333333334\n",
      "Training Epoch: 46, total loss: 42.393677\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8114035087719298\n",
      "batch_idx: 19\n",
      "0.8145833333333333\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.8143939393939394\n",
      "batch_idx: 22\n",
      "0.8152173913043478\n",
      "batch_idx: 23\n",
      "0.8159722222222222\n",
      "batch_idx: 24\n",
      "0.81\n",
      "Training Epoch: 47, total loss: 42.634195\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8269230769230769\n",
      "batch_idx: 13\n",
      "0.8333333333333334\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8382352941176471\n",
      "batch_idx: 17\n",
      "0.8379629629629629\n",
      "batch_idx: 18\n",
      "0.8399122807017544\n",
      "batch_idx: 19\n",
      "0.84375\n",
      "batch_idx: 20\n",
      "0.8392857142857143\n",
      "batch_idx: 21\n",
      "0.8390151515151515\n",
      "batch_idx: 22\n",
      "0.8369565217391305\n",
      "batch_idx: 23\n",
      "0.8368055555555556\n",
      "batch_idx: 24\n",
      "0.835\n",
      "Training Epoch: 48, total loss: 42.265448\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8257575757575758\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8269230769230769\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.8385416666666666\n",
      "batch_idx: 16\n",
      "0.8308823529411765\n",
      "batch_idx: 17\n",
      "0.8356481481481481\n",
      "batch_idx: 18\n",
      "0.8355263157894737\n",
      "batch_idx: 19\n",
      "0.8395833333333333\n",
      "batch_idx: 20\n",
      "0.8432539682539683\n",
      "batch_idx: 21\n",
      "0.8484848484848485\n",
      "batch_idx: 22\n",
      "0.8496376811594203\n",
      "batch_idx: 23\n",
      "0.8489583333333334\n",
      "batch_idx: 24\n",
      "0.85\n",
      "Training Epoch: 49, total loss: 41.932205\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.7942708333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 16\n",
      "0.7843137254901961\n",
      "batch_idx: 17\n",
      "0.7824074074074074\n",
      "batch_idx: 18\n",
      "0.7828947368421053\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7797619047619048\n",
      "batch_idx: 21\n",
      "0.7821969696969697\n",
      "batch_idx: 22\n",
      "0.7844202898550725\n",
      "batch_idx: 23\n",
      "0.7881944444444444\n",
      "batch_idx: 24\n",
      "0.7866666666666666\n",
      "Training Epoch: 50, total loss: 43.130862\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9375\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9270833333333334\n",
      "batch_idx: 4\n",
      "0.9166666666666666\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "batch_idx: 6\n",
      "0.8869047619047619\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8703703703703703\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8541666666666666\n",
      "batch_idx: 12\n",
      "0.8525641025641025\n",
      "batch_idx: 13\n",
      "0.8541666666666666\n",
      "batch_idx: 14\n",
      "0.85\n",
      "batch_idx: 15\n",
      "0.8385416666666666\n",
      "batch_idx: 16\n",
      "0.8406862745098039\n",
      "batch_idx: 17\n",
      "0.8379629629629629\n",
      "batch_idx: 18\n",
      "0.8333333333333334\n",
      "batch_idx: 19\n",
      "0.8333333333333334\n",
      "batch_idx: 20\n",
      "0.8293650793650794\n",
      "batch_idx: 21\n",
      "0.8276515151515151\n",
      "batch_idx: 22\n",
      "0.8170289855072463\n",
      "batch_idx: 23\n",
      "0.8159722222222222\n",
      "batch_idx: 24\n",
      "0.8183333333333334\n",
      "Training Epoch: 51, total loss: 42.525747\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8020833333333334\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.8055555555555556\n",
      "batch_idx: 18\n",
      "0.7960526315789473\n",
      "batch_idx: 19\n",
      "0.7979166666666667\n",
      "batch_idx: 20\n",
      "0.7876984126984127\n",
      "batch_idx: 21\n",
      "0.7821969696969697\n",
      "batch_idx: 22\n",
      "0.782608695652174\n",
      "batch_idx: 23\n",
      "0.7864583333333334\n",
      "batch_idx: 24\n",
      "0.79\n",
      "Training Epoch: 52, total loss: 42.983769\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.7976190476190477\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.8063725490196079\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8114035087719298\n",
      "batch_idx: 19\n",
      "0.8145833333333333\n",
      "batch_idx: 20\n",
      "0.8174603174603174\n",
      "batch_idx: 21\n",
      "0.8125\n",
      "batch_idx: 22\n",
      "0.8097826086956522\n",
      "batch_idx: 23\n",
      "0.8107638888888888\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 53, total loss: 42.692085\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8154761904761905\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.7941176470588235\n",
      "batch_idx: 17\n",
      "0.7870370370370371\n",
      "batch_idx: 18\n",
      "0.7916666666666666\n",
      "batch_idx: 19\n",
      "0.7916666666666666\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.7954545454545454\n",
      "batch_idx: 22\n",
      "0.7934782608695652\n",
      "batch_idx: 23\n",
      "0.796875\n",
      "batch_idx: 24\n",
      "0.8016666666666666\n",
      "Training Epoch: 54, total loss: 42.886377\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8179824561403509\n",
      "batch_idx: 19\n",
      "0.8229166666666666\n",
      "batch_idx: 20\n",
      "0.8273809523809523\n",
      "batch_idx: 21\n",
      "0.8257575757575758\n",
      "batch_idx: 22\n",
      "0.8278985507246377\n",
      "batch_idx: 23\n",
      "0.8194444444444444\n",
      "batch_idx: 24\n",
      "0.81\n",
      "Training Epoch: 55, total loss: 42.564750\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8222222222222222\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8217592592592593\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.8270833333333333\n",
      "batch_idx: 20\n",
      "0.8273809523809523\n",
      "batch_idx: 21\n",
      "0.821969696969697\n",
      "batch_idx: 22\n",
      "0.8278985507246377\n",
      "batch_idx: 23\n",
      "0.8315972222222222\n",
      "batch_idx: 24\n",
      "0.83\n",
      "Training Epoch: 56, total loss: 42.227817\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8298611111111112\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8416666666666667\n",
      "batch_idx: 15\n",
      "0.8385416666666666\n",
      "batch_idx: 16\n",
      "0.8357843137254902\n",
      "batch_idx: 17\n",
      "0.8356481481481481\n",
      "batch_idx: 18\n",
      "0.8333333333333334\n",
      "batch_idx: 19\n",
      "0.8354166666666667\n",
      "batch_idx: 20\n",
      "0.8392857142857143\n",
      "batch_idx: 21\n",
      "0.8371212121212122\n",
      "batch_idx: 22\n",
      "0.8278985507246377\n",
      "batch_idx: 23\n",
      "0.8263888888888888\n",
      "batch_idx: 24\n",
      "0.83\n",
      "Training Epoch: 57, total loss: 42.281834\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8958333333333334\n",
      "batch_idx: 6\n",
      "0.8809523809523809\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.8525641025641025\n",
      "batch_idx: 13\n",
      "0.8541666666666666\n",
      "batch_idx: 14\n",
      "0.8583333333333333\n",
      "batch_idx: 15\n",
      "0.8567708333333334\n",
      "batch_idx: 16\n",
      "0.8504901960784313\n",
      "batch_idx: 17\n",
      "0.8356481481481481\n",
      "batch_idx: 18\n",
      "0.8377192982456141\n",
      "batch_idx: 19\n",
      "0.8416666666666667\n",
      "batch_idx: 20\n",
      "0.8452380952380952\n",
      "batch_idx: 21\n",
      "0.8333333333333334\n",
      "batch_idx: 22\n",
      "0.8297101449275363\n",
      "batch_idx: 23\n",
      "0.8298611111111112\n",
      "batch_idx: 24\n",
      "0.8316666666666667\n",
      "Training Epoch: 58, total loss: 42.227862\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8303571428571429\n",
      "batch_idx: 14\n",
      "0.8305555555555556\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8135964912280702\n",
      "batch_idx: 19\n",
      "0.8229166666666666\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.8257575757575758\n",
      "batch_idx: 22\n",
      "0.8278985507246377\n",
      "batch_idx: 23\n",
      "0.8298611111111112\n",
      "batch_idx: 24\n",
      "0.825\n",
      "Training Epoch: 59, total loss: 42.236827\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.8461538461538461\n",
      "batch_idx: 13\n",
      "0.8422619047619048\n",
      "batch_idx: 14\n",
      "0.8416666666666667\n",
      "batch_idx: 15\n",
      "0.84375\n",
      "batch_idx: 16\n",
      "0.8431372549019608\n",
      "batch_idx: 17\n",
      "0.8402777777777778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 18\n",
      "0.8399122807017544\n",
      "batch_idx: 19\n",
      "0.8333333333333334\n",
      "batch_idx: 20\n",
      "0.8353174603174603\n",
      "batch_idx: 21\n",
      "0.8390151515151515\n",
      "batch_idx: 22\n",
      "0.8315217391304348\n",
      "batch_idx: 23\n",
      "0.8315972222222222\n",
      "batch_idx: 24\n",
      "0.83\n",
      "Training Epoch: 60, total loss: 42.155999\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.8257575757575758\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8020833333333334\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.7777777777777778\n",
      "batch_idx: 18\n",
      "0.7872807017543859\n",
      "batch_idx: 19\n",
      "0.7833333333333333\n",
      "batch_idx: 20\n",
      "0.7936507936507936\n",
      "batch_idx: 21\n",
      "0.7935606060606061\n",
      "batch_idx: 22\n",
      "0.7916666666666666\n",
      "batch_idx: 23\n",
      "0.7864583333333334\n",
      "batch_idx: 24\n",
      "0.7916666666666666\n",
      "Training Epoch: 61, total loss: 42.962496\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.7202380952380952\n",
      "batch_idx: 7\n",
      "0.7239583333333334\n",
      "batch_idx: 8\n",
      "0.7083333333333334\n",
      "batch_idx: 9\n",
      "0.7333333333333333\n",
      "batch_idx: 10\n",
      "0.7462121212121212\n",
      "batch_idx: 11\n",
      "0.7534722222222222\n",
      "batch_idx: 12\n",
      "0.7596153846153846\n",
      "batch_idx: 13\n",
      "0.7589285714285714\n",
      "batch_idx: 14\n",
      "0.7583333333333333\n",
      "batch_idx: 15\n",
      "0.765625\n",
      "batch_idx: 16\n",
      "0.7696078431372549\n",
      "batch_idx: 17\n",
      "0.7685185185185185\n",
      "batch_idx: 18\n",
      "0.7697368421052632\n",
      "batch_idx: 19\n",
      "0.7708333333333334\n",
      "batch_idx: 20\n",
      "0.7757936507936508\n",
      "batch_idx: 21\n",
      "0.7727272727272727\n",
      "batch_idx: 22\n",
      "0.7717391304347826\n",
      "batch_idx: 23\n",
      "0.7743055555555556\n",
      "batch_idx: 24\n",
      "0.7766666666666666\n",
      "Training Epoch: 62, total loss: 43.224370\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8452380952380952\n",
      "batch_idx: 14\n",
      "0.85\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8455882352941176\n",
      "batch_idx: 17\n",
      "0.8263888888888888\n",
      "batch_idx: 18\n",
      "0.8135964912280702\n",
      "batch_idx: 19\n",
      "0.8104166666666667\n",
      "batch_idx: 20\n",
      "0.8075396825396826\n",
      "batch_idx: 21\n",
      "0.8068181818181818\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.8003472222222222\n",
      "batch_idx: 24\n",
      "0.7983333333333333\n",
      "Training Epoch: 63, total loss: 42.806621\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7870370370370371\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7948717948717948\n",
      "batch_idx: 13\n",
      "0.8005952380952381\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.8088235294117647\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.8145833333333333\n",
      "batch_idx: 20\n",
      "0.8174603174603174\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.8170289855072463\n",
      "batch_idx: 23\n",
      "0.8159722222222222\n",
      "batch_idx: 24\n",
      "0.8166666666666667\n",
      "Training Epoch: 64, total loss: 42.453623\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9583333333333334\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.90625\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8301282051282052\n",
      "batch_idx: 13\n",
      "0.8333333333333334\n",
      "batch_idx: 14\n",
      "0.8333333333333334\n",
      "batch_idx: 15\n",
      "0.828125\n",
      "batch_idx: 16\n",
      "0.8259803921568627\n",
      "batch_idx: 17\n",
      "0.8263888888888888\n",
      "batch_idx: 18\n",
      "0.8289473684210527\n",
      "batch_idx: 19\n",
      "0.8291666666666667\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.821969696969697\n",
      "batch_idx: 22\n",
      "0.8188405797101449\n",
      "batch_idx: 23\n",
      "0.8211805555555556\n",
      "batch_idx: 24\n",
      "0.82\n",
      "Training Epoch: 65, total loss: 42.440084\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.9375\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8257575757575758\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.8365384615384616\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8305555555555556\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8263888888888888\n",
      "batch_idx: 18\n",
      "0.8289473684210527\n",
      "batch_idx: 19\n",
      "0.8291666666666667\n",
      "batch_idx: 20\n",
      "0.8313492063492064\n",
      "batch_idx: 21\n",
      "0.8314393939393939\n",
      "batch_idx: 22\n",
      "0.8297101449275363\n",
      "batch_idx: 23\n",
      "0.8263888888888888\n",
      "batch_idx: 24\n",
      "0.8183333333333334\n",
      "Training Epoch: 66, total loss: 42.513350\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7857142857142857\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.796875\n",
      "batch_idx: 16\n",
      "0.8014705882352942\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8026315789473685\n",
      "batch_idx: 19\n",
      "0.8041666666666667\n",
      "batch_idx: 20\n",
      "0.7996031746031746\n",
      "batch_idx: 21\n",
      "0.803030303030303\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8142361111111112\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 67, total loss: 42.720884\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8571428571428571\n",
      "batch_idx: 14\n",
      "0.8611111111111112\n",
      "batch_idx: 15\n",
      "0.8515625\n",
      "batch_idx: 16\n",
      "0.8529411764705882\n",
      "batch_idx: 17\n",
      "0.8518518518518519\n",
      "batch_idx: 18\n",
      "0.8377192982456141\n",
      "batch_idx: 19\n",
      "0.8395833333333333\n",
      "batch_idx: 20\n",
      "0.8432539682539683\n",
      "batch_idx: 21\n",
      "0.8409090909090909\n",
      "batch_idx: 22\n",
      "0.8387681159420289\n",
      "batch_idx: 23\n",
      "0.8368055555555556\n",
      "batch_idx: 24\n",
      "0.8333333333333334\n",
      "Training Epoch: 68, total loss: 42.133423\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.8088235294117647\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.8114035087719298\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.8095238095238095\n",
      "batch_idx: 21\n",
      "0.8068181818181818\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.8038194444444444\n",
      "batch_idx: 24\n",
      "0.7966666666666666\n",
      "Training Epoch: 69, total loss: 42.840687\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.7916666666666666\n",
      "batch_idx: 13\n",
      "0.7976190476190477\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8070175438596491\n",
      "batch_idx: 19\n",
      "0.8041666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.803030303030303\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8038194444444444\n",
      "batch_idx: 24\n",
      "0.8083333333333333\n",
      "Training Epoch: 70, total loss: 42.616541\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.7942708333333334\n",
      "batch_idx: 16\n",
      "0.7892156862745098\n",
      "batch_idx: 17\n",
      "0.7847222222222222\n",
      "batch_idx: 18\n",
      "0.7894736842105263\n",
      "batch_idx: 19\n",
      "0.7916666666666666\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.7916666666666666\n",
      "batch_idx: 22\n",
      "0.7862318840579711\n",
      "batch_idx: 23\n",
      "0.7881944444444444\n",
      "batch_idx: 24\n",
      "0.7933333333333333\n",
      "Training Epoch: 71, total loss: 42.852391\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8703703703703703\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8636363636363636\n",
      "batch_idx: 11\n",
      "0.8541666666666666\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8511904761904762\n",
      "batch_idx: 14\n",
      "0.8527777777777777\n",
      "batch_idx: 15\n",
      "0.8567708333333334\n",
      "batch_idx: 16\n",
      "0.8504901960784313\n",
      "batch_idx: 17\n",
      "0.8495370370370371\n",
      "batch_idx: 18\n",
      "0.8421052631578947\n",
      "batch_idx: 19\n",
      "0.8395833333333333\n",
      "batch_idx: 20\n",
      "0.8432539682539683\n",
      "batch_idx: 21\n",
      "0.8371212121212122\n",
      "batch_idx: 22\n",
      "0.8278985507246377\n",
      "batch_idx: 23\n",
      "0.8263888888888888\n",
      "batch_idx: 24\n",
      "0.8216666666666667\n",
      "Training Epoch: 72, total loss: 42.237179\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8303571428571429\n",
      "batch_idx: 14\n",
      "0.8333333333333334\n",
      "batch_idx: 15\n",
      "0.8333333333333334\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.8104166666666667\n",
      "batch_idx: 20\n",
      "0.8095238095238095\n",
      "batch_idx: 21\n",
      "0.8068181818181818\n",
      "batch_idx: 22\n",
      "0.8097826086956522\n",
      "batch_idx: 23\n",
      "0.8125\n",
      "batch_idx: 24\n",
      "0.8166666666666667\n",
      "Training Epoch: 73, total loss: 42.491520\n",
      "batch_idx: 0\n",
      "1.0\n",
      "batch_idx: 1\n",
      "0.9583333333333334\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8044871794871795\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.7939814814814815\n",
      "batch_idx: 18\n",
      "0.7916666666666666\n",
      "batch_idx: 19\n",
      "0.7958333333333333\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.7992424242424242\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.8003472222222222\n",
      "batch_idx: 24\n",
      "0.805\n",
      "Training Epoch: 74, total loss: 42.644339\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8461538461538461\n",
      "batch_idx: 13\n",
      "0.8363095238095238\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.8359375\n",
      "batch_idx: 16\n",
      "0.8357843137254902\n",
      "batch_idx: 17\n",
      "0.8310185185185185\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.8104166666666667\n",
      "batch_idx: 20\n",
      "0.8134920634920635\n",
      "batch_idx: 21\n",
      "0.8143939393939394\n",
      "batch_idx: 22\n",
      "0.8079710144927537\n",
      "batch_idx: 23\n",
      "0.8125\n",
      "batch_idx: 24\n",
      "0.81\n",
      "Training Epoch: 75, total loss: 42.603649\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.7791666666666667\n",
      "batch_idx: 10\n",
      "0.7689393939393939\n",
      "batch_idx: 11\n",
      "0.7604166666666666\n",
      "batch_idx: 12\n",
      "0.7724358974358975\n",
      "batch_idx: 13\n",
      "0.7767857142857143\n",
      "batch_idx: 14\n",
      "0.7611111111111111\n",
      "batch_idx: 15\n",
      "0.75\n",
      "batch_idx: 16\n",
      "0.7524509803921569\n",
      "batch_idx: 17\n",
      "0.7592592592592593\n",
      "batch_idx: 18\n",
      "0.7631578947368421\n",
      "batch_idx: 19\n",
      "0.76875\n",
      "batch_idx: 20\n",
      "0.7738095238095238\n",
      "batch_idx: 21\n",
      "0.7765151515151515\n",
      "batch_idx: 22\n",
      "0.782608695652174\n",
      "batch_idx: 23\n",
      "0.7829861111111112\n",
      "batch_idx: 24\n",
      "0.7866666666666666\n",
      "Training Epoch: 76, total loss: 43.078516\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8461538461538461\n",
      "batch_idx: 13\n",
      "0.8511904761904762\n",
      "batch_idx: 14\n",
      "0.8527777777777777\n",
      "batch_idx: 15\n",
      "0.8489583333333334\n",
      "batch_idx: 16\n",
      "0.8455882352941176\n",
      "batch_idx: 17\n",
      "0.8472222222222222\n",
      "batch_idx: 18\n",
      "0.8421052631578947\n",
      "batch_idx: 19\n",
      "0.8375\n",
      "batch_idx: 20\n",
      "0.8293650793650794\n",
      "batch_idx: 21\n",
      "0.8314393939393939\n",
      "batch_idx: 22\n",
      "0.8333333333333334\n",
      "batch_idx: 23\n",
      "0.8315972222222222\n",
      "batch_idx: 24\n",
      "0.8266666666666667\n",
      "Training Epoch: 77, total loss: 42.376498\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8333333333333334\n",
      "batch_idx: 14\n",
      "0.8333333333333334\n",
      "batch_idx: 15\n",
      "0.8359375\n",
      "batch_idx: 16\n",
      "0.8259803921568627\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8135964912280702\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8115079365079365\n",
      "batch_idx: 21\n",
      "0.8143939393939394\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8020833333333334\n",
      "batch_idx: 24\n",
      "0.7966666666666666\n",
      "Training Epoch: 78, total loss: 42.858442\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.8461538461538461\n",
      "batch_idx: 13\n",
      "0.8482142857142857\n",
      "batch_idx: 14\n",
      "0.8444444444444444\n",
      "batch_idx: 15\n",
      "0.8411458333333334\n",
      "batch_idx: 16\n",
      "0.8357843137254902\n",
      "batch_idx: 17\n",
      "0.8379629629629629\n",
      "batch_idx: 18\n",
      "0.8377192982456141\n",
      "batch_idx: 19\n",
      "0.8270833333333333\n",
      "batch_idx: 20\n",
      "0.8273809523809523\n",
      "batch_idx: 21\n",
      "0.8276515151515151\n",
      "batch_idx: 22\n",
      "0.8206521739130435\n",
      "batch_idx: 23\n",
      "0.8229166666666666\n",
      "batch_idx: 24\n",
      "0.8233333333333334\n",
      "Training Epoch: 79, total loss: 42.334820\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n",
      "batch_idx: 11\n",
      "0.8472222222222222\n",
      "batch_idx: 12\n",
      "0.8365384615384616\n",
      "batch_idx: 13\n",
      "0.8333333333333334\n",
      "batch_idx: 14\n",
      "0.8416666666666667\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8333333333333334\n",
      "batch_idx: 17\n",
      "0.8287037037037037\n",
      "batch_idx: 18\n",
      "0.8289473684210527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 19\n",
      "0.8333333333333334\n",
      "batch_idx: 20\n",
      "0.8373015873015873\n",
      "batch_idx: 21\n",
      "0.8390151515151515\n",
      "batch_idx: 22\n",
      "0.8405797101449275\n",
      "batch_idx: 23\n",
      "0.8385416666666666\n",
      "batch_idx: 24\n",
      "0.8366666666666667\n",
      "Training Epoch: 80, total loss: 41.978391\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.8611111111111112\n",
      "batch_idx: 12\n",
      "0.8621794871794872\n",
      "batch_idx: 13\n",
      "0.8571428571428571\n",
      "batch_idx: 14\n",
      "0.8527777777777777\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8455882352941176\n",
      "batch_idx: 17\n",
      "0.8425925925925926\n",
      "batch_idx: 18\n",
      "0.8442982456140351\n",
      "batch_idx: 19\n",
      "0.8395833333333333\n",
      "batch_idx: 20\n",
      "0.8333333333333334\n",
      "batch_idx: 21\n",
      "0.8314393939393939\n",
      "batch_idx: 22\n",
      "0.8297101449275363\n",
      "batch_idx: 23\n",
      "0.8333333333333334\n",
      "batch_idx: 24\n",
      "0.8366666666666667\n",
      "Training Epoch: 81, total loss: 41.933127\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8098958333333334\n",
      "batch_idx: 16\n",
      "0.8063725490196079\n",
      "batch_idx: 17\n",
      "0.8101851851851852\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8083333333333333\n",
      "batch_idx: 20\n",
      "0.8115079365079365\n",
      "batch_idx: 21\n",
      "0.8125\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8142361111111112\n",
      "batch_idx: 24\n",
      "0.8183333333333334\n",
      "Training Epoch: 82, total loss: 42.492449\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8525641025641025\n",
      "batch_idx: 13\n",
      "0.8452380952380952\n",
      "batch_idx: 14\n",
      "0.8388888888888889\n",
      "batch_idx: 15\n",
      "0.84375\n",
      "batch_idx: 16\n",
      "0.8455882352941176\n",
      "batch_idx: 17\n",
      "0.8449074074074074\n",
      "batch_idx: 18\n",
      "0.8442982456140351\n",
      "batch_idx: 19\n",
      "0.8479166666666667\n",
      "batch_idx: 20\n",
      "0.8452380952380952\n",
      "batch_idx: 21\n",
      "0.8503787878787878\n",
      "batch_idx: 22\n",
      "0.8478260869565217\n",
      "batch_idx: 23\n",
      "0.8420138888888888\n",
      "batch_idx: 24\n",
      "0.8383333333333334\n",
      "Training Epoch: 83, total loss: 42.161325\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8301282051282052\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.8222222222222222\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8310185185185185\n",
      "batch_idx: 18\n",
      "0.8289473684210527\n",
      "batch_idx: 19\n",
      "0.8291666666666667\n",
      "batch_idx: 20\n",
      "0.8214285714285714\n",
      "batch_idx: 21\n",
      "0.8200757575757576\n",
      "batch_idx: 22\n",
      "0.822463768115942\n",
      "batch_idx: 23\n",
      "0.8211805555555556\n",
      "batch_idx: 24\n",
      "0.8183333333333334\n",
      "Training Epoch: 84, total loss: 42.489095\n",
      "batch_idx: 0\n",
      "1.0\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8365384615384616\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8444444444444444\n",
      "batch_idx: 15\n",
      "0.8359375\n",
      "batch_idx: 16\n",
      "0.8357843137254902\n",
      "batch_idx: 17\n",
      "0.8379629629629629\n",
      "batch_idx: 18\n",
      "0.8333333333333334\n",
      "batch_idx: 19\n",
      "0.8208333333333333\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.8143939393939394\n",
      "batch_idx: 22\n",
      "0.8152173913043478\n",
      "batch_idx: 23\n",
      "0.8177083333333334\n",
      "batch_idx: 24\n",
      "0.8216666666666667\n",
      "Training Epoch: 85, total loss: 42.297143\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.8044871794871795\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8055555555555556\n",
      "batch_idx: 15\n",
      "0.8098958333333334\n",
      "batch_idx: 16\n",
      "0.8063725490196079\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.8041666666666667\n",
      "batch_idx: 20\n",
      "0.8075396825396826\n",
      "batch_idx: 21\n",
      "0.8087121212121212\n",
      "batch_idx: 22\n",
      "0.8079710144927537\n",
      "batch_idx: 23\n",
      "0.8055555555555556\n",
      "batch_idx: 24\n",
      "0.8083333333333333\n",
      "Training Epoch: 86, total loss: 42.550327\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8287037037037037\n",
      "batch_idx: 18\n",
      "0.8223684210526315\n",
      "batch_idx: 19\n",
      "0.8291666666666667\n",
      "batch_idx: 20\n",
      "0.8273809523809523\n",
      "batch_idx: 21\n",
      "0.8257575757575758\n",
      "batch_idx: 22\n",
      "0.8278985507246377\n",
      "batch_idx: 23\n",
      "0.828125\n",
      "batch_idx: 24\n",
      "0.8316666666666667\n",
      "Training Epoch: 87, total loss: 42.207488\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.8222222222222222\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8382352941176471\n",
      "batch_idx: 17\n",
      "0.8402777777777778\n",
      "batch_idx: 18\n",
      "0.8442982456140351\n",
      "batch_idx: 19\n",
      "0.8395833333333333\n",
      "batch_idx: 20\n",
      "0.8373015873015873\n",
      "batch_idx: 21\n",
      "0.8371212121212122\n",
      "batch_idx: 22\n",
      "0.8333333333333334\n",
      "batch_idx: 23\n",
      "0.8385416666666666\n",
      "batch_idx: 24\n",
      "0.8366666666666667\n",
      "Training Epoch: 88, total loss: 41.913560\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.8365384615384616\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.8222222222222222\n",
      "batch_idx: 15\n",
      "0.796875\n",
      "batch_idx: 16\n",
      "0.8014705882352942\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.8095238095238095\n",
      "batch_idx: 21\n",
      "0.8068181818181818\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8003472222222222\n",
      "batch_idx: 24\n",
      "0.8\n",
      "Training Epoch: 89, total loss: 42.658242\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7638888888888888\n",
      "batch_idx: 9\n",
      "0.7708333333333334\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.8005952380952381\n",
      "batch_idx: 14\n",
      "0.8\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.8014705882352942\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.8135964912280702\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8154761904761905\n",
      "batch_idx: 21\n",
      "0.8162878787878788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8142361111111112\n",
      "batch_idx: 24\n",
      "0.8183333333333334\n",
      "Training Epoch: 90, total loss: 42.400456\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.8223684210526315\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.8276515151515151\n",
      "batch_idx: 22\n",
      "0.8242753623188406\n",
      "batch_idx: 23\n",
      "0.8229166666666666\n",
      "batch_idx: 24\n",
      "0.8266666666666667\n",
      "Training Epoch: 91, total loss: 42.313406\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8625\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8611111111111112\n",
      "batch_idx: 12\n",
      "0.8461538461538461\n",
      "batch_idx: 13\n",
      "0.8511904761904762\n",
      "batch_idx: 14\n",
      "0.8472222222222222\n",
      "batch_idx: 15\n",
      "0.8489583333333334\n",
      "batch_idx: 16\n",
      "0.8406862745098039\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.8245614035087719\n",
      "batch_idx: 19\n",
      "0.8291666666666667\n",
      "batch_idx: 20\n",
      "0.8273809523809523\n",
      "batch_idx: 21\n",
      "0.8200757575757576\n",
      "batch_idx: 22\n",
      "0.822463768115942\n",
      "batch_idx: 23\n",
      "0.8177083333333334\n",
      "batch_idx: 24\n",
      "0.815\n",
      "Training Epoch: 92, total loss: 42.358349\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8802083333333334\n",
      "batch_idx: 8\n",
      "0.8796296296296297\n",
      "batch_idx: 9\n",
      "0.875\n",
      "batch_idx: 10\n",
      "0.8636363636363636\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.8589743589743589\n",
      "batch_idx: 13\n",
      "0.8541666666666666\n",
      "batch_idx: 14\n",
      "0.8527777777777777\n",
      "batch_idx: 15\n",
      "0.8515625\n",
      "batch_idx: 16\n",
      "0.8504901960784313\n",
      "batch_idx: 17\n",
      "0.8495370370370371\n",
      "batch_idx: 18\n",
      "0.8486842105263158\n",
      "batch_idx: 19\n",
      "0.8458333333333333\n",
      "batch_idx: 20\n",
      "0.8432539682539683\n",
      "batch_idx: 21\n",
      "0.8446969696969697\n",
      "batch_idx: 22\n",
      "0.8405797101449275\n",
      "batch_idx: 23\n",
      "0.8454861111111112\n",
      "batch_idx: 24\n",
      "0.8433333333333334\n",
      "Training Epoch: 93, total loss: 41.892285\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8301282051282052\n",
      "batch_idx: 13\n",
      "0.8333333333333334\n",
      "batch_idx: 14\n",
      "0.8388888888888889\n",
      "batch_idx: 15\n",
      "0.828125\n",
      "batch_idx: 16\n",
      "0.8357843137254902\n",
      "batch_idx: 17\n",
      "0.8379629629629629\n",
      "batch_idx: 18\n",
      "0.8333333333333334\n",
      "batch_idx: 19\n",
      "0.8354166666666667\n",
      "batch_idx: 20\n",
      "0.8392857142857143\n",
      "batch_idx: 21\n",
      "0.8428030303030303\n",
      "batch_idx: 22\n",
      "0.8387681159420289\n",
      "batch_idx: 23\n",
      "0.8402777777777778\n",
      "batch_idx: 24\n",
      "0.8433333333333334\n",
      "Training Epoch: 94, total loss: 41.925830\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8269230769230769\n",
      "batch_idx: 13\n",
      "0.8303571428571429\n",
      "batch_idx: 14\n",
      "0.825\n",
      "batch_idx: 15\n",
      "0.828125\n",
      "batch_idx: 16\n",
      "0.8259803921568627\n",
      "batch_idx: 17\n",
      "0.8310185185185185\n",
      "batch_idx: 18\n",
      "0.8355263157894737\n",
      "batch_idx: 19\n",
      "0.8333333333333334\n",
      "batch_idx: 20\n",
      "0.8373015873015873\n",
      "batch_idx: 21\n",
      "0.8371212121212122\n",
      "batch_idx: 22\n",
      "0.8297101449275363\n",
      "batch_idx: 23\n",
      "0.8298611111111112\n",
      "batch_idx: 24\n",
      "0.83\n",
      "Training Epoch: 95, total loss: 42.137807\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7916666666666666\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.7916666666666666\n",
      "batch_idx: 18\n",
      "0.7916666666666666\n",
      "batch_idx: 19\n",
      "0.7916666666666666\n",
      "batch_idx: 20\n",
      "0.7896825396825397\n",
      "batch_idx: 21\n",
      "0.7954545454545454\n",
      "batch_idx: 22\n",
      "0.7971014492753623\n",
      "batch_idx: 23\n",
      "0.7899305555555556\n",
      "batch_idx: 24\n",
      "0.7916666666666666\n",
      "Training Epoch: 96, total loss: 42.875868\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.8611111111111112\n",
      "batch_idx: 12\n",
      "0.8557692307692307\n",
      "batch_idx: 13\n",
      "0.8660714285714286\n",
      "batch_idx: 14\n",
      "0.8638888888888889\n",
      "batch_idx: 15\n",
      "0.8645833333333334\n",
      "batch_idx: 16\n",
      "0.8651960784313726\n",
      "batch_idx: 17\n",
      "0.8495370370370371\n",
      "batch_idx: 18\n",
      "0.8508771929824561\n",
      "batch_idx: 19\n",
      "0.8541666666666666\n",
      "batch_idx: 20\n",
      "0.8511904761904762\n",
      "batch_idx: 21\n",
      "0.8446969696969697\n",
      "batch_idx: 22\n",
      "0.8442028985507246\n",
      "batch_idx: 23\n",
      "0.8402777777777778\n",
      "batch_idx: 24\n",
      "0.84\n",
      "Training Epoch: 97, total loss: 42.011448\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8564814814814815\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.8557692307692307\n",
      "batch_idx: 13\n",
      "0.8541666666666666\n",
      "batch_idx: 14\n",
      "0.8527777777777777\n",
      "batch_idx: 15\n",
      "0.8567708333333334\n",
      "batch_idx: 16\n",
      "0.8529411764705882\n",
      "batch_idx: 17\n",
      "0.8518518518518519\n",
      "batch_idx: 18\n",
      "0.8508771929824561\n",
      "batch_idx: 19\n",
      "0.8541666666666666\n",
      "batch_idx: 20\n",
      "0.8551587301587301\n",
      "batch_idx: 21\n",
      "0.8484848484848485\n",
      "batch_idx: 22\n",
      "0.8496376811594203\n",
      "batch_idx: 23\n",
      "0.8402777777777778\n",
      "batch_idx: 24\n",
      "0.8433333333333334\n",
      "Training Epoch: 98, total loss: 42.072195\n",
      "batch_idx: 0\n",
      "1.0\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.8869047619047619\n",
      "batch_idx: 7\n",
      "0.8802083333333334\n",
      "batch_idx: 8\n",
      "0.8796296296296297\n",
      "batch_idx: 9\n",
      "0.8708333333333333\n",
      "batch_idx: 10\n",
      "0.8636363636363636\n",
      "batch_idx: 11\n",
      "0.8541666666666666\n",
      "batch_idx: 12\n",
      "0.8557692307692307\n",
      "batch_idx: 13\n",
      "0.8601190476190477\n",
      "batch_idx: 14\n",
      "0.8611111111111112\n",
      "batch_idx: 15\n",
      "0.8645833333333334\n",
      "batch_idx: 16\n",
      "0.8651960784313726\n",
      "batch_idx: 17\n",
      "0.8564814814814815\n",
      "batch_idx: 18\n",
      "0.8552631578947368\n",
      "batch_idx: 19\n",
      "0.8583333333333333\n",
      "batch_idx: 20\n",
      "0.8551587301587301\n",
      "batch_idx: 21\n",
      "0.8503787878787878\n",
      "batch_idx: 22\n",
      "0.8514492753623188\n",
      "batch_idx: 23\n",
      "0.8559027777777778\n",
      "batch_idx: 24\n",
      "0.8483333333333334\n",
      "Training Epoch: 99, total loss: 41.894052\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.9375\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.875\n",
      "batch_idx: 8\n",
      "0.875\n",
      "batch_idx: 9\n",
      "0.8708333333333333\n",
      "batch_idx: 10\n",
      "0.875\n",
      "batch_idx: 11\n",
      "0.875\n",
      "batch_idx: 12\n",
      "0.8782051282051282\n",
      "batch_idx: 13\n",
      "0.8779761904761905\n",
      "batch_idx: 14\n",
      "0.8833333333333333\n",
      "batch_idx: 15\n",
      "0.8802083333333334\n",
      "batch_idx: 16\n",
      "0.8848039215686274\n",
      "batch_idx: 17\n",
      "0.8796296296296297\n",
      "batch_idx: 18\n",
      "0.8706140350877193\n",
      "batch_idx: 19\n",
      "0.86875\n",
      "batch_idx: 20\n",
      "0.871031746031746\n",
      "batch_idx: 21\n",
      "0.8731060606060606\n",
      "batch_idx: 22\n",
      "0.8713768115942029\n",
      "batch_idx: 23\n",
      "0.8715277777777778\n",
      "batch_idx: 24\n",
      "0.8683333333333333\n",
      "Training Epoch: 100, total loss: 41.559717\n",
      "batch_idx: 0\n",
      "0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8636363636363636\n",
      "batch_idx: 11\n",
      "0.8645833333333334\n",
      "batch_idx: 12\n",
      "0.8653846153846154\n",
      "batch_idx: 13\n",
      "0.8660714285714286\n",
      "batch_idx: 14\n",
      "0.8555555555555555\n",
      "batch_idx: 15\n",
      "0.8541666666666666\n",
      "batch_idx: 16\n",
      "0.8504901960784313\n",
      "batch_idx: 17\n",
      "0.8449074074074074\n",
      "batch_idx: 18\n",
      "0.8486842105263158\n",
      "batch_idx: 19\n",
      "0.8520833333333333\n",
      "batch_idx: 20\n",
      "0.8353174603174603\n",
      "batch_idx: 21\n",
      "0.8314393939393939\n",
      "batch_idx: 22\n",
      "0.8260869565217391\n",
      "batch_idx: 23\n",
      "0.828125\n",
      "batch_idx: 24\n",
      "0.83\n",
      "Training Epoch: 101, total loss: 42.218319\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.875\n",
      "batch_idx: 8\n",
      "0.875\n",
      "batch_idx: 9\n",
      "0.875\n",
      "batch_idx: 10\n",
      "0.8674242424242424\n",
      "batch_idx: 11\n",
      "0.8611111111111112\n",
      "batch_idx: 12\n",
      "0.8653846153846154\n",
      "batch_idx: 13\n",
      "0.8541666666666666\n",
      "batch_idx: 14\n",
      "0.8527777777777777\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8431372549019608\n",
      "batch_idx: 17\n",
      "0.8472222222222222\n",
      "batch_idx: 18\n",
      "0.8486842105263158\n",
      "batch_idx: 19\n",
      "0.85\n",
      "batch_idx: 20\n",
      "0.8412698412698413\n",
      "batch_idx: 21\n",
      "0.8390151515151515\n",
      "batch_idx: 22\n",
      "0.8405797101449275\n",
      "batch_idx: 23\n",
      "0.8454861111111112\n",
      "batch_idx: 24\n",
      "0.8416666666666667\n",
      "Training Epoch: 102, total loss: 42.022128\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8472222222222222\n",
      "batch_idx: 12\n",
      "0.8525641025641025\n",
      "batch_idx: 13\n",
      "0.8452380952380952\n",
      "batch_idx: 14\n",
      "0.8472222222222222\n",
      "batch_idx: 15\n",
      "0.8489583333333334\n",
      "batch_idx: 16\n",
      "0.8504901960784313\n",
      "batch_idx: 17\n",
      "0.8518518518518519\n",
      "batch_idx: 18\n",
      "0.8530701754385965\n",
      "batch_idx: 19\n",
      "0.85625\n",
      "batch_idx: 20\n",
      "0.8492063492063492\n",
      "batch_idx: 21\n",
      "0.8503787878787878\n",
      "batch_idx: 22\n",
      "0.8460144927536232\n",
      "batch_idx: 23\n",
      "0.8402777777777778\n",
      "batch_idx: 24\n",
      "0.8383333333333334\n",
      "Training Epoch: 103, total loss: 41.902167\n",
      "batch_idx: 0\n",
      "1.0\n",
      "batch_idx: 1\n",
      "0.9791666666666666\n",
      "batch_idx: 2\n",
      "0.9722222222222222\n",
      "batch_idx: 3\n",
      "0.96875\n",
      "batch_idx: 4\n",
      "0.925\n",
      "batch_idx: 5\n",
      "0.9236111111111112\n",
      "batch_idx: 6\n",
      "0.8988095238095238\n",
      "batch_idx: 7\n",
      "0.8854166666666666\n",
      "batch_idx: 8\n",
      "0.8842592592592593\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8712121212121212\n",
      "batch_idx: 11\n",
      "0.875\n",
      "batch_idx: 12\n",
      "0.8717948717948718\n",
      "batch_idx: 13\n",
      "0.8720238095238095\n",
      "batch_idx: 14\n",
      "0.875\n",
      "batch_idx: 15\n",
      "0.8776041666666666\n",
      "batch_idx: 16\n",
      "0.8700980392156863\n",
      "batch_idx: 17\n",
      "0.8657407407407407\n",
      "batch_idx: 18\n",
      "0.8662280701754386\n",
      "batch_idx: 19\n",
      "0.8645833333333334\n",
      "batch_idx: 20\n",
      "0.8670634920634921\n",
      "batch_idx: 21\n",
      "0.8693181818181818\n",
      "batch_idx: 22\n",
      "0.8568840579710145\n",
      "batch_idx: 23\n",
      "0.8524305555555556\n",
      "batch_idx: 24\n",
      "0.855\n",
      "Training Epoch: 104, total loss: 41.791368\n",
      "batch_idx: 0\n",
      "1.0\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8657407407407407\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8611111111111112\n",
      "batch_idx: 12\n",
      "0.8653846153846154\n",
      "batch_idx: 13\n",
      "0.8720238095238095\n",
      "batch_idx: 14\n",
      "0.875\n",
      "batch_idx: 15\n",
      "0.8671875\n",
      "batch_idx: 16\n",
      "0.8651960784313726\n",
      "batch_idx: 17\n",
      "0.8564814814814815\n",
      "batch_idx: 18\n",
      "0.8508771929824561\n",
      "batch_idx: 19\n",
      "0.8458333333333333\n",
      "batch_idx: 20\n",
      "0.8452380952380952\n",
      "batch_idx: 21\n",
      "0.8465909090909091\n",
      "batch_idx: 22\n",
      "0.8514492753623188\n",
      "batch_idx: 23\n",
      "0.8489583333333334\n",
      "batch_idx: 24\n",
      "0.85\n",
      "Training Epoch: 105, total loss: 41.851712\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8422619047619048\n",
      "batch_idx: 14\n",
      "0.8444444444444444\n",
      "batch_idx: 15\n",
      "0.8333333333333334\n",
      "batch_idx: 16\n",
      "0.8308823529411765\n",
      "batch_idx: 17\n",
      "0.8310185185185185\n",
      "batch_idx: 18\n",
      "0.8289473684210527\n",
      "batch_idx: 19\n",
      "0.8354166666666667\n",
      "batch_idx: 20\n",
      "0.8373015873015873\n",
      "batch_idx: 21\n",
      "0.8352272727272727\n",
      "batch_idx: 22\n",
      "0.8387681159420289\n",
      "batch_idx: 23\n",
      "0.84375\n",
      "batch_idx: 24\n",
      "0.8433333333333334\n",
      "Training Epoch: 106, total loss: 41.776517\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8388888888888889\n",
      "batch_idx: 15\n",
      "0.84375\n",
      "batch_idx: 16\n",
      "0.8333333333333334\n",
      "batch_idx: 17\n",
      "0.8333333333333334\n",
      "batch_idx: 18\n",
      "0.8289473684210527\n",
      "batch_idx: 19\n",
      "0.8354166666666667\n",
      "batch_idx: 20\n",
      "0.8333333333333334\n",
      "batch_idx: 21\n",
      "0.8409090909090909\n",
      "batch_idx: 22\n",
      "0.8387681159420289\n",
      "batch_idx: 23\n",
      "0.8402777777777778\n",
      "batch_idx: 24\n",
      "0.8366666666666667\n",
      "Training Epoch: 107, total loss: 42.024449\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8703703703703703\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8244047619047619\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.8259803921568627\n",
      "batch_idx: 17\n",
      "0.8287037037037037\n",
      "batch_idx: 18\n",
      "0.831140350877193\n",
      "batch_idx: 19\n",
      "0.8375\n",
      "batch_idx: 20\n",
      "0.8373015873015873\n",
      "batch_idx: 21\n",
      "0.8352272727272727\n",
      "batch_idx: 22\n",
      "0.8333333333333334\n",
      "batch_idx: 23\n",
      "0.828125\n",
      "batch_idx: 24\n",
      "0.83\n",
      "Training Epoch: 108, total loss: 42.096610\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.8557692307692307\n",
      "batch_idx: 13\n",
      "0.8601190476190477\n",
      "batch_idx: 14\n",
      "0.8527777777777777\n",
      "batch_idx: 15\n",
      "0.8567708333333334\n",
      "batch_idx: 16\n",
      "0.8553921568627451\n",
      "batch_idx: 17\n",
      "0.8541666666666666\n",
      "batch_idx: 18\n",
      "0.8552631578947368\n",
      "batch_idx: 19\n",
      "0.8520833333333333\n",
      "batch_idx: 20\n",
      "0.8591269841269841\n",
      "batch_idx: 21\n",
      "0.8598484848484849\n",
      "batch_idx: 22\n",
      "0.8605072463768116\n",
      "batch_idx: 23\n",
      "0.8506944444444444\n",
      "batch_idx: 24\n",
      "0.845\n",
      "Training Epoch: 109, total loss: 41.776456\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7916666666666666\n",
      "batch_idx: 14\n",
      "0.7861111111111111\n",
      "batch_idx: 15\n",
      "0.7942708333333334\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.8026315789473685\n",
      "batch_idx: 19\n",
      "0.8104166666666667\n",
      "batch_idx: 20\n",
      "0.8154761904761905\n",
      "batch_idx: 21\n",
      "0.8162878787878788\n",
      "batch_idx: 22\n",
      "0.8206521739130435\n",
      "batch_idx: 23\n",
      "0.8246527777777778\n",
      "batch_idx: 24\n",
      "0.8233333333333334\n",
      "Training Epoch: 110, total loss: 42.255219\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8333333333333334\n",
      "batch_idx: 14\n",
      "0.8333333333333334\n",
      "batch_idx: 15\n",
      "0.8333333333333334\n",
      "batch_idx: 16\n",
      "0.8382352941176471\n",
      "batch_idx: 17\n",
      "0.8333333333333334\n",
      "batch_idx: 18\n",
      "0.831140350877193\n",
      "batch_idx: 19\n",
      "0.8354166666666667\n",
      "batch_idx: 20\n",
      "0.8373015873015873\n",
      "batch_idx: 21\n",
      "0.8409090909090909\n",
      "batch_idx: 22\n",
      "0.8442028985507246\n",
      "batch_idx: 23\n",
      "0.8402777777777778\n",
      "batch_idx: 24\n",
      "0.8383333333333334\n",
      "Training Epoch: 111, total loss: 42.152128\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.8958333333333334\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.8557692307692307\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8416666666666667\n",
      "batch_idx: 15\n",
      "0.8359375\n",
      "batch_idx: 16\n",
      "0.8406862745098039\n",
      "batch_idx: 17\n",
      "0.8402777777777778\n",
      "batch_idx: 18\n",
      "0.8464912280701754\n",
      "batch_idx: 19\n",
      "0.8520833333333333\n",
      "batch_idx: 20\n",
      "0.8492063492063492\n",
      "batch_idx: 21\n",
      "0.8541666666666666\n",
      "batch_idx: 22\n",
      "0.8532608695652174\n",
      "batch_idx: 23\n",
      "0.8524305555555556\n",
      "batch_idx: 24\n",
      "0.85\n",
      "Training Epoch: 112, total loss: 41.753987\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8222222222222222\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.8259803921568627\n",
      "batch_idx: 17\n",
      "0.8287037037037037\n",
      "batch_idx: 18\n",
      "0.831140350877193\n",
      "batch_idx: 19\n",
      "0.8375\n",
      "batch_idx: 20\n",
      "0.8353174603174603\n",
      "batch_idx: 21\n",
      "0.8371212121212122\n",
      "batch_idx: 22\n",
      "0.8369565217391305\n",
      "batch_idx: 23\n",
      "0.84375\n",
      "batch_idx: 24\n",
      "0.845\n",
      "Training Epoch: 113, total loss: 41.931373\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9583333333333334\n",
      "batch_idx: 2\n",
      "0.9444444444444444\n",
      "batch_idx: 3\n",
      "0.9375\n",
      "batch_idx: 4\n",
      "0.9333333333333333\n",
      "batch_idx: 5\n",
      "0.9027777777777778\n",
      "batch_idx: 6\n",
      "0.8988095238095238\n",
      "batch_idx: 7\n",
      "0.890625\n",
      "batch_idx: 8\n",
      "0.8981481481481481\n",
      "batch_idx: 9\n",
      "0.9\n",
      "batch_idx: 10\n",
      "0.8863636363636364\n",
      "batch_idx: 11\n",
      "0.8854166666666666\n",
      "batch_idx: 12\n",
      "0.875\n",
      "batch_idx: 13\n",
      "0.8720238095238095\n",
      "batch_idx: 14\n",
      "0.8694444444444445\n",
      "batch_idx: 15\n",
      "0.8645833333333334\n",
      "batch_idx: 16\n",
      "0.8651960784313726\n",
      "batch_idx: 17\n",
      "0.8680555555555556\n",
      "batch_idx: 18\n",
      "0.8618421052631579\n",
      "batch_idx: 19\n",
      "0.8604166666666667\n",
      "batch_idx: 20\n",
      "0.8630952380952381\n",
      "batch_idx: 21\n",
      "0.8598484848484849\n",
      "batch_idx: 22\n",
      "0.8586956521739131\n",
      "batch_idx: 23\n",
      "0.859375\n",
      "batch_idx: 24\n",
      "0.86\n",
      "Training Epoch: 114, total loss: 41.613754\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8269230769230769\n",
      "batch_idx: 13\n",
      "0.8154761904761905\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8263888888888888\n",
      "batch_idx: 18\n",
      "0.8355263157894737\n",
      "batch_idx: 19\n",
      "0.8395833333333333\n",
      "batch_idx: 20\n",
      "0.8373015873015873\n",
      "batch_idx: 21\n",
      "0.8371212121212122\n",
      "batch_idx: 22\n",
      "0.842391304347826\n",
      "batch_idx: 23\n",
      "0.8402777777777778\n",
      "batch_idx: 24\n",
      "0.8383333333333334\n",
      "Training Epoch: 115, total loss: 41.918159\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.9083333333333333\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8657407407407407\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8712121212121212\n",
      "batch_idx: 11\n",
      "0.8680555555555556\n",
      "batch_idx: 12\n",
      "0.8717948717948718\n",
      "batch_idx: 13\n",
      "0.8720238095238095\n",
      "batch_idx: 14\n",
      "0.875\n",
      "batch_idx: 15\n",
      "0.8645833333333334\n",
      "batch_idx: 16\n",
      "0.8529411764705882\n",
      "batch_idx: 17\n",
      "0.8518518518518519\n",
      "batch_idx: 18\n",
      "0.8552631578947368\n",
      "batch_idx: 19\n",
      "0.85625\n",
      "batch_idx: 20\n",
      "0.8531746031746031\n",
      "batch_idx: 21\n",
      "0.8522727272727273\n",
      "batch_idx: 22\n",
      "0.855072463768116\n",
      "batch_idx: 23\n",
      "0.8611111111111112\n",
      "batch_idx: 24\n",
      "0.8583333333333333\n",
      "Training Epoch: 116, total loss: 41.698447\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8703703703703703\n",
      "batch_idx: 9\n",
      "0.8625\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8680555555555556\n",
      "batch_idx: 12\n",
      "0.8685897435897436\n",
      "batch_idx: 13\n",
      "0.8571428571428571\n",
      "batch_idx: 14\n",
      "0.8583333333333333\n",
      "batch_idx: 15\n",
      "0.8619791666666666\n",
      "batch_idx: 16\n",
      "0.8578431372549019\n",
      "batch_idx: 17\n",
      "0.8587962962962963\n",
      "batch_idx: 18\n",
      "0.8618421052631579\n",
      "batch_idx: 19\n",
      "0.8666666666666667\n",
      "batch_idx: 20\n",
      "0.8670634920634921\n",
      "batch_idx: 21\n",
      "0.8712121212121212\n",
      "batch_idx: 22\n",
      "0.875\n",
      "batch_idx: 23\n",
      "0.8732638888888888\n",
      "batch_idx: 24\n",
      "0.8716666666666667\n",
      "Training Epoch: 117, total loss: 41.410914\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8298611111111112\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8422619047619048\n",
      "batch_idx: 14\n",
      "0.8388888888888889\n",
      "batch_idx: 15\n",
      "0.828125\n",
      "batch_idx: 16\n",
      "0.8259803921568627\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.8245614035087719\n",
      "batch_idx: 19\n",
      "0.83125\n",
      "batch_idx: 20\n",
      "0.8333333333333334\n",
      "batch_idx: 21\n",
      "0.8390151515151515\n",
      "batch_idx: 22\n",
      "0.842391304347826\n",
      "batch_idx: 23\n",
      "0.8385416666666666\n",
      "batch_idx: 24\n",
      "0.8416666666666667\n",
      "Training Epoch: 118, total loss: 41.983387\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8796296296296297\n",
      "batch_idx: 9\n",
      "0.8833333333333333\n",
      "batch_idx: 10\n",
      "0.8825757575757576\n",
      "batch_idx: 11\n",
      "0.8819444444444444\n",
      "batch_idx: 12\n",
      "0.8878205128205128\n",
      "batch_idx: 13\n",
      "0.8928571428571429\n",
      "batch_idx: 14\n",
      "0.8916666666666667\n",
      "batch_idx: 15\n",
      "0.8828125\n",
      "batch_idx: 16\n",
      "0.8872549019607843\n",
      "batch_idx: 17\n",
      "0.8912037037037037\n",
      "batch_idx: 18\n",
      "0.8881578947368421\n",
      "batch_idx: 19\n",
      "0.8833333333333333\n",
      "batch_idx: 20\n",
      "0.8809523809523809\n",
      "batch_idx: 21\n",
      "0.884469696969697\n",
      "batch_idx: 22\n",
      "0.8840579710144928\n",
      "batch_idx: 23\n",
      "0.8802083333333334\n",
      "batch_idx: 24\n",
      "0.88\n",
      "Training Epoch: 119, total loss: 41.121454\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.8869047619047619\n",
      "batch_idx: 7\n",
      "0.8802083333333334\n",
      "batch_idx: 8\n",
      "0.8888888888888888\n",
      "batch_idx: 9\n",
      "0.8958333333333334\n",
      "batch_idx: 10\n",
      "0.8901515151515151\n",
      "batch_idx: 11\n",
      "0.8680555555555556\n",
      "batch_idx: 12\n",
      "0.875\n",
      "batch_idx: 13\n",
      "0.8809523809523809\n",
      "batch_idx: 14\n",
      "0.8861111111111111\n",
      "batch_idx: 15\n",
      "0.8880208333333334\n",
      "batch_idx: 16\n",
      "0.8799019607843137\n",
      "batch_idx: 17\n",
      "0.8796296296296297\n",
      "batch_idx: 18\n",
      "0.881578947368421\n",
      "batch_idx: 19\n",
      "0.8854166666666666\n",
      "batch_idx: 20\n",
      "0.8888888888888888\n",
      "batch_idx: 21\n",
      "0.8901515151515151\n",
      "batch_idx: 22\n",
      "0.8894927536231884\n",
      "batch_idx: 23\n",
      "0.8854166666666666\n",
      "batch_idx: 24\n",
      "0.885\n",
      "Training Epoch: 120, total loss: 41.213236\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8854166666666666\n",
      "batch_idx: 8\n",
      "0.875\n",
      "batch_idx: 9\n",
      "0.8791666666666667\n",
      "batch_idx: 10\n",
      "0.8825757575757576\n",
      "batch_idx: 11\n",
      "0.8715277777777778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 12\n",
      "0.8717948717948718\n",
      "batch_idx: 13\n",
      "0.875\n",
      "batch_idx: 14\n",
      "0.8833333333333333\n",
      "batch_idx: 15\n",
      "0.8776041666666666\n",
      "batch_idx: 16\n",
      "0.8774509803921569\n",
      "batch_idx: 17\n",
      "0.8726851851851852\n",
      "batch_idx: 18\n",
      "0.868421052631579\n",
      "batch_idx: 19\n",
      "0.8541666666666666\n",
      "batch_idx: 20\n",
      "0.8551587301587301\n",
      "batch_idx: 21\n",
      "0.8522727272727273\n",
      "batch_idx: 22\n",
      "0.8496376811594203\n",
      "batch_idx: 23\n",
      "0.8506944444444444\n",
      "batch_idx: 24\n",
      "0.85\n",
      "Training Epoch: 121, total loss: 41.740676\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8186274509803921\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.8208333333333333\n",
      "batch_idx: 20\n",
      "0.8174603174603174\n",
      "batch_idx: 21\n",
      "0.8238636363636364\n",
      "batch_idx: 22\n",
      "0.8242753623188406\n",
      "batch_idx: 23\n",
      "0.8246527777777778\n",
      "batch_idx: 24\n",
      "0.8283333333333334\n",
      "Training Epoch: 122, total loss: 42.239538\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.9305555555555556\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8888888888888888\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.8472222222222222\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8388888888888889\n",
      "batch_idx: 15\n",
      "0.8385416666666666\n",
      "batch_idx: 16\n",
      "0.8308823529411765\n",
      "batch_idx: 17\n",
      "0.8217592592592593\n",
      "batch_idx: 18\n",
      "0.8245614035087719\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8154761904761905\n",
      "batch_idx: 21\n",
      "0.821969696969697\n",
      "batch_idx: 22\n",
      "0.8242753623188406\n",
      "batch_idx: 23\n",
      "0.8194444444444444\n",
      "batch_idx: 24\n",
      "0.8216666666666667\n",
      "Training Epoch: 123, total loss: 42.228965\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8301282051282052\n",
      "batch_idx: 13\n",
      "0.8303571428571429\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.8333333333333334\n",
      "batch_idx: 16\n",
      "0.8333333333333334\n",
      "batch_idx: 17\n",
      "0.8356481481481481\n",
      "batch_idx: 18\n",
      "0.8355263157894737\n",
      "batch_idx: 19\n",
      "0.8375\n",
      "batch_idx: 20\n",
      "0.8273809523809523\n",
      "batch_idx: 21\n",
      "0.8314393939393939\n",
      "batch_idx: 22\n",
      "0.8351449275362319\n",
      "batch_idx: 23\n",
      "0.8402777777777778\n",
      "batch_idx: 24\n",
      "0.8383333333333334\n",
      "Training Epoch: 124, total loss: 41.956692\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8422619047619048\n",
      "batch_idx: 14\n",
      "0.8472222222222222\n",
      "batch_idx: 15\n",
      "0.8411458333333334\n",
      "batch_idx: 16\n",
      "0.8480392156862745\n",
      "batch_idx: 17\n",
      "0.8518518518518519\n",
      "batch_idx: 18\n",
      "0.8530701754385965\n",
      "batch_idx: 19\n",
      "0.8541666666666666\n",
      "batch_idx: 20\n",
      "0.8591269841269841\n",
      "batch_idx: 21\n",
      "0.8503787878787878\n",
      "batch_idx: 22\n",
      "0.8532608695652174\n",
      "batch_idx: 23\n",
      "0.8524305555555556\n",
      "batch_idx: 24\n",
      "0.8566666666666667\n",
      "Training Epoch: 125, total loss: 41.639775\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8257575757575758\n",
      "batch_idx: 11\n",
      "0.8298611111111112\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8472222222222222\n",
      "batch_idx: 15\n",
      "0.8411458333333334\n",
      "batch_idx: 16\n",
      "0.8406862745098039\n",
      "batch_idx: 17\n",
      "0.8379629629629629\n",
      "batch_idx: 18\n",
      "0.8399122807017544\n",
      "batch_idx: 19\n",
      "0.8354166666666667\n",
      "batch_idx: 20\n",
      "0.8412698412698413\n",
      "batch_idx: 21\n",
      "0.8390151515151515\n",
      "batch_idx: 22\n",
      "0.8333333333333334\n",
      "batch_idx: 23\n",
      "0.8350694444444444\n",
      "batch_idx: 24\n",
      "0.8366666666666667\n",
      "Training Epoch: 126, total loss: 41.946464\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8269230769230769\n",
      "batch_idx: 13\n",
      "0.8333333333333334\n",
      "batch_idx: 14\n",
      "0.8333333333333334\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.8333333333333334\n",
      "batch_idx: 17\n",
      "0.8356481481481481\n",
      "batch_idx: 18\n",
      "0.8289473684210527\n",
      "batch_idx: 19\n",
      "0.8333333333333334\n",
      "batch_idx: 20\n",
      "0.8353174603174603\n",
      "batch_idx: 21\n",
      "0.8390151515151515\n",
      "batch_idx: 22\n",
      "0.8369565217391305\n",
      "batch_idx: 23\n",
      "0.8402777777777778\n",
      "batch_idx: 24\n",
      "0.8416666666666667\n",
      "Training Epoch: 127, total loss: 41.825809\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8388888888888889\n",
      "batch_idx: 15\n",
      "0.84375\n",
      "batch_idx: 16\n",
      "0.8406862745098039\n",
      "batch_idx: 17\n",
      "0.8449074074074074\n",
      "batch_idx: 18\n",
      "0.8421052631578947\n",
      "batch_idx: 19\n",
      "0.85\n",
      "batch_idx: 20\n",
      "0.8472222222222222\n",
      "batch_idx: 21\n",
      "0.8503787878787878\n",
      "batch_idx: 22\n",
      "0.8514492753623188\n",
      "batch_idx: 23\n",
      "0.8524305555555556\n",
      "batch_idx: 24\n",
      "0.8516666666666667\n",
      "Training Epoch: 128, total loss: 41.695998\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.90625\n",
      "batch_idx: 4\n",
      "0.9083333333333333\n",
      "batch_idx: 5\n",
      "0.8888888888888888\n",
      "batch_idx: 6\n",
      "0.8928571428571429\n",
      "batch_idx: 7\n",
      "0.8802083333333334\n",
      "batch_idx: 8\n",
      "0.8888888888888888\n",
      "batch_idx: 9\n",
      "0.8875\n",
      "batch_idx: 10\n",
      "0.8901515151515151\n",
      "batch_idx: 11\n",
      "0.8854166666666666\n",
      "batch_idx: 12\n",
      "0.8814102564102564\n",
      "batch_idx: 13\n",
      "0.8601190476190477\n",
      "batch_idx: 14\n",
      "0.8611111111111112\n",
      "batch_idx: 15\n",
      "0.8619791666666666\n",
      "batch_idx: 16\n",
      "0.8578431372549019\n",
      "batch_idx: 17\n",
      "0.8564814814814815\n",
      "batch_idx: 18\n",
      "0.8486842105263158\n",
      "batch_idx: 19\n",
      "0.85\n",
      "batch_idx: 20\n",
      "0.8531746031746031\n",
      "batch_idx: 21\n",
      "0.8579545454545454\n",
      "batch_idx: 22\n",
      "0.8532608695652174\n",
      "batch_idx: 23\n",
      "0.8559027777777778\n",
      "batch_idx: 24\n",
      "0.8533333333333334\n",
      "Training Epoch: 129, total loss: 41.858595\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.9583333333333334\n",
      "batch_idx: 2\n",
      "0.9583333333333334\n",
      "batch_idx: 3\n",
      "0.9583333333333334\n",
      "batch_idx: 4\n",
      "0.9416666666666667\n",
      "batch_idx: 5\n",
      "0.9236111111111112\n",
      "batch_idx: 6\n",
      "0.9107142857142857\n",
      "batch_idx: 7\n",
      "0.9010416666666666\n",
      "batch_idx: 8\n",
      "0.8842592592592593\n",
      "batch_idx: 9\n",
      "0.8833333333333333\n",
      "batch_idx: 10\n",
      "0.8863636363636364\n",
      "batch_idx: 11\n",
      "0.8888888888888888\n",
      "batch_idx: 12\n",
      "0.8942307692307693\n",
      "batch_idx: 13\n",
      "0.875\n",
      "batch_idx: 14\n",
      "0.8694444444444445\n",
      "batch_idx: 15\n",
      "0.875\n",
      "batch_idx: 16\n",
      "0.8676470588235294\n",
      "batch_idx: 17\n",
      "0.8657407407407407\n",
      "batch_idx: 18\n",
      "0.8640350877192983\n",
      "batch_idx: 19\n",
      "0.8604166666666667\n",
      "batch_idx: 20\n",
      "0.8611111111111112\n",
      "batch_idx: 21\n",
      "0.8579545454545454\n",
      "batch_idx: 22\n",
      "0.855072463768116\n",
      "batch_idx: 23\n",
      "0.8524305555555556\n",
      "batch_idx: 24\n",
      "0.8566666666666667\n",
      "Training Epoch: 130, total loss: 41.574529\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.9097222222222222\n",
      "batch_idx: 6\n",
      "0.8988095238095238\n",
      "batch_idx: 7\n",
      "0.8854166666666666\n",
      "batch_idx: 8\n",
      "0.8842592592592593\n",
      "batch_idx: 9\n",
      "0.8916666666666667\n",
      "batch_idx: 10\n",
      "0.8977272727272727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 11\n",
      "0.8888888888888888\n",
      "batch_idx: 12\n",
      "0.8910256410256411\n",
      "batch_idx: 13\n",
      "0.8869047619047619\n",
      "batch_idx: 14\n",
      "0.8861111111111111\n",
      "batch_idx: 15\n",
      "0.875\n",
      "batch_idx: 16\n",
      "0.8774509803921569\n",
      "batch_idx: 17\n",
      "0.8773148148148148\n",
      "batch_idx: 18\n",
      "0.8793859649122807\n",
      "batch_idx: 19\n",
      "0.875\n",
      "batch_idx: 20\n",
      "0.875\n",
      "batch_idx: 21\n",
      "0.8768939393939394\n",
      "batch_idx: 22\n",
      "0.8768115942028986\n",
      "batch_idx: 23\n",
      "0.8802083333333334\n",
      "batch_idx: 24\n",
      "0.885\n",
      "Training Epoch: 131, total loss: 41.086026\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.8854166666666666\n",
      "batch_idx: 8\n",
      "0.875\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8674242424242424\n",
      "batch_idx: 11\n",
      "0.8680555555555556\n",
      "batch_idx: 12\n",
      "0.8685897435897436\n",
      "batch_idx: 13\n",
      "0.8690476190476191\n",
      "batch_idx: 14\n",
      "0.8694444444444445\n",
      "batch_idx: 15\n",
      "0.8489583333333334\n",
      "batch_idx: 16\n",
      "0.8455882352941176\n",
      "batch_idx: 17\n",
      "0.8495370370370371\n",
      "batch_idx: 18\n",
      "0.8464912280701754\n",
      "batch_idx: 19\n",
      "0.8333333333333334\n",
      "batch_idx: 20\n",
      "0.8353174603174603\n",
      "batch_idx: 21\n",
      "0.8390151515151515\n",
      "batch_idx: 22\n",
      "0.8351449275362319\n",
      "batch_idx: 23\n",
      "0.8350694444444444\n",
      "batch_idx: 24\n",
      "0.8316666666666667\n",
      "Training Epoch: 132, total loss: 42.087800\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8916666666666667\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8703703703703703\n",
      "batch_idx: 9\n",
      "0.8708333333333333\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.8611111111111112\n",
      "batch_idx: 12\n",
      "0.8621794871794872\n",
      "batch_idx: 13\n",
      "0.8452380952380952\n",
      "batch_idx: 14\n",
      "0.8444444444444444\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8504901960784313\n",
      "batch_idx: 17\n",
      "0.8541666666666666\n",
      "batch_idx: 18\n",
      "0.8442982456140351\n",
      "batch_idx: 19\n",
      "0.85\n",
      "batch_idx: 20\n",
      "0.8531746031746031\n",
      "batch_idx: 21\n",
      "0.8503787878787878\n",
      "batch_idx: 22\n",
      "0.8442028985507246\n",
      "batch_idx: 23\n",
      "0.84375\n",
      "batch_idx: 24\n",
      "0.8483333333333334\n",
      "Training Epoch: 133, total loss: 41.773519\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8452380952380952\n",
      "batch_idx: 14\n",
      "0.85\n",
      "batch_idx: 15\n",
      "0.8489583333333334\n",
      "batch_idx: 16\n",
      "0.8553921568627451\n",
      "batch_idx: 17\n",
      "0.8541666666666666\n",
      "batch_idx: 18\n",
      "0.8552631578947368\n",
      "batch_idx: 19\n",
      "0.8541666666666666\n",
      "batch_idx: 20\n",
      "0.8551587301587301\n",
      "batch_idx: 21\n",
      "0.8579545454545454\n",
      "batch_idx: 22\n",
      "0.8532608695652174\n",
      "batch_idx: 23\n",
      "0.8541666666666666\n",
      "batch_idx: 24\n",
      "0.8566666666666667\n",
      "Training Epoch: 134, total loss: 41.607772\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.90625\n",
      "batch_idx: 4\n",
      "0.9083333333333333\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8363095238095238\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.84375\n",
      "batch_idx: 16\n",
      "0.8504901960784313\n",
      "batch_idx: 17\n",
      "0.8564814814814815\n",
      "batch_idx: 18\n",
      "0.8486842105263158\n",
      "batch_idx: 19\n",
      "0.8458333333333333\n",
      "batch_idx: 20\n",
      "0.8412698412698413\n",
      "batch_idx: 21\n",
      "0.8465909090909091\n",
      "batch_idx: 22\n",
      "0.8405797101449275\n",
      "batch_idx: 23\n",
      "0.84375\n",
      "batch_idx: 24\n",
      "0.845\n",
      "Training Epoch: 135, total loss: 41.861608\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.875\n",
      "batch_idx: 9\n",
      "0.8708333333333333\n",
      "batch_idx: 10\n",
      "0.8636363636363636\n",
      "batch_idx: 11\n",
      "0.8645833333333334\n",
      "batch_idx: 12\n",
      "0.8685897435897436\n",
      "batch_idx: 13\n",
      "0.8630952380952381\n",
      "batch_idx: 14\n",
      "0.8527777777777777\n",
      "batch_idx: 15\n",
      "0.8515625\n",
      "batch_idx: 16\n",
      "0.8504901960784313\n",
      "batch_idx: 17\n",
      "0.8564814814814815\n",
      "batch_idx: 18\n",
      "0.8596491228070176\n",
      "batch_idx: 19\n",
      "0.8604166666666667\n",
      "batch_idx: 20\n",
      "0.8591269841269841\n",
      "batch_idx: 21\n",
      "0.8522727272727273\n",
      "batch_idx: 22\n",
      "0.855072463768116\n",
      "batch_idx: 23\n",
      "0.8524305555555556\n",
      "batch_idx: 24\n",
      "0.85\n",
      "Training Epoch: 136, total loss: 41.696630\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.8305555555555556\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.8308823529411765\n",
      "batch_idx: 17\n",
      "0.8379629629629629\n",
      "batch_idx: 18\n",
      "0.8399122807017544\n",
      "batch_idx: 19\n",
      "0.8458333333333333\n",
      "batch_idx: 20\n",
      "0.8511904761904762\n",
      "batch_idx: 21\n",
      "0.8503787878787878\n",
      "batch_idx: 22\n",
      "0.8442028985507246\n",
      "batch_idx: 23\n",
      "0.84375\n",
      "batch_idx: 24\n",
      "0.845\n",
      "Training Epoch: 137, total loss: 41.884337\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8482142857142857\n",
      "batch_idx: 14\n",
      "0.8583333333333333\n",
      "batch_idx: 15\n",
      "0.8619791666666666\n",
      "batch_idx: 16\n",
      "0.8651960784313726\n",
      "batch_idx: 17\n",
      "0.8680555555555556\n",
      "batch_idx: 18\n",
      "0.8728070175438597\n",
      "batch_idx: 19\n",
      "0.8729166666666667\n",
      "batch_idx: 20\n",
      "0.876984126984127\n",
      "batch_idx: 21\n",
      "0.8768939393939394\n",
      "batch_idx: 22\n",
      "0.8786231884057971\n",
      "batch_idx: 23\n",
      "0.8802083333333334\n",
      "batch_idx: 24\n",
      "0.8766666666666667\n",
      "Training Epoch: 138, total loss: 41.249628\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8888888888888888\n",
      "batch_idx: 6\n",
      "0.8988095238095238\n",
      "batch_idx: 7\n",
      "0.90625\n",
      "batch_idx: 8\n",
      "0.8981481481481481\n",
      "batch_idx: 9\n",
      "0.8958333333333334\n",
      "batch_idx: 10\n",
      "0.8901515151515151\n",
      "batch_idx: 11\n",
      "0.8993055555555556\n",
      "batch_idx: 12\n",
      "0.9038461538461539\n",
      "batch_idx: 13\n",
      "0.9047619047619048\n",
      "batch_idx: 14\n",
      "0.9027777777777778\n",
      "batch_idx: 15\n",
      "0.9036458333333334\n",
      "batch_idx: 16\n",
      "0.8970588235294118\n",
      "batch_idx: 17\n",
      "0.8842592592592593\n",
      "batch_idx: 18\n",
      "0.8837719298245614\n",
      "batch_idx: 19\n",
      "0.8791666666666667\n",
      "batch_idx: 20\n",
      "0.8829365079365079\n",
      "batch_idx: 21\n",
      "0.8863636363636364\n",
      "batch_idx: 22\n",
      "0.8768115942028986\n",
      "batch_idx: 23\n",
      "0.8802083333333334\n",
      "batch_idx: 24\n",
      "0.8783333333333333\n",
      "Training Epoch: 139, total loss: 41.278389\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8657407407407407\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.8525641025641025\n",
      "batch_idx: 13\n",
      "0.8571428571428571\n",
      "batch_idx: 14\n",
      "0.8583333333333333\n",
      "batch_idx: 15\n",
      "0.8619791666666666\n",
      "batch_idx: 16\n",
      "0.8651960784313726\n",
      "batch_idx: 17\n",
      "0.8587962962962963\n",
      "batch_idx: 18\n",
      "0.8552631578947368\n",
      "batch_idx: 19\n",
      "0.8520833333333333\n",
      "batch_idx: 20\n",
      "0.8551587301587301\n",
      "batch_idx: 21\n",
      "0.8560606060606061\n",
      "batch_idx: 22\n",
      "0.8605072463768116\n",
      "batch_idx: 23\n",
      "0.859375\n",
      "batch_idx: 24\n",
      "0.8616666666666667\n",
      "Training Epoch: 140, total loss: 41.477513\n",
      "batch_idx: 0\n",
      "1.0\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.8809523809523809\n",
      "batch_idx: 7\n",
      "0.8854166666666666\n",
      "batch_idx: 8\n",
      "0.875\n",
      "batch_idx: 9\n",
      "0.875\n",
      "batch_idx: 10\n",
      "0.8712121212121212\n",
      "batch_idx: 11\n",
      "0.8680555555555556\n",
      "batch_idx: 12\n",
      "0.8685897435897436\n",
      "batch_idx: 13\n",
      "0.8630952380952381\n",
      "batch_idx: 14\n",
      "0.8611111111111112\n",
      "batch_idx: 15\n",
      "0.8489583333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 16\n",
      "0.8406862745098039\n",
      "batch_idx: 17\n",
      "0.8449074074074074\n",
      "batch_idx: 18\n",
      "0.8486842105263158\n",
      "batch_idx: 19\n",
      "0.8479166666666667\n",
      "batch_idx: 20\n",
      "0.8511904761904762\n",
      "batch_idx: 21\n",
      "0.8503787878787878\n",
      "batch_idx: 22\n",
      "0.8478260869565217\n",
      "batch_idx: 23\n",
      "0.8541666666666666\n",
      "batch_idx: 24\n",
      "0.85\n",
      "Training Epoch: 141, total loss: 41.770214\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.8461538461538461\n",
      "batch_idx: 13\n",
      "0.8511904761904762\n",
      "batch_idx: 14\n",
      "0.8555555555555555\n",
      "batch_idx: 15\n",
      "0.859375\n",
      "batch_idx: 16\n",
      "0.8602941176470589\n",
      "batch_idx: 17\n",
      "0.8634259259259259\n",
      "batch_idx: 18\n",
      "0.8618421052631579\n",
      "batch_idx: 19\n",
      "0.8645833333333334\n",
      "batch_idx: 20\n",
      "0.8611111111111112\n",
      "batch_idx: 21\n",
      "0.8674242424242424\n",
      "batch_idx: 22\n",
      "0.8641304347826086\n",
      "batch_idx: 23\n",
      "0.8645833333333334\n",
      "batch_idx: 24\n",
      "0.8633333333333333\n",
      "Training Epoch: 142, total loss: 41.516374\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8657407407407407\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n",
      "batch_idx: 11\n",
      "0.8541666666666666\n",
      "batch_idx: 12\n",
      "0.8621794871794872\n",
      "batch_idx: 13\n",
      "0.8452380952380952\n",
      "batch_idx: 14\n",
      "0.85\n",
      "batch_idx: 15\n",
      "0.8489583333333334\n",
      "batch_idx: 16\n",
      "0.8529411764705882\n",
      "batch_idx: 17\n",
      "0.8587962962962963\n",
      "batch_idx: 18\n",
      "0.8640350877192983\n",
      "batch_idx: 19\n",
      "0.8645833333333334\n",
      "batch_idx: 20\n",
      "0.8690476190476191\n",
      "batch_idx: 21\n",
      "0.8674242424242424\n",
      "batch_idx: 22\n",
      "0.8568840579710145\n",
      "batch_idx: 23\n",
      "0.8576388888888888\n",
      "batch_idx: 24\n",
      "0.8583333333333333\n",
      "Training Epoch: 143, total loss: 41.617984\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8098958333333334\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.8101851851851852\n",
      "batch_idx: 18\n",
      "0.8026315789473685\n",
      "batch_idx: 19\n",
      "0.8041666666666667\n",
      "batch_idx: 20\n",
      "0.8095238095238095\n",
      "batch_idx: 21\n",
      "0.803030303030303\n",
      "batch_idx: 22\n",
      "0.8097826086956522\n",
      "batch_idx: 23\n",
      "0.8055555555555556\n",
      "batch_idx: 24\n",
      "0.805\n",
      "Training Epoch: 144, total loss: 42.701305\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8809523809523809\n",
      "batch_idx: 7\n",
      "0.8854166666666666\n",
      "batch_idx: 8\n",
      "0.875\n",
      "batch_idx: 9\n",
      "0.8833333333333333\n",
      "batch_idx: 10\n",
      "0.8825757575757576\n",
      "batch_idx: 11\n",
      "0.875\n",
      "batch_idx: 12\n",
      "0.875\n",
      "batch_idx: 13\n",
      "0.875\n",
      "batch_idx: 14\n",
      "0.875\n",
      "batch_idx: 15\n",
      "0.8802083333333334\n",
      "batch_idx: 16\n",
      "0.8848039215686274\n",
      "batch_idx: 17\n",
      "0.8888888888888888\n",
      "batch_idx: 18\n",
      "0.8881578947368421\n",
      "batch_idx: 19\n",
      "0.8854166666666666\n",
      "batch_idx: 20\n",
      "0.8829365079365079\n",
      "batch_idx: 21\n",
      "0.884469696969697\n",
      "batch_idx: 22\n",
      "0.8840579710144928\n",
      "batch_idx: 23\n",
      "0.8819444444444444\n",
      "batch_idx: 24\n",
      "0.8816666666666667\n",
      "Training Epoch: 145, total loss: 41.128994\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8564814814814815\n",
      "batch_idx: 9\n",
      "0.8708333333333333\n",
      "batch_idx: 10\n",
      "0.8712121212121212\n",
      "batch_idx: 11\n",
      "0.8680555555555556\n",
      "batch_idx: 12\n",
      "0.8621794871794872\n",
      "batch_idx: 13\n",
      "0.8690476190476191\n",
      "batch_idx: 14\n",
      "0.8722222222222222\n",
      "batch_idx: 15\n",
      "0.8697916666666666\n",
      "batch_idx: 16\n",
      "0.8627450980392157\n",
      "batch_idx: 17\n",
      "0.8634259259259259\n",
      "batch_idx: 18\n",
      "0.8618421052631579\n",
      "batch_idx: 19\n",
      "0.8666666666666667\n",
      "batch_idx: 20\n",
      "0.8690476190476191\n",
      "batch_idx: 21\n",
      "0.8674242424242424\n",
      "batch_idx: 22\n",
      "0.8695652173913043\n",
      "batch_idx: 23\n",
      "0.8732638888888888\n",
      "batch_idx: 24\n",
      "0.8716666666666667\n",
      "Training Epoch: 146, total loss: 41.256298\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8869047619047619\n",
      "batch_idx: 7\n",
      "0.8854166666666666\n",
      "batch_idx: 8\n",
      "0.8888888888888888\n",
      "batch_idx: 9\n",
      "0.8833333333333333\n",
      "batch_idx: 10\n",
      "0.8825757575757576\n",
      "batch_idx: 11\n",
      "0.8888888888888888\n",
      "batch_idx: 12\n",
      "0.875\n",
      "batch_idx: 13\n",
      "0.8720238095238095\n",
      "batch_idx: 14\n",
      "0.8722222222222222\n",
      "batch_idx: 15\n",
      "0.8776041666666666\n",
      "batch_idx: 16\n",
      "0.8725490196078431\n",
      "batch_idx: 17\n",
      "0.8726851851851852\n",
      "batch_idx: 18\n",
      "0.868421052631579\n",
      "batch_idx: 19\n",
      "0.8666666666666667\n",
      "batch_idx: 20\n",
      "0.8650793650793651\n",
      "batch_idx: 21\n",
      "0.8522727272727273\n",
      "batch_idx: 22\n",
      "0.8514492753623188\n",
      "batch_idx: 23\n",
      "0.8576388888888888\n",
      "batch_idx: 24\n",
      "0.8583333333333333\n",
      "Training Epoch: 147, total loss: 41.592464\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8657407407407407\n",
      "batch_idx: 9\n",
      "0.8625\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8611111111111112\n",
      "batch_idx: 12\n",
      "0.8653846153846154\n",
      "batch_idx: 13\n",
      "0.8660714285714286\n",
      "batch_idx: 14\n",
      "0.8694444444444445\n",
      "batch_idx: 15\n",
      "0.8723958333333334\n",
      "batch_idx: 16\n",
      "0.8627450980392157\n",
      "batch_idx: 17\n",
      "0.8587962962962963\n",
      "batch_idx: 18\n",
      "0.8552631578947368\n",
      "batch_idx: 19\n",
      "0.85625\n",
      "batch_idx: 20\n",
      "0.8591269841269841\n",
      "batch_idx: 21\n",
      "0.8579545454545454\n",
      "batch_idx: 22\n",
      "0.8605072463768116\n",
      "batch_idx: 23\n",
      "0.8559027777777778\n",
      "batch_idx: 24\n",
      "0.8483333333333334\n",
      "Training Epoch: 148, total loss: 41.863254\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.9305555555555556\n",
      "batch_idx: 3\n",
      "0.9166666666666666\n",
      "batch_idx: 4\n",
      "0.9083333333333333\n",
      "batch_idx: 5\n",
      "0.9027777777777778\n",
      "batch_idx: 6\n",
      "0.8928571428571429\n",
      "batch_idx: 7\n",
      "0.890625\n",
      "batch_idx: 8\n",
      "0.9027777777777778\n",
      "batch_idx: 9\n",
      "0.9\n",
      "batch_idx: 10\n",
      "0.9053030303030303\n",
      "batch_idx: 11\n",
      "0.90625\n",
      "batch_idx: 12\n",
      "0.9134615384615384\n",
      "batch_idx: 13\n",
      "0.9107142857142857\n",
      "batch_idx: 14\n",
      "0.9111111111111111\n",
      "batch_idx: 15\n",
      "0.9088541666666666\n",
      "batch_idx: 16\n",
      "0.9093137254901961\n",
      "batch_idx: 17\n",
      "0.9143518518518519\n",
      "batch_idx: 18\n",
      "0.9122807017543859\n",
      "batch_idx: 19\n",
      "0.9041666666666667\n",
      "batch_idx: 20\n",
      "0.8928571428571429\n",
      "batch_idx: 21\n",
      "0.8901515151515151\n",
      "batch_idx: 22\n",
      "0.894927536231884\n",
      "batch_idx: 23\n",
      "0.8923611111111112\n",
      "batch_idx: 24\n",
      "0.8866666666666667\n",
      "Training Epoch: 149, total loss: 41.180846\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9375\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8303571428571429\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.8333333333333334\n",
      "batch_idx: 16\n",
      "0.8357843137254902\n",
      "batch_idx: 17\n",
      "0.8356481481481481\n",
      "batch_idx: 18\n",
      "0.8355263157894737\n",
      "batch_idx: 19\n",
      "0.8375\n",
      "batch_idx: 20\n",
      "0.8353174603174603\n",
      "batch_idx: 21\n",
      "0.8371212121212122\n",
      "batch_idx: 22\n",
      "0.8405797101449275\n",
      "batch_idx: 23\n",
      "0.8420138888888888\n",
      "batch_idx: 24\n",
      "0.8433333333333334\n",
      "Training Epoch: 150, total loss: 41.819091\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.90625\n",
      "batch_idx: 4\n",
      "0.9083333333333333\n",
      "batch_idx: 5\n",
      "0.8888888888888888\n",
      "batch_idx: 6\n",
      "0.8928571428571429\n",
      "batch_idx: 7\n",
      "0.8854166666666666\n",
      "batch_idx: 8\n",
      "0.8842592592592593\n",
      "batch_idx: 9\n",
      "0.8791666666666667\n",
      "batch_idx: 10\n",
      "0.8825757575757576\n",
      "batch_idx: 11\n",
      "0.8888888888888888\n",
      "batch_idx: 12\n",
      "0.8814102564102564\n",
      "batch_idx: 13\n",
      "0.8839285714285714\n",
      "batch_idx: 14\n",
      "0.8833333333333333\n",
      "batch_idx: 15\n",
      "0.8828125\n",
      "batch_idx: 16\n",
      "0.8799019607843137\n",
      "batch_idx: 17\n",
      "0.8819444444444444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 18\n",
      "0.881578947368421\n",
      "batch_idx: 19\n",
      "0.8770833333333333\n",
      "batch_idx: 20\n",
      "0.875\n",
      "batch_idx: 21\n",
      "0.875\n",
      "batch_idx: 22\n",
      "0.8623188405797102\n",
      "batch_idx: 23\n",
      "0.8628472222222222\n",
      "batch_idx: 24\n",
      "0.8633333333333333\n",
      "Training Epoch: 151, total loss: 41.440466\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8888888888888888\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8680555555555556\n",
      "batch_idx: 12\n",
      "0.8621794871794872\n",
      "batch_idx: 13\n",
      "0.8541666666666666\n",
      "batch_idx: 14\n",
      "0.8611111111111112\n",
      "batch_idx: 15\n",
      "0.859375\n",
      "batch_idx: 16\n",
      "0.8553921568627451\n",
      "batch_idx: 17\n",
      "0.8564814814814815\n",
      "batch_idx: 18\n",
      "0.8574561403508771\n",
      "batch_idx: 19\n",
      "0.8583333333333333\n",
      "batch_idx: 20\n",
      "0.8630952380952381\n",
      "batch_idx: 21\n",
      "0.8674242424242424\n",
      "batch_idx: 22\n",
      "0.8641304347826086\n",
      "batch_idx: 23\n",
      "0.8645833333333334\n",
      "batch_idx: 24\n",
      "0.8683333333333333\n",
      "Training Epoch: 152, total loss: 41.412276\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.90625\n",
      "batch_idx: 4\n",
      "0.9083333333333333\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.875\n",
      "batch_idx: 9\n",
      "0.8833333333333333\n",
      "batch_idx: 10\n",
      "0.8825757575757576\n",
      "batch_idx: 11\n",
      "0.8888888888888888\n",
      "batch_idx: 12\n",
      "0.8942307692307693\n",
      "batch_idx: 13\n",
      "0.8958333333333334\n",
      "batch_idx: 14\n",
      "0.9\n",
      "batch_idx: 15\n",
      "0.9036458333333334\n",
      "batch_idx: 16\n",
      "0.9044117647058824\n",
      "batch_idx: 17\n",
      "0.9074074074074074\n",
      "batch_idx: 18\n",
      "0.9100877192982456\n",
      "batch_idx: 19\n",
      "0.9041666666666667\n",
      "batch_idx: 20\n",
      "0.9067460317460317\n",
      "batch_idx: 21\n",
      "0.9071969696969697\n",
      "batch_idx: 22\n",
      "0.9021739130434783\n",
      "batch_idx: 23\n",
      "0.8975694444444444\n",
      "batch_idx: 24\n",
      "0.9\n",
      "Training Epoch: 153, total loss: 40.797702\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.8869047619047619\n",
      "batch_idx: 7\n",
      "0.8854166666666666\n",
      "batch_idx: 8\n",
      "0.875\n",
      "batch_idx: 9\n",
      "0.875\n",
      "batch_idx: 10\n",
      "0.8787878787878788\n",
      "batch_idx: 11\n",
      "0.8819444444444444\n",
      "batch_idx: 12\n",
      "0.875\n",
      "batch_idx: 13\n",
      "0.875\n",
      "batch_idx: 14\n",
      "0.875\n",
      "batch_idx: 15\n",
      "0.8723958333333334\n",
      "batch_idx: 16\n",
      "0.8700980392156863\n",
      "batch_idx: 17\n",
      "0.8657407407407407\n",
      "batch_idx: 18\n",
      "0.868421052631579\n",
      "batch_idx: 19\n",
      "0.8666666666666667\n",
      "batch_idx: 20\n",
      "0.8630952380952381\n",
      "batch_idx: 21\n",
      "0.865530303030303\n",
      "batch_idx: 22\n",
      "0.8641304347826086\n",
      "batch_idx: 23\n",
      "0.8628472222222222\n",
      "batch_idx: 24\n",
      "0.86\n",
      "Training Epoch: 154, total loss: 41.523664\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.875\n",
      "batch_idx: 8\n",
      "0.8796296296296297\n",
      "batch_idx: 9\n",
      "0.8791666666666667\n",
      "batch_idx: 10\n",
      "0.8712121212121212\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.8621794871794872\n",
      "batch_idx: 13\n",
      "0.8422619047619048\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.828125\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.8223684210526315\n",
      "batch_idx: 19\n",
      "0.8270833333333333\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.8188405797101449\n",
      "batch_idx: 23\n",
      "0.8142361111111112\n",
      "batch_idx: 24\n",
      "0.8183333333333334\n",
      "Training Epoch: 155, total loss: 42.354861\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.9166666666666666\n",
      "batch_idx: 5\n",
      "0.9097222222222222\n",
      "batch_idx: 6\n",
      "0.8928571428571429\n",
      "batch_idx: 7\n",
      "0.890625\n",
      "batch_idx: 8\n",
      "0.8935185185185185\n",
      "batch_idx: 9\n",
      "0.8875\n",
      "batch_idx: 10\n",
      "0.8825757575757576\n",
      "batch_idx: 11\n",
      "0.875\n",
      "batch_idx: 12\n",
      "0.8653846153846154\n",
      "batch_idx: 13\n",
      "0.8690476190476191\n",
      "batch_idx: 14\n",
      "0.8694444444444445\n",
      "batch_idx: 15\n",
      "0.8723958333333334\n",
      "batch_idx: 16\n",
      "0.8774509803921569\n",
      "batch_idx: 17\n",
      "0.875\n",
      "batch_idx: 18\n",
      "0.8793859649122807\n",
      "batch_idx: 19\n",
      "0.875\n",
      "batch_idx: 20\n",
      "0.8630952380952381\n",
      "batch_idx: 21\n",
      "0.8579545454545454\n",
      "batch_idx: 22\n",
      "0.8568840579710145\n",
      "batch_idx: 23\n",
      "0.8541666666666666\n",
      "batch_idx: 24\n",
      "0.8466666666666667\n",
      "Training Epoch: 156, total loss: 41.831612\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.90625\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.8809523809523809\n",
      "batch_idx: 7\n",
      "0.8802083333333334\n",
      "batch_idx: 8\n",
      "0.8796296296296297\n",
      "batch_idx: 9\n",
      "0.875\n",
      "batch_idx: 10\n",
      "0.875\n",
      "batch_idx: 11\n",
      "0.8784722222222222\n",
      "batch_idx: 12\n",
      "0.8878205128205128\n",
      "batch_idx: 13\n",
      "0.8869047619047619\n",
      "batch_idx: 14\n",
      "0.8861111111111111\n",
      "batch_idx: 15\n",
      "0.8854166666666666\n",
      "batch_idx: 16\n",
      "0.8848039215686274\n",
      "batch_idx: 17\n",
      "0.8773148148148148\n",
      "batch_idx: 18\n",
      "0.8640350877192983\n",
      "batch_idx: 19\n",
      "0.8541666666666666\n",
      "batch_idx: 20\n",
      "0.8591269841269841\n",
      "batch_idx: 21\n",
      "0.8598484848484849\n",
      "batch_idx: 22\n",
      "0.8641304347826086\n",
      "batch_idx: 23\n",
      "0.8697916666666666\n",
      "batch_idx: 24\n",
      "0.8666666666666667\n",
      "Training Epoch: 157, total loss: 41.447407\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8525641025641025\n",
      "batch_idx: 13\n",
      "0.8541666666666666\n",
      "batch_idx: 14\n",
      "0.8472222222222222\n",
      "batch_idx: 15\n",
      "0.8515625\n",
      "batch_idx: 16\n",
      "0.8529411764705882\n",
      "batch_idx: 17\n",
      "0.8587962962962963\n",
      "batch_idx: 18\n",
      "0.8618421052631579\n",
      "batch_idx: 19\n",
      "0.8541666666666666\n",
      "batch_idx: 20\n",
      "0.8571428571428571\n",
      "batch_idx: 21\n",
      "0.8579545454545454\n",
      "batch_idx: 22\n",
      "0.8532608695652174\n",
      "batch_idx: 23\n",
      "0.8524305555555556\n",
      "batch_idx: 24\n",
      "0.8533333333333334\n",
      "Training Epoch: 158, total loss: 41.608685\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8703703703703703\n",
      "batch_idx: 9\n",
      "0.8708333333333333\n",
      "batch_idx: 10\n",
      "0.8636363636363636\n",
      "batch_idx: 11\n",
      "0.8645833333333334\n",
      "batch_idx: 12\n",
      "0.8525641025641025\n",
      "batch_idx: 13\n",
      "0.8571428571428571\n",
      "batch_idx: 14\n",
      "0.8638888888888889\n",
      "batch_idx: 15\n",
      "0.8697916666666666\n",
      "batch_idx: 16\n",
      "0.875\n",
      "batch_idx: 17\n",
      "0.8773148148148148\n",
      "batch_idx: 18\n",
      "0.8706140350877193\n",
      "batch_idx: 19\n",
      "0.8729166666666667\n",
      "batch_idx: 20\n",
      "0.875\n",
      "batch_idx: 21\n",
      "0.8768939393939394\n",
      "batch_idx: 22\n",
      "0.8804347826086957\n",
      "batch_idx: 23\n",
      "0.8784722222222222\n",
      "batch_idx: 24\n",
      "0.8783333333333333\n",
      "Training Epoch: 159, total loss: 41.343455\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.8802083333333334\n",
      "batch_idx: 8\n",
      "0.8935185185185185\n",
      "batch_idx: 9\n",
      "0.8958333333333334\n",
      "batch_idx: 10\n",
      "0.8939393939393939\n",
      "batch_idx: 11\n",
      "0.8958333333333334\n",
      "batch_idx: 12\n",
      "0.8814102564102564\n",
      "batch_idx: 13\n",
      "0.875\n",
      "batch_idx: 14\n",
      "0.875\n",
      "batch_idx: 15\n",
      "0.8802083333333334\n",
      "batch_idx: 16\n",
      "0.8872549019607843\n",
      "batch_idx: 17\n",
      "0.8842592592592593\n",
      "batch_idx: 18\n",
      "0.8859649122807017\n",
      "batch_idx: 19\n",
      "0.8833333333333333\n",
      "batch_idx: 20\n",
      "0.8888888888888888\n",
      "batch_idx: 21\n",
      "0.8806818181818182\n",
      "batch_idx: 22\n",
      "0.8840579710144928\n",
      "batch_idx: 23\n",
      "0.8767361111111112\n",
      "batch_idx: 24\n",
      "0.8683333333333333\n",
      "Training Epoch: 160, total loss: 41.491904\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.90625\n",
      "batch_idx: 4\n",
      "0.9166666666666666\n",
      "batch_idx: 5\n",
      "0.9236111111111112\n",
      "batch_idx: 6\n",
      "0.9107142857142857\n",
      "batch_idx: 7\n",
      "0.9166666666666666\n",
      "batch_idx: 8\n",
      "0.9120370370370371\n",
      "batch_idx: 9\n",
      "0.9\n",
      "batch_idx: 10\n",
      "0.9015151515151515\n",
      "batch_idx: 11\n",
      "0.8993055555555556\n",
      "batch_idx: 12\n",
      "0.9006410256410257\n",
      "batch_idx: 13\n",
      "0.8988095238095238\n",
      "batch_idx: 14\n",
      "0.8972222222222223\n",
      "batch_idx: 15\n",
      "0.8984375\n",
      "batch_idx: 16\n",
      "0.8848039215686274\n",
      "batch_idx: 17\n",
      "0.8819444444444444\n",
      "batch_idx: 18\n",
      "0.8837719298245614\n",
      "batch_idx: 19\n",
      "0.8854166666666666\n",
      "batch_idx: 20\n",
      "0.8888888888888888\n",
      "batch_idx: 21\n",
      "0.8901515151515151\n",
      "batch_idx: 22\n",
      "0.8913043478260869\n",
      "batch_idx: 23\n",
      "0.8888888888888888\n",
      "batch_idx: 24\n",
      "0.89\n",
      "Training Epoch: 161, total loss: 41.040859\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8680555555555556\n",
      "batch_idx: 12\n",
      "0.8653846153846154\n",
      "batch_idx: 13\n",
      "0.8630952380952381\n",
      "batch_idx: 14\n",
      "0.8583333333333333\n",
      "batch_idx: 15\n",
      "0.8541666666666666\n",
      "batch_idx: 16\n",
      "0.8529411764705882\n",
      "batch_idx: 17\n",
      "0.8611111111111112\n",
      "batch_idx: 18\n",
      "0.8640350877192983\n",
      "batch_idx: 19\n",
      "0.8645833333333334\n",
      "batch_idx: 20\n",
      "0.8630952380952381\n",
      "batch_idx: 21\n",
      "0.8693181818181818\n",
      "batch_idx: 22\n",
      "0.8731884057971014\n",
      "batch_idx: 23\n",
      "0.875\n",
      "batch_idx: 24\n",
      "0.875\n",
      "Training Epoch: 162, total loss: 41.325256\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8301282051282052\n",
      "batch_idx: 13\n",
      "0.8303571428571429\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8287037037037037\n",
      "batch_idx: 18\n",
      "0.831140350877193\n",
      "batch_idx: 19\n",
      "0.8375\n",
      "batch_idx: 20\n",
      "0.8392857142857143\n",
      "batch_idx: 21\n",
      "0.8371212121212122\n",
      "batch_idx: 22\n",
      "0.8387681159420289\n",
      "batch_idx: 23\n",
      "0.8420138888888888\n",
      "batch_idx: 24\n",
      "0.8466666666666667\n",
      "Training Epoch: 163, total loss: 41.889332\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.875\n",
      "batch_idx: 9\n",
      "0.8625\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8680555555555556\n",
      "batch_idx: 12\n",
      "0.8685897435897436\n",
      "batch_idx: 13\n",
      "0.8660714285714286\n",
      "batch_idx: 14\n",
      "0.8638888888888889\n",
      "batch_idx: 15\n",
      "0.8671875\n",
      "batch_idx: 16\n",
      "0.8700980392156863\n",
      "batch_idx: 17\n",
      "0.8726851851851852\n",
      "batch_idx: 18\n",
      "0.868421052631579\n",
      "batch_idx: 19\n",
      "0.8666666666666667\n",
      "batch_idx: 20\n",
      "0.8650793650793651\n",
      "batch_idx: 21\n",
      "0.8693181818181818\n",
      "batch_idx: 22\n",
      "0.8677536231884058\n",
      "batch_idx: 23\n",
      "0.8697916666666666\n",
      "batch_idx: 24\n",
      "0.8716666666666667\n",
      "Training Epoch: 164, total loss: 41.362255\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.90625\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8303571428571429\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.8385416666666666\n",
      "batch_idx: 16\n",
      "0.8406862745098039\n",
      "batch_idx: 17\n",
      "0.8425925925925926\n",
      "batch_idx: 18\n",
      "0.8421052631578947\n",
      "batch_idx: 19\n",
      "0.83125\n",
      "batch_idx: 20\n",
      "0.8373015873015873\n",
      "batch_idx: 21\n",
      "0.8390151515151515\n",
      "batch_idx: 22\n",
      "0.842391304347826\n",
      "batch_idx: 23\n",
      "0.8472222222222222\n",
      "batch_idx: 24\n",
      "0.8466666666666667\n",
      "Training Epoch: 165, total loss: 41.716554\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.9375\n",
      "batch_idx: 2\n",
      "0.9444444444444444\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8916666666666667\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8796296296296297\n",
      "batch_idx: 9\n",
      "0.8833333333333333\n",
      "batch_idx: 10\n",
      "0.8712121212121212\n",
      "batch_idx: 11\n",
      "0.8645833333333334\n",
      "batch_idx: 12\n",
      "0.8653846153846154\n",
      "batch_idx: 13\n",
      "0.8660714285714286\n",
      "batch_idx: 14\n",
      "0.8611111111111112\n",
      "batch_idx: 15\n",
      "0.8697916666666666\n",
      "batch_idx: 16\n",
      "0.8700980392156863\n",
      "batch_idx: 17\n",
      "0.8657407407407407\n",
      "batch_idx: 18\n",
      "0.8662280701754386\n",
      "batch_idx: 19\n",
      "0.8708333333333333\n",
      "batch_idx: 20\n",
      "0.871031746031746\n",
      "batch_idx: 21\n",
      "0.865530303030303\n",
      "batch_idx: 22\n",
      "0.8659420289855072\n",
      "batch_idx: 23\n",
      "0.8663194444444444\n",
      "batch_idx: 24\n",
      "0.8666666666666667\n",
      "Training Epoch: 166, total loss: 41.511450\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.9027777777777778\n",
      "batch_idx: 6\n",
      "0.8869047619047619\n",
      "batch_idx: 7\n",
      "0.8854166666666666\n",
      "batch_idx: 8\n",
      "0.8935185185185185\n",
      "batch_idx: 9\n",
      "0.8958333333333334\n",
      "batch_idx: 10\n",
      "0.8939393939393939\n",
      "batch_idx: 11\n",
      "0.8819444444444444\n",
      "batch_idx: 12\n",
      "0.8717948717948718\n",
      "batch_idx: 13\n",
      "0.8690476190476191\n",
      "batch_idx: 14\n",
      "0.8666666666666667\n",
      "batch_idx: 15\n",
      "0.8697916666666666\n",
      "batch_idx: 16\n",
      "0.8676470588235294\n",
      "batch_idx: 17\n",
      "0.8657407407407407\n",
      "batch_idx: 18\n",
      "0.8618421052631579\n",
      "batch_idx: 19\n",
      "0.8645833333333334\n",
      "batch_idx: 20\n",
      "0.8650793650793651\n",
      "batch_idx: 21\n",
      "0.8617424242424242\n",
      "batch_idx: 22\n",
      "0.8568840579710145\n",
      "batch_idx: 23\n",
      "0.8576388888888888\n",
      "batch_idx: 24\n",
      "0.8566666666666667\n",
      "Training Epoch: 167, total loss: 41.530717\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8564814814814815\n",
      "batch_idx: 9\n",
      "0.8625\n",
      "batch_idx: 10\n",
      "0.8636363636363636\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.8621794871794872\n",
      "batch_idx: 13\n",
      "0.8541666666666666\n",
      "batch_idx: 14\n",
      "0.8527777777777777\n",
      "batch_idx: 15\n",
      "0.8541666666666666\n",
      "batch_idx: 16\n",
      "0.8578431372549019\n",
      "batch_idx: 17\n",
      "0.8634259259259259\n",
      "batch_idx: 18\n",
      "0.8662280701754386\n",
      "batch_idx: 19\n",
      "0.8666666666666667\n",
      "batch_idx: 20\n",
      "0.871031746031746\n",
      "batch_idx: 21\n",
      "0.875\n",
      "batch_idx: 22\n",
      "0.8731884057971014\n",
      "batch_idx: 23\n",
      "0.8680555555555556\n",
      "batch_idx: 24\n",
      "0.87\n",
      "Training Epoch: 168, total loss: 41.404230\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8854166666666666\n",
      "batch_idx: 8\n",
      "0.8935185185185185\n",
      "batch_idx: 9\n",
      "0.8958333333333334\n",
      "batch_idx: 10\n",
      "0.8977272727272727\n",
      "batch_idx: 11\n",
      "0.9027777777777778\n",
      "batch_idx: 12\n",
      "0.8910256410256411\n",
      "batch_idx: 13\n",
      "0.8928571428571429\n",
      "batch_idx: 14\n",
      "0.8916666666666667\n",
      "batch_idx: 15\n",
      "0.8828125\n",
      "batch_idx: 16\n",
      "0.8774509803921569\n",
      "batch_idx: 17\n",
      "0.875\n",
      "batch_idx: 18\n",
      "0.8793859649122807\n",
      "batch_idx: 19\n",
      "0.8854166666666666\n",
      "batch_idx: 20\n",
      "0.8869047619047619\n",
      "batch_idx: 21\n",
      "0.8901515151515151\n",
      "batch_idx: 22\n",
      "0.8876811594202898\n",
      "batch_idx: 23\n",
      "0.890625\n",
      "batch_idx: 24\n",
      "0.8866666666666667\n",
      "Training Epoch: 169, total loss: 41.013567\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9270833333333334\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8625\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8645833333333334\n",
      "batch_idx: 12\n",
      "0.8685897435897436\n",
      "batch_idx: 13\n",
      "0.875\n",
      "batch_idx: 14\n",
      "0.875\n",
      "batch_idx: 15\n",
      "0.8697916666666666\n",
      "batch_idx: 16\n",
      "0.8651960784313726\n",
      "batch_idx: 17\n",
      "0.8634259259259259\n",
      "batch_idx: 18\n",
      "0.8618421052631579\n",
      "batch_idx: 19\n",
      "0.8583333333333333\n",
      "batch_idx: 20\n",
      "0.8531746031746031\n",
      "batch_idx: 21\n",
      "0.8560606060606061\n",
      "batch_idx: 22\n",
      "0.8586956521739131\n",
      "batch_idx: 23\n",
      "0.8524305555555556\n",
      "batch_idx: 24\n",
      "0.8483333333333334\n",
      "Training Epoch: 170, total loss: 41.707006\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8703703703703703\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8674242424242424\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.8621794871794872\n",
      "batch_idx: 13\n",
      "0.8660714285714286\n",
      "batch_idx: 14\n",
      "0.8611111111111112\n",
      "batch_idx: 15\n",
      "0.8567708333333334\n",
      "batch_idx: 16\n",
      "0.8627450980392157\n",
      "batch_idx: 17\n",
      "0.8657407407407407\n",
      "batch_idx: 18\n",
      "0.8552631578947368\n",
      "batch_idx: 19\n",
      "0.85625\n",
      "batch_idx: 20\n",
      "0.8571428571428571\n",
      "batch_idx: 21\n",
      "0.8636363636363636\n",
      "batch_idx: 22\n",
      "0.8641304347826086\n",
      "batch_idx: 23\n",
      "0.8611111111111112\n",
      "batch_idx: 24\n",
      "0.8616666666666667\n",
      "Training Epoch: 171, total loss: 41.381615\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8657407407407407\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8645833333333334\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8482142857142857\n",
      "batch_idx: 14\n",
      "0.8555555555555555\n",
      "batch_idx: 15\n",
      "0.8619791666666666\n",
      "batch_idx: 16\n",
      "0.8651960784313726\n",
      "batch_idx: 17\n",
      "0.8564814814814815\n",
      "batch_idx: 18\n",
      "0.8596491228070176\n",
      "batch_idx: 19\n",
      "0.8604166666666667\n",
      "batch_idx: 20\n",
      "0.8611111111111112\n",
      "batch_idx: 21\n",
      "0.865530303030303\n",
      "batch_idx: 22\n",
      "0.8623188405797102\n",
      "batch_idx: 23\n",
      "0.8576388888888888\n",
      "batch_idx: 24\n",
      "0.8583333333333333\n",
      "Training Epoch: 172, total loss: 41.684879\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8674242424242424\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.8557692307692307\n",
      "batch_idx: 13\n",
      "0.8571428571428571\n",
      "batch_idx: 14\n",
      "0.8638888888888889\n",
      "batch_idx: 15\n",
      "0.8671875\n",
      "batch_idx: 16\n",
      "0.8651960784313726\n",
      "batch_idx: 17\n",
      "0.8587962962962963\n",
      "batch_idx: 18\n",
      "0.8486842105263158\n",
      "batch_idx: 19\n",
      "0.8458333333333333\n",
      "batch_idx: 20\n",
      "0.8472222222222222\n",
      "batch_idx: 21\n",
      "0.8484848484848485\n",
      "batch_idx: 22\n",
      "0.8442028985507246\n",
      "batch_idx: 23\n",
      "0.8472222222222222\n",
      "batch_idx: 24\n",
      "0.85\n",
      "Training Epoch: 173, total loss: 41.743110\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9166666666666666\n",
      "batch_idx: 4\n",
      "0.9166666666666666\n",
      "batch_idx: 5\n",
      "0.9236111111111112\n",
      "batch_idx: 6\n",
      "0.9285714285714286\n",
      "batch_idx: 7\n",
      "0.9270833333333334\n",
      "batch_idx: 8\n",
      "0.9259259259259259\n",
      "batch_idx: 9\n",
      "0.925\n",
      "batch_idx: 10\n",
      "0.928030303030303\n",
      "batch_idx: 11\n",
      "0.9201388888888888\n",
      "batch_idx: 12\n",
      "0.9134615384615384\n",
      "batch_idx: 13\n",
      "0.9047619047619048\n",
      "batch_idx: 14\n",
      "0.9027777777777778\n",
      "batch_idx: 15\n",
      "0.8984375\n",
      "batch_idx: 16\n",
      "0.8995098039215687\n",
      "batch_idx: 17\n",
      "0.8958333333333334\n",
      "batch_idx: 18\n",
      "0.8903508771929824\n",
      "batch_idx: 19\n",
      "0.8875\n",
      "batch_idx: 20\n",
      "0.8849206349206349\n",
      "batch_idx: 21\n",
      "0.884469696969697\n",
      "batch_idx: 22\n",
      "0.8822463768115942\n",
      "batch_idx: 23\n",
      "0.8802083333333334\n",
      "batch_idx: 24\n",
      "0.8816666666666667\n",
      "Training Epoch: 174, total loss: 41.086109\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.8645833333333334\n",
      "batch_idx: 12\n",
      "0.8653846153846154\n",
      "batch_idx: 13\n",
      "0.8630952380952381\n",
      "batch_idx: 14\n",
      "0.8638888888888889\n",
      "batch_idx: 15\n",
      "0.859375\n",
      "batch_idx: 16\n",
      "0.8627450980392157\n",
      "batch_idx: 17\n",
      "0.8657407407407407\n",
      "batch_idx: 18\n",
      "0.8662280701754386\n",
      "batch_idx: 19\n",
      "0.85625\n",
      "batch_idx: 20\n",
      "0.8611111111111112\n",
      "batch_idx: 21\n",
      "0.8579545454545454\n",
      "batch_idx: 22\n",
      "0.8605072463768116\n",
      "batch_idx: 23\n",
      "0.8628472222222222\n",
      "batch_idx: 24\n",
      "0.865\n",
      "Training Epoch: 175, total loss: 41.468675\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.9305555555555556\n",
      "batch_idx: 3\n",
      "0.9166666666666666\n",
      "batch_idx: 4\n",
      "0.9166666666666666\n",
      "batch_idx: 5\n",
      "0.9097222222222222\n",
      "batch_idx: 6\n",
      "0.8809523809523809\n",
      "batch_idx: 7\n",
      "0.8958333333333334\n",
      "batch_idx: 8\n",
      "0.8935185185185185\n",
      "batch_idx: 9\n",
      "0.8833333333333333\n",
      "batch_idx: 10\n",
      "0.8863636363636364\n",
      "batch_idx: 11\n",
      "0.8888888888888888\n",
      "batch_idx: 12\n",
      "0.8942307692307693\n",
      "batch_idx: 13\n",
      "0.8779761904761905\n",
      "batch_idx: 14\n",
      "0.8833333333333333\n",
      "batch_idx: 15\n",
      "0.8776041666666666\n",
      "batch_idx: 16\n",
      "0.8725490196078431\n",
      "batch_idx: 17\n",
      "0.8773148148148148\n",
      "batch_idx: 18\n",
      "0.8771929824561403\n",
      "batch_idx: 19\n",
      "0.875\n",
      "batch_idx: 20\n",
      "0.871031746031746\n",
      "batch_idx: 21\n",
      "0.8617424242424242\n",
      "batch_idx: 22\n",
      "0.8659420289855072\n",
      "batch_idx: 23\n",
      "0.8697916666666666\n",
      "batch_idx: 24\n",
      "0.87\n",
      "Training Epoch: 176, total loss: 41.521450\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9583333333333334\n",
      "batch_idx: 2\n",
      "0.9305555555555556\n",
      "batch_idx: 3\n",
      "0.9270833333333334\n",
      "batch_idx: 4\n",
      "0.9333333333333333\n",
      "batch_idx: 5\n",
      "0.9236111111111112\n",
      "batch_idx: 6\n",
      "0.9166666666666666\n",
      "batch_idx: 7\n",
      "0.90625\n",
      "batch_idx: 8\n",
      "0.8703703703703703\n",
      "batch_idx: 9\n",
      "0.875\n",
      "batch_idx: 10\n",
      "0.8787878787878788\n",
      "batch_idx: 11\n",
      "0.8784722222222222\n",
      "batch_idx: 12\n",
      "0.8782051282051282\n",
      "batch_idx: 13\n",
      "0.8809523809523809\n",
      "batch_idx: 14\n",
      "0.8777777777777778\n",
      "batch_idx: 15\n",
      "0.8854166666666666\n",
      "batch_idx: 16\n",
      "0.8872549019607843\n",
      "batch_idx: 17\n",
      "0.8935185185185185\n",
      "batch_idx: 18\n",
      "0.8903508771929824\n",
      "batch_idx: 19\n",
      "0.8916666666666667\n",
      "batch_idx: 20\n",
      "0.8869047619047619\n",
      "batch_idx: 21\n",
      "0.884469696969697\n",
      "batch_idx: 22\n",
      "0.8695652173913043\n",
      "batch_idx: 23\n",
      "0.8680555555555556\n",
      "batch_idx: 24\n",
      "0.87\n",
      "Training Epoch: 177, total loss: 41.317897\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.8557692307692307\n",
      "batch_idx: 13\n",
      "0.8601190476190477\n",
      "batch_idx: 14\n",
      "0.8611111111111112\n",
      "batch_idx: 15\n",
      "0.8567708333333334\n",
      "batch_idx: 16\n",
      "0.8529411764705882\n",
      "batch_idx: 17\n",
      "0.8587962962962963\n",
      "batch_idx: 18\n",
      "0.8640350877192983\n",
      "batch_idx: 19\n",
      "0.8645833333333334\n",
      "batch_idx: 20\n",
      "0.8630952380952381\n",
      "batch_idx: 21\n",
      "0.8617424242424242\n",
      "batch_idx: 22\n",
      "0.8659420289855072\n",
      "batch_idx: 23\n",
      "0.8697916666666666\n",
      "batch_idx: 24\n",
      "0.87\n",
      "Training Epoch: 178, total loss: 41.374071\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8564814814814815\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8715277777777778\n",
      "batch_idx: 12\n",
      "0.875\n",
      "batch_idx: 13\n",
      "0.8779761904761905\n",
      "batch_idx: 14\n",
      "0.8722222222222222\n",
      "batch_idx: 15\n",
      "0.8645833333333334\n",
      "batch_idx: 16\n",
      "0.8676470588235294\n",
      "batch_idx: 17\n",
      "0.8680555555555556\n",
      "batch_idx: 18\n",
      "0.868421052631579\n",
      "batch_idx: 19\n",
      "0.8708333333333333\n",
      "batch_idx: 20\n",
      "0.871031746031746\n",
      "batch_idx: 21\n",
      "0.8693181818181818\n",
      "batch_idx: 22\n",
      "0.8677536231884058\n",
      "batch_idx: 23\n",
      "0.8663194444444444\n",
      "batch_idx: 24\n",
      "0.8716666666666667\n",
      "Training Epoch: 179, total loss: 41.264649\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.8869047619047619\n",
      "batch_idx: 7\n",
      "0.890625\n",
      "batch_idx: 8\n",
      "0.8935185185185185\n",
      "batch_idx: 9\n",
      "0.8916666666666667\n",
      "batch_idx: 10\n",
      "0.875\n",
      "batch_idx: 11\n",
      "0.8854166666666666\n",
      "batch_idx: 12\n",
      "0.8878205128205128\n",
      "batch_idx: 13\n",
      "0.8869047619047619\n",
      "batch_idx: 14\n",
      "0.8833333333333333\n",
      "batch_idx: 15\n",
      "0.8880208333333334\n",
      "batch_idx: 16\n",
      "0.8799019607843137\n",
      "batch_idx: 17\n",
      "0.8680555555555556\n",
      "batch_idx: 18\n",
      "0.8728070175438597\n",
      "batch_idx: 19\n",
      "0.8770833333333333\n",
      "batch_idx: 20\n",
      "0.876984126984127\n",
      "batch_idx: 21\n",
      "0.8806818181818182\n",
      "batch_idx: 22\n",
      "0.8713768115942029\n",
      "batch_idx: 23\n",
      "0.8697916666666666\n",
      "batch_idx: 24\n",
      "0.8733333333333333\n",
      "Training Epoch: 180, total loss: 41.414212\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8708333333333333\n",
      "batch_idx: 10\n",
      "0.8787878787878788\n",
      "batch_idx: 11\n",
      "0.8888888888888888\n",
      "batch_idx: 12\n",
      "0.8910256410256411\n",
      "batch_idx: 13\n",
      "0.8928571428571429\n",
      "batch_idx: 14\n",
      "0.8972222222222223\n",
      "batch_idx: 15\n",
      "0.8854166666666666\n",
      "batch_idx: 16\n",
      "0.8774509803921569\n",
      "batch_idx: 17\n",
      "0.8726851851851852\n",
      "batch_idx: 18\n",
      "0.8574561403508771\n",
      "batch_idx: 19\n",
      "0.8541666666666666\n",
      "batch_idx: 20\n",
      "0.8571428571428571\n",
      "batch_idx: 21\n",
      "0.8541666666666666\n",
      "batch_idx: 22\n",
      "0.8514492753623188\n",
      "batch_idx: 23\n",
      "0.8506944444444444\n",
      "batch_idx: 24\n",
      "0.845\n",
      "Training Epoch: 181, total loss: 41.802533\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.8928571428571429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 7\n",
      "0.8958333333333334\n",
      "batch_idx: 8\n",
      "0.8796296296296297\n",
      "batch_idx: 9\n",
      "0.8833333333333333\n",
      "batch_idx: 10\n",
      "0.8863636363636364\n",
      "batch_idx: 11\n",
      "0.8819444444444444\n",
      "batch_idx: 12\n",
      "0.875\n",
      "batch_idx: 13\n",
      "0.8809523809523809\n",
      "batch_idx: 14\n",
      "0.8694444444444445\n",
      "batch_idx: 15\n",
      "0.8619791666666666\n",
      "batch_idx: 16\n",
      "0.8627450980392157\n",
      "batch_idx: 17\n",
      "0.8634259259259259\n",
      "batch_idx: 18\n",
      "0.8574561403508771\n",
      "batch_idx: 19\n",
      "0.8604166666666667\n",
      "batch_idx: 20\n",
      "0.8650793650793651\n",
      "batch_idx: 21\n",
      "0.8693181818181818\n",
      "batch_idx: 22\n",
      "0.8695652173913043\n",
      "batch_idx: 23\n",
      "0.8732638888888888\n",
      "batch_idx: 24\n",
      "0.8716666666666667\n",
      "Training Epoch: 182, total loss: 41.367519\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-082b1b5f79b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnfm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNFM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnfm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#加了device防止出现GPU CPU两种设备的错误提示\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nfm:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnfm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnfm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnfm_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'dataset/xiaoguan/RF/RF_for_train/train_class_9/model/2_gene_4000_NFM.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-165f23e5dbad>\u001b[0m in \u001b[0;36mtrain_data\u001b[0;34m(model, data_loader, batch_size, model_path)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0;31m#print(\"y_predict:\",y_predict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m#loss = loss_func(y_predict.view(-1), labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NFM-pyorch-master/NFM-pyorch-master/new_nfm_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m#linear_output=self.BN_linear(linear_output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# 求出稀疏特征的embedding向量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0msparse_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0msparse_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NFM-pyorch-master/NFM-pyorch-master/new_nfm_network.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m#linear_output=self.BN_linear(linear_output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# 求出稀疏特征的embedding向量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0msparse_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0msparse_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset,train_loader=prepare_dataset(nfm_config['train_data'],nfm_config['train_label'],nfm_config['batch_size'],nfm_config['n_class'])\n",
    "nfm = NFM(nfm_config).cuda()#加了device防止出现GPU CPU两种设备的错误提示\n",
    "print(\"nfm:\",nfm)\n",
    "train_data(nfm,train_loader,nfm_config['batch_size'],'dataset/xiaoguan/RF/RF_for_train/train_class_9/model/2_gene_4000_NFM.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb0b833",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(nfm.state_dict(),'dataset/xiaoguan/RF/RF_for_train/train_class_9/model/2_gene_4000_NFM.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e20eacd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97958780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8322e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "####MLPmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8f4bd747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP: MLP(\n",
      "  (fc1): Linear(in_features=4225, out_features=1000, bias=True)\n",
      "  (bn1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=100, out_features=9, bias=True)\n",
      "  (bn3): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.08333333333333333\n",
      "batch_idx: 1\n",
      "0.14583333333333334\n",
      "batch_idx: 2\n",
      "0.16666666666666666\n",
      "batch_idx: 3\n",
      "0.14583333333333334\n",
      "batch_idx: 4\n",
      "0.14166666666666666\n",
      "batch_idx: 5\n",
      "0.1736111111111111\n",
      "batch_idx: 6\n",
      "0.17261904761904762\n",
      "batch_idx: 7\n",
      "0.18229166666666666\n",
      "batch_idx: 8\n",
      "0.18981481481481483\n",
      "batch_idx: 9\n",
      "0.1875\n",
      "batch_idx: 10\n",
      "0.1856060606060606\n",
      "batch_idx: 11\n",
      "0.19791666666666666\n",
      "batch_idx: 12\n",
      "0.19230769230769232\n",
      "batch_idx: 13\n",
      "0.20238095238095238\n",
      "batch_idx: 14\n",
      "0.20833333333333334\n",
      "batch_idx: 15\n",
      "0.21875\n",
      "batch_idx: 16\n",
      "0.23039215686274508\n",
      "batch_idx: 17\n",
      "0.23148148148148148\n",
      "batch_idx: 18\n",
      "0.23903508771929824\n",
      "batch_idx: 19\n",
      "0.24166666666666667\n",
      "batch_idx: 20\n",
      "0.24206349206349206\n",
      "batch_idx: 21\n",
      "0.24242424242424243\n",
      "batch_idx: 22\n",
      "0.24818840579710144\n",
      "batch_idx: 23\n",
      "0.2465277777777778\n",
      "batch_idx: 24\n",
      "0.24166666666666667\n",
      "Training Epoch: 0, total loss: 54.049360\n",
      "batch_idx: 0\n",
      "0.2916666666666667\n",
      "batch_idx: 1\n",
      "0.2916666666666667\n",
      "batch_idx: 2\n",
      "0.3055555555555556\n",
      "batch_idx: 3\n",
      "0.34375\n",
      "batch_idx: 4\n",
      "0.3416666666666667\n",
      "batch_idx: 5\n",
      "0.3611111111111111\n",
      "batch_idx: 6\n",
      "0.3392857142857143\n",
      "batch_idx: 7\n",
      "0.3385416666666667\n",
      "batch_idx: 8\n",
      "0.3425925925925926\n",
      "batch_idx: 9\n",
      "0.3416666666666667\n",
      "batch_idx: 10\n",
      "0.3484848484848485\n",
      "batch_idx: 11\n",
      "0.3368055555555556\n",
      "batch_idx: 12\n",
      "0.3301282051282051\n",
      "batch_idx: 13\n",
      "0.3273809523809524\n",
      "batch_idx: 14\n",
      "0.32222222222222224\n",
      "batch_idx: 15\n",
      "0.3307291666666667\n",
      "batch_idx: 16\n",
      "0.32598039215686275\n",
      "batch_idx: 17\n",
      "0.32407407407407407\n",
      "batch_idx: 18\n",
      "0.3223684210526316\n",
      "batch_idx: 19\n",
      "0.3145833333333333\n",
      "batch_idx: 20\n",
      "0.3115079365079365\n",
      "batch_idx: 21\n",
      "0.3106060606060606\n",
      "batch_idx: 22\n",
      "0.31521739130434784\n",
      "batch_idx: 23\n",
      "0.3177083333333333\n",
      "batch_idx: 24\n",
      "0.32166666666666666\n",
      "Training Epoch: 1, total loss: 53.104415\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.5625\n",
      "batch_idx: 2\n",
      "0.5\n",
      "batch_idx: 3\n",
      "0.4583333333333333\n",
      "batch_idx: 4\n",
      "0.4666666666666667\n",
      "batch_idx: 5\n",
      "0.4375\n",
      "batch_idx: 6\n",
      "0.43452380952380953\n",
      "batch_idx: 7\n",
      "0.4270833333333333\n",
      "batch_idx: 8\n",
      "0.41203703703703703\n",
      "batch_idx: 9\n",
      "0.4125\n",
      "batch_idx: 10\n",
      "0.4090909090909091\n",
      "batch_idx: 11\n",
      "0.3993055555555556\n",
      "batch_idx: 12\n",
      "0.40064102564102566\n",
      "batch_idx: 13\n",
      "0.4107142857142857\n",
      "batch_idx: 14\n",
      "0.4166666666666667\n",
      "batch_idx: 15\n",
      "0.4166666666666667\n",
      "batch_idx: 16\n",
      "0.42401960784313725\n",
      "batch_idx: 17\n",
      "0.4212962962962963\n",
      "batch_idx: 18\n",
      "0.41228070175438597\n",
      "batch_idx: 19\n",
      "0.41458333333333336\n",
      "batch_idx: 20\n",
      "0.4166666666666667\n",
      "batch_idx: 21\n",
      "0.42045454545454547\n",
      "batch_idx: 22\n",
      "0.4221014492753623\n",
      "batch_idx: 23\n",
      "0.4236111111111111\n",
      "batch_idx: 24\n",
      "0.42333333333333334\n",
      "Training Epoch: 2, total loss: 52.256824\n",
      "batch_idx: 0\n",
      "0.375\n",
      "batch_idx: 1\n",
      "0.4791666666666667\n",
      "batch_idx: 2\n",
      "0.4583333333333333\n",
      "batch_idx: 3\n",
      "0.4895833333333333\n",
      "batch_idx: 4\n",
      "0.475\n",
      "batch_idx: 5\n",
      "0.4791666666666667\n",
      "batch_idx: 6\n",
      "0.4583333333333333\n",
      "batch_idx: 7\n",
      "0.4791666666666667\n",
      "batch_idx: 8\n",
      "0.4722222222222222\n",
      "batch_idx: 9\n",
      "0.4583333333333333\n",
      "batch_idx: 10\n",
      "0.45075757575757575\n",
      "batch_idx: 11\n",
      "0.4548611111111111\n",
      "batch_idx: 12\n",
      "0.4551282051282051\n",
      "batch_idx: 13\n",
      "0.4583333333333333\n",
      "batch_idx: 14\n",
      "0.4527777777777778\n",
      "batch_idx: 15\n",
      "0.4505208333333333\n",
      "batch_idx: 16\n",
      "0.46078431372549017\n",
      "batch_idx: 17\n",
      "0.46296296296296297\n",
      "batch_idx: 18\n",
      "0.46710526315789475\n",
      "batch_idx: 19\n",
      "0.4583333333333333\n",
      "batch_idx: 20\n",
      "0.45436507936507936\n",
      "batch_idx: 21\n",
      "0.45075757575757575\n",
      "batch_idx: 22\n",
      "0.447463768115942\n",
      "batch_idx: 23\n",
      "0.4392361111111111\n",
      "batch_idx: 24\n",
      "0.44666666666666666\n",
      "Training Epoch: 3, total loss: 51.793534\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.5625\n",
      "batch_idx: 2\n",
      "0.4583333333333333\n",
      "batch_idx: 3\n",
      "0.4791666666666667\n",
      "batch_idx: 4\n",
      "0.43333333333333335\n",
      "batch_idx: 5\n",
      "0.4583333333333333\n",
      "batch_idx: 6\n",
      "0.47023809523809523\n",
      "batch_idx: 7\n",
      "0.484375\n",
      "batch_idx: 8\n",
      "0.49074074074074076\n",
      "batch_idx: 9\n",
      "0.5\n",
      "batch_idx: 10\n",
      "0.5037878787878788\n",
      "batch_idx: 11\n",
      "0.5104166666666666\n",
      "batch_idx: 12\n",
      "0.49038461538461536\n",
      "batch_idx: 13\n",
      "0.4851190476190476\n",
      "batch_idx: 14\n",
      "0.48333333333333334\n",
      "batch_idx: 15\n",
      "0.4791666666666667\n",
      "batch_idx: 16\n",
      "0.4803921568627451\n",
      "batch_idx: 17\n",
      "0.48148148148148145\n",
      "batch_idx: 18\n",
      "0.48464912280701755\n",
      "batch_idx: 19\n",
      "0.5\n",
      "batch_idx: 20\n",
      "0.503968253968254\n",
      "batch_idx: 21\n",
      "0.5132575757575758\n",
      "batch_idx: 22\n",
      "0.5126811594202898\n",
      "batch_idx: 23\n",
      "0.5138888888888888\n",
      "batch_idx: 24\n",
      "0.5083333333333333\n",
      "Training Epoch: 4, total loss: 51.063769\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.5694444444444444\n",
      "batch_idx: 3\n",
      "0.5729166666666666\n",
      "batch_idx: 4\n",
      "0.5833333333333334\n",
      "batch_idx: 5\n",
      "0.5763888888888888\n",
      "batch_idx: 6\n",
      "0.5714285714285714\n",
      "batch_idx: 7\n",
      "0.5520833333333334\n",
      "batch_idx: 8\n",
      "0.5601851851851852\n",
      "batch_idx: 9\n",
      "0.5541666666666667\n",
      "batch_idx: 10\n",
      "0.553030303030303\n",
      "batch_idx: 11\n",
      "0.5555555555555556\n",
      "batch_idx: 12\n",
      "0.5608974358974359\n",
      "batch_idx: 13\n",
      "0.5625\n",
      "batch_idx: 14\n",
      "0.5638888888888889\n",
      "batch_idx: 15\n",
      "0.5546875\n",
      "batch_idx: 16\n",
      "0.553921568627451\n",
      "batch_idx: 17\n",
      "0.5462962962962963\n",
      "batch_idx: 18\n",
      "0.5394736842105263\n",
      "batch_idx: 19\n",
      "0.5375\n",
      "batch_idx: 20\n",
      "0.5436507936507936\n",
      "batch_idx: 21\n",
      "0.5435606060606061\n",
      "batch_idx: 22\n",
      "0.5434782608695652\n",
      "batch_idx: 23\n",
      "0.5364583333333334\n",
      "batch_idx: 24\n",
      "0.53\n",
      "Training Epoch: 5, total loss: 50.441350\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.5555555555555556\n",
      "batch_idx: 3\n",
      "0.5416666666666666\n",
      "batch_idx: 4\n",
      "0.5666666666666667\n",
      "batch_idx: 5\n",
      "0.5902777777777778\n",
      "batch_idx: 6\n",
      "0.5892857142857143\n",
      "batch_idx: 7\n",
      "0.609375\n",
      "batch_idx: 8\n",
      "0.6157407407407407\n",
      "batch_idx: 9\n",
      "0.6083333333333333\n",
      "batch_idx: 10\n",
      "0.6060606060606061\n",
      "batch_idx: 11\n",
      "0.6145833333333334\n",
      "batch_idx: 12\n",
      "0.6121794871794872\n",
      "batch_idx: 13\n",
      "0.6041666666666666\n",
      "batch_idx: 14\n",
      "0.5972222222222222\n",
      "batch_idx: 15\n",
      "0.5989583333333334\n",
      "batch_idx: 16\n",
      "0.5980392156862745\n",
      "batch_idx: 17\n",
      "0.5972222222222222\n",
      "batch_idx: 18\n",
      "0.5921052631578947\n",
      "batch_idx: 19\n",
      "0.5875\n",
      "batch_idx: 20\n",
      "0.5873015873015873\n",
      "batch_idx: 21\n",
      "0.5700757575757576\n",
      "batch_idx: 22\n",
      "0.5742753623188406\n",
      "batch_idx: 23\n",
      "0.578125\n",
      "batch_idx: 24\n",
      "0.5783333333333334\n",
      "Training Epoch: 6, total loss: 50.313615\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.6833333333333333\n",
      "batch_idx: 5\n",
      "0.6388888888888888\n",
      "batch_idx: 6\n",
      "0.6130952380952381\n",
      "batch_idx: 7\n",
      "0.5833333333333334\n",
      "batch_idx: 8\n",
      "0.5648148148148148\n",
      "batch_idx: 9\n",
      "0.5541666666666667\n",
      "batch_idx: 10\n",
      "0.5681818181818182\n",
      "batch_idx: 11\n",
      "0.5694444444444444\n",
      "batch_idx: 12\n",
      "0.5673076923076923\n",
      "batch_idx: 13\n",
      "0.5714285714285714\n",
      "batch_idx: 14\n",
      "0.575\n",
      "batch_idx: 15\n",
      "0.5807291666666666\n",
      "batch_idx: 16\n",
      "0.5808823529411765\n",
      "batch_idx: 17\n",
      "0.5810185185185185\n",
      "batch_idx: 18\n",
      "0.581140350877193\n",
      "batch_idx: 19\n",
      "0.5791666666666667\n",
      "batch_idx: 20\n",
      "0.5694444444444444\n",
      "batch_idx: 21\n",
      "0.5681818181818182\n",
      "batch_idx: 22\n",
      "0.572463768115942\n",
      "batch_idx: 23\n",
      "0.5746527777777778\n",
      "batch_idx: 24\n",
      "0.58\n",
      "Training Epoch: 7, total loss: 49.998692\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.6770833333333334\n",
      "batch_idx: 4\n",
      "0.6583333333333333\n",
      "batch_idx: 5\n",
      "0.6388888888888888\n",
      "batch_idx: 6\n",
      "0.6428571428571429\n",
      "batch_idx: 7\n",
      "0.6302083333333334\n",
      "batch_idx: 8\n",
      "0.6435185185185185\n",
      "batch_idx: 9\n",
      "0.6208333333333333\n",
      "batch_idx: 10\n",
      "0.6136363636363636\n",
      "batch_idx: 11\n",
      "0.6180555555555556\n",
      "batch_idx: 12\n",
      "0.6121794871794872\n",
      "batch_idx: 13\n",
      "0.6041666666666666\n",
      "batch_idx: 14\n",
      "0.5972222222222222\n",
      "batch_idx: 15\n",
      "0.6067708333333334\n",
      "batch_idx: 16\n",
      "0.6053921568627451\n",
      "batch_idx: 17\n",
      "0.6064814814814815\n",
      "batch_idx: 18\n",
      "0.6008771929824561\n",
      "batch_idx: 19\n",
      "0.6083333333333333\n",
      "batch_idx: 20\n",
      "0.6071428571428571\n",
      "batch_idx: 21\n",
      "0.6079545454545454\n",
      "batch_idx: 22\n",
      "0.605072463768116\n",
      "batch_idx: 23\n",
      "0.59375\n",
      "batch_idx: 24\n",
      "0.5966666666666667\n",
      "Training Epoch: 8, total loss: 49.649088\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.5972222222222222\n",
      "batch_idx: 3\n",
      "0.5833333333333334\n",
      "batch_idx: 4\n",
      "0.6\n",
      "batch_idx: 5\n",
      "0.5763888888888888\n",
      "batch_idx: 6\n",
      "0.5952380952380952\n",
      "batch_idx: 7\n",
      "0.59375\n",
      "batch_idx: 8\n",
      "0.6111111111111112\n",
      "batch_idx: 9\n",
      "0.6166666666666667\n",
      "batch_idx: 10\n",
      "0.625\n",
      "batch_idx: 11\n",
      "0.6180555555555556\n",
      "batch_idx: 12\n",
      "0.6153846153846154\n",
      "batch_idx: 13\n",
      "0.6011904761904762\n",
      "batch_idx: 14\n",
      "0.5972222222222222\n",
      "batch_idx: 15\n",
      "0.5963541666666666\n",
      "batch_idx: 16\n",
      "0.6004901960784313\n",
      "batch_idx: 17\n",
      "0.6041666666666666\n",
      "batch_idx: 18\n",
      "0.6140350877192983\n",
      "batch_idx: 19\n",
      "0.6125\n",
      "batch_idx: 20\n",
      "0.6130952380952381\n",
      "batch_idx: 21\n",
      "0.615530303030303\n",
      "batch_idx: 22\n",
      "0.6123188405797102\n",
      "batch_idx: 23\n",
      "0.6197916666666666\n",
      "batch_idx: 24\n",
      "0.62\n",
      "Training Epoch: 9, total loss: 49.376554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.6388888888888888\n",
      "batch_idx: 3\n",
      "0.6458333333333334\n",
      "batch_idx: 4\n",
      "0.6583333333333333\n",
      "batch_idx: 5\n",
      "0.625\n",
      "batch_idx: 6\n",
      "0.6309523809523809\n",
      "batch_idx: 7\n",
      "0.6354166666666666\n",
      "batch_idx: 8\n",
      "0.6203703703703703\n",
      "batch_idx: 9\n",
      "0.6125\n",
      "batch_idx: 10\n",
      "0.625\n",
      "batch_idx: 11\n",
      "0.6180555555555556\n",
      "batch_idx: 12\n",
      "0.6217948717948718\n",
      "batch_idx: 13\n",
      "0.6130952380952381\n",
      "batch_idx: 14\n",
      "0.6111111111111112\n",
      "batch_idx: 15\n",
      "0.6145833333333334\n",
      "batch_idx: 16\n",
      "0.6176470588235294\n",
      "batch_idx: 17\n",
      "0.6180555555555556\n",
      "batch_idx: 18\n",
      "0.6162280701754386\n",
      "batch_idx: 19\n",
      "0.6145833333333334\n",
      "batch_idx: 20\n",
      "0.6190476190476191\n",
      "batch_idx: 21\n",
      "0.615530303030303\n",
      "batch_idx: 22\n",
      "0.6159420289855072\n",
      "batch_idx: 23\n",
      "0.6128472222222222\n",
      "batch_idx: 24\n",
      "0.61\n",
      "Training Epoch: 10, total loss: 49.113001\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.6111111111111112\n",
      "batch_idx: 3\n",
      "0.6145833333333334\n",
      "batch_idx: 4\n",
      "0.575\n",
      "batch_idx: 5\n",
      "0.5972222222222222\n",
      "batch_idx: 6\n",
      "0.6190476190476191\n",
      "batch_idx: 7\n",
      "0.6197916666666666\n",
      "batch_idx: 8\n",
      "0.625\n",
      "batch_idx: 9\n",
      "0.6375\n",
      "batch_idx: 10\n",
      "0.6363636363636364\n",
      "batch_idx: 11\n",
      "0.6284722222222222\n",
      "batch_idx: 12\n",
      "0.625\n",
      "batch_idx: 13\n",
      "0.6279761904761905\n",
      "batch_idx: 14\n",
      "0.6361111111111111\n",
      "batch_idx: 15\n",
      "0.6276041666666666\n",
      "batch_idx: 16\n",
      "0.6225490196078431\n",
      "batch_idx: 17\n",
      "0.6342592592592593\n",
      "batch_idx: 18\n",
      "0.6359649122807017\n",
      "batch_idx: 19\n",
      "0.6333333333333333\n",
      "batch_idx: 20\n",
      "0.6349206349206349\n",
      "batch_idx: 21\n",
      "0.6325757575757576\n",
      "batch_idx: 22\n",
      "0.6322463768115942\n",
      "batch_idx: 23\n",
      "0.640625\n",
      "batch_idx: 24\n",
      "0.6433333333333333\n",
      "Training Epoch: 11, total loss: 48.859823\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.6111111111111112\n",
      "batch_idx: 3\n",
      "0.6770833333333334\n",
      "batch_idx: 4\n",
      "0.625\n",
      "batch_idx: 5\n",
      "0.6736111111111112\n",
      "batch_idx: 6\n",
      "0.6785714285714286\n",
      "batch_idx: 7\n",
      "0.6770833333333334\n",
      "batch_idx: 8\n",
      "0.6805555555555556\n",
      "batch_idx: 9\n",
      "0.6875\n",
      "batch_idx: 10\n",
      "0.6742424242424242\n",
      "batch_idx: 11\n",
      "0.6770833333333334\n",
      "batch_idx: 12\n",
      "0.6698717948717948\n",
      "batch_idx: 13\n",
      "0.6726190476190477\n",
      "batch_idx: 14\n",
      "0.6666666666666666\n",
      "batch_idx: 15\n",
      "0.6536458333333334\n",
      "batch_idx: 16\n",
      "0.6568627450980392\n",
      "batch_idx: 17\n",
      "0.6550925925925926\n",
      "batch_idx: 18\n",
      "0.6622807017543859\n",
      "batch_idx: 19\n",
      "0.6604166666666667\n",
      "batch_idx: 20\n",
      "0.6587301587301587\n",
      "batch_idx: 21\n",
      "0.6590909090909091\n",
      "batch_idx: 22\n",
      "0.657608695652174\n",
      "batch_idx: 23\n",
      "0.65625\n",
      "batch_idx: 24\n",
      "0.655\n",
      "Training Epoch: 12, total loss: 48.645673\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.6145833333333334\n",
      "batch_idx: 4\n",
      "0.625\n",
      "batch_idx: 5\n",
      "0.6458333333333334\n",
      "batch_idx: 6\n",
      "0.6666666666666666\n",
      "batch_idx: 7\n",
      "0.671875\n",
      "batch_idx: 8\n",
      "0.6574074074074074\n",
      "batch_idx: 9\n",
      "0.6625\n",
      "batch_idx: 10\n",
      "0.6590909090909091\n",
      "batch_idx: 11\n",
      "0.6597222222222222\n",
      "batch_idx: 12\n",
      "0.6602564102564102\n",
      "batch_idx: 13\n",
      "0.6517857142857143\n",
      "batch_idx: 14\n",
      "0.65\n",
      "batch_idx: 15\n",
      "0.6510416666666666\n",
      "batch_idx: 16\n",
      "0.6446078431372549\n",
      "batch_idx: 17\n",
      "0.6435185185185185\n",
      "batch_idx: 18\n",
      "0.6425438596491229\n",
      "batch_idx: 19\n",
      "0.64375\n",
      "batch_idx: 20\n",
      "0.6507936507936508\n",
      "batch_idx: 21\n",
      "0.6571969696969697\n",
      "batch_idx: 22\n",
      "0.6648550724637681\n",
      "batch_idx: 23\n",
      "0.6579861111111112\n",
      "batch_idx: 24\n",
      "0.6583333333333333\n",
      "Training Epoch: 13, total loss: 48.560014\n",
      "batch_idx: 0\n",
      "0.4583333333333333\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.65625\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6319444444444444\n",
      "batch_idx: 6\n",
      "0.6369047619047619\n",
      "batch_idx: 7\n",
      "0.6614583333333334\n",
      "batch_idx: 8\n",
      "0.6527777777777778\n",
      "batch_idx: 9\n",
      "0.6666666666666666\n",
      "batch_idx: 10\n",
      "0.6742424242424242\n",
      "batch_idx: 11\n",
      "0.6736111111111112\n",
      "batch_idx: 12\n",
      "0.6762820512820513\n",
      "batch_idx: 13\n",
      "0.6845238095238095\n",
      "batch_idx: 14\n",
      "0.6805555555555556\n",
      "batch_idx: 15\n",
      "0.6875\n",
      "batch_idx: 16\n",
      "0.6838235294117647\n",
      "batch_idx: 17\n",
      "0.6736111111111112\n",
      "batch_idx: 18\n",
      "0.6600877192982456\n",
      "batch_idx: 19\n",
      "0.6625\n",
      "batch_idx: 20\n",
      "0.6626984126984127\n",
      "batch_idx: 21\n",
      "0.6628787878787878\n",
      "batch_idx: 22\n",
      "0.6594202898550725\n",
      "batch_idx: 23\n",
      "0.6579861111111112\n",
      "batch_idx: 24\n",
      "0.6566666666666666\n",
      "Training Epoch: 14, total loss: 48.340482\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.6111111111111112\n",
      "batch_idx: 3\n",
      "0.5833333333333334\n",
      "batch_idx: 4\n",
      "0.5916666666666667\n",
      "batch_idx: 5\n",
      "0.6180555555555556\n",
      "batch_idx: 6\n",
      "0.6369047619047619\n",
      "batch_idx: 7\n",
      "0.6510416666666666\n",
      "batch_idx: 8\n",
      "0.6666666666666666\n",
      "batch_idx: 9\n",
      "0.6791666666666667\n",
      "batch_idx: 10\n",
      "0.6856060606060606\n",
      "batch_idx: 11\n",
      "0.6805555555555556\n",
      "batch_idx: 12\n",
      "0.6602564102564102\n",
      "batch_idx: 13\n",
      "0.6636904761904762\n",
      "batch_idx: 14\n",
      "0.6777777777777778\n",
      "batch_idx: 15\n",
      "0.6744791666666666\n",
      "batch_idx: 16\n",
      "0.678921568627451\n",
      "batch_idx: 17\n",
      "0.6736111111111112\n",
      "batch_idx: 18\n",
      "0.6710526315789473\n",
      "batch_idx: 19\n",
      "0.675\n",
      "batch_idx: 20\n",
      "0.6746031746031746\n",
      "batch_idx: 21\n",
      "0.6761363636363636\n",
      "batch_idx: 22\n",
      "0.6739130434782609\n",
      "batch_idx: 23\n",
      "0.671875\n",
      "batch_idx: 24\n",
      "0.6733333333333333\n",
      "Training Epoch: 15, total loss: 48.106938\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.6666666666666666\n",
      "batch_idx: 4\n",
      "0.65\n",
      "batch_idx: 5\n",
      "0.6805555555555556\n",
      "batch_idx: 6\n",
      "0.6904761904761905\n",
      "batch_idx: 7\n",
      "0.7083333333333334\n",
      "batch_idx: 8\n",
      "0.7175925925925926\n",
      "batch_idx: 9\n",
      "0.725\n",
      "batch_idx: 10\n",
      "0.7310606060606061\n",
      "batch_idx: 11\n",
      "0.71875\n",
      "batch_idx: 12\n",
      "0.7083333333333334\n",
      "batch_idx: 13\n",
      "0.7113095238095238\n",
      "batch_idx: 14\n",
      "0.7027777777777777\n",
      "batch_idx: 15\n",
      "0.7057291666666666\n",
      "batch_idx: 16\n",
      "0.6985294117647058\n",
      "batch_idx: 17\n",
      "0.7013888888888888\n",
      "batch_idx: 18\n",
      "0.706140350877193\n",
      "batch_idx: 19\n",
      "0.7\n",
      "batch_idx: 20\n",
      "0.6924603174603174\n",
      "batch_idx: 21\n",
      "0.6893939393939394\n",
      "batch_idx: 22\n",
      "0.6865942028985508\n",
      "batch_idx: 23\n",
      "0.6822916666666666\n",
      "batch_idx: 24\n",
      "0.6816666666666666\n",
      "Training Epoch: 16, total loss: 47.882871\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.725\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.7261904761904762\n",
      "batch_idx: 7\n",
      "0.7291666666666666\n",
      "batch_idx: 8\n",
      "0.7361111111111112\n",
      "batch_idx: 9\n",
      "0.7458333333333333\n",
      "batch_idx: 10\n",
      "0.7386363636363636\n",
      "batch_idx: 11\n",
      "0.7326388888888888\n",
      "batch_idx: 12\n",
      "0.7339743589743589\n",
      "batch_idx: 13\n",
      "0.7380952380952381\n",
      "batch_idx: 14\n",
      "0.7305555555555555\n",
      "batch_idx: 15\n",
      "0.7265625\n",
      "batch_idx: 16\n",
      "0.7230392156862745\n",
      "batch_idx: 17\n",
      "0.7129629629629629\n",
      "batch_idx: 18\n",
      "0.7105263157894737\n",
      "batch_idx: 19\n",
      "0.7104166666666667\n",
      "batch_idx: 20\n",
      "0.7103174603174603\n",
      "batch_idx: 21\n",
      "0.7121212121212122\n",
      "batch_idx: 22\n",
      "0.7065217391304348\n",
      "batch_idx: 23\n",
      "0.7118055555555556\n",
      "batch_idx: 24\n",
      "0.7033333333333334\n",
      "Training Epoch: 17, total loss: 47.660113\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.6833333333333333\n",
      "batch_idx: 5\n",
      "0.6666666666666666\n",
      "batch_idx: 6\n",
      "0.6726190476190477\n",
      "batch_idx: 7\n",
      "0.671875\n",
      "batch_idx: 8\n",
      "0.6851851851851852\n",
      "batch_idx: 9\n",
      "0.6916666666666667\n",
      "batch_idx: 10\n",
      "0.7007575757575758\n",
      "batch_idx: 11\n",
      "0.6909722222222222\n",
      "batch_idx: 12\n",
      "0.6923076923076923\n",
      "batch_idx: 13\n",
      "0.6934523809523809\n",
      "batch_idx: 14\n",
      "0.6805555555555556\n",
      "batch_idx: 15\n",
      "0.6848958333333334\n",
      "batch_idx: 16\n",
      "0.678921568627451\n",
      "batch_idx: 17\n",
      "0.6805555555555556\n",
      "batch_idx: 18\n",
      "0.6776315789473685\n",
      "batch_idx: 19\n",
      "0.6770833333333334\n",
      "batch_idx: 20\n",
      "0.6805555555555556\n",
      "batch_idx: 21\n",
      "0.6761363636363636\n",
      "batch_idx: 22\n",
      "0.6847826086956522\n",
      "batch_idx: 23\n",
      "0.6822916666666666\n",
      "batch_idx: 24\n",
      "0.6933333333333334\n",
      "Training Epoch: 18, total loss: 47.634831\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6736111111111112\n",
      "batch_idx: 6\n",
      "0.6726190476190477\n",
      "batch_idx: 7\n",
      "0.6927083333333334\n",
      "batch_idx: 8\n",
      "0.7037037037037037\n",
      "batch_idx: 9\n",
      "0.7125\n",
      "batch_idx: 10\n",
      "0.7083333333333334\n",
      "batch_idx: 11\n",
      "0.7152777777777778\n",
      "batch_idx: 12\n",
      "0.7211538461538461\n",
      "batch_idx: 13\n",
      "0.7142857142857143\n",
      "batch_idx: 14\n",
      "0.7083333333333334\n",
      "batch_idx: 15\n",
      "0.7057291666666666\n",
      "batch_idx: 16\n",
      "0.7107843137254902\n",
      "batch_idx: 17\n",
      "0.7129629629629629\n",
      "batch_idx: 18\n",
      "0.7192982456140351\n",
      "batch_idx: 19\n",
      "0.71875\n",
      "batch_idx: 20\n",
      "0.7182539682539683\n",
      "batch_idx: 21\n",
      "0.7140151515151515\n",
      "batch_idx: 22\n",
      "0.7192028985507246\n",
      "batch_idx: 23\n",
      "0.71875\n",
      "batch_idx: 24\n",
      "0.7133333333333334\n",
      "Training Epoch: 19, total loss: 47.345314\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.6770833333333334\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6875\n",
      "batch_idx: 6\n",
      "0.6964285714285714\n",
      "batch_idx: 7\n",
      "0.6927083333333334\n",
      "batch_idx: 8\n",
      "0.6898148148148148\n",
      "batch_idx: 9\n",
      "0.7\n",
      "batch_idx: 10\n",
      "0.696969696969697\n",
      "batch_idx: 11\n",
      "0.6979166666666666\n",
      "batch_idx: 12\n",
      "0.7083333333333334\n",
      "batch_idx: 13\n",
      "0.7083333333333334\n",
      "batch_idx: 14\n",
      "0.7111111111111111\n",
      "batch_idx: 15\n",
      "0.7083333333333334\n",
      "batch_idx: 16\n",
      "0.7083333333333334\n",
      "batch_idx: 17\n",
      "0.7129629629629629\n",
      "batch_idx: 18\n",
      "0.7105263157894737\n",
      "batch_idx: 19\n",
      "0.7125\n",
      "batch_idx: 20\n",
      "0.7063492063492064\n",
      "batch_idx: 21\n",
      "0.7121212121212122\n",
      "batch_idx: 22\n",
      "0.7155797101449275\n",
      "batch_idx: 23\n",
      "0.7239583333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 24\n",
      "0.7233333333333334\n",
      "Training Epoch: 20, total loss: 47.249690\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6666666666666666\n",
      "batch_idx: 6\n",
      "0.6726190476190477\n",
      "batch_idx: 7\n",
      "0.6666666666666666\n",
      "batch_idx: 8\n",
      "0.6712962962962963\n",
      "batch_idx: 9\n",
      "0.6625\n",
      "batch_idx: 10\n",
      "0.6363636363636364\n",
      "batch_idx: 11\n",
      "0.6527777777777778\n",
      "batch_idx: 12\n",
      "0.6506410256410257\n",
      "batch_idx: 13\n",
      "0.6547619047619048\n",
      "batch_idx: 14\n",
      "0.6583333333333333\n",
      "batch_idx: 15\n",
      "0.6666666666666666\n",
      "batch_idx: 16\n",
      "0.6642156862745098\n",
      "batch_idx: 17\n",
      "0.6689814814814815\n",
      "batch_idx: 18\n",
      "0.6754385964912281\n",
      "batch_idx: 19\n",
      "0.675\n",
      "batch_idx: 20\n",
      "0.6825396825396826\n",
      "batch_idx: 21\n",
      "0.678030303030303\n",
      "batch_idx: 22\n",
      "0.6865942028985508\n",
      "batch_idx: 23\n",
      "0.6909722222222222\n",
      "batch_idx: 24\n",
      "0.695\n",
      "Training Epoch: 21, total loss: 47.412985\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7407407407407407\n",
      "batch_idx: 9\n",
      "0.7416666666666667\n",
      "batch_idx: 10\n",
      "0.7272727272727273\n",
      "batch_idx: 11\n",
      "0.7152777777777778\n",
      "batch_idx: 12\n",
      "0.7019230769230769\n",
      "batch_idx: 13\n",
      "0.7113095238095238\n",
      "batch_idx: 14\n",
      "0.7166666666666667\n",
      "batch_idx: 15\n",
      "0.71875\n",
      "batch_idx: 16\n",
      "0.7205882352941176\n",
      "batch_idx: 17\n",
      "0.7268518518518519\n",
      "batch_idx: 18\n",
      "0.7214912280701754\n",
      "batch_idx: 19\n",
      "0.71875\n",
      "batch_idx: 20\n",
      "0.7261904761904762\n",
      "batch_idx: 21\n",
      "0.7253787878787878\n",
      "batch_idx: 22\n",
      "0.7246376811594203\n",
      "batch_idx: 23\n",
      "0.7256944444444444\n",
      "batch_idx: 24\n",
      "0.7216666666666667\n",
      "Training Epoch: 22, total loss: 46.897491\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.6833333333333333\n",
      "batch_idx: 5\n",
      "0.6944444444444444\n",
      "batch_idx: 6\n",
      "0.6845238095238095\n",
      "batch_idx: 7\n",
      "0.6875\n",
      "batch_idx: 8\n",
      "0.7083333333333334\n",
      "batch_idx: 9\n",
      "0.7125\n",
      "batch_idx: 10\n",
      "0.7083333333333334\n",
      "batch_idx: 11\n",
      "0.7048611111111112\n",
      "batch_idx: 12\n",
      "0.6987179487179487\n",
      "batch_idx: 13\n",
      "0.7053571428571429\n",
      "batch_idx: 14\n",
      "0.7111111111111111\n",
      "batch_idx: 15\n",
      "0.7135416666666666\n",
      "batch_idx: 16\n",
      "0.7156862745098039\n",
      "batch_idx: 17\n",
      "0.7129629629629629\n",
      "batch_idx: 18\n",
      "0.7149122807017544\n",
      "batch_idx: 19\n",
      "0.7208333333333333\n",
      "batch_idx: 20\n",
      "0.7261904761904762\n",
      "batch_idx: 21\n",
      "0.7178030303030303\n",
      "batch_idx: 22\n",
      "0.7155797101449275\n",
      "batch_idx: 23\n",
      "0.7204861111111112\n",
      "batch_idx: 24\n",
      "0.7116666666666667\n",
      "Training Epoch: 23, total loss: 46.924322\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7407407407407407\n",
      "batch_idx: 9\n",
      "0.7333333333333333\n",
      "batch_idx: 10\n",
      "0.7310606060606061\n",
      "batch_idx: 11\n",
      "0.7361111111111112\n",
      "batch_idx: 12\n",
      "0.7532051282051282\n",
      "batch_idx: 13\n",
      "0.7559523809523809\n",
      "batch_idx: 14\n",
      "0.7444444444444445\n",
      "batch_idx: 15\n",
      "0.734375\n",
      "batch_idx: 16\n",
      "0.7352941176470589\n",
      "batch_idx: 17\n",
      "0.7291666666666666\n",
      "batch_idx: 18\n",
      "0.7302631578947368\n",
      "batch_idx: 19\n",
      "0.7270833333333333\n",
      "batch_idx: 20\n",
      "0.7242063492063492\n",
      "batch_idx: 21\n",
      "0.7234848484848485\n",
      "batch_idx: 22\n",
      "0.7192028985507246\n",
      "batch_idx: 23\n",
      "0.71875\n",
      "batch_idx: 24\n",
      "0.7216666666666667\n",
      "Training Epoch: 24, total loss: 46.902861\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.6770833333333334\n",
      "batch_idx: 4\n",
      "0.65\n",
      "batch_idx: 5\n",
      "0.6805555555555556\n",
      "batch_idx: 6\n",
      "0.6964285714285714\n",
      "batch_idx: 7\n",
      "0.6875\n",
      "batch_idx: 8\n",
      "0.7037037037037037\n",
      "batch_idx: 9\n",
      "0.7041666666666667\n",
      "batch_idx: 10\n",
      "0.7045454545454546\n",
      "batch_idx: 11\n",
      "0.7118055555555556\n",
      "batch_idx: 12\n",
      "0.7083333333333334\n",
      "batch_idx: 13\n",
      "0.7142857142857143\n",
      "batch_idx: 14\n",
      "0.7194444444444444\n",
      "batch_idx: 15\n",
      "0.71875\n",
      "batch_idx: 16\n",
      "0.7230392156862745\n",
      "batch_idx: 17\n",
      "0.7222222222222222\n",
      "batch_idx: 18\n",
      "0.7214912280701754\n",
      "batch_idx: 19\n",
      "0.7270833333333333\n",
      "batch_idx: 20\n",
      "0.7281746031746031\n",
      "batch_idx: 21\n",
      "0.7215909090909091\n",
      "batch_idx: 22\n",
      "0.7210144927536232\n",
      "batch_idx: 23\n",
      "0.7204861111111112\n",
      "batch_idx: 24\n",
      "0.7233333333333334\n",
      "Training Epoch: 25, total loss: 46.700481\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.6388888888888888\n",
      "batch_idx: 3\n",
      "0.6354166666666666\n",
      "batch_idx: 4\n",
      "0.65\n",
      "batch_idx: 5\n",
      "0.6388888888888888\n",
      "batch_idx: 6\n",
      "0.6309523809523809\n",
      "batch_idx: 7\n",
      "0.6354166666666666\n",
      "batch_idx: 8\n",
      "0.6527777777777778\n",
      "batch_idx: 9\n",
      "0.6625\n",
      "batch_idx: 10\n",
      "0.678030303030303\n",
      "batch_idx: 11\n",
      "0.6840277777777778\n",
      "batch_idx: 12\n",
      "0.6826923076923077\n",
      "batch_idx: 13\n",
      "0.6904761904761905\n",
      "batch_idx: 14\n",
      "0.6916666666666667\n",
      "batch_idx: 15\n",
      "0.6848958333333334\n",
      "batch_idx: 16\n",
      "0.6936274509803921\n",
      "batch_idx: 17\n",
      "0.6898148148148148\n",
      "batch_idx: 18\n",
      "0.6885964912280702\n",
      "batch_idx: 19\n",
      "0.69375\n",
      "batch_idx: 20\n",
      "0.6924603174603174\n",
      "batch_idx: 21\n",
      "0.6875\n",
      "batch_idx: 22\n",
      "0.6938405797101449\n",
      "batch_idx: 23\n",
      "0.6875\n",
      "batch_idx: 24\n",
      "0.6916666666666667\n",
      "Training Epoch: 26, total loss: 47.078441\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.725\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.71875\n",
      "batch_idx: 8\n",
      "0.7222222222222222\n",
      "batch_idx: 9\n",
      "0.725\n",
      "batch_idx: 10\n",
      "0.7310606060606061\n",
      "batch_idx: 11\n",
      "0.7430555555555556\n",
      "batch_idx: 12\n",
      "0.7275641025641025\n",
      "batch_idx: 13\n",
      "0.7232142857142857\n",
      "batch_idx: 14\n",
      "0.7333333333333333\n",
      "batch_idx: 15\n",
      "0.7395833333333334\n",
      "batch_idx: 16\n",
      "0.7352941176470589\n",
      "batch_idx: 17\n",
      "0.7361111111111112\n",
      "batch_idx: 18\n",
      "0.7390350877192983\n",
      "batch_idx: 19\n",
      "0.7375\n",
      "batch_idx: 20\n",
      "0.7440476190476191\n",
      "batch_idx: 21\n",
      "0.75\n",
      "batch_idx: 22\n",
      "0.7481884057971014\n",
      "batch_idx: 23\n",
      "0.75\n",
      "batch_idx: 24\n",
      "0.755\n",
      "Training Epoch: 27, total loss: 46.154481\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7407407407407407\n",
      "batch_idx: 9\n",
      "0.7333333333333333\n",
      "batch_idx: 10\n",
      "0.7348484848484849\n",
      "batch_idx: 11\n",
      "0.7361111111111112\n",
      "batch_idx: 12\n",
      "0.7371794871794872\n",
      "batch_idx: 13\n",
      "0.7261904761904762\n",
      "batch_idx: 14\n",
      "0.7333333333333333\n",
      "batch_idx: 15\n",
      "0.734375\n",
      "batch_idx: 16\n",
      "0.7303921568627451\n",
      "batch_idx: 17\n",
      "0.7291666666666666\n",
      "batch_idx: 18\n",
      "0.7302631578947368\n",
      "batch_idx: 19\n",
      "0.725\n",
      "batch_idx: 20\n",
      "0.7281746031746031\n",
      "batch_idx: 21\n",
      "0.7253787878787878\n",
      "batch_idx: 22\n",
      "0.7210144927536232\n",
      "batch_idx: 23\n",
      "0.71875\n",
      "batch_idx: 24\n",
      "0.7216666666666667\n",
      "Training Epoch: 28, total loss: 46.402241\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.7361111111111112\n",
      "batch_idx: 6\n",
      "0.7142857142857143\n",
      "batch_idx: 7\n",
      "0.7083333333333334\n",
      "batch_idx: 8\n",
      "0.7175925925925926\n",
      "batch_idx: 9\n",
      "0.725\n",
      "batch_idx: 10\n",
      "0.7348484848484849\n",
      "batch_idx: 11\n",
      "0.7361111111111112\n",
      "batch_idx: 12\n",
      "0.7275641025641025\n",
      "batch_idx: 13\n",
      "0.7380952380952381\n",
      "batch_idx: 14\n",
      "0.7416666666666667\n",
      "batch_idx: 15\n",
      "0.7291666666666666\n",
      "batch_idx: 16\n",
      "0.7279411764705882\n",
      "batch_idx: 17\n",
      "0.7314814814814815\n",
      "batch_idx: 18\n",
      "0.7390350877192983\n",
      "batch_idx: 19\n",
      "0.7458333333333333\n",
      "batch_idx: 20\n",
      "0.75\n",
      "batch_idx: 21\n",
      "0.7575757575757576\n",
      "batch_idx: 22\n",
      "0.7572463768115942\n",
      "batch_idx: 23\n",
      "0.7621527777777778\n",
      "batch_idx: 24\n",
      "0.7633333333333333\n",
      "Training Epoch: 29, total loss: 46.023957\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.7453703703703703\n",
      "batch_idx: 9\n",
      "0.7375\n",
      "batch_idx: 10\n",
      "0.7348484848484849\n",
      "batch_idx: 11\n",
      "0.7430555555555556\n",
      "batch_idx: 12\n",
      "0.7467948717948718\n",
      "batch_idx: 13\n",
      "0.75\n",
      "batch_idx: 14\n",
      "0.7444444444444445\n",
      "batch_idx: 15\n",
      "0.7526041666666666\n",
      "batch_idx: 16\n",
      "0.7475490196078431\n",
      "batch_idx: 17\n",
      "0.7453703703703703\n",
      "batch_idx: 18\n",
      "0.7390350877192983\n",
      "batch_idx: 19\n",
      "0.7416666666666667\n",
      "batch_idx: 20\n",
      "0.746031746031746\n",
      "batch_idx: 21\n",
      "0.7443181818181818\n",
      "batch_idx: 22\n",
      "0.7518115942028986\n",
      "batch_idx: 23\n",
      "0.75\n",
      "batch_idx: 24\n",
      "0.7516666666666667\n",
      "Training Epoch: 30, total loss: 46.028265\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.7314814814814815\n",
      "batch_idx: 9\n",
      "0.7458333333333333\n",
      "batch_idx: 10\n",
      "0.7537878787878788\n",
      "batch_idx: 11\n",
      "0.7673611111111112\n",
      "batch_idx: 12\n",
      "0.7692307692307693\n",
      "batch_idx: 13\n",
      "0.7708333333333334\n",
      "batch_idx: 14\n",
      "0.7722222222222223\n",
      "batch_idx: 15\n",
      "0.7734375\n",
      "batch_idx: 16\n",
      "0.7745098039215687\n",
      "batch_idx: 17\n",
      "0.7777777777777778\n",
      "batch_idx: 18\n",
      "0.7741228070175439\n",
      "batch_idx: 19\n",
      "0.7770833333333333\n",
      "batch_idx: 20\n",
      "0.7738095238095238\n",
      "batch_idx: 21\n",
      "0.7727272727272727\n",
      "batch_idx: 22\n",
      "0.769927536231884\n",
      "batch_idx: 23\n",
      "0.7673611111111112\n",
      "batch_idx: 24\n",
      "0.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 31, total loss: 45.593422\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.7361111111111112\n",
      "batch_idx: 9\n",
      "0.7416666666666667\n",
      "batch_idx: 10\n",
      "0.7234848484848485\n",
      "batch_idx: 11\n",
      "0.71875\n",
      "batch_idx: 12\n",
      "0.7115384615384616\n",
      "batch_idx: 13\n",
      "0.7142857142857143\n",
      "batch_idx: 14\n",
      "0.7138888888888889\n",
      "batch_idx: 15\n",
      "0.7213541666666666\n",
      "batch_idx: 16\n",
      "0.7107843137254902\n",
      "batch_idx: 17\n",
      "0.7129629629629629\n",
      "batch_idx: 18\n",
      "0.7127192982456141\n",
      "batch_idx: 19\n",
      "0.7270833333333333\n",
      "batch_idx: 20\n",
      "0.7301587301587301\n",
      "batch_idx: 21\n",
      "0.7348484848484849\n",
      "batch_idx: 22\n",
      "0.7282608695652174\n",
      "batch_idx: 23\n",
      "0.7309027777777778\n",
      "batch_idx: 24\n",
      "0.7266666666666667\n",
      "Training Epoch: 32, total loss: 46.225611\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.5625\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.65625\n",
      "batch_idx: 4\n",
      "0.6666666666666666\n",
      "batch_idx: 5\n",
      "0.6944444444444444\n",
      "batch_idx: 6\n",
      "0.7202380952380952\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7453703703703703\n",
      "batch_idx: 9\n",
      "0.7333333333333333\n",
      "batch_idx: 10\n",
      "0.7386363636363636\n",
      "batch_idx: 11\n",
      "0.75\n",
      "batch_idx: 12\n",
      "0.7628205128205128\n",
      "batch_idx: 13\n",
      "0.7589285714285714\n",
      "batch_idx: 14\n",
      "0.7527777777777778\n",
      "batch_idx: 15\n",
      "0.75\n",
      "batch_idx: 16\n",
      "0.7524509803921569\n",
      "batch_idx: 17\n",
      "0.7453703703703703\n",
      "batch_idx: 18\n",
      "0.743421052631579\n",
      "batch_idx: 19\n",
      "0.7416666666666667\n",
      "batch_idx: 20\n",
      "0.7400793650793651\n",
      "batch_idx: 21\n",
      "0.7443181818181818\n",
      "batch_idx: 22\n",
      "0.7518115942028986\n",
      "batch_idx: 23\n",
      "0.7482638888888888\n",
      "batch_idx: 24\n",
      "0.74\n",
      "Training Epoch: 33, total loss: 46.078042\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.7541666666666667\n",
      "batch_idx: 10\n",
      "0.7424242424242424\n",
      "batch_idx: 11\n",
      "0.7465277777777778\n",
      "batch_idx: 12\n",
      "0.75\n",
      "batch_idx: 13\n",
      "0.7440476190476191\n",
      "batch_idx: 14\n",
      "0.7527777777777778\n",
      "batch_idx: 15\n",
      "0.7578125\n",
      "batch_idx: 16\n",
      "0.7598039215686274\n",
      "batch_idx: 17\n",
      "0.7615740740740741\n",
      "batch_idx: 18\n",
      "0.756578947368421\n",
      "batch_idx: 19\n",
      "0.7583333333333333\n",
      "batch_idx: 20\n",
      "0.7559523809523809\n",
      "batch_idx: 21\n",
      "0.7575757575757576\n",
      "batch_idx: 22\n",
      "0.7608695652173914\n",
      "batch_idx: 23\n",
      "0.7586805555555556\n",
      "batch_idx: 24\n",
      "0.7583333333333333\n",
      "Training Epoch: 34, total loss: 45.694887\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.7743055555555556\n",
      "batch_idx: 12\n",
      "0.7660256410256411\n",
      "batch_idx: 13\n",
      "0.7619047619047619\n",
      "batch_idx: 14\n",
      "0.7555555555555555\n",
      "batch_idx: 15\n",
      "0.7604166666666666\n",
      "batch_idx: 16\n",
      "0.7622549019607843\n",
      "batch_idx: 17\n",
      "0.7708333333333334\n",
      "batch_idx: 18\n",
      "0.7631578947368421\n",
      "batch_idx: 19\n",
      "0.75625\n",
      "batch_idx: 20\n",
      "0.7599206349206349\n",
      "batch_idx: 21\n",
      "0.7632575757575758\n",
      "batch_idx: 22\n",
      "0.7608695652173914\n",
      "batch_idx: 23\n",
      "0.7604166666666666\n",
      "batch_idx: 24\n",
      "0.7616666666666667\n",
      "Training Epoch: 35, total loss: 45.694894\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.7458333333333333\n",
      "batch_idx: 10\n",
      "0.7424242424242424\n",
      "batch_idx: 11\n",
      "0.7430555555555556\n",
      "batch_idx: 12\n",
      "0.75\n",
      "batch_idx: 13\n",
      "0.7529761904761905\n",
      "batch_idx: 14\n",
      "0.7583333333333333\n",
      "batch_idx: 15\n",
      "0.7552083333333334\n",
      "batch_idx: 16\n",
      "0.7549019607843137\n",
      "batch_idx: 17\n",
      "0.7546296296296297\n",
      "batch_idx: 18\n",
      "0.756578947368421\n",
      "batch_idx: 19\n",
      "0.7625\n",
      "batch_idx: 20\n",
      "0.7579365079365079\n",
      "batch_idx: 21\n",
      "0.7651515151515151\n",
      "batch_idx: 22\n",
      "0.7717391304347826\n",
      "batch_idx: 23\n",
      "0.7760416666666666\n",
      "batch_idx: 24\n",
      "0.7766666666666666\n",
      "Training Epoch: 36, total loss: 45.379940\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.7777777777777778\n",
      "batch_idx: 12\n",
      "0.7788461538461539\n",
      "batch_idx: 13\n",
      "0.7857142857142857\n",
      "batch_idx: 14\n",
      "0.7777777777777778\n",
      "batch_idx: 15\n",
      "0.7864583333333334\n",
      "batch_idx: 16\n",
      "0.7867647058823529\n",
      "batch_idx: 17\n",
      "0.7824074074074074\n",
      "batch_idx: 18\n",
      "0.7763157894736842\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7876984126984127\n",
      "batch_idx: 21\n",
      "0.7821969696969697\n",
      "batch_idx: 22\n",
      "0.7807971014492754\n",
      "batch_idx: 23\n",
      "0.7795138888888888\n",
      "batch_idx: 24\n",
      "0.7716666666666666\n",
      "Training Epoch: 37, total loss: 45.503430\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7625\n",
      "batch_idx: 10\n",
      "0.7689393939393939\n",
      "batch_idx: 11\n",
      "0.7569444444444444\n",
      "batch_idx: 12\n",
      "0.7628205128205128\n",
      "batch_idx: 13\n",
      "0.7738095238095238\n",
      "batch_idx: 14\n",
      "0.7777777777777778\n",
      "batch_idx: 15\n",
      "0.7734375\n",
      "batch_idx: 16\n",
      "0.7720588235294118\n",
      "batch_idx: 17\n",
      "0.7731481481481481\n",
      "batch_idx: 18\n",
      "0.7741228070175439\n",
      "batch_idx: 19\n",
      "0.7729166666666667\n",
      "batch_idx: 20\n",
      "0.7738095238095238\n",
      "batch_idx: 21\n",
      "0.7746212121212122\n",
      "batch_idx: 22\n",
      "0.7807971014492754\n",
      "batch_idx: 23\n",
      "0.7829861111111112\n",
      "batch_idx: 24\n",
      "0.7816666666666666\n",
      "Training Epoch: 38, total loss: 45.199808\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.90625\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.8958333333333334\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8298611111111112\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8244047619047619\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8070175438596491\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.7956349206349206\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.7971014492753623\n",
      "batch_idx: 23\n",
      "0.7916666666666666\n",
      "batch_idx: 24\n",
      "0.7933333333333333\n",
      "Training Epoch: 39, total loss: 45.038068\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.7395833333333334\n",
      "batch_idx: 8\n",
      "0.7314814814814815\n",
      "batch_idx: 9\n",
      "0.7291666666666666\n",
      "batch_idx: 10\n",
      "0.7462121212121212\n",
      "batch_idx: 11\n",
      "0.75\n",
      "batch_idx: 12\n",
      "0.7435897435897436\n",
      "batch_idx: 13\n",
      "0.75\n",
      "batch_idx: 14\n",
      "0.7527777777777778\n",
      "batch_idx: 15\n",
      "0.75\n",
      "batch_idx: 16\n",
      "0.7549019607843137\n",
      "batch_idx: 17\n",
      "0.7523148148148148\n",
      "batch_idx: 18\n",
      "0.7456140350877193\n",
      "batch_idx: 19\n",
      "0.74375\n",
      "batch_idx: 20\n",
      "0.746031746031746\n",
      "batch_idx: 21\n",
      "0.7518939393939394\n",
      "batch_idx: 22\n",
      "0.7536231884057971\n",
      "batch_idx: 23\n",
      "0.7552083333333334\n",
      "batch_idx: 24\n",
      "0.7566666666666667\n",
      "Training Epoch: 40, total loss: 45.397335\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7407407407407407\n",
      "batch_idx: 9\n",
      "0.75\n",
      "batch_idx: 10\n",
      "0.7424242424242424\n",
      "batch_idx: 11\n",
      "0.7569444444444444\n",
      "batch_idx: 12\n",
      "0.7435897435897436\n",
      "batch_idx: 13\n",
      "0.7529761904761905\n",
      "batch_idx: 14\n",
      "0.7527777777777778\n",
      "batch_idx: 15\n",
      "0.75\n",
      "batch_idx: 16\n",
      "0.7524509803921569\n",
      "batch_idx: 17\n",
      "0.7592592592592593\n",
      "batch_idx: 18\n",
      "0.7697368421052632\n",
      "batch_idx: 19\n",
      "0.76875\n",
      "batch_idx: 20\n",
      "0.7678571428571429\n",
      "batch_idx: 21\n",
      "0.7651515151515151\n",
      "batch_idx: 22\n",
      "0.7608695652173914\n",
      "batch_idx: 23\n",
      "0.7621527777777778\n",
      "batch_idx: 24\n",
      "0.7633333333333333\n",
      "Training Epoch: 41, total loss: 45.469333\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8044871794871795\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.7838541666666666\n",
      "batch_idx: 16\n",
      "0.7818627450980392\n",
      "batch_idx: 17\n",
      "0.7847222222222222\n",
      "batch_idx: 18\n",
      "0.7850877192982456\n",
      "batch_idx: 19\n",
      "0.7791666666666667\n",
      "batch_idx: 20\n",
      "0.7757936507936508\n",
      "batch_idx: 21\n",
      "0.7803030303030303\n",
      "batch_idx: 22\n",
      "0.7753623188405797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 23\n",
      "0.7708333333333334\n",
      "batch_idx: 24\n",
      "0.77\n",
      "Training Epoch: 42, total loss: 45.345353\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.7638888888888888\n",
      "batch_idx: 9\n",
      "0.7708333333333334\n",
      "batch_idx: 10\n",
      "0.7651515151515151\n",
      "batch_idx: 11\n",
      "0.7777777777777778\n",
      "batch_idx: 12\n",
      "0.7788461538461539\n",
      "batch_idx: 13\n",
      "0.7797619047619048\n",
      "batch_idx: 14\n",
      "0.7694444444444445\n",
      "batch_idx: 15\n",
      "0.7734375\n",
      "batch_idx: 16\n",
      "0.7745098039215687\n",
      "batch_idx: 17\n",
      "0.7777777777777778\n",
      "batch_idx: 18\n",
      "0.7675438596491229\n",
      "batch_idx: 19\n",
      "0.7625\n",
      "batch_idx: 20\n",
      "0.7638888888888888\n",
      "batch_idx: 21\n",
      "0.7670454545454546\n",
      "batch_idx: 22\n",
      "0.7608695652173914\n",
      "batch_idx: 23\n",
      "0.7534722222222222\n",
      "batch_idx: 24\n",
      "0.75\n",
      "Training Epoch: 43, total loss: 45.392520\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n",
      "batch_idx: 12\n",
      "0.7788461538461539\n",
      "batch_idx: 13\n",
      "0.7797619047619048\n",
      "batch_idx: 14\n",
      "0.7611111111111111\n",
      "batch_idx: 15\n",
      "0.7578125\n",
      "batch_idx: 16\n",
      "0.7549019607843137\n",
      "batch_idx: 17\n",
      "0.7546296296296297\n",
      "batch_idx: 18\n",
      "0.75\n",
      "batch_idx: 19\n",
      "0.75\n",
      "batch_idx: 20\n",
      "0.7559523809523809\n",
      "batch_idx: 21\n",
      "0.7537878787878788\n",
      "batch_idx: 22\n",
      "0.7626811594202898\n",
      "batch_idx: 23\n",
      "0.7604166666666666\n",
      "batch_idx: 24\n",
      "0.7616666666666667\n",
      "Training Epoch: 44, total loss: 45.157828\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.8\n",
      "batch_idx: 15\n",
      "0.7942708333333334\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.7893518518518519\n",
      "batch_idx: 18\n",
      "0.7872807017543859\n",
      "batch_idx: 19\n",
      "0.7833333333333333\n",
      "batch_idx: 20\n",
      "0.7777777777777778\n",
      "batch_idx: 21\n",
      "0.7803030303030303\n",
      "batch_idx: 22\n",
      "0.782608695652174\n",
      "batch_idx: 23\n",
      "0.7847222222222222\n",
      "batch_idx: 24\n",
      "0.785\n",
      "Training Epoch: 45, total loss: 44.988365\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7638888888888888\n",
      "batch_idx: 9\n",
      "0.7791666666666667\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7827380952380952\n",
      "batch_idx: 14\n",
      "0.7888888888888889\n",
      "batch_idx: 15\n",
      "0.7864583333333334\n",
      "batch_idx: 16\n",
      "0.7867647058823529\n",
      "batch_idx: 17\n",
      "0.7962962962962963\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.7996031746031746\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.7986111111111112\n",
      "batch_idx: 24\n",
      "0.8033333333333333\n",
      "Training Epoch: 46, total loss: 44.475876\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7638888888888888\n",
      "batch_idx: 9\n",
      "0.7625\n",
      "batch_idx: 10\n",
      "0.7462121212121212\n",
      "batch_idx: 11\n",
      "0.7395833333333334\n",
      "batch_idx: 12\n",
      "0.75\n",
      "batch_idx: 13\n",
      "0.75\n",
      "batch_idx: 14\n",
      "0.7527777777777778\n",
      "batch_idx: 15\n",
      "0.7578125\n",
      "batch_idx: 16\n",
      "0.7524509803921569\n",
      "batch_idx: 17\n",
      "0.7523148148148148\n",
      "batch_idx: 18\n",
      "0.7412280701754386\n",
      "batch_idx: 19\n",
      "0.7479166666666667\n",
      "batch_idx: 20\n",
      "0.748015873015873\n",
      "batch_idx: 21\n",
      "0.7481060606060606\n",
      "batch_idx: 22\n",
      "0.7518115942028986\n",
      "batch_idx: 23\n",
      "0.7552083333333334\n",
      "batch_idx: 24\n",
      "0.7516666666666667\n",
      "Training Epoch: 47, total loss: 44.984729\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.7946428571428571\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.7994791666666666\n",
      "batch_idx: 16\n",
      "0.7916666666666666\n",
      "batch_idx: 17\n",
      "0.7986111111111112\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.803030303030303\n",
      "batch_idx: 22\n",
      "0.8043478260869565\n",
      "batch_idx: 23\n",
      "0.8055555555555556\n",
      "batch_idx: 24\n",
      "0.8066666666666666\n",
      "Training Epoch: 48, total loss: 44.604688\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.8101851851851852\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8041666666666667\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.8087121212121212\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8038194444444444\n",
      "batch_idx: 24\n",
      "0.8\n",
      "Training Epoch: 49, total loss: 44.380516\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7727272727272727\n",
      "batch_idx: 11\n",
      "0.7777777777777778\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7916666666666666\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.7994791666666666\n",
      "batch_idx: 16\n",
      "0.7941176470588235\n",
      "batch_idx: 17\n",
      "0.7847222222222222\n",
      "batch_idx: 18\n",
      "0.7807017543859649\n",
      "batch_idx: 19\n",
      "0.775\n",
      "batch_idx: 20\n",
      "0.7738095238095238\n",
      "batch_idx: 21\n",
      "0.7708333333333334\n",
      "batch_idx: 22\n",
      "0.7717391304347826\n",
      "batch_idx: 23\n",
      "0.7725694444444444\n",
      "batch_idx: 24\n",
      "0.7766666666666666\n",
      "Training Epoch: 50, total loss: 44.772616\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.725\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.7546296296296297\n",
      "batch_idx: 9\n",
      "0.7625\n",
      "batch_idx: 10\n",
      "0.7537878787878788\n",
      "batch_idx: 11\n",
      "0.7569444444444444\n",
      "batch_idx: 12\n",
      "0.7660256410256411\n",
      "batch_idx: 13\n",
      "0.7648809523809523\n",
      "batch_idx: 14\n",
      "0.7722222222222223\n",
      "batch_idx: 15\n",
      "0.7682291666666666\n",
      "batch_idx: 16\n",
      "0.7647058823529411\n",
      "batch_idx: 17\n",
      "0.7615740740740741\n",
      "batch_idx: 18\n",
      "0.7631578947368421\n",
      "batch_idx: 19\n",
      "0.7645833333333333\n",
      "batch_idx: 20\n",
      "0.7619047619047619\n",
      "batch_idx: 21\n",
      "0.759469696969697\n",
      "batch_idx: 22\n",
      "0.7590579710144928\n",
      "batch_idx: 23\n",
      "0.7586805555555556\n",
      "batch_idx: 24\n",
      "0.76\n",
      "Training Epoch: 51, total loss: 45.082312\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7678571428571429\n",
      "batch_idx: 14\n",
      "0.7694444444444445\n",
      "batch_idx: 15\n",
      "0.7682291666666666\n",
      "batch_idx: 16\n",
      "0.7745098039215687\n",
      "batch_idx: 17\n",
      "0.7800925925925926\n",
      "batch_idx: 18\n",
      "0.7785087719298246\n",
      "batch_idx: 19\n",
      "0.7770833333333333\n",
      "batch_idx: 20\n",
      "0.7797619047619048\n",
      "batch_idx: 21\n",
      "0.7784090909090909\n",
      "batch_idx: 22\n",
      "0.769927536231884\n",
      "batch_idx: 23\n",
      "0.7795138888888888\n",
      "batch_idx: 24\n",
      "0.7783333333333333\n",
      "Training Epoch: 52, total loss: 44.579370\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7765151515151515\n",
      "batch_idx: 11\n",
      "0.7743055555555556\n",
      "batch_idx: 12\n",
      "0.7724358974358975\n",
      "batch_idx: 13\n",
      "0.7797619047619048\n",
      "batch_idx: 14\n",
      "0.7861111111111111\n",
      "batch_idx: 15\n",
      "0.7786458333333334\n",
      "batch_idx: 16\n",
      "0.7818627450980392\n",
      "batch_idx: 17\n",
      "0.7685185185185185\n",
      "batch_idx: 18\n",
      "0.7697368421052632\n",
      "batch_idx: 19\n",
      "0.7770833333333333\n",
      "batch_idx: 20\n",
      "0.7837301587301587\n",
      "batch_idx: 21\n",
      "0.7840909090909091\n",
      "batch_idx: 22\n",
      "0.7862318840579711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 23\n",
      "0.7829861111111112\n",
      "batch_idx: 24\n",
      "0.7783333333333333\n",
      "Training Epoch: 53, total loss: 44.736432\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.8088235294117647\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.7960526315789473\n",
      "batch_idx: 19\n",
      "0.7875\n",
      "batch_idx: 20\n",
      "0.7876984126984127\n",
      "batch_idx: 21\n",
      "0.7878787878787878\n",
      "batch_idx: 22\n",
      "0.7880434782608695\n",
      "batch_idx: 23\n",
      "0.7934027777777778\n",
      "batch_idx: 24\n",
      "0.7883333333333333\n",
      "Training Epoch: 54, total loss: 44.533562\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7878787878787878\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7797619047619048\n",
      "batch_idx: 14\n",
      "0.7888888888888889\n",
      "batch_idx: 15\n",
      "0.7864583333333334\n",
      "batch_idx: 16\n",
      "0.7867647058823529\n",
      "batch_idx: 17\n",
      "0.7824074074074074\n",
      "batch_idx: 18\n",
      "0.7807017543859649\n",
      "batch_idx: 19\n",
      "0.7770833333333333\n",
      "batch_idx: 20\n",
      "0.7738095238095238\n",
      "batch_idx: 21\n",
      "0.7708333333333334\n",
      "batch_idx: 22\n",
      "0.7717391304347826\n",
      "batch_idx: 23\n",
      "0.7760416666666666\n",
      "batch_idx: 24\n",
      "0.78\n",
      "Training Epoch: 55, total loss: 44.699290\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7765151515151515\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.7660256410256411\n",
      "batch_idx: 13\n",
      "0.7619047619047619\n",
      "batch_idx: 14\n",
      "0.7638888888888888\n",
      "batch_idx: 15\n",
      "0.7578125\n",
      "batch_idx: 16\n",
      "0.7573529411764706\n",
      "batch_idx: 17\n",
      "0.7615740740740741\n",
      "batch_idx: 18\n",
      "0.7675438596491229\n",
      "batch_idx: 19\n",
      "0.7583333333333333\n",
      "batch_idx: 20\n",
      "0.751984126984127\n",
      "batch_idx: 21\n",
      "0.7537878787878788\n",
      "batch_idx: 22\n",
      "0.7572463768115942\n",
      "batch_idx: 23\n",
      "0.7621527777777778\n",
      "batch_idx: 24\n",
      "0.7583333333333333\n",
      "Training Epoch: 56, total loss: 44.775143\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7878787878787878\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.7948717948717948\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.7916666666666666\n",
      "batch_idx: 15\n",
      "0.7864583333333334\n",
      "batch_idx: 16\n",
      "0.7794117647058824\n",
      "batch_idx: 17\n",
      "0.7731481481481481\n",
      "batch_idx: 18\n",
      "0.7697368421052632\n",
      "batch_idx: 19\n",
      "0.7645833333333333\n",
      "batch_idx: 20\n",
      "0.7678571428571429\n",
      "batch_idx: 21\n",
      "0.7651515151515151\n",
      "batch_idx: 22\n",
      "0.7735507246376812\n",
      "batch_idx: 23\n",
      "0.7777777777777778\n",
      "batch_idx: 24\n",
      "0.785\n",
      "Training Epoch: 57, total loss: 44.553887\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7731481481481481\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7946428571428571\n",
      "batch_idx: 14\n",
      "0.7916666666666666\n",
      "batch_idx: 15\n",
      "0.7864583333333334\n",
      "batch_idx: 16\n",
      "0.7892156862745098\n",
      "batch_idx: 17\n",
      "0.7847222222222222\n",
      "batch_idx: 18\n",
      "0.7872807017543859\n",
      "batch_idx: 19\n",
      "0.7916666666666666\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.7935606060606061\n",
      "batch_idx: 22\n",
      "0.7934782608695652\n",
      "batch_idx: 23\n",
      "0.7916666666666666\n",
      "batch_idx: 24\n",
      "0.7916666666666666\n",
      "Training Epoch: 58, total loss: 44.319550\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.7976190476190477\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.7994791666666666\n",
      "batch_idx: 16\n",
      "0.8014705882352942\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.8004385964912281\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.7952898550724637\n",
      "batch_idx: 23\n",
      "0.7951388888888888\n",
      "batch_idx: 24\n",
      "0.795\n",
      "Training Epoch: 59, total loss: 44.322315\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.7942708333333334\n",
      "batch_idx: 16\n",
      "0.8014705882352942\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8104166666666667\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.8087121212121212\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8107638888888888\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 60, total loss: 44.179298\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.7948717948717948\n",
      "batch_idx: 13\n",
      "0.7946428571428571\n",
      "batch_idx: 14\n",
      "0.8\n",
      "batch_idx: 15\n",
      "0.7916666666666666\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.8055555555555556\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8104166666666667\n",
      "batch_idx: 20\n",
      "0.8095238095238095\n",
      "batch_idx: 21\n",
      "0.8087121212121212\n",
      "batch_idx: 22\n",
      "0.8097826086956522\n",
      "batch_idx: 23\n",
      "0.8020833333333334\n",
      "batch_idx: 24\n",
      "0.8\n",
      "Training Epoch: 61, total loss: 44.199197\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.8005952380952381\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8098958333333334\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.7986111111111112\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.7979166666666667\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.8020833333333334\n",
      "batch_idx: 24\n",
      "0.8066666666666666\n",
      "Training Epoch: 62, total loss: 44.126864\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.8088235294117647\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8135964912280702\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.8095238095238095\n",
      "batch_idx: 21\n",
      "0.8087121212121212\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8090277777777778\n",
      "batch_idx: 24\n",
      "0.8133333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 63, total loss: 44.056386\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.7916666666666666\n",
      "batch_idx: 13\n",
      "0.7976190476190477\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.8014705882352942\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.8004385964912281\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.7973484848484849\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.7986111111111112\n",
      "batch_idx: 24\n",
      "0.8016666666666666\n",
      "Training Epoch: 64, total loss: 44.110716\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7857142857142857\n",
      "batch_idx: 14\n",
      "0.7888888888888889\n",
      "batch_idx: 15\n",
      "0.7942708333333334\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.7847222222222222\n",
      "batch_idx: 18\n",
      "0.7850877192982456\n",
      "batch_idx: 19\n",
      "0.7833333333333333\n",
      "batch_idx: 20\n",
      "0.7857142857142857\n",
      "batch_idx: 21\n",
      "0.7935606060606061\n",
      "batch_idx: 22\n",
      "0.7916666666666666\n",
      "batch_idx: 23\n",
      "0.7934027777777778\n",
      "batch_idx: 24\n",
      "0.7966666666666666\n",
      "Training Epoch: 65, total loss: 44.174353\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7951388888888888\n",
      "batch_idx: 12\n",
      "0.7916666666666666\n",
      "batch_idx: 13\n",
      "0.7916666666666666\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.8041666666666667\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.7992424242424242\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.7951388888888888\n",
      "batch_idx: 24\n",
      "0.8\n",
      "Training Epoch: 66, total loss: 44.121733\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.7731481481481481\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7878787878787878\n",
      "batch_idx: 11\n",
      "0.7673611111111112\n",
      "batch_idx: 12\n",
      "0.7564102564102564\n",
      "batch_idx: 13\n",
      "0.7708333333333334\n",
      "batch_idx: 14\n",
      "0.775\n",
      "batch_idx: 15\n",
      "0.7682291666666666\n",
      "batch_idx: 16\n",
      "0.7696078431372549\n",
      "batch_idx: 17\n",
      "0.7731481481481481\n",
      "batch_idx: 18\n",
      "0.7653508771929824\n",
      "batch_idx: 19\n",
      "0.7666666666666667\n",
      "batch_idx: 20\n",
      "0.7678571428571429\n",
      "batch_idx: 21\n",
      "0.7708333333333334\n",
      "batch_idx: 22\n",
      "0.7663043478260869\n",
      "batch_idx: 23\n",
      "0.7673611111111112\n",
      "batch_idx: 24\n",
      "0.7716666666666666\n",
      "Training Epoch: 67, total loss: 44.584688\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.8088235294117647\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8114035087719298\n",
      "batch_idx: 19\n",
      "0.8166666666666667\n",
      "batch_idx: 20\n",
      "0.8174603174603174\n",
      "batch_idx: 21\n",
      "0.8125\n",
      "batch_idx: 22\n",
      "0.8097826086956522\n",
      "batch_idx: 23\n",
      "0.8142361111111112\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 68, total loss: 44.021603\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8625\n",
      "batch_idx: 10\n",
      "0.8674242424242424\n",
      "batch_idx: 11\n",
      "0.8611111111111112\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8363095238095238\n",
      "batch_idx: 14\n",
      "0.8305555555555556\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8308823529411765\n",
      "batch_idx: 17\n",
      "0.8310185185185185\n",
      "batch_idx: 18\n",
      "0.8223684210526315\n",
      "batch_idx: 19\n",
      "0.81875\n",
      "batch_idx: 20\n",
      "0.8234126984126984\n",
      "batch_idx: 21\n",
      "0.8238636363636364\n",
      "batch_idx: 22\n",
      "0.8260869565217391\n",
      "batch_idx: 23\n",
      "0.8211805555555556\n",
      "batch_idx: 24\n",
      "0.8166666666666667\n",
      "Training Epoch: 69, total loss: 43.893973\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n",
      "batch_idx: 12\n",
      "0.7692307692307693\n",
      "batch_idx: 13\n",
      "0.7678571428571429\n",
      "batch_idx: 14\n",
      "0.7666666666666667\n",
      "batch_idx: 15\n",
      "0.7682291666666666\n",
      "batch_idx: 16\n",
      "0.7671568627450981\n",
      "batch_idx: 17\n",
      "0.7662037037037037\n",
      "batch_idx: 18\n",
      "0.7741228070175439\n",
      "batch_idx: 19\n",
      "0.775\n",
      "batch_idx: 20\n",
      "0.7837301587301587\n",
      "batch_idx: 21\n",
      "0.7784090909090909\n",
      "batch_idx: 22\n",
      "0.7771739130434783\n",
      "batch_idx: 23\n",
      "0.7708333333333334\n",
      "batch_idx: 24\n",
      "0.7733333333333333\n",
      "Training Epoch: 70, total loss: 44.284842\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8482142857142857\n",
      "batch_idx: 14\n",
      "0.8388888888888889\n",
      "batch_idx: 15\n",
      "0.8411458333333334\n",
      "batch_idx: 16\n",
      "0.8382352941176471\n",
      "batch_idx: 17\n",
      "0.8310185185185185\n",
      "batch_idx: 18\n",
      "0.8223684210526315\n",
      "batch_idx: 19\n",
      "0.8208333333333333\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.8134057971014492\n",
      "batch_idx: 23\n",
      "0.8125\n",
      "batch_idx: 24\n",
      "0.815\n",
      "Training Epoch: 71, total loss: 43.870225\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.8186274509803921\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.81875\n",
      "batch_idx: 20\n",
      "0.8154761904761905\n",
      "batch_idx: 21\n",
      "0.821969696969697\n",
      "batch_idx: 22\n",
      "0.822463768115942\n",
      "batch_idx: 23\n",
      "0.8211805555555556\n",
      "batch_idx: 24\n",
      "0.8216666666666667\n",
      "Training Epoch: 72, total loss: 43.709574\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.7916666666666666\n",
      "batch_idx: 13\n",
      "0.7946428571428571\n",
      "batch_idx: 14\n",
      "0.7944444444444444\n",
      "batch_idx: 15\n",
      "0.7916666666666666\n",
      "batch_idx: 16\n",
      "0.7892156862745098\n",
      "batch_idx: 17\n",
      "0.7939814814814815\n",
      "batch_idx: 18\n",
      "0.7807017543859649\n",
      "batch_idx: 19\n",
      "0.7833333333333333\n",
      "batch_idx: 20\n",
      "0.7837301587301587\n",
      "batch_idx: 21\n",
      "0.7859848484848485\n",
      "batch_idx: 22\n",
      "0.7880434782608695\n",
      "batch_idx: 23\n",
      "0.7934027777777778\n",
      "batch_idx: 24\n",
      "0.7983333333333333\n",
      "Training Epoch: 73, total loss: 44.038085\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8303571428571429\n",
      "batch_idx: 14\n",
      "0.8305555555555556\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.8308823529411765\n",
      "batch_idx: 17\n",
      "0.8356481481481481\n",
      "batch_idx: 18\n",
      "0.831140350877193\n",
      "batch_idx: 19\n",
      "0.8333333333333334\n",
      "batch_idx: 20\n",
      "0.8293650793650794\n",
      "batch_idx: 21\n",
      "0.8314393939393939\n",
      "batch_idx: 22\n",
      "0.8315217391304348\n",
      "batch_idx: 23\n",
      "0.8333333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 24\n",
      "0.8333333333333334\n",
      "Training Epoch: 74, total loss: 43.395806\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.8359375\n",
      "batch_idx: 16\n",
      "0.8308823529411765\n",
      "batch_idx: 17\n",
      "0.8310185185185185\n",
      "batch_idx: 18\n",
      "0.8245614035087719\n",
      "batch_idx: 19\n",
      "0.8270833333333333\n",
      "batch_idx: 20\n",
      "0.8234126984126984\n",
      "batch_idx: 21\n",
      "0.8200757575757576\n",
      "batch_idx: 22\n",
      "0.8170289855072463\n",
      "batch_idx: 23\n",
      "0.8159722222222222\n",
      "batch_idx: 24\n",
      "0.81\n",
      "Training Epoch: 75, total loss: 43.871760\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8044871794871795\n",
      "batch_idx: 13\n",
      "0.7946428571428571\n",
      "batch_idx: 14\n",
      "0.7944444444444444\n",
      "batch_idx: 15\n",
      "0.7916666666666666\n",
      "batch_idx: 16\n",
      "0.7916666666666666\n",
      "batch_idx: 17\n",
      "0.7939814814814815\n",
      "batch_idx: 18\n",
      "0.8004385964912281\n",
      "batch_idx: 19\n",
      "0.7958333333333333\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.8049242424242424\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.8055555555555556\n",
      "batch_idx: 24\n",
      "0.8083333333333333\n",
      "Training Epoch: 76, total loss: 44.038534\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.8014705882352942\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8083333333333333\n",
      "batch_idx: 20\n",
      "0.8134920634920635\n",
      "batch_idx: 21\n",
      "0.8143939393939394\n",
      "batch_idx: 22\n",
      "0.8134057971014492\n",
      "batch_idx: 23\n",
      "0.8125\n",
      "batch_idx: 24\n",
      "0.815\n",
      "Training Epoch: 77, total loss: 43.827303\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8179824561403509\n",
      "batch_idx: 19\n",
      "0.8145833333333333\n",
      "batch_idx: 20\n",
      "0.8154761904761905\n",
      "batch_idx: 21\n",
      "0.8162878787878788\n",
      "batch_idx: 22\n",
      "0.8170289855072463\n",
      "batch_idx: 23\n",
      "0.8177083333333334\n",
      "batch_idx: 24\n",
      "0.8183333333333334\n",
      "Training Epoch: 78, total loss: 43.733743\n",
      "batch_idx: 0\n",
      "1.0\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8055555555555556\n",
      "batch_idx: 15\n",
      "0.7942708333333334\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.7939814814814815\n",
      "batch_idx: 18\n",
      "0.7960526315789473\n",
      "batch_idx: 19\n",
      "0.7958333333333333\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.7954545454545454\n",
      "batch_idx: 22\n",
      "0.7916666666666666\n",
      "batch_idx: 23\n",
      "0.7951388888888888\n",
      "batch_idx: 24\n",
      "0.7916666666666666\n",
      "Training Epoch: 79, total loss: 44.007171\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8098958333333334\n",
      "batch_idx: 16\n",
      "0.8088235294117647\n",
      "batch_idx: 17\n",
      "0.8055555555555556\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.7996031746031746\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.802536231884058\n",
      "batch_idx: 23\n",
      "0.8038194444444444\n",
      "batch_idx: 24\n",
      "0.8016666666666666\n",
      "Training Epoch: 80, total loss: 43.900710\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8269230769230769\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8222222222222222\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.81875\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.8125\n",
      "batch_idx: 22\n",
      "0.8134057971014492\n",
      "batch_idx: 23\n",
      "0.8072916666666666\n",
      "batch_idx: 24\n",
      "0.8083333333333333\n",
      "Training Epoch: 81, total loss: 43.801978\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.7552083333333334\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.7724358974358975\n",
      "batch_idx: 13\n",
      "0.7648809523809523\n",
      "batch_idx: 14\n",
      "0.7666666666666667\n",
      "batch_idx: 15\n",
      "0.765625\n",
      "batch_idx: 16\n",
      "0.7696078431372549\n",
      "batch_idx: 17\n",
      "0.7754629629629629\n",
      "batch_idx: 18\n",
      "0.7719298245614035\n",
      "batch_idx: 19\n",
      "0.7708333333333334\n",
      "batch_idx: 20\n",
      "0.7738095238095238\n",
      "batch_idx: 21\n",
      "0.7765151515151515\n",
      "batch_idx: 22\n",
      "0.7753623188405797\n",
      "batch_idx: 23\n",
      "0.7777777777777778\n",
      "batch_idx: 24\n",
      "0.785\n",
      "Training Epoch: 82, total loss: 44.191738\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.8088235294117647\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8114035087719298\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.8068181818181818\n",
      "batch_idx: 22\n",
      "0.8097826086956522\n",
      "batch_idx: 23\n",
      "0.8125\n",
      "batch_idx: 24\n",
      "0.8083333333333333\n",
      "Training Epoch: 83, total loss: 43.594386\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8303571428571429\n",
      "batch_idx: 14\n",
      "0.825\n",
      "batch_idx: 15\n",
      "0.828125\n",
      "batch_idx: 16\n",
      "0.8259803921568627\n",
      "batch_idx: 17\n",
      "0.8263888888888888\n",
      "batch_idx: 18\n",
      "0.8289473684210527\n",
      "batch_idx: 19\n",
      "0.83125\n",
      "batch_idx: 20\n",
      "0.8313492063492064\n",
      "batch_idx: 21\n",
      "0.8276515151515151\n",
      "batch_idx: 22\n",
      "0.8260869565217391\n",
      "batch_idx: 23\n",
      "0.8246527777777778\n",
      "batch_idx: 24\n",
      "0.8233333333333334\n",
      "Training Epoch: 84, total loss: 43.681755\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7447916666666666\n",
      "batch_idx: 8\n",
      "0.7592592592592593\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7689393939393939\n",
      "batch_idx: 11\n",
      "0.7777777777777778\n",
      "batch_idx: 12\n",
      "0.7724358974358975\n",
      "batch_idx: 13\n",
      "0.7648809523809523\n",
      "batch_idx: 14\n",
      "0.7777777777777778\n",
      "batch_idx: 15\n",
      "0.7760416666666666\n",
      "batch_idx: 16\n",
      "0.7818627450980392\n",
      "batch_idx: 17\n",
      "0.7824074074074074\n",
      "batch_idx: 18\n",
      "0.7828947368421053\n",
      "batch_idx: 19\n",
      "0.7770833333333333\n",
      "batch_idx: 20\n",
      "0.7817460317460317\n",
      "batch_idx: 21\n",
      "0.7821969696969697\n",
      "batch_idx: 22\n",
      "0.7807971014492754\n",
      "batch_idx: 23\n",
      "0.78125\n",
      "batch_idx: 24\n",
      "0.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 85, total loss: 44.094308\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8269230769230769\n",
      "batch_idx: 13\n",
      "0.8303571428571429\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8114035087719298\n",
      "batch_idx: 19\n",
      "0.8145833333333333\n",
      "batch_idx: 20\n",
      "0.8174603174603174\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.8188405797101449\n",
      "batch_idx: 23\n",
      "0.8211805555555556\n",
      "batch_idx: 24\n",
      "0.8233333333333334\n",
      "Training Epoch: 86, total loss: 43.333304\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8257575757575758\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.8186274509803921\n",
      "batch_idx: 17\n",
      "0.8217592592592593\n",
      "batch_idx: 18\n",
      "0.8245614035087719\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.8238636363636364\n",
      "batch_idx: 22\n",
      "0.822463768115942\n",
      "batch_idx: 23\n",
      "0.8229166666666666\n",
      "batch_idx: 24\n",
      "0.8216666666666667\n",
      "Training Epoch: 87, total loss: 43.449594\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.8005952380952381\n",
      "batch_idx: 14\n",
      "0.8\n",
      "batch_idx: 15\n",
      "0.7994791666666666\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8095238095238095\n",
      "batch_idx: 21\n",
      "0.8125\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8142361111111112\n",
      "batch_idx: 24\n",
      "0.8166666666666667\n",
      "Training Epoch: 88, total loss: 43.378945\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8\n",
      "batch_idx: 15\n",
      "0.796875\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.8026315789473685\n",
      "batch_idx: 19\n",
      "0.8041666666666667\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.8087121212121212\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.8055555555555556\n",
      "batch_idx: 24\n",
      "0.81\n",
      "Training Epoch: 89, total loss: 43.652462\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8095238095238095\n",
      "batch_idx: 21\n",
      "0.8125\n",
      "batch_idx: 22\n",
      "0.8134057971014492\n",
      "batch_idx: 23\n",
      "0.8090277777777778\n",
      "batch_idx: 24\n",
      "0.8066666666666666\n",
      "Training Epoch: 90, total loss: 43.685183\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8044871794871795\n",
      "batch_idx: 13\n",
      "0.8154761904761905\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8217592592592593\n",
      "batch_idx: 18\n",
      "0.8289473684210527\n",
      "batch_idx: 19\n",
      "0.83125\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.8152173913043478\n",
      "batch_idx: 23\n",
      "0.8125\n",
      "batch_idx: 24\n",
      "0.8083333333333333\n",
      "Training Epoch: 91, total loss: 43.852901\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.8044871794871795\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.7944444444444444\n",
      "batch_idx: 15\n",
      "0.7994791666666666\n",
      "batch_idx: 16\n",
      "0.7941176470588235\n",
      "batch_idx: 17\n",
      "0.7986111111111112\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.7996031746031746\n",
      "batch_idx: 21\n",
      "0.7954545454545454\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.8003472222222222\n",
      "batch_idx: 24\n",
      "0.8016666666666666\n",
      "Training Epoch: 92, total loss: 43.866489\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.7361111111111112\n",
      "batch_idx: 6\n",
      "0.7440476190476191\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7592592592592593\n",
      "batch_idx: 9\n",
      "0.7458333333333333\n",
      "batch_idx: 10\n",
      "0.7613636363636364\n",
      "batch_idx: 11\n",
      "0.7673611111111112\n",
      "batch_idx: 12\n",
      "0.7628205128205128\n",
      "batch_idx: 13\n",
      "0.7738095238095238\n",
      "batch_idx: 14\n",
      "0.7666666666666667\n",
      "batch_idx: 15\n",
      "0.7760416666666666\n",
      "batch_idx: 16\n",
      "0.7867647058823529\n",
      "batch_idx: 17\n",
      "0.7847222222222222\n",
      "batch_idx: 18\n",
      "0.7872807017543859\n",
      "batch_idx: 19\n",
      "0.7875\n",
      "batch_idx: 20\n",
      "0.7857142857142857\n",
      "batch_idx: 21\n",
      "0.7897727272727273\n",
      "batch_idx: 22\n",
      "0.7880434782608695\n",
      "batch_idx: 23\n",
      "0.7916666666666666\n",
      "batch_idx: 24\n",
      "0.7933333333333333\n",
      "Training Epoch: 93, total loss: 43.888137\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.7202380952380952\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7453703703703703\n",
      "batch_idx: 9\n",
      "0.75\n",
      "batch_idx: 10\n",
      "0.7575757575757576\n",
      "batch_idx: 11\n",
      "0.7638888888888888\n",
      "batch_idx: 12\n",
      "0.7628205128205128\n",
      "batch_idx: 13\n",
      "0.7648809523809523\n",
      "batch_idx: 14\n",
      "0.7694444444444445\n",
      "batch_idx: 15\n",
      "0.7630208333333334\n",
      "batch_idx: 16\n",
      "0.7647058823529411\n",
      "batch_idx: 17\n",
      "0.7662037037037037\n",
      "batch_idx: 18\n",
      "0.7653508771929824\n",
      "batch_idx: 19\n",
      "0.7604166666666666\n",
      "batch_idx: 20\n",
      "0.7619047619047619\n",
      "batch_idx: 21\n",
      "0.7651515151515151\n",
      "batch_idx: 22\n",
      "0.7717391304347826\n",
      "batch_idx: 23\n",
      "0.7673611111111112\n",
      "batch_idx: 24\n",
      "0.7683333333333333\n",
      "Training Epoch: 94, total loss: 44.073702\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8363095238095238\n",
      "batch_idx: 14\n",
      "0.8416666666666667\n",
      "batch_idx: 15\n",
      "0.8411458333333334\n",
      "batch_idx: 16\n",
      "0.8382352941176471\n",
      "batch_idx: 17\n",
      "0.8402777777777778\n",
      "batch_idx: 18\n",
      "0.8377192982456141\n",
      "batch_idx: 19\n",
      "0.83125\n",
      "batch_idx: 20\n",
      "0.8333333333333334\n",
      "batch_idx: 21\n",
      "0.8295454545454546\n",
      "batch_idx: 22\n",
      "0.8278985507246377\n",
      "batch_idx: 23\n",
      "0.828125\n",
      "batch_idx: 24\n",
      "0.8283333333333334\n",
      "Training Epoch: 95, total loss: 43.602246\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.7791666666666667\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7976190476190477\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.8088235294117647\n",
      "batch_idx: 17\n",
      "0.8055555555555556\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8134920634920635\n",
      "batch_idx: 21\n",
      "0.8106060606060606\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8107638888888888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 24\n",
      "0.8133333333333334\n",
      "Training Epoch: 96, total loss: 43.420962\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7767857142857143\n",
      "batch_idx: 14\n",
      "0.775\n",
      "batch_idx: 15\n",
      "0.7864583333333334\n",
      "batch_idx: 16\n",
      "0.7892156862745098\n",
      "batch_idx: 17\n",
      "0.7939814814814815\n",
      "batch_idx: 18\n",
      "0.7960526315789473\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.8015873015873016\n",
      "batch_idx: 21\n",
      "0.8106060606060606\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8090277777777778\n",
      "batch_idx: 24\n",
      "0.8083333333333333\n",
      "Training Epoch: 97, total loss: 43.684983\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8257575757575758\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.825\n",
      "batch_idx: 15\n",
      "0.828125\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8287037037037037\n",
      "batch_idx: 18\n",
      "0.8289473684210527\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8234126984126984\n",
      "batch_idx: 21\n",
      "0.8257575757575758\n",
      "batch_idx: 22\n",
      "0.8278985507246377\n",
      "batch_idx: 23\n",
      "0.8246527777777778\n",
      "batch_idx: 24\n",
      "0.825\n",
      "Training Epoch: 98, total loss: 43.431206\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.9166666666666666\n",
      "batch_idx: 5\n",
      "0.9027777777777778\n",
      "batch_idx: 6\n",
      "0.8988095238095238\n",
      "batch_idx: 7\n",
      "0.890625\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8636363636363636\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.8621794871794872\n",
      "batch_idx: 13\n",
      "0.8630952380952381\n",
      "batch_idx: 14\n",
      "0.8555555555555555\n",
      "batch_idx: 15\n",
      "0.8541666666666666\n",
      "batch_idx: 16\n",
      "0.8553921568627451\n",
      "batch_idx: 17\n",
      "0.8518518518518519\n",
      "batch_idx: 18\n",
      "0.8464912280701754\n",
      "batch_idx: 19\n",
      "0.8375\n",
      "batch_idx: 20\n",
      "0.8353174603174603\n",
      "batch_idx: 21\n",
      "0.8333333333333334\n",
      "batch_idx: 22\n",
      "0.8387681159420289\n",
      "batch_idx: 23\n",
      "0.8420138888888888\n",
      "batch_idx: 24\n",
      "0.8383333333333334\n",
      "Training Epoch: 99, total loss: 43.003692\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n",
      "batch_idx: 11\n",
      "0.8541666666666666\n",
      "batch_idx: 12\n",
      "0.8525641025641025\n",
      "batch_idx: 13\n",
      "0.8571428571428571\n",
      "batch_idx: 14\n",
      "0.8527777777777777\n",
      "batch_idx: 15\n",
      "0.84375\n",
      "batch_idx: 16\n",
      "0.8333333333333334\n",
      "batch_idx: 17\n",
      "0.8310185185185185\n",
      "batch_idx: 18\n",
      "0.8333333333333334\n",
      "batch_idx: 19\n",
      "0.8333333333333334\n",
      "batch_idx: 20\n",
      "0.8353174603174603\n",
      "batch_idx: 21\n",
      "0.8352272727272727\n",
      "batch_idx: 22\n",
      "0.8405797101449275\n",
      "batch_idx: 23\n",
      "0.84375\n",
      "batch_idx: 24\n",
      "0.8383333333333334\n",
      "Training Epoch: 100, total loss: 43.214379\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8564814814814815\n",
      "batch_idx: 9\n",
      "0.8625\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.8541666666666666\n",
      "batch_idx: 12\n",
      "0.8525641025641025\n",
      "batch_idx: 13\n",
      "0.8363095238095238\n",
      "batch_idx: 14\n",
      "0.8444444444444444\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8480392156862745\n",
      "batch_idx: 17\n",
      "0.8472222222222222\n",
      "batch_idx: 18\n",
      "0.8442982456140351\n",
      "batch_idx: 19\n",
      "0.84375\n",
      "batch_idx: 20\n",
      "0.8452380952380952\n",
      "batch_idx: 21\n",
      "0.8465909090909091\n",
      "batch_idx: 22\n",
      "0.8478260869565217\n",
      "batch_idx: 23\n",
      "0.8420138888888888\n",
      "batch_idx: 24\n",
      "0.8433333333333334\n",
      "Training Epoch: 101, total loss: 43.124067\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8244047619047619\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8179824561403509\n",
      "batch_idx: 19\n",
      "0.81875\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.821969696969697\n",
      "batch_idx: 22\n",
      "0.8206521739130435\n",
      "batch_idx: 23\n",
      "0.8211805555555556\n",
      "batch_idx: 24\n",
      "0.8266666666666667\n",
      "Training Epoch: 102, total loss: 43.316116\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8301282051282052\n",
      "batch_idx: 13\n",
      "0.8363095238095238\n",
      "batch_idx: 14\n",
      "0.825\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.7979166666666667\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.803030303030303\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.8020833333333334\n",
      "batch_idx: 24\n",
      "0.7983333333333333\n",
      "Training Epoch: 103, total loss: 43.416816\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.7857142857142857\n",
      "batch_idx: 14\n",
      "0.7888888888888889\n",
      "batch_idx: 15\n",
      "0.7838541666666666\n",
      "batch_idx: 16\n",
      "0.7794117647058824\n",
      "batch_idx: 17\n",
      "0.7754629629629629\n",
      "batch_idx: 18\n",
      "0.7741228070175439\n",
      "batch_idx: 19\n",
      "0.7770833333333333\n",
      "batch_idx: 20\n",
      "0.7837301587301587\n",
      "batch_idx: 21\n",
      "0.7859848484848485\n",
      "batch_idx: 22\n",
      "0.7862318840579711\n",
      "batch_idx: 23\n",
      "0.7864583333333334\n",
      "batch_idx: 24\n",
      "0.795\n",
      "Training Epoch: 104, total loss: 43.748045\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8269230769230769\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8217592592592593\n",
      "batch_idx: 18\n",
      "0.8245614035087719\n",
      "batch_idx: 19\n",
      "0.8270833333333333\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.8276515151515151\n",
      "batch_idx: 22\n",
      "0.8170289855072463\n",
      "batch_idx: 23\n",
      "0.8055555555555556\n",
      "batch_idx: 24\n",
      "0.805\n",
      "Training Epoch: 105, total loss: 43.456148\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.7916666666666666\n",
      "batch_idx: 13\n",
      "0.7916666666666666\n",
      "batch_idx: 14\n",
      "0.7888888888888889\n",
      "batch_idx: 15\n",
      "0.78125\n",
      "batch_idx: 16\n",
      "0.7867647058823529\n",
      "batch_idx: 17\n",
      "0.7847222222222222\n",
      "batch_idx: 18\n",
      "0.7872807017543859\n",
      "batch_idx: 19\n",
      "0.7875\n",
      "batch_idx: 20\n",
      "0.7936507936507936\n",
      "batch_idx: 21\n",
      "0.7973484848484849\n",
      "batch_idx: 22\n",
      "0.802536231884058\n",
      "batch_idx: 23\n",
      "0.8072916666666666\n",
      "batch_idx: 24\n",
      "0.8133333333333334\n",
      "Training Epoch: 106, total loss: 43.396359\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.8541666666666666\n",
      "batch_idx: 12\n",
      "0.8589743589743589\n",
      "batch_idx: 13\n",
      "0.8422619047619048\n",
      "batch_idx: 14\n",
      "0.8333333333333334\n",
      "batch_idx: 15\n",
      "0.8385416666666666\n",
      "batch_idx: 16\n",
      "0.8333333333333334\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.8245614035087719\n",
      "batch_idx: 19\n",
      "0.8229166666666666\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 21\n",
      "0.8276515151515151\n",
      "batch_idx: 22\n",
      "0.8242753623188406\n",
      "batch_idx: 23\n",
      "0.8194444444444444\n",
      "batch_idx: 24\n",
      "0.8183333333333334\n",
      "Training Epoch: 107, total loss: 43.397306\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8257575757575758\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8301282051282052\n",
      "batch_idx: 13\n",
      "0.8333333333333334\n",
      "batch_idx: 14\n",
      "0.8333333333333334\n",
      "batch_idx: 15\n",
      "0.8333333333333334\n",
      "batch_idx: 16\n",
      "0.8308823529411765\n",
      "batch_idx: 17\n",
      "0.8310185185185185\n",
      "batch_idx: 18\n",
      "0.8289473684210527\n",
      "batch_idx: 19\n",
      "0.8333333333333334\n",
      "batch_idx: 20\n",
      "0.8333333333333334\n",
      "batch_idx: 21\n",
      "0.8314393939393939\n",
      "batch_idx: 22\n",
      "0.8278985507246377\n",
      "batch_idx: 23\n",
      "0.8229166666666666\n",
      "batch_idx: 24\n",
      "0.8233333333333334\n",
      "Training Epoch: 108, total loss: 43.229425\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8\n",
      "batch_idx: 15\n",
      "0.796875\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.7996031746031746\n",
      "batch_idx: 21\n",
      "0.7992424242424242\n",
      "batch_idx: 22\n",
      "0.7971014492753623\n",
      "batch_idx: 23\n",
      "0.7986111111111112\n",
      "batch_idx: 24\n",
      "0.8\n",
      "Training Epoch: 109, total loss: 43.808288\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8303571428571429\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8114035087719298\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8154761904761905\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.8206521739130435\n",
      "batch_idx: 23\n",
      "0.8159722222222222\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 110, total loss: 43.550211\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.8525641025641025\n",
      "batch_idx: 13\n",
      "0.8541666666666666\n",
      "batch_idx: 14\n",
      "0.8555555555555555\n",
      "batch_idx: 15\n",
      "0.8489583333333334\n",
      "batch_idx: 16\n",
      "0.8357843137254902\n",
      "batch_idx: 17\n",
      "0.8402777777777778\n",
      "batch_idx: 18\n",
      "0.8399122807017544\n",
      "batch_idx: 19\n",
      "0.8395833333333333\n",
      "batch_idx: 20\n",
      "0.8333333333333334\n",
      "batch_idx: 21\n",
      "0.8333333333333334\n",
      "batch_idx: 22\n",
      "0.8333333333333334\n",
      "batch_idx: 23\n",
      "0.8315972222222222\n",
      "batch_idx: 24\n",
      "0.8316666666666667\n",
      "Training Epoch: 111, total loss: 43.117332\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.7976190476190477\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.8063725490196079\n",
      "batch_idx: 17\n",
      "0.8055555555555556\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.8075396825396826\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.802536231884058\n",
      "batch_idx: 23\n",
      "0.7986111111111112\n",
      "batch_idx: 24\n",
      "0.7983333333333333\n",
      "Training Epoch: 112, total loss: 43.531043\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.8044871794871795\n",
      "batch_idx: 13\n",
      "0.8005952380952381\n",
      "batch_idx: 14\n",
      "0.8\n",
      "batch_idx: 15\n",
      "0.796875\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.7986111111111112\n",
      "batch_idx: 18\n",
      "0.7960526315789473\n",
      "batch_idx: 19\n",
      "0.79375\n",
      "batch_idx: 20\n",
      "0.7956349206349206\n",
      "batch_idx: 21\n",
      "0.803030303030303\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8038194444444444\n",
      "batch_idx: 24\n",
      "0.8033333333333333\n",
      "Training Epoch: 113, total loss: 43.568890\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.8365384615384616\n",
      "batch_idx: 13\n",
      "0.8363095238095238\n",
      "batch_idx: 14\n",
      "0.8333333333333334\n",
      "batch_idx: 15\n",
      "0.8333333333333334\n",
      "batch_idx: 16\n",
      "0.8333333333333334\n",
      "batch_idx: 17\n",
      "0.8379629629629629\n",
      "batch_idx: 18\n",
      "0.8333333333333334\n",
      "batch_idx: 19\n",
      "0.8291666666666667\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.8134057971014492\n",
      "batch_idx: 23\n",
      "0.8125\n",
      "batch_idx: 24\n",
      "0.815\n",
      "Training Epoch: 114, total loss: 43.469391\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8472222222222222\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.825\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8214285714285714\n",
      "batch_idx: 21\n",
      "0.821969696969697\n",
      "batch_idx: 22\n",
      "0.8188405797101449\n",
      "batch_idx: 23\n",
      "0.8246527777777778\n",
      "batch_idx: 24\n",
      "0.82\n",
      "Training Epoch: 115, total loss: 43.286813\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.7948717948717948\n",
      "batch_idx: 13\n",
      "0.7886904761904762\n",
      "batch_idx: 14\n",
      "0.7861111111111111\n",
      "batch_idx: 15\n",
      "0.7916666666666666\n",
      "batch_idx: 16\n",
      "0.7941176470588235\n",
      "batch_idx: 17\n",
      "0.7939814814814815\n",
      "batch_idx: 18\n",
      "0.7916666666666666\n",
      "batch_idx: 19\n",
      "0.7916666666666666\n",
      "batch_idx: 20\n",
      "0.7996031746031746\n",
      "batch_idx: 21\n",
      "0.8087121212121212\n",
      "batch_idx: 22\n",
      "0.8097826086956522\n",
      "batch_idx: 23\n",
      "0.8072916666666666\n",
      "batch_idx: 24\n",
      "0.81\n",
      "Training Epoch: 116, total loss: 43.397745\n",
      "batch_idx: 0\n",
      "1.0\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.8916666666666667\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8298611111111112\n",
      "batch_idx: 12\n",
      "0.8301282051282052\n",
      "batch_idx: 13\n",
      "0.8303571428571429\n",
      "batch_idx: 14\n",
      "0.8333333333333334\n",
      "batch_idx: 15\n",
      "0.8333333333333334\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8263888888888888\n",
      "batch_idx: 18\n",
      "0.8267543859649122\n",
      "batch_idx: 19\n",
      "0.8270833333333333\n",
      "batch_idx: 20\n",
      "0.8293650793650794\n",
      "batch_idx: 21\n",
      "0.8314393939393939\n",
      "batch_idx: 22\n",
      "0.8369565217391305\n",
      "batch_idx: 23\n",
      "0.8368055555555556\n",
      "batch_idx: 24\n",
      "0.835\n",
      "Training Epoch: 117, total loss: 43.069432\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8044871794871795\n",
      "batch_idx: 13\n",
      "0.8005952380952381\n",
      "batch_idx: 14\n",
      "0.8055555555555556\n",
      "batch_idx: 15\n",
      "0.8020833333333334\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.8101851851851852\n",
      "batch_idx: 18\n",
      "0.8070175438596491\n",
      "batch_idx: 19\n",
      "0.8083333333333333\n",
      "batch_idx: 20\n",
      "0.8134920634920635\n",
      "batch_idx: 21\n",
      "0.8143939393939394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 22\n",
      "0.8170289855072463\n",
      "batch_idx: 23\n",
      "0.8177083333333334\n",
      "batch_idx: 24\n",
      "0.8216666666666667\n",
      "Training Epoch: 118, total loss: 43.158746\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.8411458333333334\n",
      "batch_idx: 16\n",
      "0.8431372549019608\n",
      "batch_idx: 17\n",
      "0.8425925925925926\n",
      "batch_idx: 18\n",
      "0.8333333333333334\n",
      "batch_idx: 19\n",
      "0.8395833333333333\n",
      "batch_idx: 20\n",
      "0.8412698412698413\n",
      "batch_idx: 21\n",
      "0.8371212121212122\n",
      "batch_idx: 22\n",
      "0.8351449275362319\n",
      "batch_idx: 23\n",
      "0.8263888888888888\n",
      "batch_idx: 24\n",
      "0.83\n",
      "Training Epoch: 119, total loss: 43.310351\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.8186274509803921\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.8267543859649122\n",
      "batch_idx: 19\n",
      "0.8333333333333334\n",
      "batch_idx: 20\n",
      "0.8313492063492064\n",
      "batch_idx: 21\n",
      "0.8276515151515151\n",
      "batch_idx: 22\n",
      "0.822463768115942\n",
      "batch_idx: 23\n",
      "0.8263888888888888\n",
      "batch_idx: 24\n",
      "0.8283333333333334\n",
      "Training Epoch: 120, total loss: 43.019213\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.7951388888888888\n",
      "batch_idx: 12\n",
      "0.7916666666666666\n",
      "batch_idx: 13\n",
      "0.7857142857142857\n",
      "batch_idx: 14\n",
      "0.7861111111111111\n",
      "batch_idx: 15\n",
      "0.7890625\n",
      "batch_idx: 16\n",
      "0.7916666666666666\n",
      "batch_idx: 17\n",
      "0.7893518518518519\n",
      "batch_idx: 18\n",
      "0.7894736842105263\n",
      "batch_idx: 19\n",
      "0.7875\n",
      "batch_idx: 20\n",
      "0.7916666666666666\n",
      "batch_idx: 21\n",
      "0.7973484848484849\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.796875\n",
      "batch_idx: 24\n",
      "0.7916666666666666\n",
      "Training Epoch: 121, total loss: 43.633374\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.825\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.8259803921568627\n",
      "batch_idx: 17\n",
      "0.8333333333333334\n",
      "batch_idx: 18\n",
      "0.8355263157894737\n",
      "batch_idx: 19\n",
      "0.8333333333333334\n",
      "batch_idx: 20\n",
      "0.8333333333333334\n",
      "batch_idx: 21\n",
      "0.8314393939393939\n",
      "batch_idx: 22\n",
      "0.8315217391304348\n",
      "batch_idx: 23\n",
      "0.8298611111111112\n",
      "batch_idx: 24\n",
      "0.8316666666666667\n",
      "Training Epoch: 122, total loss: 43.112511\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8186274509803921\n",
      "batch_idx: 17\n",
      "0.8217592592592593\n",
      "batch_idx: 18\n",
      "0.831140350877193\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8273809523809523\n",
      "batch_idx: 21\n",
      "0.8238636363636364\n",
      "batch_idx: 22\n",
      "0.8260869565217391\n",
      "batch_idx: 23\n",
      "0.8263888888888888\n",
      "batch_idx: 24\n",
      "0.8266666666666667\n",
      "Training Epoch: 123, total loss: 43.196038\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8703703703703703\n",
      "batch_idx: 9\n",
      "0.8708333333333333\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.8557692307692307\n",
      "batch_idx: 13\n",
      "0.8511904761904762\n",
      "batch_idx: 14\n",
      "0.8555555555555555\n",
      "batch_idx: 15\n",
      "0.8515625\n",
      "batch_idx: 16\n",
      "0.8553921568627451\n",
      "batch_idx: 17\n",
      "0.8518518518518519\n",
      "batch_idx: 18\n",
      "0.8530701754385965\n",
      "batch_idx: 19\n",
      "0.85625\n",
      "batch_idx: 20\n",
      "0.8531746031746031\n",
      "batch_idx: 21\n",
      "0.8446969696969697\n",
      "batch_idx: 22\n",
      "0.842391304347826\n",
      "batch_idx: 23\n",
      "0.84375\n",
      "batch_idx: 24\n",
      "0.8433333333333334\n",
      "Training Epoch: 124, total loss: 42.907673\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8298611111111112\n",
      "batch_idx: 12\n",
      "0.8301282051282052\n",
      "batch_idx: 13\n",
      "0.8363095238095238\n",
      "batch_idx: 14\n",
      "0.825\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8137254901960784\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8223684210526315\n",
      "batch_idx: 19\n",
      "0.8208333333333333\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.821969696969697\n",
      "batch_idx: 22\n",
      "0.8242753623188406\n",
      "batch_idx: 23\n",
      "0.828125\n",
      "batch_idx: 24\n",
      "0.8283333333333334\n",
      "Training Epoch: 125, total loss: 43.175493\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8388888888888889\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8480392156862745\n",
      "batch_idx: 17\n",
      "0.8425925925925926\n",
      "batch_idx: 18\n",
      "0.8421052631578947\n",
      "batch_idx: 19\n",
      "0.84375\n",
      "batch_idx: 20\n",
      "0.8472222222222222\n",
      "batch_idx: 21\n",
      "0.8465909090909091\n",
      "batch_idx: 22\n",
      "0.8387681159420289\n",
      "batch_idx: 23\n",
      "0.8385416666666666\n",
      "batch_idx: 24\n",
      "0.8316666666666667\n",
      "Training Epoch: 126, total loss: 43.076701\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7878787878787878\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8179824561403509\n",
      "batch_idx: 19\n",
      "0.8208333333333333\n",
      "batch_idx: 20\n",
      "0.8234126984126984\n",
      "batch_idx: 21\n",
      "0.8238636363636364\n",
      "batch_idx: 22\n",
      "0.8297101449275363\n",
      "batch_idx: 23\n",
      "0.8298611111111112\n",
      "batch_idx: 24\n",
      "0.8316666666666667\n",
      "Training Epoch: 127, total loss: 42.983083\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8482142857142857\n",
      "batch_idx: 14\n",
      "0.8416666666666667\n",
      "batch_idx: 15\n",
      "0.8385416666666666\n",
      "batch_idx: 16\n",
      "0.8333333333333334\n",
      "batch_idx: 17\n",
      "0.8356481481481481\n",
      "batch_idx: 18\n",
      "0.8333333333333334\n",
      "batch_idx: 19\n",
      "0.8395833333333333\n",
      "batch_idx: 20\n",
      "0.8412698412698413\n",
      "batch_idx: 21\n",
      "0.8409090909090909\n",
      "batch_idx: 22\n",
      "0.842391304347826\n",
      "batch_idx: 23\n",
      "0.8385416666666666\n",
      "batch_idx: 24\n",
      "0.8383333333333334\n",
      "Training Epoch: 128, total loss: 42.822649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8186274509803921\n",
      "batch_idx: 17\n",
      "0.8217592592592593\n",
      "batch_idx: 18\n",
      "0.8223684210526315\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8293650793650794\n",
      "batch_idx: 21\n",
      "0.8314393939393939\n",
      "batch_idx: 22\n",
      "0.8260869565217391\n",
      "batch_idx: 23\n",
      "0.8177083333333334\n",
      "batch_idx: 24\n",
      "0.82\n",
      "Training Epoch: 129, total loss: 43.195231\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8333333333333334\n",
      "batch_idx: 14\n",
      "0.8305555555555556\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8259803921568627\n",
      "batch_idx: 17\n",
      "0.8287037037037037\n",
      "batch_idx: 18\n",
      "0.831140350877193\n",
      "batch_idx: 19\n",
      "0.8270833333333333\n",
      "batch_idx: 20\n",
      "0.8333333333333334\n",
      "batch_idx: 21\n",
      "0.8352272727272727\n",
      "batch_idx: 22\n",
      "0.8351449275362319\n",
      "batch_idx: 23\n",
      "0.8315972222222222\n",
      "batch_idx: 24\n",
      "0.8333333333333334\n",
      "Training Epoch: 130, total loss: 43.043814\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9166666666666666\n",
      "batch_idx: 4\n",
      "0.925\n",
      "batch_idx: 5\n",
      "0.9027777777777778\n",
      "batch_idx: 6\n",
      "0.8928571428571429\n",
      "batch_idx: 7\n",
      "0.890625\n",
      "batch_idx: 8\n",
      "0.8842592592592593\n",
      "batch_idx: 9\n",
      "0.8791666666666667\n",
      "batch_idx: 10\n",
      "0.8825757575757576\n",
      "batch_idx: 11\n",
      "0.8784722222222222\n",
      "batch_idx: 12\n",
      "0.875\n",
      "batch_idx: 13\n",
      "0.875\n",
      "batch_idx: 14\n",
      "0.8805555555555555\n",
      "batch_idx: 15\n",
      "0.8802083333333334\n",
      "batch_idx: 16\n",
      "0.8774509803921569\n",
      "batch_idx: 17\n",
      "0.8819444444444444\n",
      "batch_idx: 18\n",
      "0.8728070175438597\n",
      "batch_idx: 19\n",
      "0.86875\n",
      "batch_idx: 20\n",
      "0.8650793650793651\n",
      "batch_idx: 21\n",
      "0.8636363636363636\n",
      "batch_idx: 22\n",
      "0.8623188405797102\n",
      "batch_idx: 23\n",
      "0.8611111111111112\n",
      "batch_idx: 24\n",
      "0.8516666666666667\n",
      "Training Epoch: 131, total loss: 42.775244\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8222222222222222\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.8083333333333333\n",
      "batch_idx: 20\n",
      "0.8115079365079365\n",
      "batch_idx: 21\n",
      "0.8143939393939394\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8125\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 132, total loss: 43.282830\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7948717948717948\n",
      "batch_idx: 13\n",
      "0.7916666666666666\n",
      "batch_idx: 14\n",
      "0.7833333333333333\n",
      "batch_idx: 15\n",
      "0.7890625\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.7986111111111112\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.7979166666666667\n",
      "batch_idx: 20\n",
      "0.7996031746031746\n",
      "batch_idx: 21\n",
      "0.7973484848484849\n",
      "batch_idx: 22\n",
      "0.7971014492753623\n",
      "batch_idx: 23\n",
      "0.7951388888888888\n",
      "batch_idx: 24\n",
      "0.795\n",
      "Training Epoch: 133, total loss: 43.326524\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.8557692307692307\n",
      "batch_idx: 13\n",
      "0.8452380952380952\n",
      "batch_idx: 14\n",
      "0.8527777777777777\n",
      "batch_idx: 15\n",
      "0.8515625\n",
      "batch_idx: 16\n",
      "0.8504901960784313\n",
      "batch_idx: 17\n",
      "0.8425925925925926\n",
      "batch_idx: 18\n",
      "0.8377192982456141\n",
      "batch_idx: 19\n",
      "0.8416666666666667\n",
      "batch_idx: 20\n",
      "0.8432539682539683\n",
      "batch_idx: 21\n",
      "0.8465909090909091\n",
      "batch_idx: 22\n",
      "0.842391304347826\n",
      "batch_idx: 23\n",
      "0.8454861111111112\n",
      "batch_idx: 24\n",
      "0.8466666666666667\n",
      "Training Epoch: 134, total loss: 42.557467\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.8186274509803921\n",
      "batch_idx: 17\n",
      "0.8263888888888888\n",
      "batch_idx: 18\n",
      "0.8223684210526315\n",
      "batch_idx: 19\n",
      "0.8291666666666667\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.8276515151515151\n",
      "batch_idx: 22\n",
      "0.8315217391304348\n",
      "batch_idx: 23\n",
      "0.8315972222222222\n",
      "batch_idx: 24\n",
      "0.8316666666666667\n",
      "Training Epoch: 135, total loss: 42.924733\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.7994791666666666\n",
      "batch_idx: 16\n",
      "0.8014705882352942\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.7979166666666667\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.803030303030303\n",
      "batch_idx: 22\n",
      "0.8043478260869565\n",
      "batch_idx: 23\n",
      "0.8055555555555556\n",
      "batch_idx: 24\n",
      "0.8066666666666666\n",
      "Training Epoch: 136, total loss: 43.048407\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7361111111111112\n",
      "batch_idx: 6\n",
      "0.7321428571428571\n",
      "batch_idx: 7\n",
      "0.7291666666666666\n",
      "batch_idx: 8\n",
      "0.75\n",
      "batch_idx: 9\n",
      "0.7708333333333334\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7886904761904762\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.8020833333333334\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.8055555555555556\n",
      "batch_idx: 18\n",
      "0.8004385964912281\n",
      "batch_idx: 19\n",
      "0.8041666666666667\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.8049242424242424\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.7934027777777778\n",
      "batch_idx: 24\n",
      "0.7983333333333333\n",
      "Training Epoch: 137, total loss: 43.429913\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.7870370370370371\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.7946428571428571\n",
      "batch_idx: 14\n",
      "0.8\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.8015873015873016\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.802536231884058\n",
      "batch_idx: 23\n",
      "0.8107638888888888\n",
      "batch_idx: 24\n",
      "0.815\n",
      "Training Epoch: 138, total loss: 43.098103\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8657407407407407\n",
      "batch_idx: 9\n",
      "0.875\n",
      "batch_idx: 10\n",
      "0.8825757575757576\n",
      "batch_idx: 11\n",
      "0.8854166666666666\n",
      "batch_idx: 12\n",
      "0.8878205128205128\n",
      "batch_idx: 13\n",
      "0.8720238095238095\n",
      "batch_idx: 14\n",
      "0.8583333333333333\n",
      "batch_idx: 15\n",
      "0.8567708333333334\n",
      "batch_idx: 16\n",
      "0.8578431372549019\n",
      "batch_idx: 17\n",
      "0.8587962962962963\n",
      "batch_idx: 18\n",
      "0.8508771929824561\n",
      "batch_idx: 19\n",
      "0.85625\n",
      "batch_idx: 20\n",
      "0.8591269841269841\n",
      "batch_idx: 21\n",
      "0.8560606060606061\n",
      "batch_idx: 22\n",
      "0.855072463768116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 23\n",
      "0.8559027777777778\n",
      "batch_idx: 24\n",
      "0.8566666666666667\n",
      "Training Epoch: 139, total loss: 42.564067\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8303571428571429\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.8186274509803921\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.8245614035087719\n",
      "batch_idx: 19\n",
      "0.8270833333333333\n",
      "batch_idx: 20\n",
      "0.8313492063492064\n",
      "batch_idx: 21\n",
      "0.8314393939393939\n",
      "batch_idx: 22\n",
      "0.8278985507246377\n",
      "batch_idx: 23\n",
      "0.828125\n",
      "batch_idx: 24\n",
      "0.8283333333333334\n",
      "Training Epoch: 140, total loss: 42.922401\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.8557692307692307\n",
      "batch_idx: 13\n",
      "0.8571428571428571\n",
      "batch_idx: 14\n",
      "0.8583333333333333\n",
      "batch_idx: 15\n",
      "0.8567708333333334\n",
      "batch_idx: 16\n",
      "0.8553921568627451\n",
      "batch_idx: 17\n",
      "0.8611111111111112\n",
      "batch_idx: 18\n",
      "0.8552631578947368\n",
      "batch_idx: 19\n",
      "0.85625\n",
      "batch_idx: 20\n",
      "0.8551587301587301\n",
      "batch_idx: 21\n",
      "0.8541666666666666\n",
      "batch_idx: 22\n",
      "0.8532608695652174\n",
      "batch_idx: 23\n",
      "0.8489583333333334\n",
      "batch_idx: 24\n",
      "0.8516666666666667\n",
      "Training Epoch: 141, total loss: 42.607953\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8298611111111112\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.825\n",
      "batch_idx: 15\n",
      "0.828125\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8263888888888888\n",
      "batch_idx: 18\n",
      "0.8245614035087719\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8234126984126984\n",
      "batch_idx: 21\n",
      "0.8238636363636364\n",
      "batch_idx: 22\n",
      "0.8278985507246377\n",
      "batch_idx: 23\n",
      "0.8315972222222222\n",
      "batch_idx: 24\n",
      "0.8266666666666667\n",
      "Training Epoch: 142, total loss: 43.019585\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9375\n",
      "batch_idx: 2\n",
      "0.9444444444444444\n",
      "batch_idx: 3\n",
      "0.9270833333333334\n",
      "batch_idx: 4\n",
      "0.9333333333333333\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "batch_idx: 6\n",
      "0.8988095238095238\n",
      "batch_idx: 7\n",
      "0.8854166666666666\n",
      "batch_idx: 8\n",
      "0.8842592592592593\n",
      "batch_idx: 9\n",
      "0.8958333333333334\n",
      "batch_idx: 10\n",
      "0.8939393939393939\n",
      "batch_idx: 11\n",
      "0.8958333333333334\n",
      "batch_idx: 12\n",
      "0.8846153846153846\n",
      "batch_idx: 13\n",
      "0.8898809523809523\n",
      "batch_idx: 14\n",
      "0.8777777777777778\n",
      "batch_idx: 15\n",
      "0.8671875\n",
      "batch_idx: 16\n",
      "0.8602941176470589\n",
      "batch_idx: 17\n",
      "0.8518518518518519\n",
      "batch_idx: 18\n",
      "0.8530701754385965\n",
      "batch_idx: 19\n",
      "0.84375\n",
      "batch_idx: 20\n",
      "0.8412698412698413\n",
      "batch_idx: 21\n",
      "0.8409090909090909\n",
      "batch_idx: 22\n",
      "0.8442028985507246\n",
      "batch_idx: 23\n",
      "0.8454861111111112\n",
      "batch_idx: 24\n",
      "0.85\n",
      "Training Epoch: 143, total loss: 42.828960\n",
      "batch_idx: 0\n",
      "1.0\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8452380952380952\n",
      "batch_idx: 14\n",
      "0.8472222222222222\n",
      "batch_idx: 15\n",
      "0.8515625\n",
      "batch_idx: 16\n",
      "0.8553921568627451\n",
      "batch_idx: 17\n",
      "0.8587962962962963\n",
      "batch_idx: 18\n",
      "0.8574561403508771\n",
      "batch_idx: 19\n",
      "0.8583333333333333\n",
      "batch_idx: 20\n",
      "0.8551587301587301\n",
      "batch_idx: 21\n",
      "0.8560606060606061\n",
      "batch_idx: 22\n",
      "0.8532608695652174\n",
      "batch_idx: 23\n",
      "0.8454861111111112\n",
      "batch_idx: 24\n",
      "0.8466666666666667\n",
      "Training Epoch: 144, total loss: 42.723796\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.825\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8223684210526315\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8214285714285714\n",
      "batch_idx: 21\n",
      "0.8200757575757576\n",
      "batch_idx: 22\n",
      "0.8152173913043478\n",
      "batch_idx: 23\n",
      "0.8107638888888888\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 145, total loss: 43.182522\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8269230769230769\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.8333333333333334\n",
      "batch_idx: 16\n",
      "0.8382352941176471\n",
      "batch_idx: 17\n",
      "0.8425925925925926\n",
      "batch_idx: 18\n",
      "0.8399122807017544\n",
      "batch_idx: 19\n",
      "0.8354166666666667\n",
      "batch_idx: 20\n",
      "0.8313492063492064\n",
      "batch_idx: 21\n",
      "0.8333333333333334\n",
      "batch_idx: 22\n",
      "0.8315217391304348\n",
      "batch_idx: 23\n",
      "0.8333333333333334\n",
      "batch_idx: 24\n",
      "0.835\n",
      "Training Epoch: 146, total loss: 42.906212\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9583333333333334\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.8958333333333334\n",
      "batch_idx: 6\n",
      "0.8809523809523809\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8564814814814815\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.8461538461538461\n",
      "batch_idx: 13\n",
      "0.8452380952380952\n",
      "batch_idx: 14\n",
      "0.8444444444444444\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8480392156862745\n",
      "batch_idx: 17\n",
      "0.8518518518518519\n",
      "batch_idx: 18\n",
      "0.8530701754385965\n",
      "batch_idx: 19\n",
      "0.85625\n",
      "batch_idx: 20\n",
      "0.8531746031746031\n",
      "batch_idx: 21\n",
      "0.8541666666666666\n",
      "batch_idx: 22\n",
      "0.8532608695652174\n",
      "batch_idx: 23\n",
      "0.8559027777777778\n",
      "batch_idx: 24\n",
      "0.8533333333333334\n",
      "Training Epoch: 147, total loss: 42.548730\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7870370370370371\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.7878787878787878\n",
      "batch_idx: 11\n",
      "0.7951388888888888\n",
      "batch_idx: 12\n",
      "0.8044871794871795\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8217592592592593\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.81875\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.821969696969697\n",
      "batch_idx: 22\n",
      "0.8242753623188406\n",
      "batch_idx: 23\n",
      "0.8246527777777778\n",
      "batch_idx: 24\n",
      "0.8233333333333334\n",
      "Training Epoch: 148, total loss: 43.090203\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.825\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.8259803921568627\n",
      "batch_idx: 17\n",
      "0.8310185185185185\n",
      "batch_idx: 18\n",
      "0.8289473684210527\n",
      "batch_idx: 19\n",
      "0.8333333333333334\n",
      "batch_idx: 20\n",
      "0.8313492063492064\n",
      "batch_idx: 21\n",
      "0.8352272727272727\n",
      "batch_idx: 22\n",
      "0.8333333333333334\n",
      "batch_idx: 23\n",
      "0.8315972222222222\n",
      "batch_idx: 24\n",
      "0.835\n",
      "Training Epoch: 149, total loss: 42.757857\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n",
      "batch_idx: 11\n",
      "0.8541666666666666\n",
      "batch_idx: 12\n",
      "0.8557692307692307\n",
      "batch_idx: 13\n",
      "0.8571428571428571\n",
      "batch_idx: 14\n",
      "0.8555555555555555\n",
      "batch_idx: 15\n",
      "0.8567708333333334\n",
      "batch_idx: 16\n",
      "0.8602941176470589\n",
      "batch_idx: 17\n",
      "0.8564814814814815\n",
      "batch_idx: 18\n",
      "0.8574561403508771\n",
      "batch_idx: 19\n",
      "0.8604166666666667\n",
      "batch_idx: 20\n",
      "0.8611111111111112\n",
      "batch_idx: 21\n",
      "0.865530303030303\n",
      "batch_idx: 22\n",
      "0.8586956521739131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 23\n",
      "0.8559027777777778\n",
      "batch_idx: 24\n",
      "0.8533333333333334\n",
      "Training Epoch: 150, total loss: 42.513214\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8564814814814815\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8452380952380952\n",
      "batch_idx: 14\n",
      "0.8444444444444444\n",
      "batch_idx: 15\n",
      "0.84375\n",
      "batch_idx: 16\n",
      "0.8382352941176471\n",
      "batch_idx: 17\n",
      "0.8379629629629629\n",
      "batch_idx: 18\n",
      "0.8421052631578947\n",
      "batch_idx: 19\n",
      "0.8375\n",
      "batch_idx: 20\n",
      "0.8432539682539683\n",
      "batch_idx: 21\n",
      "0.8484848484848485\n",
      "batch_idx: 22\n",
      "0.8478260869565217\n",
      "batch_idx: 23\n",
      "0.8489583333333334\n",
      "batch_idx: 24\n",
      "0.8533333333333334\n",
      "Training Epoch: 151, total loss: 42.520767\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8244047619047619\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8217592592592593\n",
      "batch_idx: 18\n",
      "0.8114035087719298\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8115079365079365\n",
      "batch_idx: 21\n",
      "0.8106060606060606\n",
      "batch_idx: 22\n",
      "0.8134057971014492\n",
      "batch_idx: 23\n",
      "0.8125\n",
      "batch_idx: 24\n",
      "0.815\n",
      "Training Epoch: 152, total loss: 43.029986\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.8611111111111112\n",
      "batch_idx: 12\n",
      "0.8557692307692307\n",
      "batch_idx: 13\n",
      "0.8511904761904762\n",
      "batch_idx: 14\n",
      "0.8527777777777777\n",
      "batch_idx: 15\n",
      "0.8541666666666666\n",
      "batch_idx: 16\n",
      "0.8553921568627451\n",
      "batch_idx: 17\n",
      "0.8495370370370371\n",
      "batch_idx: 18\n",
      "0.8486842105263158\n",
      "batch_idx: 19\n",
      "0.8541666666666666\n",
      "batch_idx: 20\n",
      "0.8492063492063492\n",
      "batch_idx: 21\n",
      "0.8390151515151515\n",
      "batch_idx: 22\n",
      "0.842391304347826\n",
      "batch_idx: 23\n",
      "0.8368055555555556\n",
      "batch_idx: 24\n",
      "0.8366666666666667\n",
      "Training Epoch: 153, total loss: 42.819903\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8564814814814815\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8301282051282052\n",
      "batch_idx: 13\n",
      "0.8363095238095238\n",
      "batch_idx: 14\n",
      "0.8416666666666667\n",
      "batch_idx: 15\n",
      "0.84375\n",
      "batch_idx: 16\n",
      "0.8455882352941176\n",
      "batch_idx: 17\n",
      "0.8425925925925926\n",
      "batch_idx: 18\n",
      "0.8421052631578947\n",
      "batch_idx: 19\n",
      "0.8416666666666667\n",
      "batch_idx: 20\n",
      "0.8373015873015873\n",
      "batch_idx: 21\n",
      "0.8352272727272727\n",
      "batch_idx: 22\n",
      "0.8351449275362319\n",
      "batch_idx: 23\n",
      "0.8333333333333334\n",
      "batch_idx: 24\n",
      "0.8366666666666667\n",
      "Training Epoch: 154, total loss: 42.683120\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8333333333333334\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8135964912280702\n",
      "batch_idx: 19\n",
      "0.81875\n",
      "batch_idx: 20\n",
      "0.8234126984126984\n",
      "batch_idx: 21\n",
      "0.8238636363636364\n",
      "batch_idx: 22\n",
      "0.8188405797101449\n",
      "batch_idx: 23\n",
      "0.8194444444444444\n",
      "batch_idx: 24\n",
      "0.8183333333333334\n",
      "Training Epoch: 155, total loss: 43.168927\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8472222222222222\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8482142857142857\n",
      "batch_idx: 14\n",
      "0.8333333333333334\n",
      "batch_idx: 15\n",
      "0.8359375\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.8223684210526315\n",
      "batch_idx: 19\n",
      "0.8270833333333333\n",
      "batch_idx: 20\n",
      "0.8234126984126984\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.822463768115942\n",
      "batch_idx: 23\n",
      "0.8177083333333334\n",
      "batch_idx: 24\n",
      "0.8216666666666667\n",
      "Training Epoch: 156, total loss: 43.115339\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7791666666666667\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7916666666666666\n",
      "batch_idx: 14\n",
      "0.7916666666666666\n",
      "batch_idx: 15\n",
      "0.7890625\n",
      "batch_idx: 16\n",
      "0.7867647058823529\n",
      "batch_idx: 17\n",
      "0.7916666666666666\n",
      "batch_idx: 18\n",
      "0.7872807017543859\n",
      "batch_idx: 19\n",
      "0.7916666666666666\n",
      "batch_idx: 20\n",
      "0.7896825396825397\n",
      "batch_idx: 21\n",
      "0.7916666666666666\n",
      "batch_idx: 22\n",
      "0.7916666666666666\n",
      "batch_idx: 23\n",
      "0.7951388888888888\n",
      "batch_idx: 24\n",
      "0.7983333333333333\n",
      "Training Epoch: 157, total loss: 43.534041\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8657407407407407\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8680555555555556\n",
      "batch_idx: 12\n",
      "0.8685897435897436\n",
      "batch_idx: 13\n",
      "0.8660714285714286\n",
      "batch_idx: 14\n",
      "0.8666666666666667\n",
      "batch_idx: 15\n",
      "0.8619791666666666\n",
      "batch_idx: 16\n",
      "0.8578431372549019\n",
      "batch_idx: 17\n",
      "0.8564814814814815\n",
      "batch_idx: 18\n",
      "0.8552631578947368\n",
      "batch_idx: 19\n",
      "0.8520833333333333\n",
      "batch_idx: 20\n",
      "0.8492063492063492\n",
      "batch_idx: 21\n",
      "0.8484848484848485\n",
      "batch_idx: 22\n",
      "0.8442028985507246\n",
      "batch_idx: 23\n",
      "0.8420138888888888\n",
      "batch_idx: 24\n",
      "0.8366666666666667\n",
      "Training Epoch: 158, total loss: 42.714563\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.8365384615384616\n",
      "batch_idx: 13\n",
      "0.8333333333333334\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.8359375\n",
      "batch_idx: 16\n",
      "0.8455882352941176\n",
      "batch_idx: 17\n",
      "0.8472222222222222\n",
      "batch_idx: 18\n",
      "0.8442982456140351\n",
      "batch_idx: 19\n",
      "0.8458333333333333\n",
      "batch_idx: 20\n",
      "0.8373015873015873\n",
      "batch_idx: 21\n",
      "0.8333333333333334\n",
      "batch_idx: 22\n",
      "0.8351449275362319\n",
      "batch_idx: 23\n",
      "0.8298611111111112\n",
      "batch_idx: 24\n",
      "0.83\n",
      "Training Epoch: 159, total loss: 42.818173\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8452380952380952\n",
      "batch_idx: 14\n",
      "0.8472222222222222\n",
      "batch_idx: 15\n",
      "0.8489583333333334\n",
      "batch_idx: 16\n",
      "0.8455882352941176\n",
      "batch_idx: 17\n",
      "0.8472222222222222\n",
      "batch_idx: 18\n",
      "0.8530701754385965\n",
      "batch_idx: 19\n",
      "0.85625\n",
      "batch_idx: 20\n",
      "0.8611111111111112\n",
      "batch_idx: 21\n",
      "0.8579545454545454\n",
      "batch_idx: 22\n",
      "0.855072463768116\n",
      "batch_idx: 23\n",
      "0.8559027777777778\n",
      "batch_idx: 24\n",
      "0.8583333333333333\n",
      "Training Epoch: 160, total loss: 42.326017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8179824561403509\n",
      "batch_idx: 19\n",
      "0.8166666666666667\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.8162878787878788\n",
      "batch_idx: 22\n",
      "0.8206521739130435\n",
      "batch_idx: 23\n",
      "0.8263888888888888\n",
      "batch_idx: 24\n",
      "0.825\n",
      "Training Epoch: 161, total loss: 42.938626\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.8611111111111112\n",
      "batch_idx: 12\n",
      "0.8653846153846154\n",
      "batch_idx: 13\n",
      "0.8660714285714286\n",
      "batch_idx: 14\n",
      "0.8611111111111112\n",
      "batch_idx: 15\n",
      "0.8619791666666666\n",
      "batch_idx: 16\n",
      "0.8578431372549019\n",
      "batch_idx: 17\n",
      "0.8587962962962963\n",
      "batch_idx: 18\n",
      "0.8574561403508771\n",
      "batch_idx: 19\n",
      "0.8541666666666666\n",
      "batch_idx: 20\n",
      "0.8531746031746031\n",
      "batch_idx: 21\n",
      "0.8541666666666666\n",
      "batch_idx: 22\n",
      "0.8514492753623188\n",
      "batch_idx: 23\n",
      "0.8541666666666666\n",
      "batch_idx: 24\n",
      "0.8566666666666667\n",
      "Training Epoch: 162, total loss: 42.545926\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8541666666666666\n",
      "batch_idx: 14\n",
      "0.8583333333333333\n",
      "batch_idx: 15\n",
      "0.8619791666666666\n",
      "batch_idx: 16\n",
      "0.8480392156862745\n",
      "batch_idx: 17\n",
      "0.8472222222222222\n",
      "batch_idx: 18\n",
      "0.8464912280701754\n",
      "batch_idx: 19\n",
      "0.8479166666666667\n",
      "batch_idx: 20\n",
      "0.8432539682539683\n",
      "batch_idx: 21\n",
      "0.8465909090909091\n",
      "batch_idx: 22\n",
      "0.8460144927536232\n",
      "batch_idx: 23\n",
      "0.8402777777777778\n",
      "batch_idx: 24\n",
      "0.835\n",
      "Training Epoch: 163, total loss: 42.827810\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n",
      "batch_idx: 11\n",
      "0.8472222222222222\n",
      "batch_idx: 12\n",
      "0.8589743589743589\n",
      "batch_idx: 13\n",
      "0.8541666666666666\n",
      "batch_idx: 14\n",
      "0.8555555555555555\n",
      "batch_idx: 15\n",
      "0.8515625\n",
      "batch_idx: 16\n",
      "0.8504901960784313\n",
      "batch_idx: 17\n",
      "0.8541666666666666\n",
      "batch_idx: 18\n",
      "0.8596491228070176\n",
      "batch_idx: 19\n",
      "0.8604166666666667\n",
      "batch_idx: 20\n",
      "0.8611111111111112\n",
      "batch_idx: 21\n",
      "0.8522727272727273\n",
      "batch_idx: 22\n",
      "0.8478260869565217\n",
      "batch_idx: 23\n",
      "0.8454861111111112\n",
      "batch_idx: 24\n",
      "0.8466666666666667\n",
      "Training Epoch: 164, total loss: 42.569991\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.9375\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.8888888888888888\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8461538461538461\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8333333333333334\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.8259803921568627\n",
      "batch_idx: 17\n",
      "0.8287037037037037\n",
      "batch_idx: 18\n",
      "0.8377192982456141\n",
      "batch_idx: 19\n",
      "0.8375\n",
      "batch_idx: 20\n",
      "0.8373015873015873\n",
      "batch_idx: 21\n",
      "0.8371212121212122\n",
      "batch_idx: 22\n",
      "0.8369565217391305\n",
      "batch_idx: 23\n",
      "0.8333333333333334\n",
      "batch_idx: 24\n",
      "0.8383333333333334\n",
      "Training Epoch: 165, total loss: 42.569034\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8708333333333333\n",
      "batch_idx: 10\n",
      "0.8825757575757576\n",
      "batch_idx: 11\n",
      "0.875\n",
      "batch_idx: 12\n",
      "0.8717948717948718\n",
      "batch_idx: 13\n",
      "0.875\n",
      "batch_idx: 14\n",
      "0.875\n",
      "batch_idx: 15\n",
      "0.8697916666666666\n",
      "batch_idx: 16\n",
      "0.8725490196078431\n",
      "batch_idx: 17\n",
      "0.8773148148148148\n",
      "batch_idx: 18\n",
      "0.881578947368421\n",
      "batch_idx: 19\n",
      "0.8791666666666667\n",
      "batch_idx: 20\n",
      "0.875\n",
      "batch_idx: 21\n",
      "0.875\n",
      "batch_idx: 22\n",
      "0.8731884057971014\n",
      "batch_idx: 23\n",
      "0.8628472222222222\n",
      "batch_idx: 24\n",
      "0.8566666666666667\n",
      "Training Epoch: 166, total loss: 42.289734\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8564814814814815\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8712121212121212\n",
      "batch_idx: 11\n",
      "0.8611111111111112\n",
      "batch_idx: 12\n",
      "0.8653846153846154\n",
      "batch_idx: 13\n",
      "0.8630952380952381\n",
      "batch_idx: 14\n",
      "0.8666666666666667\n",
      "batch_idx: 15\n",
      "0.8723958333333334\n",
      "batch_idx: 16\n",
      "0.8799019607843137\n",
      "batch_idx: 17\n",
      "0.8726851851851852\n",
      "batch_idx: 18\n",
      "0.8662280701754386\n",
      "batch_idx: 19\n",
      "0.8625\n",
      "batch_idx: 20\n",
      "0.8611111111111112\n",
      "batch_idx: 21\n",
      "0.8503787878787878\n",
      "batch_idx: 22\n",
      "0.8496376811594203\n",
      "batch_idx: 23\n",
      "0.8524305555555556\n",
      "batch_idx: 24\n",
      "0.8483333333333334\n",
      "Training Epoch: 167, total loss: 42.477531\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8888888888888888\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8657407407407407\n",
      "batch_idx: 9\n",
      "0.8625\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.8472222222222222\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8482142857142857\n",
      "batch_idx: 14\n",
      "0.8555555555555555\n",
      "batch_idx: 15\n",
      "0.8567708333333334\n",
      "batch_idx: 16\n",
      "0.8602941176470589\n",
      "batch_idx: 17\n",
      "0.8587962962962963\n",
      "batch_idx: 18\n",
      "0.8596491228070176\n",
      "batch_idx: 19\n",
      "0.85625\n",
      "batch_idx: 20\n",
      "0.8611111111111112\n",
      "batch_idx: 21\n",
      "0.8598484848484849\n",
      "batch_idx: 22\n",
      "0.8623188405797102\n",
      "batch_idx: 23\n",
      "0.8611111111111112\n",
      "batch_idx: 24\n",
      "0.865\n",
      "Training Epoch: 168, total loss: 42.185040\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8301282051282052\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.8333333333333334\n",
      "batch_idx: 16\n",
      "0.8308823529411765\n",
      "batch_idx: 17\n",
      "0.8310185185185185\n",
      "batch_idx: 18\n",
      "0.831140350877193\n",
      "batch_idx: 19\n",
      "0.8270833333333333\n",
      "batch_idx: 20\n",
      "0.8273809523809523\n",
      "batch_idx: 21\n",
      "0.8238636363636364\n",
      "batch_idx: 22\n",
      "0.822463768115942\n",
      "batch_idx: 23\n",
      "0.8211805555555556\n",
      "batch_idx: 24\n",
      "0.8216666666666667\n",
      "Training Epoch: 169, total loss: 42.662916\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.8359375\n",
      "batch_idx: 16\n",
      "0.8406862745098039\n",
      "batch_idx: 17\n",
      "0.8449074074074074\n",
      "batch_idx: 18\n",
      "0.8486842105263158\n",
      "batch_idx: 19\n",
      "0.8416666666666667\n",
      "batch_idx: 20\n",
      "0.8452380952380952\n",
      "batch_idx: 21\n",
      "0.8428030303030303\n",
      "batch_idx: 22\n",
      "0.842391304347826\n",
      "batch_idx: 23\n",
      "0.8402777777777778\n",
      "batch_idx: 24\n",
      "0.8416666666666667\n",
      "Training Epoch: 170, total loss: 42.591743\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.9583333333333334\n",
      "batch_idx: 2\n",
      "0.9305555555555556\n",
      "batch_idx: 3\n",
      "0.9270833333333334\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.8472222222222222\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8482142857142857\n",
      "batch_idx: 14\n",
      "0.8416666666666667\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8186274509803921\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.8223684210526315\n",
      "batch_idx: 19\n",
      "0.8229166666666666\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.821969696969697\n",
      "batch_idx: 22\n",
      "0.8242753623188406\n",
      "batch_idx: 23\n",
      "0.8246527777777778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 24\n",
      "0.8283333333333334\n",
      "Training Epoch: 171, total loss: 42.733896\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.8472222222222222\n",
      "batch_idx: 12\n",
      "0.8365384615384616\n",
      "batch_idx: 13\n",
      "0.8363095238095238\n",
      "batch_idx: 14\n",
      "0.8444444444444444\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8455882352941176\n",
      "batch_idx: 17\n",
      "0.8472222222222222\n",
      "batch_idx: 18\n",
      "0.8442982456140351\n",
      "batch_idx: 19\n",
      "0.8520833333333333\n",
      "batch_idx: 20\n",
      "0.8452380952380952\n",
      "batch_idx: 21\n",
      "0.8409090909090909\n",
      "batch_idx: 22\n",
      "0.8405797101449275\n",
      "batch_idx: 23\n",
      "0.8420138888888888\n",
      "batch_idx: 24\n",
      "0.84\n",
      "Training Epoch: 172, total loss: 42.433591\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.8385416666666666\n",
      "batch_idx: 16\n",
      "0.8357843137254902\n",
      "batch_idx: 17\n",
      "0.8356481481481481\n",
      "batch_idx: 18\n",
      "0.8377192982456141\n",
      "batch_idx: 19\n",
      "0.8333333333333334\n",
      "batch_idx: 20\n",
      "0.8353174603174603\n",
      "batch_idx: 21\n",
      "0.8371212121212122\n",
      "batch_idx: 22\n",
      "0.8387681159420289\n",
      "batch_idx: 23\n",
      "0.8420138888888888\n",
      "batch_idx: 24\n",
      "0.8433333333333334\n",
      "Training Epoch: 173, total loss: 42.549661\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8482142857142857\n",
      "batch_idx: 14\n",
      "0.8555555555555555\n",
      "batch_idx: 15\n",
      "0.859375\n",
      "batch_idx: 16\n",
      "0.8627450980392157\n",
      "batch_idx: 17\n",
      "0.8657407407407407\n",
      "batch_idx: 18\n",
      "0.8552631578947368\n",
      "batch_idx: 19\n",
      "0.8604166666666667\n",
      "batch_idx: 20\n",
      "0.8630952380952381\n",
      "batch_idx: 21\n",
      "0.8617424242424242\n",
      "batch_idx: 22\n",
      "0.8641304347826086\n",
      "batch_idx: 23\n",
      "0.8628472222222222\n",
      "batch_idx: 24\n",
      "0.86\n",
      "Training Epoch: 174, total loss: 42.235898\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.9027777777777778\n",
      "batch_idx: 6\n",
      "0.9107142857142857\n",
      "batch_idx: 7\n",
      "0.9114583333333334\n",
      "batch_idx: 8\n",
      "0.9074074074074074\n",
      "batch_idx: 9\n",
      "0.8708333333333333\n",
      "batch_idx: 10\n",
      "0.8787878787878788\n",
      "batch_idx: 11\n",
      "0.8715277777777778\n",
      "batch_idx: 12\n",
      "0.8717948717948718\n",
      "batch_idx: 13\n",
      "0.8630952380952381\n",
      "batch_idx: 14\n",
      "0.8638888888888889\n",
      "batch_idx: 15\n",
      "0.8619791666666666\n",
      "batch_idx: 16\n",
      "0.8602941176470589\n",
      "batch_idx: 17\n",
      "0.8634259259259259\n",
      "batch_idx: 18\n",
      "0.8662280701754386\n",
      "batch_idx: 19\n",
      "0.85625\n",
      "batch_idx: 20\n",
      "0.8531746031746031\n",
      "batch_idx: 21\n",
      "0.8465909090909091\n",
      "batch_idx: 22\n",
      "0.8496376811594203\n",
      "batch_idx: 23\n",
      "0.8472222222222222\n",
      "batch_idx: 24\n",
      "0.8483333333333334\n",
      "Training Epoch: 175, total loss: 42.491807\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8636363636363636\n",
      "batch_idx: 11\n",
      "0.8680555555555556\n",
      "batch_idx: 12\n",
      "0.8685897435897436\n",
      "batch_idx: 13\n",
      "0.8601190476190477\n",
      "batch_idx: 14\n",
      "0.8638888888888889\n",
      "batch_idx: 15\n",
      "0.8567708333333334\n",
      "batch_idx: 16\n",
      "0.8602941176470589\n",
      "batch_idx: 17\n",
      "0.8657407407407407\n",
      "batch_idx: 18\n",
      "0.8618421052631579\n",
      "batch_idx: 19\n",
      "0.8625\n",
      "batch_idx: 20\n",
      "0.8611111111111112\n",
      "batch_idx: 21\n",
      "0.8579545454545454\n",
      "batch_idx: 22\n",
      "0.8586956521739131\n",
      "batch_idx: 23\n",
      "0.8559027777777778\n",
      "batch_idx: 24\n",
      "0.8566666666666667\n",
      "Training Epoch: 176, total loss: 42.348794\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8482142857142857\n",
      "batch_idx: 14\n",
      "0.85\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8553921568627451\n",
      "batch_idx: 17\n",
      "0.8611111111111112\n",
      "batch_idx: 18\n",
      "0.8662280701754386\n",
      "batch_idx: 19\n",
      "0.8666666666666667\n",
      "batch_idx: 20\n",
      "0.871031746031746\n",
      "batch_idx: 21\n",
      "0.8693181818181818\n",
      "batch_idx: 22\n",
      "0.8605072463768116\n",
      "batch_idx: 23\n",
      "0.8628472222222222\n",
      "batch_idx: 24\n",
      "0.86\n",
      "Training Epoch: 177, total loss: 42.447223\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.7951388888888888\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.7916666666666666\n",
      "batch_idx: 14\n",
      "0.7916666666666666\n",
      "batch_idx: 15\n",
      "0.7890625\n",
      "batch_idx: 16\n",
      "0.7916666666666666\n",
      "batch_idx: 17\n",
      "0.7939814814814815\n",
      "batch_idx: 18\n",
      "0.8004385964912281\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.8095238095238095\n",
      "batch_idx: 21\n",
      "0.8125\n",
      "batch_idx: 22\n",
      "0.8134057971014492\n",
      "batch_idx: 23\n",
      "0.8125\n",
      "batch_idx: 24\n",
      "0.81\n",
      "Training Epoch: 178, total loss: 43.044071\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.8988095238095238\n",
      "batch_idx: 7\n",
      "0.9114583333333334\n",
      "batch_idx: 8\n",
      "0.9120370370370371\n",
      "batch_idx: 9\n",
      "0.9083333333333333\n",
      "batch_idx: 10\n",
      "0.9090909090909091\n",
      "batch_idx: 11\n",
      "0.8888888888888888\n",
      "batch_idx: 12\n",
      "0.8910256410256411\n",
      "batch_idx: 13\n",
      "0.8869047619047619\n",
      "batch_idx: 14\n",
      "0.8833333333333333\n",
      "batch_idx: 15\n",
      "0.8880208333333334\n",
      "batch_idx: 16\n",
      "0.8823529411764706\n",
      "batch_idx: 17\n",
      "0.8773148148148148\n",
      "batch_idx: 18\n",
      "0.8728070175438597\n",
      "batch_idx: 19\n",
      "0.875\n",
      "batch_idx: 20\n",
      "0.876984126984127\n",
      "batch_idx: 21\n",
      "0.875\n",
      "batch_idx: 22\n",
      "0.8786231884057971\n",
      "batch_idx: 23\n",
      "0.8767361111111112\n",
      "batch_idx: 24\n",
      "0.8733333333333333\n",
      "Training Epoch: 179, total loss: 42.168656\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8888888888888888\n",
      "batch_idx: 6\n",
      "0.9047619047619048\n",
      "batch_idx: 7\n",
      "0.9010416666666666\n",
      "batch_idx: 8\n",
      "0.8981481481481481\n",
      "batch_idx: 9\n",
      "0.9\n",
      "batch_idx: 10\n",
      "0.8977272727272727\n",
      "batch_idx: 11\n",
      "0.8923611111111112\n",
      "batch_idx: 12\n",
      "0.8974358974358975\n",
      "batch_idx: 13\n",
      "0.8958333333333334\n",
      "batch_idx: 14\n",
      "0.8888888888888888\n",
      "batch_idx: 15\n",
      "0.8880208333333334\n",
      "batch_idx: 16\n",
      "0.8799019607843137\n",
      "batch_idx: 17\n",
      "0.8773148148148148\n",
      "batch_idx: 18\n",
      "0.875\n",
      "batch_idx: 19\n",
      "0.86875\n",
      "batch_idx: 20\n",
      "0.8670634920634921\n",
      "batch_idx: 21\n",
      "0.8617424242424242\n",
      "batch_idx: 22\n",
      "0.8623188405797102\n",
      "batch_idx: 23\n",
      "0.8559027777777778\n",
      "batch_idx: 24\n",
      "0.8466666666666667\n",
      "Training Epoch: 180, total loss: 42.409559\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.875\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8636363636363636\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8511904761904762\n",
      "batch_idx: 14\n",
      "0.8388888888888889\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8431372549019608\n",
      "batch_idx: 17\n",
      "0.8425925925925926\n",
      "batch_idx: 18\n",
      "0.8442982456140351\n",
      "batch_idx: 19\n",
      "0.8395833333333333\n",
      "batch_idx: 20\n",
      "0.8452380952380952\n",
      "batch_idx: 21\n",
      "0.8428030303030303\n",
      "batch_idx: 22\n",
      "0.8442028985507246\n",
      "batch_idx: 23\n",
      "0.8489583333333334\n",
      "batch_idx: 24\n",
      "0.8466666666666667\n",
      "Training Epoch: 181, total loss: 42.471443\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8564814814814815\n",
      "batch_idx: 9\n",
      "0.8625\n",
      "batch_idx: 10\n",
      "0.8636363636363636\n",
      "batch_idx: 11\n",
      "0.8645833333333334\n",
      "batch_idx: 12\n",
      "0.8653846153846154\n",
      "batch_idx: 13\n",
      "0.8660714285714286\n",
      "batch_idx: 14\n",
      "0.8638888888888889\n",
      "batch_idx: 15\n",
      "0.8567708333333334\n",
      "batch_idx: 16\n",
      "0.8553921568627451\n",
      "batch_idx: 17\n",
      "0.8611111111111112\n",
      "batch_idx: 18\n",
      "0.8618421052631579\n",
      "batch_idx: 19\n",
      "0.8479166666666667\n",
      "batch_idx: 20\n",
      "0.8412698412698413\n",
      "batch_idx: 21\n",
      "0.8409090909090909\n",
      "batch_idx: 22\n",
      "0.8369565217391305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 23\n",
      "0.8350694444444444\n",
      "batch_idx: 24\n",
      "0.8366666666666667\n",
      "Training Epoch: 182, total loss: 42.663350\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8333333333333334\n",
      "batch_idx: 15\n",
      "0.8385416666666666\n",
      "batch_idx: 16\n",
      "0.8333333333333334\n",
      "batch_idx: 17\n",
      "0.8356481481481481\n",
      "batch_idx: 18\n",
      "0.8333333333333334\n",
      "batch_idx: 19\n",
      "0.8354166666666667\n",
      "batch_idx: 20\n",
      "0.8373015873015873\n",
      "batch_idx: 21\n",
      "0.8314393939393939\n",
      "batch_idx: 22\n",
      "0.8315217391304348\n",
      "batch_idx: 23\n",
      "0.828125\n",
      "batch_idx: 24\n",
      "0.8283333333333334\n",
      "Training Epoch: 183, total loss: 42.663520\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8564814814814815\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8712121212121212\n",
      "batch_idx: 11\n",
      "0.8680555555555556\n",
      "batch_idx: 12\n",
      "0.8685897435897436\n",
      "batch_idx: 13\n",
      "0.8541666666666666\n",
      "batch_idx: 14\n",
      "0.8472222222222222\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8382352941176471\n",
      "batch_idx: 17\n",
      "0.8333333333333334\n",
      "batch_idx: 18\n",
      "0.8377192982456141\n",
      "batch_idx: 19\n",
      "0.8416666666666667\n",
      "batch_idx: 20\n",
      "0.8432539682539683\n",
      "batch_idx: 21\n",
      "0.8446969696969697\n",
      "batch_idx: 22\n",
      "0.842391304347826\n",
      "batch_idx: 23\n",
      "0.8420138888888888\n",
      "batch_idx: 24\n",
      "0.8433333333333334\n",
      "Training Epoch: 184, total loss: 42.441815\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8287037037037037\n",
      "batch_idx: 18\n",
      "0.8267543859649122\n",
      "batch_idx: 19\n",
      "0.8270833333333333\n",
      "batch_idx: 20\n",
      "0.8234126984126984\n",
      "batch_idx: 21\n",
      "0.8276515151515151\n",
      "batch_idx: 22\n",
      "0.8315217391304348\n",
      "batch_idx: 23\n",
      "0.8263888888888888\n",
      "batch_idx: 24\n",
      "0.825\n",
      "Training Epoch: 185, total loss: 42.670794\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.8916666666666667\n",
      "batch_idx: 5\n",
      "0.9027777777777778\n",
      "batch_idx: 6\n",
      "0.8928571428571429\n",
      "batch_idx: 7\n",
      "0.875\n",
      "batch_idx: 8\n",
      "0.8564814814814815\n",
      "batch_idx: 9\n",
      "0.8625\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.8653846153846154\n",
      "batch_idx: 13\n",
      "0.8720238095238095\n",
      "batch_idx: 14\n",
      "0.875\n",
      "batch_idx: 15\n",
      "0.875\n",
      "batch_idx: 16\n",
      "0.875\n",
      "batch_idx: 17\n",
      "0.8703703703703703\n",
      "batch_idx: 18\n",
      "0.8728070175438597\n",
      "batch_idx: 19\n",
      "0.8729166666666667\n",
      "batch_idx: 20\n",
      "0.873015873015873\n",
      "batch_idx: 21\n",
      "0.8731060606060606\n",
      "batch_idx: 22\n",
      "0.8713768115942029\n",
      "batch_idx: 23\n",
      "0.8697916666666666\n",
      "batch_idx: 24\n",
      "0.865\n",
      "Training Epoch: 186, total loss: 42.294208\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.90625\n",
      "batch_idx: 4\n",
      "0.9083333333333333\n",
      "batch_idx: 5\n",
      "0.9097222222222222\n",
      "batch_idx: 6\n",
      "0.9166666666666666\n",
      "batch_idx: 7\n",
      "0.9010416666666666\n",
      "batch_idx: 8\n",
      "0.8935185185185185\n",
      "batch_idx: 9\n",
      "0.8916666666666667\n",
      "batch_idx: 10\n",
      "0.8901515151515151\n",
      "batch_idx: 11\n",
      "0.8784722222222222\n",
      "batch_idx: 12\n",
      "0.8685897435897436\n",
      "batch_idx: 13\n",
      "0.8720238095238095\n",
      "batch_idx: 14\n",
      "0.8694444444444445\n",
      "batch_idx: 15\n",
      "0.8645833333333334\n",
      "batch_idx: 16\n",
      "0.8602941176470589\n",
      "batch_idx: 17\n",
      "0.8611111111111112\n",
      "batch_idx: 18\n",
      "0.8618421052631579\n",
      "batch_idx: 19\n",
      "0.8583333333333333\n",
      "batch_idx: 20\n",
      "0.8571428571428571\n",
      "batch_idx: 21\n",
      "0.8598484848484849\n",
      "batch_idx: 22\n",
      "0.8641304347826086\n",
      "batch_idx: 23\n",
      "0.8645833333333334\n",
      "batch_idx: 24\n",
      "0.8616666666666667\n",
      "Training Epoch: 187, total loss: 42.344220\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8305555555555556\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8382352941176471\n",
      "batch_idx: 17\n",
      "0.8356481481481481\n",
      "batch_idx: 18\n",
      "0.8333333333333334\n",
      "batch_idx: 19\n",
      "0.8333333333333334\n",
      "batch_idx: 20\n",
      "0.8373015873015873\n",
      "batch_idx: 21\n",
      "0.8428030303030303\n",
      "batch_idx: 22\n",
      "0.8405797101449275\n",
      "batch_idx: 23\n",
      "0.8368055555555556\n",
      "batch_idx: 24\n",
      "0.8366666666666667\n",
      "Training Epoch: 188, total loss: 42.745026\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.9083333333333333\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8564814814814815\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8525641025641025\n",
      "batch_idx: 13\n",
      "0.8601190476190477\n",
      "batch_idx: 14\n",
      "0.8555555555555555\n",
      "batch_idx: 15\n",
      "0.8541666666666666\n",
      "batch_idx: 16\n",
      "0.8455882352941176\n",
      "batch_idx: 17\n",
      "0.8402777777777778\n",
      "batch_idx: 18\n",
      "0.8355263157894737\n",
      "batch_idx: 19\n",
      "0.8354166666666667\n",
      "batch_idx: 20\n",
      "0.8353174603174603\n",
      "batch_idx: 21\n",
      "0.8333333333333334\n",
      "batch_idx: 22\n",
      "0.8333333333333334\n",
      "batch_idx: 23\n",
      "0.8315972222222222\n",
      "batch_idx: 24\n",
      "0.835\n",
      "Training Epoch: 189, total loss: 42.760224\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.8653846153846154\n",
      "batch_idx: 13\n",
      "0.8660714285714286\n",
      "batch_idx: 14\n",
      "0.8555555555555555\n",
      "batch_idx: 15\n",
      "0.8489583333333334\n",
      "batch_idx: 16\n",
      "0.8504901960784313\n",
      "batch_idx: 17\n",
      "0.8495370370370371\n",
      "batch_idx: 18\n",
      "0.8486842105263158\n",
      "batch_idx: 19\n",
      "0.85\n",
      "batch_idx: 20\n",
      "0.8551587301587301\n",
      "batch_idx: 21\n",
      "0.8541666666666666\n",
      "batch_idx: 22\n",
      "0.8514492753623188\n",
      "batch_idx: 23\n",
      "0.8472222222222222\n",
      "batch_idx: 24\n",
      "0.8533333333333334\n",
      "Training Epoch: 190, total loss: 42.260181\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.8270833333333333\n",
      "batch_idx: 20\n",
      "0.8313492063492064\n",
      "batch_idx: 21\n",
      "0.8314393939393939\n",
      "batch_idx: 22\n",
      "0.8351449275362319\n",
      "batch_idx: 23\n",
      "0.8350694444444444\n",
      "batch_idx: 24\n",
      "0.8383333333333334\n",
      "Training Epoch: 191, total loss: 42.506429\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.8802083333333334\n",
      "batch_idx: 8\n",
      "0.875\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8674242424242424\n",
      "batch_idx: 11\n",
      "0.8715277777777778\n",
      "batch_idx: 12\n",
      "0.8717948717948718\n",
      "batch_idx: 13\n",
      "0.8630952380952381\n",
      "batch_idx: 14\n",
      "0.8694444444444445\n",
      "batch_idx: 15\n",
      "0.8671875\n",
      "batch_idx: 16\n",
      "0.8529411764705882\n",
      "batch_idx: 17\n",
      "0.8564814814814815\n",
      "batch_idx: 18\n",
      "0.8552631578947368\n",
      "batch_idx: 19\n",
      "0.85625\n",
      "batch_idx: 20\n",
      "0.8611111111111112\n",
      "batch_idx: 21\n",
      "0.8598484848484849\n",
      "batch_idx: 22\n",
      "0.8568840579710145\n",
      "batch_idx: 23\n",
      "0.8559027777777778\n",
      "batch_idx: 24\n",
      "0.855\n",
      "Training Epoch: 192, total loss: 42.473779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8422619047619048\n",
      "batch_idx: 14\n",
      "0.85\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8504901960784313\n",
      "batch_idx: 17\n",
      "0.8518518518518519\n",
      "batch_idx: 18\n",
      "0.8508771929824561\n",
      "batch_idx: 19\n",
      "0.8541666666666666\n",
      "batch_idx: 20\n",
      "0.8551587301587301\n",
      "batch_idx: 21\n",
      "0.8560606060606061\n",
      "batch_idx: 22\n",
      "0.8568840579710145\n",
      "batch_idx: 23\n",
      "0.8576388888888888\n",
      "batch_idx: 24\n",
      "0.8566666666666667\n",
      "Training Epoch: 193, total loss: 42.393041\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n",
      "batch_idx: 11\n",
      "0.8541666666666666\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8541666666666666\n",
      "batch_idx: 14\n",
      "0.8611111111111112\n",
      "batch_idx: 15\n",
      "0.8619791666666666\n",
      "batch_idx: 16\n",
      "0.8602941176470589\n",
      "batch_idx: 17\n",
      "0.8634259259259259\n",
      "batch_idx: 18\n",
      "0.8640350877192983\n",
      "batch_idx: 19\n",
      "0.8666666666666667\n",
      "batch_idx: 20\n",
      "0.871031746031746\n",
      "batch_idx: 21\n",
      "0.8693181818181818\n",
      "batch_idx: 22\n",
      "0.8677536231884058\n",
      "batch_idx: 23\n",
      "0.8680555555555556\n",
      "batch_idx: 24\n",
      "0.8716666666666667\n",
      "Training Epoch: 194, total loss: 42.139922\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8363095238095238\n",
      "batch_idx: 14\n",
      "0.8333333333333334\n",
      "batch_idx: 15\n",
      "0.8333333333333334\n",
      "batch_idx: 16\n",
      "0.8357843137254902\n",
      "batch_idx: 17\n",
      "0.8402777777777778\n",
      "batch_idx: 18\n",
      "0.8377192982456141\n",
      "batch_idx: 19\n",
      "0.84375\n",
      "batch_idx: 20\n",
      "0.8412698412698413\n",
      "batch_idx: 21\n",
      "0.8390151515151515\n",
      "batch_idx: 22\n",
      "0.8405797101449275\n",
      "batch_idx: 23\n",
      "0.84375\n",
      "batch_idx: 24\n",
      "0.8416666666666667\n",
      "Training Epoch: 195, total loss: 42.588124\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8452380952380952\n",
      "batch_idx: 14\n",
      "0.8444444444444444\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8480392156862745\n",
      "batch_idx: 17\n",
      "0.8356481481481481\n",
      "batch_idx: 18\n",
      "0.8399122807017544\n",
      "batch_idx: 19\n",
      "0.8395833333333333\n",
      "batch_idx: 20\n",
      "0.8432539682539683\n",
      "batch_idx: 21\n",
      "0.8428030303030303\n",
      "batch_idx: 22\n",
      "0.8405797101449275\n",
      "batch_idx: 23\n",
      "0.8350694444444444\n",
      "batch_idx: 24\n",
      "0.8383333333333334\n",
      "Training Epoch: 196, total loss: 42.379041\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.8472222222222222\n",
      "batch_idx: 12\n",
      "0.8525641025641025\n",
      "batch_idx: 13\n",
      "0.8541666666666666\n",
      "batch_idx: 14\n",
      "0.8555555555555555\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8382352941176471\n",
      "batch_idx: 17\n",
      "0.8379629629629629\n",
      "batch_idx: 18\n",
      "0.8421052631578947\n",
      "batch_idx: 19\n",
      "0.8458333333333333\n",
      "batch_idx: 20\n",
      "0.8412698412698413\n",
      "batch_idx: 21\n",
      "0.8465909090909091\n",
      "batch_idx: 22\n",
      "0.8442028985507246\n",
      "batch_idx: 23\n",
      "0.8420138888888888\n",
      "batch_idx: 24\n",
      "0.84\n",
      "Training Epoch: 197, total loss: 42.444369\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8511904761904762\n",
      "batch_idx: 14\n",
      "0.8472222222222222\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8480392156862745\n",
      "batch_idx: 17\n",
      "0.8449074074074074\n",
      "batch_idx: 18\n",
      "0.8464912280701754\n",
      "batch_idx: 19\n",
      "0.8416666666666667\n",
      "batch_idx: 20\n",
      "0.8412698412698413\n",
      "batch_idx: 21\n",
      "0.8446969696969697\n",
      "batch_idx: 22\n",
      "0.8405797101449275\n",
      "batch_idx: 23\n",
      "0.8368055555555556\n",
      "batch_idx: 24\n",
      "0.8366666666666667\n",
      "Training Epoch: 198, total loss: 42.563476\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8388888888888889\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8357843137254902\n",
      "batch_idx: 17\n",
      "0.8333333333333334\n",
      "batch_idx: 18\n",
      "0.8377192982456141\n",
      "batch_idx: 19\n",
      "0.8395833333333333\n",
      "batch_idx: 20\n",
      "0.8353174603174603\n",
      "batch_idx: 21\n",
      "0.8371212121212122\n",
      "batch_idx: 22\n",
      "0.8278985507246377\n",
      "batch_idx: 23\n",
      "0.8159722222222222\n",
      "batch_idx: 24\n",
      "0.8183333333333334\n",
      "Training Epoch: 199, total loss: 42.914136\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8482142857142857\n",
      "batch_idx: 14\n",
      "0.8444444444444444\n",
      "batch_idx: 15\n",
      "0.8333333333333334\n",
      "batch_idx: 16\n",
      "0.8308823529411765\n",
      "batch_idx: 17\n",
      "0.8287037037037037\n",
      "batch_idx: 18\n",
      "0.8267543859649122\n",
      "batch_idx: 19\n",
      "0.83125\n",
      "batch_idx: 20\n",
      "0.8214285714285714\n",
      "batch_idx: 21\n",
      "0.8257575757575758\n",
      "batch_idx: 22\n",
      "0.8188405797101449\n",
      "batch_idx: 23\n",
      "0.8194444444444444\n",
      "batch_idx: 24\n",
      "0.8183333333333334\n",
      "Training Epoch: 200, total loss: 43.135789\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8263888888888888\n",
      "batch_idx: 18\n",
      "0.8223684210526315\n",
      "batch_idx: 19\n",
      "0.8166666666666667\n",
      "batch_idx: 20\n",
      "0.8115079365079365\n",
      "batch_idx: 21\n",
      "0.8087121212121212\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8177083333333334\n",
      "batch_idx: 24\n",
      "0.81\n",
      "Training Epoch: 201, total loss: 43.036796\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8796296296296297\n",
      "batch_idx: 9\n",
      "0.8791666666666667\n",
      "batch_idx: 10\n",
      "0.8863636363636364\n",
      "batch_idx: 11\n",
      "0.8854166666666666\n",
      "batch_idx: 12\n",
      "0.875\n",
      "batch_idx: 13\n",
      "0.8660714285714286\n",
      "batch_idx: 14\n",
      "0.8638888888888889\n",
      "batch_idx: 15\n",
      "0.8541666666666666\n",
      "batch_idx: 16\n",
      "0.8504901960784313\n",
      "batch_idx: 17\n",
      "0.8495370370370371\n",
      "batch_idx: 18\n",
      "0.8508771929824561\n",
      "batch_idx: 19\n",
      "0.8520833333333333\n",
      "batch_idx: 20\n",
      "0.8511904761904762\n",
      "batch_idx: 21\n",
      "0.8541666666666666\n",
      "batch_idx: 22\n",
      "0.855072463768116\n",
      "batch_idx: 23\n",
      "0.8524305555555556\n",
      "batch_idx: 24\n",
      "0.8516666666666667\n",
      "Training Epoch: 202, total loss: 42.328868\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.9305555555555556\n",
      "batch_idx: 3\n",
      "0.9166666666666666\n",
      "batch_idx: 4\n",
      "0.9083333333333333\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "batch_idx: 6\n",
      "0.8928571428571429\n",
      "batch_idx: 7\n",
      "0.8802083333333334\n",
      "batch_idx: 8\n",
      "0.8703703703703703\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8674242424242424\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.8525641025641025\n",
      "batch_idx: 13\n",
      "0.8541666666666666\n",
      "batch_idx: 14\n",
      "0.8555555555555555\n",
      "batch_idx: 15\n",
      "0.8619791666666666\n",
      "batch_idx: 16\n",
      "0.8700980392156863\n",
      "batch_idx: 17\n",
      "0.8680555555555556\n",
      "batch_idx: 18\n",
      "0.8640350877192983\n",
      "batch_idx: 19\n",
      "0.8604166666666667\n",
      "batch_idx: 20\n",
      "0.8650793650793651\n",
      "batch_idx: 21\n",
      "0.865530303030303\n",
      "batch_idx: 22\n",
      "0.8695652173913043\n",
      "batch_idx: 23\n",
      "0.8697916666666666\n",
      "batch_idx: 24\n",
      "0.8666666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 203, total loss: 41.983440\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8333333333333334\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.8359375\n",
      "batch_idx: 16\n",
      "0.8382352941176471\n",
      "batch_idx: 17\n",
      "0.8425925925925926\n",
      "batch_idx: 18\n",
      "0.8399122807017544\n",
      "batch_idx: 19\n",
      "0.8416666666666667\n",
      "batch_idx: 20\n",
      "0.8472222222222222\n",
      "batch_idx: 21\n",
      "0.8484848484848485\n",
      "batch_idx: 22\n",
      "0.8514492753623188\n",
      "batch_idx: 23\n",
      "0.8472222222222222\n",
      "batch_idx: 24\n",
      "0.8483333333333334\n",
      "Training Epoch: 204, total loss: 42.404382\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.8541666666666666\n",
      "batch_idx: 12\n",
      "0.8525641025641025\n",
      "batch_idx: 13\n",
      "0.8511904761904762\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.8385416666666666\n",
      "batch_idx: 16\n",
      "0.8431372549019608\n",
      "batch_idx: 17\n",
      "0.8356481481481481\n",
      "batch_idx: 18\n",
      "0.8399122807017544\n",
      "batch_idx: 19\n",
      "0.8354166666666667\n",
      "batch_idx: 20\n",
      "0.8392857142857143\n",
      "batch_idx: 21\n",
      "0.8446969696969697\n",
      "batch_idx: 22\n",
      "0.8496376811594203\n",
      "batch_idx: 23\n",
      "0.8489583333333334\n",
      "batch_idx: 24\n",
      "0.8516666666666667\n",
      "Training Epoch: 205, total loss: 42.209194\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8657407407407407\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.8589743589743589\n",
      "batch_idx: 13\n",
      "0.8601190476190477\n",
      "batch_idx: 14\n",
      "0.8638888888888889\n",
      "batch_idx: 15\n",
      "0.8723958333333334\n",
      "batch_idx: 16\n",
      "0.8651960784313726\n",
      "batch_idx: 17\n",
      "0.8634259259259259\n",
      "batch_idx: 18\n",
      "0.8618421052631579\n",
      "batch_idx: 19\n",
      "0.8625\n",
      "batch_idx: 20\n",
      "0.8650793650793651\n",
      "batch_idx: 21\n",
      "0.8674242424242424\n",
      "batch_idx: 22\n",
      "0.8713768115942029\n",
      "batch_idx: 23\n",
      "0.8680555555555556\n",
      "batch_idx: 24\n",
      "0.865\n",
      "Training Epoch: 206, total loss: 42.121282\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.8541666666666666\n",
      "batch_idx: 12\n",
      "0.8589743589743589\n",
      "batch_idx: 13\n",
      "0.8511904761904762\n",
      "batch_idx: 14\n",
      "0.8527777777777777\n",
      "batch_idx: 15\n",
      "0.84375\n",
      "batch_idx: 16\n",
      "0.8480392156862745\n",
      "batch_idx: 17\n",
      "0.8518518518518519\n",
      "batch_idx: 18\n",
      "0.8574561403508771\n",
      "batch_idx: 19\n",
      "0.8604166666666667\n",
      "batch_idx: 20\n",
      "0.8630952380952381\n",
      "batch_idx: 21\n",
      "0.8598484848484849\n",
      "batch_idx: 22\n",
      "0.8623188405797102\n",
      "batch_idx: 23\n",
      "0.8628472222222222\n",
      "batch_idx: 24\n",
      "0.8666666666666667\n",
      "Training Epoch: 207, total loss: 42.056424\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.8916666666666667\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.8869047619047619\n",
      "batch_idx: 7\n",
      "0.8802083333333334\n",
      "batch_idx: 8\n",
      "0.8703703703703703\n",
      "batch_idx: 9\n",
      "0.875\n",
      "batch_idx: 10\n",
      "0.8825757575757576\n",
      "batch_idx: 11\n",
      "0.8819444444444444\n",
      "batch_idx: 12\n",
      "0.8814102564102564\n",
      "batch_idx: 13\n",
      "0.8869047619047619\n",
      "batch_idx: 14\n",
      "0.8833333333333333\n",
      "batch_idx: 15\n",
      "0.8828125\n",
      "batch_idx: 16\n",
      "0.8799019607843137\n",
      "batch_idx: 17\n",
      "0.8726851851851852\n",
      "batch_idx: 18\n",
      "0.8706140350877193\n",
      "batch_idx: 19\n",
      "0.8625\n",
      "batch_idx: 20\n",
      "0.8591269841269841\n",
      "batch_idx: 21\n",
      "0.8560606060606061\n",
      "batch_idx: 22\n",
      "0.855072463768116\n",
      "batch_idx: 23\n",
      "0.859375\n",
      "batch_idx: 24\n",
      "0.8566666666666667\n",
      "Training Epoch: 208, total loss: 42.098546\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8657407407407407\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.8525641025641025\n",
      "batch_idx: 13\n",
      "0.8541666666666666\n",
      "batch_idx: 14\n",
      "0.8611111111111112\n",
      "batch_idx: 15\n",
      "0.8567708333333334\n",
      "batch_idx: 16\n",
      "0.8553921568627451\n",
      "batch_idx: 17\n",
      "0.8634259259259259\n",
      "batch_idx: 18\n",
      "0.8618421052631579\n",
      "batch_idx: 19\n",
      "0.8645833333333334\n",
      "batch_idx: 20\n",
      "0.8650793650793651\n",
      "batch_idx: 21\n",
      "0.865530303030303\n",
      "batch_idx: 22\n",
      "0.8659420289855072\n",
      "batch_idx: 23\n",
      "0.8628472222222222\n",
      "batch_idx: 24\n",
      "0.865\n",
      "Training Epoch: 209, total loss: 42.190432\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8541666666666666\n",
      "batch_idx: 14\n",
      "0.8527777777777777\n",
      "batch_idx: 15\n",
      "0.8515625\n",
      "batch_idx: 16\n",
      "0.8553921568627451\n",
      "batch_idx: 17\n",
      "0.8587962962962963\n",
      "batch_idx: 18\n",
      "0.8552631578947368\n",
      "batch_idx: 19\n",
      "0.8541666666666666\n",
      "batch_idx: 20\n",
      "0.8511904761904762\n",
      "batch_idx: 21\n",
      "0.8560606060606061\n",
      "batch_idx: 22\n",
      "0.8586956521739131\n",
      "batch_idx: 23\n",
      "0.8576388888888888\n",
      "batch_idx: 24\n",
      "0.8483333333333334\n",
      "Training Epoch: 210, total loss: 42.263438\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9166666666666666\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.9027777777777778\n",
      "batch_idx: 6\n",
      "0.8928571428571429\n",
      "batch_idx: 7\n",
      "0.8854166666666666\n",
      "batch_idx: 8\n",
      "0.8657407407407407\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.8645833333333334\n",
      "batch_idx: 12\n",
      "0.8557692307692307\n",
      "batch_idx: 13\n",
      "0.8511904761904762\n",
      "batch_idx: 14\n",
      "0.8527777777777777\n",
      "batch_idx: 15\n",
      "0.8619791666666666\n",
      "batch_idx: 16\n",
      "0.8700980392156863\n",
      "batch_idx: 17\n",
      "0.8726851851851852\n",
      "batch_idx: 18\n",
      "0.8706140350877193\n",
      "batch_idx: 19\n",
      "0.8666666666666667\n",
      "batch_idx: 20\n",
      "0.8690476190476191\n",
      "batch_idx: 21\n",
      "0.8712121212121212\n",
      "batch_idx: 22\n",
      "0.8713768115942029\n",
      "batch_idx: 23\n",
      "0.8697916666666666\n",
      "batch_idx: 24\n",
      "0.87\n",
      "Training Epoch: 211, total loss: 42.045105\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.8916666666666667\n",
      "batch_idx: 5\n",
      "0.8888888888888888\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8703703703703703\n",
      "batch_idx: 9\n",
      "0.8625\n",
      "batch_idx: 10\n",
      "0.8636363636363636\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.8525641025641025\n",
      "batch_idx: 13\n",
      "0.8511904761904762\n",
      "batch_idx: 14\n",
      "0.8527777777777777\n",
      "batch_idx: 15\n",
      "0.8489583333333334\n",
      "batch_idx: 16\n",
      "0.8553921568627451\n",
      "batch_idx: 17\n",
      "0.8472222222222222\n",
      "batch_idx: 18\n",
      "0.8442982456140351\n",
      "batch_idx: 19\n",
      "0.8479166666666667\n",
      "batch_idx: 20\n",
      "0.8472222222222222\n",
      "batch_idx: 21\n",
      "0.8484848484848485\n",
      "batch_idx: 22\n",
      "0.8514492753623188\n",
      "batch_idx: 23\n",
      "0.8559027777777778\n",
      "batch_idx: 24\n",
      "0.8583333333333333\n",
      "Training Epoch: 212, total loss: 42.094310\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.875\n",
      "batch_idx: 8\n",
      "0.8842592592592593\n",
      "batch_idx: 9\n",
      "0.8875\n",
      "batch_idx: 10\n",
      "0.8977272727272727\n",
      "batch_idx: 11\n",
      "0.8923611111111112\n",
      "batch_idx: 12\n",
      "0.875\n",
      "batch_idx: 13\n",
      "0.8809523809523809\n",
      "batch_idx: 14\n",
      "0.875\n",
      "batch_idx: 15\n",
      "0.8723958333333334\n",
      "batch_idx: 16\n",
      "0.8725490196078431\n",
      "batch_idx: 17\n",
      "0.875\n",
      "batch_idx: 18\n",
      "0.8771929824561403\n",
      "batch_idx: 19\n",
      "0.8770833333333333\n",
      "batch_idx: 20\n",
      "0.875\n",
      "batch_idx: 21\n",
      "0.8768939393939394\n",
      "batch_idx: 22\n",
      "0.8768115942028986\n",
      "batch_idx: 23\n",
      "0.8767361111111112\n",
      "batch_idx: 24\n",
      "0.8783333333333333\n",
      "Training Epoch: 213, total loss: 41.808542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8712121212121212\n",
      "batch_idx: 11\n",
      "0.8645833333333334\n",
      "batch_idx: 12\n",
      "0.8685897435897436\n",
      "batch_idx: 13\n",
      "0.8660714285714286\n",
      "batch_idx: 14\n",
      "0.8638888888888889\n",
      "batch_idx: 15\n",
      "0.8645833333333334\n",
      "batch_idx: 16\n",
      "0.8700980392156863\n",
      "batch_idx: 17\n",
      "0.8680555555555556\n",
      "batch_idx: 18\n",
      "0.8706140350877193\n",
      "batch_idx: 19\n",
      "0.8708333333333333\n",
      "batch_idx: 20\n",
      "0.873015873015873\n",
      "batch_idx: 21\n",
      "0.8731060606060606\n",
      "batch_idx: 22\n",
      "0.875\n",
      "batch_idx: 23\n",
      "0.8767361111111112\n",
      "batch_idx: 24\n",
      "0.8766666666666667\n",
      "Training Epoch: 214, total loss: 42.059796\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.9305555555555556\n",
      "batch_idx: 3\n",
      "0.9270833333333334\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8657407407407407\n",
      "batch_idx: 9\n",
      "0.8625\n",
      "batch_idx: 10\n",
      "0.8636363636363636\n",
      "batch_idx: 11\n",
      "0.8611111111111112\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8482142857142857\n",
      "batch_idx: 14\n",
      "0.8472222222222222\n",
      "batch_idx: 15\n",
      "0.8567708333333334\n",
      "batch_idx: 16\n",
      "0.8651960784313726\n",
      "batch_idx: 17\n",
      "0.8611111111111112\n",
      "batch_idx: 18\n",
      "0.8640350877192983\n",
      "batch_idx: 19\n",
      "0.8604166666666667\n",
      "batch_idx: 20\n",
      "0.8432539682539683\n",
      "batch_idx: 21\n",
      "0.8465909090909091\n",
      "batch_idx: 22\n",
      "0.8387681159420289\n",
      "batch_idx: 23\n",
      "0.8333333333333334\n",
      "batch_idx: 24\n",
      "0.8316666666666667\n",
      "Training Epoch: 215, total loss: 42.577911\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8916666666666667\n",
      "batch_idx: 5\n",
      "0.8888888888888888\n",
      "batch_idx: 6\n",
      "0.8928571428571429\n",
      "batch_idx: 7\n",
      "0.8958333333333334\n",
      "batch_idx: 8\n",
      "0.8842592592592593\n",
      "batch_idx: 9\n",
      "0.8875\n",
      "batch_idx: 10\n",
      "0.8825757575757576\n",
      "batch_idx: 11\n",
      "0.8819444444444444\n",
      "batch_idx: 12\n",
      "0.8910256410256411\n",
      "batch_idx: 13\n",
      "0.8898809523809523\n",
      "batch_idx: 14\n",
      "0.8833333333333333\n",
      "batch_idx: 15\n",
      "0.8828125\n",
      "batch_idx: 16\n",
      "0.8823529411764706\n",
      "batch_idx: 17\n",
      "0.8865740740740741\n",
      "batch_idx: 18\n",
      "0.8837719298245614\n",
      "batch_idx: 19\n",
      "0.88125\n",
      "batch_idx: 20\n",
      "0.878968253968254\n",
      "batch_idx: 21\n",
      "0.875\n",
      "batch_idx: 22\n",
      "0.875\n",
      "batch_idx: 23\n",
      "0.875\n",
      "batch_idx: 24\n",
      "0.8783333333333333\n",
      "Training Epoch: 216, total loss: 41.923725\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.8928571428571429\n",
      "batch_idx: 7\n",
      "0.8854166666666666\n",
      "batch_idx: 8\n",
      "0.875\n",
      "batch_idx: 9\n",
      "0.8833333333333333\n",
      "batch_idx: 10\n",
      "0.8863636363636364\n",
      "batch_idx: 11\n",
      "0.8888888888888888\n",
      "batch_idx: 12\n",
      "0.8878205128205128\n",
      "batch_idx: 13\n",
      "0.8869047619047619\n",
      "batch_idx: 14\n",
      "0.8916666666666667\n",
      "batch_idx: 15\n",
      "0.890625\n",
      "batch_idx: 16\n",
      "0.8897058823529411\n",
      "batch_idx: 17\n",
      "0.8912037037037037\n",
      "batch_idx: 18\n",
      "0.8903508771929824\n",
      "batch_idx: 19\n",
      "0.8875\n",
      "batch_idx: 20\n",
      "0.8869047619047619\n",
      "batch_idx: 21\n",
      "0.8882575757575758\n",
      "batch_idx: 22\n",
      "0.8876811594202898\n",
      "batch_idx: 23\n",
      "0.8802083333333334\n",
      "batch_idx: 24\n",
      "0.8783333333333333\n",
      "Training Epoch: 217, total loss: 41.919233\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.875\n",
      "batch_idx: 8\n",
      "0.8842592592592593\n",
      "batch_idx: 9\n",
      "0.8833333333333333\n",
      "batch_idx: 10\n",
      "0.8901515151515151\n",
      "batch_idx: 11\n",
      "0.8854166666666666\n",
      "batch_idx: 12\n",
      "0.875\n",
      "batch_idx: 13\n",
      "0.8779761904761905\n",
      "batch_idx: 14\n",
      "0.8722222222222222\n",
      "batch_idx: 15\n",
      "0.875\n",
      "batch_idx: 16\n",
      "0.8774509803921569\n",
      "batch_idx: 17\n",
      "0.8773148148148148\n",
      "batch_idx: 18\n",
      "0.8662280701754386\n",
      "batch_idx: 19\n",
      "0.86875\n",
      "batch_idx: 20\n",
      "0.8670634920634921\n",
      "batch_idx: 21\n",
      "0.8579545454545454\n",
      "batch_idx: 22\n",
      "0.8586956521739131\n",
      "batch_idx: 23\n",
      "0.8628472222222222\n",
      "batch_idx: 24\n",
      "0.865\n",
      "Training Epoch: 218, total loss: 42.066909\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8363095238095238\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.8385416666666666\n",
      "batch_idx: 16\n",
      "0.8431372549019608\n",
      "batch_idx: 17\n",
      "0.8518518518518519\n",
      "batch_idx: 18\n",
      "0.8530701754385965\n",
      "batch_idx: 19\n",
      "0.8583333333333333\n",
      "batch_idx: 20\n",
      "0.8571428571428571\n",
      "batch_idx: 21\n",
      "0.8484848484848485\n",
      "batch_idx: 22\n",
      "0.8496376811594203\n",
      "batch_idx: 23\n",
      "0.8454861111111112\n",
      "batch_idx: 24\n",
      "0.8416666666666667\n",
      "Training Epoch: 219, total loss: 42.539823\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.90625\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.8888888888888888\n",
      "batch_idx: 6\n",
      "0.8928571428571429\n",
      "batch_idx: 7\n",
      "0.890625\n",
      "batch_idx: 8\n",
      "0.8888888888888888\n",
      "batch_idx: 9\n",
      "0.8875\n",
      "batch_idx: 10\n",
      "0.8901515151515151\n",
      "batch_idx: 11\n",
      "0.8888888888888888\n",
      "batch_idx: 12\n",
      "0.8782051282051282\n",
      "batch_idx: 13\n",
      "0.8779761904761905\n",
      "batch_idx: 14\n",
      "0.8777777777777778\n",
      "batch_idx: 15\n",
      "0.8802083333333334\n",
      "batch_idx: 16\n",
      "0.8823529411764706\n",
      "batch_idx: 17\n",
      "0.8865740740740741\n",
      "batch_idx: 18\n",
      "0.8881578947368421\n",
      "batch_idx: 19\n",
      "0.8895833333333333\n",
      "batch_idx: 20\n",
      "0.8869047619047619\n",
      "batch_idx: 21\n",
      "0.8882575757575758\n",
      "batch_idx: 22\n",
      "0.8840579710144928\n",
      "batch_idx: 23\n",
      "0.8836805555555556\n",
      "batch_idx: 24\n",
      "0.8866666666666667\n",
      "Training Epoch: 220, total loss: 41.687449\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8869047619047619\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8796296296296297\n",
      "batch_idx: 9\n",
      "0.875\n",
      "batch_idx: 10\n",
      "0.8674242424242424\n",
      "batch_idx: 11\n",
      "0.875\n",
      "batch_idx: 12\n",
      "0.875\n",
      "batch_idx: 13\n",
      "0.875\n",
      "batch_idx: 14\n",
      "0.8722222222222222\n",
      "batch_idx: 15\n",
      "0.8776041666666666\n",
      "batch_idx: 16\n",
      "0.8799019607843137\n",
      "batch_idx: 17\n",
      "0.8726851851851852\n",
      "batch_idx: 18\n",
      "0.8728070175438597\n",
      "batch_idx: 19\n",
      "0.8729166666666667\n",
      "batch_idx: 20\n",
      "0.875\n",
      "batch_idx: 21\n",
      "0.8731060606060606\n",
      "batch_idx: 22\n",
      "0.8768115942028986\n",
      "batch_idx: 23\n",
      "0.8784722222222222\n",
      "batch_idx: 24\n",
      "0.875\n",
      "Training Epoch: 221, total loss: 42.059428\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9375\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8703703703703703\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8674242424242424\n",
      "batch_idx: 11\n",
      "0.8715277777777778\n",
      "batch_idx: 12\n",
      "0.8717948717948718\n",
      "batch_idx: 13\n",
      "0.8779761904761905\n",
      "batch_idx: 14\n",
      "0.8805555555555555\n",
      "batch_idx: 15\n",
      "0.8776041666666666\n",
      "batch_idx: 16\n",
      "0.8725490196078431\n",
      "batch_idx: 17\n",
      "0.875\n",
      "batch_idx: 18\n",
      "0.8728070175438597\n",
      "batch_idx: 19\n",
      "0.8645833333333334\n",
      "batch_idx: 20\n",
      "0.8591269841269841\n",
      "batch_idx: 21\n",
      "0.8560606060606061\n",
      "batch_idx: 22\n",
      "0.8568840579710145\n",
      "batch_idx: 23\n",
      "0.8576388888888888\n",
      "batch_idx: 24\n",
      "0.86\n",
      "Training Epoch: 222, total loss: 42.137126\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.8916666666666667\n",
      "batch_idx: 5\n",
      "0.8888888888888888\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.8645833333333334\n",
      "batch_idx: 12\n",
      "0.8621794871794872\n",
      "batch_idx: 13\n",
      "0.8690476190476191\n",
      "batch_idx: 14\n",
      "0.8694444444444445\n",
      "batch_idx: 15\n",
      "0.8697916666666666\n",
      "batch_idx: 16\n",
      "0.8651960784313726\n",
      "batch_idx: 17\n",
      "0.8680555555555556\n",
      "batch_idx: 18\n",
      "0.8640350877192983\n",
      "batch_idx: 19\n",
      "0.8645833333333334\n",
      "batch_idx: 20\n",
      "0.8670634920634921\n",
      "batch_idx: 21\n",
      "0.8674242424242424\n",
      "batch_idx: 22\n",
      "0.8605072463768116\n",
      "batch_idx: 23\n",
      "0.859375\n",
      "batch_idx: 24\n",
      "0.8583333333333333\n",
      "Training Epoch: 223, total loss: 42.194171\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.8472222222222222\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8482142857142857\n",
      "batch_idx: 14\n",
      "0.8472222222222222\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8553921568627451\n",
      "batch_idx: 17\n",
      "0.8564814814814815\n",
      "batch_idx: 18\n",
      "0.8596491228070176\n",
      "batch_idx: 19\n",
      "0.85625\n",
      "batch_idx: 20\n",
      "0.8591269841269841\n",
      "batch_idx: 21\n",
      "0.8560606060606061\n",
      "batch_idx: 22\n",
      "0.8568840579710145\n",
      "batch_idx: 23\n",
      "0.8559027777777778\n",
      "batch_idx: 24\n",
      "0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 224, total loss: 42.117834\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.8472222222222222\n",
      "batch_idx: 12\n",
      "0.8461538461538461\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8416666666666667\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8504901960784313\n",
      "batch_idx: 17\n",
      "0.8472222222222222\n",
      "batch_idx: 18\n",
      "0.8442982456140351\n",
      "batch_idx: 19\n",
      "0.8458333333333333\n",
      "batch_idx: 20\n",
      "0.8472222222222222\n",
      "batch_idx: 21\n",
      "0.8484848484848485\n",
      "batch_idx: 22\n",
      "0.8460144927536232\n",
      "batch_idx: 23\n",
      "0.8524305555555556\n",
      "batch_idx: 24\n",
      "0.8483333333333334\n",
      "Training Epoch: 225, total loss: 42.273516\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8098958333333334\n",
      "batch_idx: 16\n",
      "0.8186274509803921\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.8295454545454546\n",
      "batch_idx: 22\n",
      "0.8333333333333334\n",
      "batch_idx: 23\n",
      "0.8368055555555556\n",
      "batch_idx: 24\n",
      "0.8366666666666667\n",
      "Training Epoch: 226, total loss: 42.386555\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8472222222222222\n",
      "batch_idx: 15\n",
      "0.8515625\n",
      "batch_idx: 16\n",
      "0.8578431372549019\n",
      "batch_idx: 17\n",
      "0.8587962962962963\n",
      "batch_idx: 18\n",
      "0.8596491228070176\n",
      "batch_idx: 19\n",
      "0.85625\n",
      "batch_idx: 20\n",
      "0.8591269841269841\n",
      "batch_idx: 21\n",
      "0.8579545454545454\n",
      "batch_idx: 22\n",
      "0.8605072463768116\n",
      "batch_idx: 23\n",
      "0.8628472222222222\n",
      "batch_idx: 24\n",
      "0.8683333333333333\n",
      "Training Epoch: 227, total loss: 42.041011\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8511904761904762\n",
      "batch_idx: 14\n",
      "0.85\n",
      "batch_idx: 15\n",
      "0.84375\n",
      "batch_idx: 16\n",
      "0.8504901960784313\n",
      "batch_idx: 17\n",
      "0.8518518518518519\n",
      "batch_idx: 18\n",
      "0.8508771929824561\n",
      "batch_idx: 19\n",
      "0.85\n",
      "batch_idx: 20\n",
      "0.8492063492063492\n",
      "batch_idx: 21\n",
      "0.8503787878787878\n",
      "batch_idx: 22\n",
      "0.842391304347826\n",
      "batch_idx: 23\n",
      "0.8472222222222222\n",
      "batch_idx: 24\n",
      "0.85\n",
      "Training Epoch: 228, total loss: 42.430829\n",
      "batch_idx: 0\n",
      "1.0\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.9166666666666666\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.8809523809523809\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8657407407407407\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8636363636363636\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8461538461538461\n",
      "batch_idx: 13\n",
      "0.8422619047619048\n",
      "batch_idx: 14\n",
      "0.85\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8480392156862745\n",
      "batch_idx: 17\n",
      "0.8402777777777778\n",
      "batch_idx: 18\n",
      "0.8377192982456141\n",
      "batch_idx: 19\n",
      "0.8291666666666667\n",
      "batch_idx: 20\n",
      "0.8313492063492064\n",
      "batch_idx: 21\n",
      "0.8390151515151515\n",
      "batch_idx: 22\n",
      "0.8460144927536232\n",
      "batch_idx: 23\n",
      "0.8506944444444444\n",
      "batch_idx: 24\n",
      "0.8566666666666667\n",
      "Training Epoch: 229, total loss: 42.156733\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.8916666666666667\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.8869047619047619\n",
      "batch_idx: 7\n",
      "0.890625\n",
      "batch_idx: 8\n",
      "0.8888888888888888\n",
      "batch_idx: 9\n",
      "0.8916666666666667\n",
      "batch_idx: 10\n",
      "0.8939393939393939\n",
      "batch_idx: 11\n",
      "0.8888888888888888\n",
      "batch_idx: 12\n",
      "0.8814102564102564\n",
      "batch_idx: 13\n",
      "0.8720238095238095\n",
      "batch_idx: 14\n",
      "0.875\n",
      "batch_idx: 15\n",
      "0.875\n",
      "batch_idx: 16\n",
      "0.8725490196078431\n",
      "batch_idx: 17\n",
      "0.8773148148148148\n",
      "batch_idx: 18\n",
      "0.8771929824561403\n",
      "batch_idx: 19\n",
      "0.8729166666666667\n",
      "batch_idx: 20\n",
      "0.875\n",
      "batch_idx: 21\n",
      "0.875\n",
      "batch_idx: 22\n",
      "0.875\n",
      "batch_idx: 23\n",
      "0.8767361111111112\n",
      "batch_idx: 24\n",
      "0.8783333333333333\n",
      "Training Epoch: 230, total loss: 41.905003\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8625\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8472222222222222\n",
      "batch_idx: 12\n",
      "0.8461538461538461\n",
      "batch_idx: 13\n",
      "0.8452380952380952\n",
      "batch_idx: 14\n",
      "0.8388888888888889\n",
      "batch_idx: 15\n",
      "0.8385416666666666\n",
      "batch_idx: 16\n",
      "0.8382352941176471\n",
      "batch_idx: 17\n",
      "0.8310185185185185\n",
      "batch_idx: 18\n",
      "0.8377192982456141\n",
      "batch_idx: 19\n",
      "0.8395833333333333\n",
      "batch_idx: 20\n",
      "0.8412698412698413\n",
      "batch_idx: 21\n",
      "0.8428030303030303\n",
      "batch_idx: 22\n",
      "0.8442028985507246\n",
      "batch_idx: 23\n",
      "0.8420138888888888\n",
      "batch_idx: 24\n",
      "0.8466666666666667\n",
      "Training Epoch: 231, total loss: 42.214158\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8222222222222222\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8263888888888888\n",
      "batch_idx: 18\n",
      "0.8333333333333334\n",
      "batch_idx: 19\n",
      "0.8333333333333334\n",
      "batch_idx: 20\n",
      "0.8353174603174603\n",
      "batch_idx: 21\n",
      "0.8409090909090909\n",
      "batch_idx: 22\n",
      "0.8460144927536232\n",
      "batch_idx: 23\n",
      "0.8454861111111112\n",
      "batch_idx: 24\n",
      "0.8483333333333334\n",
      "Training Epoch: 232, total loss: 42.346746\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.8916666666666667\n",
      "batch_idx: 5\n",
      "0.9027777777777778\n",
      "batch_idx: 6\n",
      "0.8869047619047619\n",
      "batch_idx: 7\n",
      "0.890625\n",
      "batch_idx: 8\n",
      "0.8703703703703703\n",
      "batch_idx: 9\n",
      "0.8708333333333333\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8511904761904762\n",
      "batch_idx: 14\n",
      "0.8555555555555555\n",
      "batch_idx: 15\n",
      "0.859375\n",
      "batch_idx: 16\n",
      "0.8553921568627451\n",
      "batch_idx: 17\n",
      "0.8564814814814815\n",
      "batch_idx: 18\n",
      "0.8574561403508771\n",
      "batch_idx: 19\n",
      "0.85625\n",
      "batch_idx: 20\n",
      "0.8492063492063492\n",
      "batch_idx: 21\n",
      "0.8465909090909091\n",
      "batch_idx: 22\n",
      "0.8442028985507246\n",
      "batch_idx: 23\n",
      "0.8420138888888888\n",
      "batch_idx: 24\n",
      "0.8433333333333334\n",
      "Training Epoch: 233, total loss: 42.392380\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.8869047619047619\n",
      "batch_idx: 7\n",
      "0.890625\n",
      "batch_idx: 8\n",
      "0.8888888888888888\n",
      "batch_idx: 9\n",
      "0.8833333333333333\n",
      "batch_idx: 10\n",
      "0.8825757575757576\n",
      "batch_idx: 11\n",
      "0.8784722222222222\n",
      "batch_idx: 12\n",
      "0.8814102564102564\n",
      "batch_idx: 13\n",
      "0.8809523809523809\n",
      "batch_idx: 14\n",
      "0.8833333333333333\n",
      "batch_idx: 15\n",
      "0.875\n",
      "batch_idx: 16\n",
      "0.8823529411764706\n",
      "batch_idx: 17\n",
      "0.8819444444444444\n",
      "batch_idx: 18\n",
      "0.8771929824561403\n",
      "batch_idx: 19\n",
      "0.875\n",
      "batch_idx: 20\n",
      "0.871031746031746\n",
      "batch_idx: 21\n",
      "0.8693181818181818\n",
      "batch_idx: 22\n",
      "0.8641304347826086\n",
      "batch_idx: 23\n",
      "0.8663194444444444\n",
      "batch_idx: 24\n",
      "0.86\n",
      "Training Epoch: 234, total loss: 42.029917\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.90625\n",
      "batch_idx: 4\n",
      "0.9083333333333333\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.8928571428571429\n",
      "batch_idx: 7\n",
      "0.890625\n",
      "batch_idx: 8\n",
      "0.8935185185185185\n",
      "batch_idx: 9\n",
      "0.8875\n",
      "batch_idx: 10\n",
      "0.875\n",
      "batch_idx: 11\n",
      "0.8680555555555556\n",
      "batch_idx: 12\n",
      "0.875\n",
      "batch_idx: 13\n",
      "0.8690476190476191\n",
      "batch_idx: 14\n",
      "0.8666666666666667\n",
      "batch_idx: 15\n",
      "0.8645833333333334\n",
      "batch_idx: 16\n",
      "0.8627450980392157\n",
      "batch_idx: 17\n",
      "0.8657407407407407\n",
      "batch_idx: 18\n",
      "0.8618421052631579\n",
      "batch_idx: 19\n",
      "0.8604166666666667\n",
      "batch_idx: 20\n",
      "0.8571428571428571\n",
      "batch_idx: 21\n",
      "0.8560606060606061\n",
      "batch_idx: 22\n",
      "0.8478260869565217\n",
      "batch_idx: 23\n",
      "0.8454861111111112\n",
      "batch_idx: 24\n",
      "0.8466666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 235, total loss: 42.221441\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8657407407407407\n",
      "batch_idx: 9\n",
      "0.8708333333333333\n",
      "batch_idx: 10\n",
      "0.8674242424242424\n",
      "batch_idx: 11\n",
      "0.8784722222222222\n",
      "batch_idx: 12\n",
      "0.8782051282051282\n",
      "batch_idx: 13\n",
      "0.8809523809523809\n",
      "batch_idx: 14\n",
      "0.8805555555555555\n",
      "batch_idx: 15\n",
      "0.8723958333333334\n",
      "batch_idx: 16\n",
      "0.875\n",
      "batch_idx: 17\n",
      "0.875\n",
      "batch_idx: 18\n",
      "0.8793859649122807\n",
      "batch_idx: 19\n",
      "0.8729166666666667\n",
      "batch_idx: 20\n",
      "0.873015873015873\n",
      "batch_idx: 21\n",
      "0.875\n",
      "batch_idx: 22\n",
      "0.875\n",
      "batch_idx: 23\n",
      "0.8680555555555556\n",
      "batch_idx: 24\n",
      "0.8683333333333333\n",
      "Training Epoch: 236, total loss: 42.048180\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8287037037037037\n",
      "batch_idx: 18\n",
      "0.8289473684210527\n",
      "batch_idx: 19\n",
      "0.8291666666666667\n",
      "batch_idx: 20\n",
      "0.8353174603174603\n",
      "batch_idx: 21\n",
      "0.8390151515151515\n",
      "batch_idx: 22\n",
      "0.8442028985507246\n",
      "batch_idx: 23\n",
      "0.84375\n",
      "batch_idx: 24\n",
      "0.8416666666666667\n",
      "Training Epoch: 237, total loss: 42.386527\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.90625\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "batch_idx: 6\n",
      "0.9107142857142857\n",
      "batch_idx: 7\n",
      "0.8958333333333334\n",
      "batch_idx: 8\n",
      "0.8981481481481481\n",
      "batch_idx: 9\n",
      "0.9041666666666667\n",
      "batch_idx: 10\n",
      "0.8825757575757576\n",
      "batch_idx: 11\n",
      "0.8715277777777778\n",
      "batch_idx: 12\n",
      "0.875\n",
      "batch_idx: 13\n",
      "0.8690476190476191\n",
      "batch_idx: 14\n",
      "0.875\n",
      "batch_idx: 15\n",
      "0.8802083333333334\n",
      "batch_idx: 16\n",
      "0.8799019607843137\n",
      "batch_idx: 17\n",
      "0.8773148148148148\n",
      "batch_idx: 18\n",
      "0.8728070175438597\n",
      "batch_idx: 19\n",
      "0.8708333333333333\n",
      "batch_idx: 20\n",
      "0.8630952380952381\n",
      "batch_idx: 21\n",
      "0.8617424242424242\n",
      "batch_idx: 22\n",
      "0.8623188405797102\n",
      "batch_idx: 23\n",
      "0.8680555555555556\n",
      "batch_idx: 24\n",
      "0.8666666666666667\n",
      "Training Epoch: 238, total loss: 41.960727\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n",
      "batch_idx: 11\n",
      "0.8541666666666666\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8444444444444444\n",
      "batch_idx: 15\n",
      "0.8385416666666666\n",
      "batch_idx: 16\n",
      "0.8431372549019608\n",
      "batch_idx: 17\n",
      "0.8449074074074074\n",
      "batch_idx: 18\n",
      "0.8377192982456141\n",
      "batch_idx: 19\n",
      "0.8375\n",
      "batch_idx: 20\n",
      "0.8412698412698413\n",
      "batch_idx: 21\n",
      "0.8409090909090909\n",
      "batch_idx: 22\n",
      "0.8369565217391305\n",
      "batch_idx: 23\n",
      "0.8385416666666666\n",
      "batch_idx: 24\n",
      "0.8316666666666667\n",
      "Training Epoch: 239, total loss: 42.582077\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.9027777777777778\n",
      "batch_idx: 6\n",
      "0.9107142857142857\n",
      "batch_idx: 7\n",
      "0.9166666666666666\n",
      "batch_idx: 8\n",
      "0.9074074074074074\n",
      "batch_idx: 9\n",
      "0.9\n",
      "batch_idx: 10\n",
      "0.8901515151515151\n",
      "batch_idx: 11\n",
      "0.8854166666666666\n",
      "batch_idx: 12\n",
      "0.8846153846153846\n",
      "batch_idx: 13\n",
      "0.8839285714285714\n",
      "batch_idx: 14\n",
      "0.8777777777777778\n",
      "batch_idx: 15\n",
      "0.8828125\n",
      "batch_idx: 16\n",
      "0.8823529411764706\n",
      "batch_idx: 17\n",
      "0.8726851851851852\n",
      "batch_idx: 18\n",
      "0.8793859649122807\n",
      "batch_idx: 19\n",
      "0.875\n",
      "batch_idx: 20\n",
      "0.873015873015873\n",
      "batch_idx: 21\n",
      "0.8712121212121212\n",
      "batch_idx: 22\n",
      "0.8731884057971014\n",
      "batch_idx: 23\n",
      "0.8767361111111112\n",
      "batch_idx: 24\n",
      "0.8716666666666667\n",
      "Training Epoch: 240, total loss: 41.980029\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8301282051282052\n",
      "batch_idx: 13\n",
      "0.8303571428571429\n",
      "batch_idx: 14\n",
      "0.825\n",
      "batch_idx: 15\n",
      "0.828125\n",
      "batch_idx: 16\n",
      "0.8333333333333334\n",
      "batch_idx: 17\n",
      "0.8333333333333334\n",
      "batch_idx: 18\n",
      "0.831140350877193\n",
      "batch_idx: 19\n",
      "0.8270833333333333\n",
      "batch_idx: 20\n",
      "0.8273809523809523\n",
      "batch_idx: 21\n",
      "0.8314393939393939\n",
      "batch_idx: 22\n",
      "0.8297101449275363\n",
      "batch_idx: 23\n",
      "0.828125\n",
      "batch_idx: 24\n",
      "0.8316666666666667\n",
      "Training Epoch: 241, total loss: 42.625043\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8657407407407407\n",
      "batch_idx: 9\n",
      "0.8708333333333333\n",
      "batch_idx: 10\n",
      "0.8712121212121212\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.8685897435897436\n",
      "batch_idx: 13\n",
      "0.8690476190476191\n",
      "batch_idx: 14\n",
      "0.8611111111111112\n",
      "batch_idx: 15\n",
      "0.8645833333333334\n",
      "batch_idx: 16\n",
      "0.8700980392156863\n",
      "batch_idx: 17\n",
      "0.8703703703703703\n",
      "batch_idx: 18\n",
      "0.8662280701754386\n",
      "batch_idx: 19\n",
      "0.8666666666666667\n",
      "batch_idx: 20\n",
      "0.8630952380952381\n",
      "batch_idx: 21\n",
      "0.8598484848484849\n",
      "batch_idx: 22\n",
      "0.8623188405797102\n",
      "batch_idx: 23\n",
      "0.8611111111111112\n",
      "batch_idx: 24\n",
      "0.8616666666666667\n",
      "Training Epoch: 242, total loss: 42.094046\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8257575757575758\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8244047619047619\n",
      "batch_idx: 14\n",
      "0.8222222222222222\n",
      "batch_idx: 15\n",
      "0.828125\n",
      "batch_idx: 16\n",
      "0.8357843137254902\n",
      "batch_idx: 17\n",
      "0.8425925925925926\n",
      "batch_idx: 18\n",
      "0.8464912280701754\n",
      "batch_idx: 19\n",
      "0.85\n",
      "batch_idx: 20\n",
      "0.8472222222222222\n",
      "batch_idx: 21\n",
      "0.8465909090909091\n",
      "batch_idx: 22\n",
      "0.8514492753623188\n",
      "batch_idx: 23\n",
      "0.84375\n",
      "batch_idx: 24\n",
      "0.8416666666666667\n",
      "Training Epoch: 243, total loss: 42.364134\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.8958333333333334\n",
      "batch_idx: 6\n",
      "0.8988095238095238\n",
      "batch_idx: 7\n",
      "0.875\n",
      "batch_idx: 8\n",
      "0.8703703703703703\n",
      "batch_idx: 9\n",
      "0.8708333333333333\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.8525641025641025\n",
      "batch_idx: 13\n",
      "0.8422619047619048\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.8359375\n",
      "batch_idx: 16\n",
      "0.8406862745098039\n",
      "batch_idx: 17\n",
      "0.8449074074074074\n",
      "batch_idx: 18\n",
      "0.8486842105263158\n",
      "batch_idx: 19\n",
      "0.85\n",
      "batch_idx: 20\n",
      "0.8492063492063492\n",
      "batch_idx: 21\n",
      "0.8522727272727273\n",
      "batch_idx: 22\n",
      "0.855072463768116\n",
      "batch_idx: 23\n",
      "0.8541666666666666\n",
      "batch_idx: 24\n",
      "0.8516666666666667\n",
      "Training Epoch: 244, total loss: 42.188253\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9375\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8625\n",
      "batch_idx: 10\n",
      "0.8674242424242424\n",
      "batch_idx: 11\n",
      "0.8680555555555556\n",
      "batch_idx: 12\n",
      "0.8653846153846154\n",
      "batch_idx: 13\n",
      "0.8720238095238095\n",
      "batch_idx: 14\n",
      "0.8638888888888889\n",
      "batch_idx: 15\n",
      "0.859375\n",
      "batch_idx: 16\n",
      "0.8602941176470589\n",
      "batch_idx: 17\n",
      "0.8495370370370371\n",
      "batch_idx: 18\n",
      "0.8530701754385965\n",
      "batch_idx: 19\n",
      "0.85625\n",
      "batch_idx: 20\n",
      "0.8591269841269841\n",
      "batch_idx: 21\n",
      "0.8598484848484849\n",
      "batch_idx: 22\n",
      "0.8532608695652174\n",
      "batch_idx: 23\n",
      "0.8541666666666666\n",
      "batch_idx: 24\n",
      "0.85\n",
      "Training Epoch: 245, total loss: 42.264749\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8657407407407407\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8712121212121212\n",
      "batch_idx: 11\n",
      "0.8784722222222222\n",
      "batch_idx: 12\n",
      "0.8685897435897436\n",
      "batch_idx: 13\n",
      "0.8630952380952381\n",
      "batch_idx: 14\n",
      "0.8638888888888889\n",
      "batch_idx: 15\n",
      "0.8619791666666666\n",
      "batch_idx: 16\n",
      "0.8651960784313726\n",
      "batch_idx: 17\n",
      "0.8634259259259259\n",
      "batch_idx: 18\n",
      "0.868421052631579\n",
      "batch_idx: 19\n",
      "0.86875\n",
      "batch_idx: 20\n",
      "0.8650793650793651\n",
      "batch_idx: 21\n",
      "0.8636363636363636\n",
      "batch_idx: 22\n",
      "0.8641304347826086\n",
      "batch_idx: 23\n",
      "0.8611111111111112\n",
      "batch_idx: 24\n",
      "0.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 246, total loss: 42.229728\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8708333333333333\n",
      "batch_idx: 10\n",
      "0.8712121212121212\n",
      "batch_idx: 11\n",
      "0.8715277777777778\n",
      "batch_idx: 12\n",
      "0.8653846153846154\n",
      "batch_idx: 13\n",
      "0.8660714285714286\n",
      "batch_idx: 14\n",
      "0.875\n",
      "batch_idx: 15\n",
      "0.8776041666666666\n",
      "batch_idx: 16\n",
      "0.8799019607843137\n",
      "batch_idx: 17\n",
      "0.875\n",
      "batch_idx: 18\n",
      "0.8728070175438597\n",
      "batch_idx: 19\n",
      "0.86875\n",
      "batch_idx: 20\n",
      "0.8690476190476191\n",
      "batch_idx: 21\n",
      "0.8693181818181818\n",
      "batch_idx: 22\n",
      "0.8731884057971014\n",
      "batch_idx: 23\n",
      "0.8715277777777778\n",
      "batch_idx: 24\n",
      "0.8733333333333333\n",
      "Training Epoch: 247, total loss: 41.884656\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.875\n",
      "batch_idx: 8\n",
      "0.875\n",
      "batch_idx: 9\n",
      "0.8791666666666667\n",
      "batch_idx: 10\n",
      "0.8825757575757576\n",
      "batch_idx: 11\n",
      "0.8854166666666666\n",
      "batch_idx: 12\n",
      "0.8782051282051282\n",
      "batch_idx: 13\n",
      "0.8839285714285714\n",
      "batch_idx: 14\n",
      "0.8805555555555555\n",
      "batch_idx: 15\n",
      "0.8828125\n",
      "batch_idx: 16\n",
      "0.8774509803921569\n",
      "batch_idx: 17\n",
      "0.875\n",
      "batch_idx: 18\n",
      "0.875\n",
      "batch_idx: 19\n",
      "0.8791666666666667\n",
      "batch_idx: 20\n",
      "0.8809523809523809\n",
      "batch_idx: 21\n",
      "0.8768939393939394\n",
      "batch_idx: 22\n",
      "0.8786231884057971\n",
      "batch_idx: 23\n",
      "0.8767361111111112\n",
      "batch_idx: 24\n",
      "0.875\n",
      "Training Epoch: 248, total loss: 41.777178\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8842592592592593\n",
      "batch_idx: 9\n",
      "0.8833333333333333\n",
      "batch_idx: 10\n",
      "0.8825757575757576\n",
      "batch_idx: 11\n",
      "0.8819444444444444\n",
      "batch_idx: 12\n",
      "0.8846153846153846\n",
      "batch_idx: 13\n",
      "0.8898809523809523\n",
      "batch_idx: 14\n",
      "0.8861111111111111\n",
      "batch_idx: 15\n",
      "0.8880208333333334\n",
      "batch_idx: 16\n",
      "0.8872549019607843\n",
      "batch_idx: 17\n",
      "0.8842592592592593\n",
      "batch_idx: 18\n",
      "0.8771929824561403\n",
      "batch_idx: 19\n",
      "0.8708333333333333\n",
      "batch_idx: 20\n",
      "0.873015873015873\n",
      "batch_idx: 21\n",
      "0.8712121212121212\n",
      "batch_idx: 22\n",
      "0.875\n",
      "batch_idx: 23\n",
      "0.8715277777777778\n",
      "batch_idx: 24\n",
      "0.8683333333333333\n",
      "Training Epoch: 249, total loss: 41.774079\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.8589743589743589\n",
      "batch_idx: 13\n",
      "0.8541666666666666\n",
      "batch_idx: 14\n",
      "0.8527777777777777\n",
      "batch_idx: 15\n",
      "0.8515625\n",
      "batch_idx: 16\n",
      "0.8578431372549019\n",
      "batch_idx: 17\n",
      "0.8611111111111112\n",
      "batch_idx: 18\n",
      "0.868421052631579\n",
      "batch_idx: 19\n",
      "0.8708333333333333\n",
      "batch_idx: 20\n",
      "0.871031746031746\n",
      "batch_idx: 21\n",
      "0.8731060606060606\n",
      "batch_idx: 22\n",
      "0.8731884057971014\n",
      "batch_idx: 23\n",
      "0.8715277777777778\n",
      "batch_idx: 24\n",
      "0.875\n",
      "Training Epoch: 250, total loss: 41.769673\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8674242424242424\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.8589743589743589\n",
      "batch_idx: 13\n",
      "0.8630952380952381\n",
      "batch_idx: 14\n",
      "0.8583333333333333\n",
      "batch_idx: 15\n",
      "0.8619791666666666\n",
      "batch_idx: 16\n",
      "0.8676470588235294\n",
      "batch_idx: 17\n",
      "0.8726851851851852\n",
      "batch_idx: 18\n",
      "0.875\n",
      "batch_idx: 19\n",
      "0.8708333333333333\n",
      "batch_idx: 20\n",
      "0.8690476190476191\n",
      "batch_idx: 21\n",
      "0.8674242424242424\n",
      "batch_idx: 22\n",
      "0.8659420289855072\n",
      "batch_idx: 23\n",
      "0.8645833333333334\n",
      "batch_idx: 24\n",
      "0.8666666666666667\n",
      "Training Epoch: 251, total loss: 41.878290\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8657407407407407\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8636363636363636\n",
      "batch_idx: 11\n",
      "0.8715277777777778\n",
      "batch_idx: 12\n",
      "0.8685897435897436\n",
      "batch_idx: 13\n",
      "0.8720238095238095\n",
      "batch_idx: 14\n",
      "0.8638888888888889\n",
      "batch_idx: 15\n",
      "0.8645833333333334\n",
      "batch_idx: 16\n",
      "0.8627450980392157\n",
      "batch_idx: 17\n",
      "0.8680555555555556\n",
      "batch_idx: 18\n",
      "0.868421052631579\n",
      "batch_idx: 19\n",
      "0.86875\n",
      "batch_idx: 20\n",
      "0.871031746031746\n",
      "batch_idx: 21\n",
      "0.8636363636363636\n",
      "batch_idx: 22\n",
      "0.8605072463768116\n",
      "batch_idx: 23\n",
      "0.8611111111111112\n",
      "batch_idx: 24\n",
      "0.855\n",
      "Training Epoch: 252, total loss: 42.077021\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.9027777777777778\n",
      "batch_idx: 6\n",
      "0.8988095238095238\n",
      "batch_idx: 7\n",
      "0.9114583333333334\n",
      "batch_idx: 8\n",
      "0.9166666666666666\n",
      "batch_idx: 9\n",
      "0.9083333333333333\n",
      "batch_idx: 10\n",
      "0.8977272727272727\n",
      "batch_idx: 11\n",
      "0.9027777777777778\n",
      "batch_idx: 12\n",
      "0.8974358974358975\n",
      "batch_idx: 13\n",
      "0.8928571428571429\n",
      "batch_idx: 14\n",
      "0.8972222222222223\n",
      "batch_idx: 15\n",
      "0.8958333333333334\n",
      "batch_idx: 16\n",
      "0.8970588235294118\n",
      "batch_idx: 17\n",
      "0.8935185185185185\n",
      "batch_idx: 18\n",
      "0.8925438596491229\n",
      "batch_idx: 19\n",
      "0.89375\n",
      "batch_idx: 20\n",
      "0.8888888888888888\n",
      "batch_idx: 21\n",
      "0.8901515151515151\n",
      "batch_idx: 22\n",
      "0.8894927536231884\n",
      "batch_idx: 23\n",
      "0.8923611111111112\n",
      "batch_idx: 24\n",
      "0.8883333333333333\n",
      "Training Epoch: 253, total loss: 41.662924\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.875\n",
      "batch_idx: 8\n",
      "0.8796296296296297\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8674242424242424\n",
      "batch_idx: 11\n",
      "0.875\n",
      "batch_idx: 12\n",
      "0.875\n",
      "batch_idx: 13\n",
      "0.8809523809523809\n",
      "batch_idx: 14\n",
      "0.8833333333333333\n",
      "batch_idx: 15\n",
      "0.8828125\n",
      "batch_idx: 16\n",
      "0.8872549019607843\n",
      "batch_idx: 17\n",
      "0.8842592592592593\n",
      "batch_idx: 18\n",
      "0.8793859649122807\n",
      "batch_idx: 19\n",
      "0.8791666666666667\n",
      "batch_idx: 20\n",
      "0.8809523809523809\n",
      "batch_idx: 21\n",
      "0.875\n",
      "batch_idx: 22\n",
      "0.8731884057971014\n",
      "batch_idx: 23\n",
      "0.8715277777777778\n",
      "batch_idx: 24\n",
      "0.8633333333333333\n",
      "Training Epoch: 254, total loss: 42.252332\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8625\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n",
      "batch_idx: 11\n",
      "0.8645833333333334\n",
      "batch_idx: 12\n",
      "0.8621794871794872\n",
      "batch_idx: 13\n",
      "0.8601190476190477\n",
      "batch_idx: 14\n",
      "0.8611111111111112\n",
      "batch_idx: 15\n",
      "0.8671875\n",
      "batch_idx: 16\n",
      "0.8676470588235294\n",
      "batch_idx: 17\n",
      "0.8680555555555556\n",
      "batch_idx: 18\n",
      "0.8728070175438597\n",
      "batch_idx: 19\n",
      "0.8666666666666667\n",
      "batch_idx: 20\n",
      "0.871031746031746\n",
      "batch_idx: 21\n",
      "0.8731060606060606\n",
      "batch_idx: 22\n",
      "0.875\n",
      "batch_idx: 23\n",
      "0.8784722222222222\n",
      "batch_idx: 24\n",
      "0.8766666666666667\n",
      "Training Epoch: 255, total loss: 41.735763\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8680555555555556\n",
      "batch_idx: 12\n",
      "0.8653846153846154\n",
      "batch_idx: 13\n",
      "0.8511904761904762\n",
      "batch_idx: 14\n",
      "0.8527777777777777\n",
      "batch_idx: 15\n",
      "0.8515625\n",
      "batch_idx: 16\n",
      "0.8553921568627451\n",
      "batch_idx: 17\n",
      "0.8564814814814815\n",
      "batch_idx: 18\n",
      "0.8552631578947368\n",
      "batch_idx: 19\n",
      "0.85625\n",
      "batch_idx: 20\n",
      "0.8571428571428571\n",
      "batch_idx: 21\n",
      "0.8617424242424242\n",
      "batch_idx: 22\n",
      "0.8659420289855072\n",
      "batch_idx: 23\n",
      "0.859375\n",
      "batch_idx: 24\n",
      "0.8566666666666667\n",
      "Training Epoch: 256, total loss: 42.238741\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.9305555555555556\n",
      "batch_idx: 3\n",
      "0.9375\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.8888888888888888\n",
      "batch_idx: 6\n",
      "0.8928571428571429\n",
      "batch_idx: 7\n",
      "0.8854166666666666\n",
      "batch_idx: 8\n",
      "0.8888888888888888\n",
      "batch_idx: 9\n",
      "0.8875\n",
      "batch_idx: 10\n",
      "0.8825757575757576\n",
      "batch_idx: 11\n",
      "0.8819444444444444\n",
      "batch_idx: 12\n",
      "0.8814102564102564\n",
      "batch_idx: 13\n",
      "0.8779761904761905\n",
      "batch_idx: 14\n",
      "0.8666666666666667\n",
      "batch_idx: 15\n",
      "0.8723958333333334\n",
      "batch_idx: 16\n",
      "0.8774509803921569\n",
      "batch_idx: 17\n",
      "0.8773148148148148\n",
      "batch_idx: 18\n",
      "0.8618421052631579\n",
      "batch_idx: 19\n",
      "0.8666666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 20\n",
      "0.8650793650793651\n",
      "batch_idx: 21\n",
      "0.8598484848484849\n",
      "batch_idx: 22\n",
      "0.8586956521739131\n",
      "batch_idx: 23\n",
      "0.8628472222222222\n",
      "batch_idx: 24\n",
      "0.8566666666666667\n",
      "Training Epoch: 257, total loss: 42.113908\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.8589743589743589\n",
      "batch_idx: 13\n",
      "0.8601190476190477\n",
      "batch_idx: 14\n",
      "0.8611111111111112\n",
      "batch_idx: 15\n",
      "0.8567708333333334\n",
      "batch_idx: 16\n",
      "0.8578431372549019\n",
      "batch_idx: 17\n",
      "0.8587962962962963\n",
      "batch_idx: 18\n",
      "0.8618421052631579\n",
      "batch_idx: 19\n",
      "0.8625\n",
      "batch_idx: 20\n",
      "0.8670634920634921\n",
      "batch_idx: 21\n",
      "0.8674242424242424\n",
      "batch_idx: 22\n",
      "0.8713768115942029\n",
      "batch_idx: 23\n",
      "0.8715277777777778\n",
      "batch_idx: 24\n",
      "0.87\n",
      "Training Epoch: 258, total loss: 41.843762\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.9305555555555556\n",
      "batch_idx: 3\n",
      "0.9375\n",
      "batch_idx: 4\n",
      "0.9166666666666666\n",
      "batch_idx: 5\n",
      "0.9097222222222222\n",
      "batch_idx: 6\n",
      "0.8988095238095238\n",
      "batch_idx: 7\n",
      "0.8854166666666666\n",
      "batch_idx: 8\n",
      "0.8796296296296297\n",
      "batch_idx: 9\n",
      "0.8708333333333333\n",
      "batch_idx: 10\n",
      "0.8787878787878788\n",
      "batch_idx: 11\n",
      "0.8680555555555556\n",
      "batch_idx: 12\n",
      "0.875\n",
      "batch_idx: 13\n",
      "0.8690476190476191\n",
      "batch_idx: 14\n",
      "0.8722222222222222\n",
      "batch_idx: 15\n",
      "0.8776041666666666\n",
      "batch_idx: 16\n",
      "0.8774509803921569\n",
      "batch_idx: 17\n",
      "0.8518518518518519\n",
      "batch_idx: 18\n",
      "0.8486842105263158\n",
      "batch_idx: 19\n",
      "0.85\n",
      "batch_idx: 20\n",
      "0.8492063492063492\n",
      "batch_idx: 21\n",
      "0.8484848484848485\n",
      "batch_idx: 22\n",
      "0.8496376811594203\n",
      "batch_idx: 23\n",
      "0.8472222222222222\n",
      "batch_idx: 24\n",
      "0.8483333333333334\n",
      "Training Epoch: 259, total loss: 42.406000\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8564814814814815\n",
      "batch_idx: 9\n",
      "0.8625\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.8589743589743589\n",
      "batch_idx: 13\n",
      "0.8511904761904762\n",
      "batch_idx: 14\n",
      "0.85\n",
      "batch_idx: 15\n",
      "0.8567708333333334\n",
      "batch_idx: 16\n",
      "0.8578431372549019\n",
      "batch_idx: 17\n",
      "0.8611111111111112\n",
      "batch_idx: 18\n",
      "0.8640350877192983\n",
      "batch_idx: 19\n",
      "0.8666666666666667\n",
      "batch_idx: 20\n",
      "0.8630952380952381\n",
      "batch_idx: 21\n",
      "0.8674242424242424\n",
      "batch_idx: 22\n",
      "0.8713768115942029\n",
      "batch_idx: 23\n",
      "0.8680555555555556\n",
      "batch_idx: 24\n",
      "0.8683333333333333\n",
      "Training Epoch: 260, total loss: 41.914778\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.9083333333333333\n",
      "batch_idx: 5\n",
      "0.9097222222222222\n",
      "batch_idx: 6\n",
      "0.9107142857142857\n",
      "batch_idx: 7\n",
      "0.9166666666666666\n",
      "batch_idx: 8\n",
      "0.9166666666666666\n",
      "batch_idx: 9\n",
      "0.8958333333333334\n",
      "batch_idx: 10\n",
      "0.8939393939393939\n",
      "batch_idx: 11\n",
      "0.8993055555555556\n",
      "batch_idx: 12\n",
      "0.8942307692307693\n",
      "batch_idx: 13\n",
      "0.8928571428571429\n",
      "batch_idx: 14\n",
      "0.9\n",
      "batch_idx: 15\n",
      "0.9010416666666666\n",
      "batch_idx: 16\n",
      "0.8995098039215687\n",
      "batch_idx: 17\n",
      "0.8958333333333334\n",
      "batch_idx: 18\n",
      "0.8969298245614035\n",
      "batch_idx: 19\n",
      "0.8979166666666667\n",
      "batch_idx: 20\n",
      "0.8928571428571429\n",
      "batch_idx: 21\n",
      "0.8863636363636364\n",
      "batch_idx: 22\n",
      "0.8768115942028986\n",
      "batch_idx: 23\n",
      "0.8784722222222222\n",
      "batch_idx: 24\n",
      "0.875\n",
      "Training Epoch: 261, total loss: 41.802286\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.8472222222222222\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8452380952380952\n",
      "batch_idx: 14\n",
      "0.8388888888888889\n",
      "batch_idx: 15\n",
      "0.84375\n",
      "batch_idx: 16\n",
      "0.8480392156862745\n",
      "batch_idx: 17\n",
      "0.8518518518518519\n",
      "batch_idx: 18\n",
      "0.8574561403508771\n",
      "batch_idx: 19\n",
      "0.8520833333333333\n",
      "batch_idx: 20\n",
      "0.8452380952380952\n",
      "batch_idx: 21\n",
      "0.8465909090909091\n",
      "batch_idx: 22\n",
      "0.8496376811594203\n",
      "batch_idx: 23\n",
      "0.8472222222222222\n",
      "batch_idx: 24\n",
      "0.8433333333333334\n",
      "Training Epoch: 262, total loss: 42.124501\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.8888888888888888\n",
      "batch_idx: 6\n",
      "0.8988095238095238\n",
      "batch_idx: 7\n",
      "0.875\n",
      "batch_idx: 8\n",
      "0.8842592592592593\n",
      "batch_idx: 9\n",
      "0.8833333333333333\n",
      "batch_idx: 10\n",
      "0.8901515151515151\n",
      "batch_idx: 11\n",
      "0.8854166666666666\n",
      "batch_idx: 12\n",
      "0.8878205128205128\n",
      "batch_idx: 13\n",
      "0.8898809523809523\n",
      "batch_idx: 14\n",
      "0.8944444444444445\n",
      "batch_idx: 15\n",
      "0.8958333333333334\n",
      "batch_idx: 16\n",
      "0.8921568627450981\n",
      "batch_idx: 17\n",
      "0.8935185185185185\n",
      "batch_idx: 18\n",
      "0.8859649122807017\n",
      "batch_idx: 19\n",
      "0.875\n",
      "batch_idx: 20\n",
      "0.876984126984127\n",
      "batch_idx: 21\n",
      "0.8712121212121212\n",
      "batch_idx: 22\n",
      "0.8713768115942029\n",
      "batch_idx: 23\n",
      "0.8767361111111112\n",
      "batch_idx: 24\n",
      "0.8733333333333333\n",
      "Training Epoch: 263, total loss: 41.890435\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8674242424242424\n",
      "batch_idx: 11\n",
      "0.8680555555555556\n",
      "batch_idx: 12\n",
      "0.8653846153846154\n",
      "batch_idx: 13\n",
      "0.8601190476190477\n",
      "batch_idx: 14\n",
      "0.8666666666666667\n",
      "batch_idx: 15\n",
      "0.8567708333333334\n",
      "batch_idx: 16\n",
      "0.8627450980392157\n",
      "batch_idx: 17\n",
      "0.8680555555555556\n",
      "batch_idx: 18\n",
      "0.8728070175438597\n",
      "batch_idx: 19\n",
      "0.8729166666666667\n",
      "batch_idx: 20\n",
      "0.8690476190476191\n",
      "batch_idx: 21\n",
      "0.8693181818181818\n",
      "batch_idx: 22\n",
      "0.8659420289855072\n",
      "batch_idx: 23\n",
      "0.8628472222222222\n",
      "batch_idx: 24\n",
      "0.8616666666666667\n",
      "Training Epoch: 264, total loss: 42.097933\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8657407407407407\n",
      "batch_idx: 9\n",
      "0.875\n",
      "batch_idx: 10\n",
      "0.8825757575757576\n",
      "batch_idx: 11\n",
      "0.8854166666666666\n",
      "batch_idx: 12\n",
      "0.8846153846153846\n",
      "batch_idx: 13\n",
      "0.8839285714285714\n",
      "batch_idx: 14\n",
      "0.8861111111111111\n",
      "batch_idx: 15\n",
      "0.890625\n",
      "batch_idx: 16\n",
      "0.8848039215686274\n",
      "batch_idx: 17\n",
      "0.8773148148148148\n",
      "batch_idx: 18\n",
      "0.881578947368421\n",
      "batch_idx: 19\n",
      "0.8854166666666666\n",
      "batch_idx: 20\n",
      "0.8849206349206349\n",
      "batch_idx: 21\n",
      "0.8787878787878788\n",
      "batch_idx: 22\n",
      "0.8768115942028986\n",
      "batch_idx: 23\n",
      "0.8767361111111112\n",
      "batch_idx: 24\n",
      "0.87\n",
      "Training Epoch: 265, total loss: 41.914899\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8564814814814815\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.8589743589743589\n",
      "batch_idx: 13\n",
      "0.8630952380952381\n",
      "batch_idx: 14\n",
      "0.8638888888888889\n",
      "batch_idx: 15\n",
      "0.8723958333333334\n",
      "batch_idx: 16\n",
      "0.8676470588235294\n",
      "batch_idx: 17\n",
      "0.8726851851851852\n",
      "batch_idx: 18\n",
      "0.8662280701754386\n",
      "batch_idx: 19\n",
      "0.8645833333333334\n",
      "batch_idx: 20\n",
      "0.8690476190476191\n",
      "batch_idx: 21\n",
      "0.8674242424242424\n",
      "batch_idx: 22\n",
      "0.8659420289855072\n",
      "batch_idx: 23\n",
      "0.8645833333333334\n",
      "batch_idx: 24\n",
      "0.8616666666666667\n",
      "Training Epoch: 266, total loss: 42.013095\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8301282051282052\n",
      "batch_idx: 13\n",
      "0.8303571428571429\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.8359375\n",
      "batch_idx: 16\n",
      "0.8406862745098039\n",
      "batch_idx: 17\n",
      "0.8425925925925926\n",
      "batch_idx: 18\n",
      "0.8377192982456141\n",
      "batch_idx: 19\n",
      "0.8395833333333333\n",
      "batch_idx: 20\n",
      "0.8353174603174603\n",
      "batch_idx: 21\n",
      "0.8352272727272727\n",
      "batch_idx: 22\n",
      "0.8369565217391305\n",
      "batch_idx: 23\n",
      "0.8385416666666666\n",
      "batch_idx: 24\n",
      "0.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 267, total loss: 42.587294\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.8461538461538461\n",
      "batch_idx: 13\n",
      "0.8482142857142857\n",
      "batch_idx: 14\n",
      "0.8472222222222222\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8480392156862745\n",
      "batch_idx: 17\n",
      "0.8449074074074074\n",
      "batch_idx: 18\n",
      "0.8421052631578947\n",
      "batch_idx: 19\n",
      "0.8333333333333334\n",
      "batch_idx: 20\n",
      "0.8353174603174603\n",
      "batch_idx: 21\n",
      "0.8390151515151515\n",
      "batch_idx: 22\n",
      "0.8387681159420289\n",
      "batch_idx: 23\n",
      "0.8402777777777778\n",
      "batch_idx: 24\n",
      "0.84\n",
      "Training Epoch: 268, total loss: 42.454117\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n",
      "batch_idx: 11\n",
      "0.8611111111111112\n",
      "batch_idx: 12\n",
      "0.8621794871794872\n",
      "batch_idx: 13\n",
      "0.8541666666666666\n",
      "batch_idx: 14\n",
      "0.8527777777777777\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8529411764705882\n",
      "batch_idx: 17\n",
      "0.8541666666666666\n",
      "batch_idx: 18\n",
      "0.8552631578947368\n",
      "batch_idx: 19\n",
      "0.85\n",
      "batch_idx: 20\n",
      "0.8511904761904762\n",
      "batch_idx: 21\n",
      "0.8503787878787878\n",
      "batch_idx: 22\n",
      "0.8405797101449275\n",
      "batch_idx: 23\n",
      "0.84375\n",
      "batch_idx: 24\n",
      "0.8416666666666667\n",
      "Training Epoch: 269, total loss: 42.550818\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8657407407407407\n",
      "batch_idx: 9\n",
      "0.8791666666666667\n",
      "batch_idx: 10\n",
      "0.875\n",
      "batch_idx: 11\n",
      "0.8715277777777778\n",
      "batch_idx: 12\n",
      "0.875\n",
      "batch_idx: 13\n",
      "0.8809523809523809\n",
      "batch_idx: 14\n",
      "0.8777777777777778\n",
      "batch_idx: 15\n",
      "0.8723958333333334\n",
      "batch_idx: 16\n",
      "0.8725490196078431\n",
      "batch_idx: 17\n",
      "0.8680555555555556\n",
      "batch_idx: 18\n",
      "0.8706140350877193\n",
      "batch_idx: 19\n",
      "0.85\n",
      "batch_idx: 20\n",
      "0.8492063492063492\n",
      "batch_idx: 21\n",
      "0.8541666666666666\n",
      "batch_idx: 22\n",
      "0.8568840579710145\n",
      "batch_idx: 23\n",
      "0.8576388888888888\n",
      "batch_idx: 24\n",
      "0.85\n",
      "Training Epoch: 270, total loss: 42.186388\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8422619047619048\n",
      "batch_idx: 14\n",
      "0.8388888888888889\n",
      "batch_idx: 15\n",
      "0.8359375\n",
      "batch_idx: 16\n",
      "0.8382352941176471\n",
      "batch_idx: 17\n",
      "0.8472222222222222\n",
      "batch_idx: 18\n",
      "0.8530701754385965\n",
      "batch_idx: 19\n",
      "0.8541666666666666\n",
      "batch_idx: 20\n",
      "0.8591269841269841\n",
      "batch_idx: 21\n",
      "0.8636363636363636\n",
      "batch_idx: 22\n",
      "0.855072463768116\n",
      "batch_idx: 23\n",
      "0.8524305555555556\n",
      "batch_idx: 24\n",
      "0.8533333333333334\n",
      "Training Epoch: 271, total loss: 42.212216\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8472222222222222\n",
      "batch_idx: 12\n",
      "0.8365384615384616\n",
      "batch_idx: 13\n",
      "0.8333333333333334\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8310185185185185\n",
      "batch_idx: 18\n",
      "0.8377192982456141\n",
      "batch_idx: 19\n",
      "0.8416666666666667\n",
      "batch_idx: 20\n",
      "0.8432539682539683\n",
      "batch_idx: 21\n",
      "0.8484848484848485\n",
      "batch_idx: 22\n",
      "0.8460144927536232\n",
      "batch_idx: 23\n",
      "0.8489583333333334\n",
      "batch_idx: 24\n",
      "0.8433333333333334\n",
      "Training Epoch: 272, total loss: 42.303511\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.90625\n",
      "batch_idx: 4\n",
      "0.9083333333333333\n",
      "batch_idx: 5\n",
      "0.9027777777777778\n",
      "batch_idx: 6\n",
      "0.8928571428571429\n",
      "batch_idx: 7\n",
      "0.8802083333333334\n",
      "batch_idx: 8\n",
      "0.8842592592592593\n",
      "batch_idx: 9\n",
      "0.8708333333333333\n",
      "batch_idx: 10\n",
      "0.8712121212121212\n",
      "batch_idx: 11\n",
      "0.875\n",
      "batch_idx: 12\n",
      "0.8717948717948718\n",
      "batch_idx: 13\n",
      "0.8690476190476191\n",
      "batch_idx: 14\n",
      "0.8722222222222222\n",
      "batch_idx: 15\n",
      "0.8776041666666666\n",
      "batch_idx: 16\n",
      "0.8774509803921569\n",
      "batch_idx: 17\n",
      "0.8796296296296297\n",
      "batch_idx: 18\n",
      "0.875\n",
      "batch_idx: 19\n",
      "0.86875\n",
      "batch_idx: 20\n",
      "0.873015873015873\n",
      "batch_idx: 21\n",
      "0.8712121212121212\n",
      "batch_idx: 22\n",
      "0.8677536231884058\n",
      "batch_idx: 23\n",
      "0.8611111111111112\n",
      "batch_idx: 24\n",
      "0.86\n",
      "Training Epoch: 273, total loss: 42.186749\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8564814814814815\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.875\n",
      "batch_idx: 11\n",
      "0.8645833333333334\n",
      "batch_idx: 12\n",
      "0.8653846153846154\n",
      "batch_idx: 13\n",
      "0.8601190476190477\n",
      "batch_idx: 14\n",
      "0.8583333333333333\n",
      "batch_idx: 15\n",
      "0.8489583333333334\n",
      "batch_idx: 16\n",
      "0.8480392156862745\n",
      "batch_idx: 17\n",
      "0.8495370370370371\n",
      "batch_idx: 18\n",
      "0.8464912280701754\n",
      "batch_idx: 19\n",
      "0.8541666666666666\n",
      "batch_idx: 20\n",
      "0.8531746031746031\n",
      "batch_idx: 21\n",
      "0.8541666666666666\n",
      "batch_idx: 22\n",
      "0.8605072463768116\n",
      "batch_idx: 23\n",
      "0.8541666666666666\n",
      "batch_idx: 24\n",
      "0.8516666666666667\n",
      "Training Epoch: 274, total loss: 42.298200\n",
      "batch_idx: 0\n",
      "1.0\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.90625\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8796296296296297\n",
      "batch_idx: 9\n",
      "0.875\n",
      "batch_idx: 10\n",
      "0.8636363636363636\n",
      "batch_idx: 11\n",
      "0.8611111111111112\n",
      "batch_idx: 12\n",
      "0.8653846153846154\n",
      "batch_idx: 13\n",
      "0.8690476190476191\n",
      "batch_idx: 14\n",
      "0.8666666666666667\n",
      "batch_idx: 15\n",
      "0.8697916666666666\n",
      "batch_idx: 16\n",
      "0.8725490196078431\n",
      "batch_idx: 17\n",
      "0.8726851851851852\n",
      "batch_idx: 18\n",
      "0.875\n",
      "batch_idx: 19\n",
      "0.8791666666666667\n",
      "batch_idx: 20\n",
      "0.8849206349206349\n",
      "batch_idx: 21\n",
      "0.8825757575757576\n",
      "batch_idx: 22\n",
      "0.8804347826086957\n",
      "batch_idx: 23\n",
      "0.875\n",
      "batch_idx: 24\n",
      "0.8716666666666667\n",
      "Training Epoch: 275, total loss: 42.039883\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8333333333333334\n",
      "batch_idx: 14\n",
      "0.8416666666666667\n",
      "batch_idx: 15\n",
      "0.8359375\n",
      "batch_idx: 16\n",
      "0.8382352941176471\n",
      "batch_idx: 17\n",
      "0.8402777777777778\n",
      "batch_idx: 18\n",
      "0.8355263157894737\n",
      "batch_idx: 19\n",
      "0.8333333333333334\n",
      "batch_idx: 20\n",
      "0.8273809523809523\n",
      "batch_idx: 21\n",
      "0.8314393939393939\n",
      "batch_idx: 22\n",
      "0.8297101449275363\n",
      "batch_idx: 23\n",
      "0.8333333333333334\n",
      "batch_idx: 24\n",
      "0.8316666666666667\n",
      "Training Epoch: 276, total loss: 42.515779\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8636363636363636\n",
      "batch_idx: 11\n",
      "0.8680555555555556\n",
      "batch_idx: 12\n",
      "0.8621794871794872\n",
      "batch_idx: 13\n",
      "0.8720238095238095\n",
      "batch_idx: 14\n",
      "0.875\n",
      "batch_idx: 15\n",
      "0.8723958333333334\n",
      "batch_idx: 16\n",
      "0.8799019607843137\n",
      "batch_idx: 17\n",
      "0.8796296296296297\n",
      "batch_idx: 18\n",
      "0.881578947368421\n",
      "batch_idx: 19\n",
      "0.8791666666666667\n",
      "batch_idx: 20\n",
      "0.8849206349206349\n",
      "batch_idx: 21\n",
      "0.8825757575757576\n",
      "batch_idx: 22\n",
      "0.875\n",
      "batch_idx: 23\n",
      "0.8767361111111112\n",
      "batch_idx: 24\n",
      "0.8783333333333333\n",
      "Training Epoch: 277, total loss: 41.774758\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.8916666666666667\n",
      "batch_idx: 5\n",
      "0.9027777777777778\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8564814814814815\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.8525641025641025\n",
      "batch_idx: 13\n",
      "0.8601190476190477\n",
      "batch_idx: 14\n",
      "0.8638888888888889\n",
      "batch_idx: 15\n",
      "0.8567708333333334\n",
      "batch_idx: 16\n",
      "0.8602941176470589\n",
      "batch_idx: 17\n",
      "0.8657407407407407\n",
      "batch_idx: 18\n",
      "0.868421052631579\n",
      "batch_idx: 19\n",
      "0.8729166666666667\n",
      "batch_idx: 20\n",
      "0.875\n",
      "batch_idx: 21\n",
      "0.875\n",
      "batch_idx: 22\n",
      "0.8713768115942029\n",
      "batch_idx: 23\n",
      "0.8715277777777778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 24\n",
      "0.8716666666666667\n",
      "Training Epoch: 278, total loss: 41.921996\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.8541666666666666\n",
      "batch_idx: 12\n",
      "0.8589743589743589\n",
      "batch_idx: 13\n",
      "0.8601190476190477\n",
      "batch_idx: 14\n",
      "0.8555555555555555\n",
      "batch_idx: 15\n",
      "0.8567708333333334\n",
      "batch_idx: 16\n",
      "0.8578431372549019\n",
      "batch_idx: 17\n",
      "0.8495370370370371\n",
      "batch_idx: 18\n",
      "0.8486842105263158\n",
      "batch_idx: 19\n",
      "0.85\n",
      "batch_idx: 20\n",
      "0.8452380952380952\n",
      "batch_idx: 21\n",
      "0.8503787878787878\n",
      "batch_idx: 22\n",
      "0.8460144927536232\n",
      "batch_idx: 23\n",
      "0.8472222222222222\n",
      "batch_idx: 24\n",
      "0.8483333333333334\n",
      "Training Epoch: 279, total loss: 42.290626\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8472222222222222\n",
      "batch_idx: 15\n",
      "0.8489583333333334\n",
      "batch_idx: 16\n",
      "0.8504901960784313\n",
      "batch_idx: 17\n",
      "0.8541666666666666\n",
      "batch_idx: 18\n",
      "0.8596491228070176\n",
      "batch_idx: 19\n",
      "0.8541666666666666\n",
      "batch_idx: 20\n",
      "0.8551587301587301\n",
      "batch_idx: 21\n",
      "0.8560606060606061\n",
      "batch_idx: 22\n",
      "0.8586956521739131\n",
      "batch_idx: 23\n",
      "0.8576388888888888\n",
      "batch_idx: 24\n",
      "0.855\n",
      "Training Epoch: 280, total loss: 42.042798\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8461538461538461\n",
      "batch_idx: 13\n",
      "0.8511904761904762\n",
      "batch_idx: 14\n",
      "0.8472222222222222\n",
      "batch_idx: 15\n",
      "0.8515625\n",
      "batch_idx: 16\n",
      "0.8529411764705882\n",
      "batch_idx: 17\n",
      "0.8495370370370371\n",
      "batch_idx: 18\n",
      "0.8552631578947368\n",
      "batch_idx: 19\n",
      "0.8604166666666667\n",
      "batch_idx: 20\n",
      "0.8670634920634921\n",
      "batch_idx: 21\n",
      "0.8693181818181818\n",
      "batch_idx: 22\n",
      "0.8659420289855072\n",
      "batch_idx: 23\n",
      "0.8680555555555556\n",
      "batch_idx: 24\n",
      "0.87\n",
      "Training Epoch: 281, total loss: 41.794410\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8916666666666667\n",
      "batch_idx: 5\n",
      "0.8888888888888888\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8796296296296297\n",
      "batch_idx: 9\n",
      "0.875\n",
      "batch_idx: 10\n",
      "0.8787878787878788\n",
      "batch_idx: 11\n",
      "0.8854166666666666\n",
      "batch_idx: 12\n",
      "0.8814102564102564\n",
      "batch_idx: 13\n",
      "0.8839285714285714\n",
      "batch_idx: 14\n",
      "0.8888888888888888\n",
      "batch_idx: 15\n",
      "0.8932291666666666\n",
      "batch_idx: 16\n",
      "0.8872549019607843\n",
      "batch_idx: 17\n",
      "0.8865740740740741\n",
      "batch_idx: 18\n",
      "0.8837719298245614\n",
      "batch_idx: 19\n",
      "0.8833333333333333\n",
      "batch_idx: 20\n",
      "0.8829365079365079\n",
      "batch_idx: 21\n",
      "0.8806818181818182\n",
      "batch_idx: 22\n",
      "0.8786231884057971\n",
      "batch_idx: 23\n",
      "0.8802083333333334\n",
      "batch_idx: 24\n",
      "0.885\n",
      "Training Epoch: 282, total loss: 41.517866\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8888888888888888\n",
      "batch_idx: 6\n",
      "0.8928571428571429\n",
      "batch_idx: 7\n",
      "0.8802083333333334\n",
      "batch_idx: 8\n",
      "0.8842592592592593\n",
      "batch_idx: 9\n",
      "0.8833333333333333\n",
      "batch_idx: 10\n",
      "0.8863636363636364\n",
      "batch_idx: 11\n",
      "0.8888888888888888\n",
      "batch_idx: 12\n",
      "0.8942307692307693\n",
      "batch_idx: 13\n",
      "0.8898809523809523\n",
      "batch_idx: 14\n",
      "0.8944444444444445\n",
      "batch_idx: 15\n",
      "0.890625\n",
      "batch_idx: 16\n",
      "0.8921568627450981\n",
      "batch_idx: 17\n",
      "0.8958333333333334\n",
      "batch_idx: 18\n",
      "0.8991228070175439\n",
      "batch_idx: 19\n",
      "0.8979166666666667\n",
      "batch_idx: 20\n",
      "0.8988095238095238\n",
      "batch_idx: 21\n",
      "0.8958333333333334\n",
      "batch_idx: 22\n",
      "0.8967391304347826\n",
      "batch_idx: 23\n",
      "0.9010416666666666\n",
      "batch_idx: 24\n",
      "0.905\n",
      "Training Epoch: 283, total loss: 41.470213\n",
      "batch_idx: 0\n",
      "1.0\n",
      "batch_idx: 1\n",
      "0.9791666666666666\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.9166666666666666\n",
      "batch_idx: 4\n",
      "0.9333333333333333\n",
      "batch_idx: 5\n",
      "0.9236111111111112\n",
      "batch_idx: 6\n",
      "0.9226190476190477\n",
      "batch_idx: 7\n",
      "0.90625\n",
      "batch_idx: 8\n",
      "0.9120370370370371\n",
      "batch_idx: 9\n",
      "0.9041666666666667\n",
      "batch_idx: 10\n",
      "0.8977272727272727\n",
      "batch_idx: 11\n",
      "0.8923611111111112\n",
      "batch_idx: 12\n",
      "0.8878205128205128\n",
      "batch_idx: 13\n",
      "0.8898809523809523\n",
      "batch_idx: 14\n",
      "0.8888888888888888\n",
      "batch_idx: 15\n",
      "0.8854166666666666\n",
      "batch_idx: 16\n",
      "0.8848039215686274\n",
      "batch_idx: 17\n",
      "0.8842592592592593\n",
      "batch_idx: 18\n",
      "0.8837719298245614\n",
      "batch_idx: 19\n",
      "0.88125\n",
      "batch_idx: 20\n",
      "0.8690476190476191\n",
      "batch_idx: 21\n",
      "0.865530303030303\n",
      "batch_idx: 22\n",
      "0.8659420289855072\n",
      "batch_idx: 23\n",
      "0.8663194444444444\n",
      "batch_idx: 24\n",
      "0.8633333333333333\n",
      "Training Epoch: 284, total loss: 42.007898\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8564814814814815\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8680555555555556\n",
      "batch_idx: 12\n",
      "0.8653846153846154\n",
      "batch_idx: 13\n",
      "0.8720238095238095\n",
      "batch_idx: 14\n",
      "0.8694444444444445\n",
      "batch_idx: 15\n",
      "0.8645833333333334\n",
      "batch_idx: 16\n",
      "0.8602941176470589\n",
      "batch_idx: 17\n",
      "0.8587962962962963\n",
      "batch_idx: 18\n",
      "0.8574561403508771\n",
      "batch_idx: 19\n",
      "0.85625\n",
      "batch_idx: 20\n",
      "0.8591269841269841\n",
      "batch_idx: 21\n",
      "0.8636363636363636\n",
      "batch_idx: 22\n",
      "0.8659420289855072\n",
      "batch_idx: 23\n",
      "0.8628472222222222\n",
      "batch_idx: 24\n",
      "0.8616666666666667\n",
      "Training Epoch: 285, total loss: 41.957406\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.9375\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.8888888888888888\n",
      "batch_idx: 6\n",
      "0.8809523809523809\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8703703703703703\n",
      "batch_idx: 9\n",
      "0.8791666666666667\n",
      "batch_idx: 10\n",
      "0.8863636363636364\n",
      "batch_idx: 11\n",
      "0.8819444444444444\n",
      "batch_idx: 12\n",
      "0.8717948717948718\n",
      "batch_idx: 13\n",
      "0.875\n",
      "batch_idx: 14\n",
      "0.8638888888888889\n",
      "batch_idx: 15\n",
      "0.8671875\n",
      "batch_idx: 16\n",
      "0.8602941176470589\n",
      "batch_idx: 17\n",
      "0.8518518518518519\n",
      "batch_idx: 18\n",
      "0.8552631578947368\n",
      "batch_idx: 19\n",
      "0.85\n",
      "batch_idx: 20\n",
      "0.8472222222222222\n",
      "batch_idx: 21\n",
      "0.8503787878787878\n",
      "batch_idx: 22\n",
      "0.8405797101449275\n",
      "batch_idx: 23\n",
      "0.8420138888888888\n",
      "batch_idx: 24\n",
      "0.8433333333333334\n",
      "Training Epoch: 286, total loss: 42.425038\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.9305555555555556\n",
      "batch_idx: 3\n",
      "0.9479166666666666\n",
      "batch_idx: 4\n",
      "0.9416666666666667\n",
      "batch_idx: 5\n",
      "0.9375\n",
      "batch_idx: 6\n",
      "0.9166666666666666\n",
      "batch_idx: 7\n",
      "0.8958333333333334\n",
      "batch_idx: 8\n",
      "0.8888888888888888\n",
      "batch_idx: 9\n",
      "0.8791666666666667\n",
      "batch_idx: 10\n",
      "0.8863636363636364\n",
      "batch_idx: 11\n",
      "0.8784722222222222\n",
      "batch_idx: 12\n",
      "0.875\n",
      "batch_idx: 13\n",
      "0.8660714285714286\n",
      "batch_idx: 14\n",
      "0.8666666666666667\n",
      "batch_idx: 15\n",
      "0.8619791666666666\n",
      "batch_idx: 16\n",
      "0.8651960784313726\n",
      "batch_idx: 17\n",
      "0.8634259259259259\n",
      "batch_idx: 18\n",
      "0.8618421052631579\n",
      "batch_idx: 19\n",
      "0.8604166666666667\n",
      "batch_idx: 20\n",
      "0.8650793650793651\n",
      "batch_idx: 21\n",
      "0.8674242424242424\n",
      "batch_idx: 22\n",
      "0.8695652173913043\n",
      "batch_idx: 23\n",
      "0.8715277777777778\n",
      "batch_idx: 24\n",
      "0.8716666666666667\n",
      "Training Epoch: 287, total loss: 41.869807\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8712121212121212\n",
      "batch_idx: 11\n",
      "0.8784722222222222\n",
      "batch_idx: 12\n",
      "0.8814102564102564\n",
      "batch_idx: 13\n",
      "0.8809523809523809\n",
      "batch_idx: 14\n",
      "0.8694444444444445\n",
      "batch_idx: 15\n",
      "0.875\n",
      "batch_idx: 16\n",
      "0.8799019607843137\n",
      "batch_idx: 17\n",
      "0.875\n",
      "batch_idx: 18\n",
      "0.875\n",
      "batch_idx: 19\n",
      "0.8729166666666667\n",
      "batch_idx: 20\n",
      "0.878968253968254\n",
      "batch_idx: 21\n",
      "0.884469696969697\n",
      "batch_idx: 22\n",
      "0.8822463768115942\n",
      "batch_idx: 23\n",
      "0.8784722222222222\n",
      "batch_idx: 24\n",
      "0.8816666666666667\n",
      "Training Epoch: 288, total loss: 41.621398\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8703703703703703\n",
      "batch_idx: 9\n",
      "0.8708333333333333\n",
      "batch_idx: 10\n",
      "0.8712121212121212\n",
      "batch_idx: 11\n",
      "0.8645833333333334\n",
      "batch_idx: 12\n",
      "0.8621794871794872\n",
      "batch_idx: 13\n",
      "0.8630952380952381\n",
      "batch_idx: 14\n",
      "0.8666666666666667\n",
      "batch_idx: 15\n",
      "0.8645833333333334\n",
      "batch_idx: 16\n",
      "0.8602941176470589\n",
      "batch_idx: 17\n",
      "0.8634259259259259\n",
      "batch_idx: 18\n",
      "0.8574561403508771\n",
      "batch_idx: 19\n",
      "0.8625\n",
      "batch_idx: 20\n",
      "0.8630952380952381\n",
      "batch_idx: 21\n",
      "0.8636363636363636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 22\n",
      "0.8677536231884058\n",
      "batch_idx: 23\n",
      "0.8697916666666666\n",
      "batch_idx: 24\n",
      "0.8733333333333333\n",
      "Training Epoch: 289, total loss: 41.774770\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8298611111111112\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8244047619047619\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.8333333333333334\n",
      "batch_idx: 16\n",
      "0.8308823529411765\n",
      "batch_idx: 17\n",
      "0.8333333333333334\n",
      "batch_idx: 18\n",
      "0.8399122807017544\n",
      "batch_idx: 19\n",
      "0.8375\n",
      "batch_idx: 20\n",
      "0.8392857142857143\n",
      "batch_idx: 21\n",
      "0.8428030303030303\n",
      "batch_idx: 22\n",
      "0.842391304347826\n",
      "batch_idx: 23\n",
      "0.8385416666666666\n",
      "batch_idx: 24\n",
      "0.845\n",
      "Training Epoch: 290, total loss: 42.262695\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.8916666666666667\n",
      "batch_idx: 5\n",
      "0.8958333333333334\n",
      "batch_idx: 6\n",
      "0.9047619047619048\n",
      "batch_idx: 7\n",
      "0.9010416666666666\n",
      "batch_idx: 8\n",
      "0.9074074074074074\n",
      "batch_idx: 9\n",
      "0.8875\n",
      "batch_idx: 10\n",
      "0.8674242424242424\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.8621794871794872\n",
      "batch_idx: 13\n",
      "0.8601190476190477\n",
      "batch_idx: 14\n",
      "0.8583333333333333\n",
      "batch_idx: 15\n",
      "0.8645833333333334\n",
      "batch_idx: 16\n",
      "0.8602941176470589\n",
      "batch_idx: 17\n",
      "0.8587962962962963\n",
      "batch_idx: 18\n",
      "0.8508771929824561\n",
      "batch_idx: 19\n",
      "0.8541666666666666\n",
      "batch_idx: 20\n",
      "0.8511904761904762\n",
      "batch_idx: 21\n",
      "0.8560606060606061\n",
      "batch_idx: 22\n",
      "0.855072463768116\n",
      "batch_idx: 23\n",
      "0.8559027777777778\n",
      "batch_idx: 24\n",
      "0.8566666666666667\n",
      "Training Epoch: 291, total loss: 42.063797\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.8809523809523809\n",
      "batch_idx: 7\n",
      "0.875\n",
      "batch_idx: 8\n",
      "0.8842592592592593\n",
      "batch_idx: 9\n",
      "0.8916666666666667\n",
      "batch_idx: 10\n",
      "0.8939393939393939\n",
      "batch_idx: 11\n",
      "0.8854166666666666\n",
      "batch_idx: 12\n",
      "0.8942307692307693\n",
      "batch_idx: 13\n",
      "0.8898809523809523\n",
      "batch_idx: 14\n",
      "0.8944444444444445\n",
      "batch_idx: 15\n",
      "0.890625\n",
      "batch_idx: 16\n",
      "0.8823529411764706\n",
      "batch_idx: 17\n",
      "0.8865740740740741\n",
      "batch_idx: 18\n",
      "0.8837719298245614\n",
      "batch_idx: 19\n",
      "0.8833333333333333\n",
      "batch_idx: 20\n",
      "0.8809523809523809\n",
      "batch_idx: 21\n",
      "0.8787878787878788\n",
      "batch_idx: 22\n",
      "0.8822463768115942\n",
      "batch_idx: 23\n",
      "0.8819444444444444\n",
      "batch_idx: 24\n",
      "0.8766666666666667\n",
      "Training Epoch: 292, total loss: 41.601749\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.8653846153846154\n",
      "batch_idx: 13\n",
      "0.8630952380952381\n",
      "batch_idx: 14\n",
      "0.8611111111111112\n",
      "batch_idx: 15\n",
      "0.8619791666666666\n",
      "batch_idx: 16\n",
      "0.8627450980392157\n",
      "batch_idx: 17\n",
      "0.8587962962962963\n",
      "batch_idx: 18\n",
      "0.8574561403508771\n",
      "batch_idx: 19\n",
      "0.8583333333333333\n",
      "batch_idx: 20\n",
      "0.8591269841269841\n",
      "batch_idx: 21\n",
      "0.8617424242424242\n",
      "batch_idx: 22\n",
      "0.8605072463768116\n",
      "batch_idx: 23\n",
      "0.859375\n",
      "batch_idx: 24\n",
      "0.8583333333333333\n",
      "Training Epoch: 293, total loss: 42.109342\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8657407407407407\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8636363636363636\n",
      "batch_idx: 11\n",
      "0.8645833333333334\n",
      "batch_idx: 12\n",
      "0.8621794871794872\n",
      "batch_idx: 13\n",
      "0.8660714285714286\n",
      "batch_idx: 14\n",
      "0.8722222222222222\n",
      "batch_idx: 15\n",
      "0.8645833333333334\n",
      "batch_idx: 16\n",
      "0.8651960784313726\n",
      "batch_idx: 17\n",
      "0.8703703703703703\n",
      "batch_idx: 18\n",
      "0.8728070175438597\n",
      "batch_idx: 19\n",
      "0.8729166666666667\n",
      "batch_idx: 20\n",
      "0.871031746031746\n",
      "batch_idx: 21\n",
      "0.8693181818181818\n",
      "batch_idx: 22\n",
      "0.8695652173913043\n",
      "batch_idx: 23\n",
      "0.8697916666666666\n",
      "batch_idx: 24\n",
      "0.8683333333333333\n",
      "Training Epoch: 294, total loss: 41.868999\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8645833333333334\n",
      "batch_idx: 12\n",
      "0.8621794871794872\n",
      "batch_idx: 13\n",
      "0.8690476190476191\n",
      "batch_idx: 14\n",
      "0.8583333333333333\n",
      "batch_idx: 15\n",
      "0.8619791666666666\n",
      "batch_idx: 16\n",
      "0.8651960784313726\n",
      "batch_idx: 17\n",
      "0.8657407407407407\n",
      "batch_idx: 18\n",
      "0.8640350877192983\n",
      "batch_idx: 19\n",
      "0.8604166666666667\n",
      "batch_idx: 20\n",
      "0.8611111111111112\n",
      "batch_idx: 21\n",
      "0.8560606060606061\n",
      "batch_idx: 22\n",
      "0.8605072463768116\n",
      "batch_idx: 23\n",
      "0.8628472222222222\n",
      "batch_idx: 24\n",
      "0.8666666666666667\n",
      "Training Epoch: 295, total loss: 41.852852\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9375\n",
      "batch_idx: 2\n",
      "0.9583333333333334\n",
      "batch_idx: 3\n",
      "0.9270833333333334\n",
      "batch_idx: 4\n",
      "0.9166666666666666\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8657407407407407\n",
      "batch_idx: 9\n",
      "0.875\n",
      "batch_idx: 10\n",
      "0.8863636363636364\n",
      "batch_idx: 11\n",
      "0.8854166666666666\n",
      "batch_idx: 12\n",
      "0.8846153846153846\n",
      "batch_idx: 13\n",
      "0.8898809523809523\n",
      "batch_idx: 14\n",
      "0.8888888888888888\n",
      "batch_idx: 15\n",
      "0.8854166666666666\n",
      "batch_idx: 16\n",
      "0.8848039215686274\n",
      "batch_idx: 17\n",
      "0.8888888888888888\n",
      "batch_idx: 18\n",
      "0.8903508771929824\n",
      "batch_idx: 19\n",
      "0.8895833333333333\n",
      "batch_idx: 20\n",
      "0.8869047619047619\n",
      "batch_idx: 21\n",
      "0.8863636363636364\n",
      "batch_idx: 22\n",
      "0.8804347826086957\n",
      "batch_idx: 23\n",
      "0.875\n",
      "batch_idx: 24\n",
      "0.8666666666666667\n",
      "Training Epoch: 296, total loss: 41.940804\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.8557692307692307\n",
      "batch_idx: 13\n",
      "0.8511904761904762\n",
      "batch_idx: 14\n",
      "0.8527777777777777\n",
      "batch_idx: 15\n",
      "0.8619791666666666\n",
      "batch_idx: 16\n",
      "0.8651960784313726\n",
      "batch_idx: 17\n",
      "0.8611111111111112\n",
      "batch_idx: 18\n",
      "0.8596491228070176\n",
      "batch_idx: 19\n",
      "0.8416666666666667\n",
      "batch_idx: 20\n",
      "0.8452380952380952\n",
      "batch_idx: 21\n",
      "0.8428030303030303\n",
      "batch_idx: 22\n",
      "0.8369565217391305\n",
      "batch_idx: 23\n",
      "0.8368055555555556\n",
      "batch_idx: 24\n",
      "0.835\n",
      "Training Epoch: 297, total loss: 42.548451\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8452380952380952\n",
      "batch_idx: 14\n",
      "0.8472222222222222\n",
      "batch_idx: 15\n",
      "0.8515625\n",
      "batch_idx: 16\n",
      "0.8553921568627451\n",
      "batch_idx: 17\n",
      "0.8564814814814815\n",
      "batch_idx: 18\n",
      "0.8530701754385965\n",
      "batch_idx: 19\n",
      "0.85625\n",
      "batch_idx: 20\n",
      "0.8611111111111112\n",
      "batch_idx: 21\n",
      "0.8598484848484849\n",
      "batch_idx: 22\n",
      "0.8623188405797102\n",
      "batch_idx: 23\n",
      "0.8559027777777778\n",
      "batch_idx: 24\n",
      "0.8583333333333333\n",
      "Training Epoch: 298, total loss: 42.212658\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.8958333333333334\n",
      "batch_idx: 6\n",
      "0.8809523809523809\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.8472222222222222\n",
      "batch_idx: 12\n",
      "0.8461538461538461\n",
      "batch_idx: 13\n",
      "0.8422619047619048\n",
      "batch_idx: 14\n",
      "0.8444444444444444\n",
      "batch_idx: 15\n",
      "0.8385416666666666\n",
      "batch_idx: 16\n",
      "0.8455882352941176\n",
      "batch_idx: 17\n",
      "0.8402777777777778\n",
      "batch_idx: 18\n",
      "0.8421052631578947\n",
      "batch_idx: 19\n",
      "0.8458333333333333\n",
      "batch_idx: 20\n",
      "0.8511904761904762\n",
      "batch_idx: 21\n",
      "0.8541666666666666\n",
      "batch_idx: 22\n",
      "0.8532608695652174\n",
      "batch_idx: 23\n",
      "0.8559027777777778\n",
      "batch_idx: 24\n",
      "0.8566666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 299, total loss: 42.146661\n",
      "batch_idx: 0\n",
      "1.0\n",
      "batch_idx: 1\n",
      "0.9583333333333334\n",
      "batch_idx: 2\n",
      "0.9305555555555556\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.8802083333333334\n",
      "batch_idx: 8\n",
      "0.8703703703703703\n",
      "batch_idx: 9\n",
      "0.8625\n",
      "batch_idx: 10\n",
      "0.8674242424242424\n",
      "batch_idx: 11\n",
      "0.8645833333333334\n",
      "batch_idx: 12\n",
      "0.8621794871794872\n",
      "batch_idx: 13\n",
      "0.8690476190476191\n",
      "batch_idx: 14\n",
      "0.8722222222222222\n",
      "batch_idx: 15\n",
      "0.875\n",
      "batch_idx: 16\n",
      "0.8774509803921569\n",
      "batch_idx: 17\n",
      "0.875\n",
      "batch_idx: 18\n",
      "0.8793859649122807\n",
      "batch_idx: 19\n",
      "0.88125\n",
      "batch_idx: 20\n",
      "0.878968253968254\n",
      "batch_idx: 21\n",
      "0.8806818181818182\n",
      "batch_idx: 22\n",
      "0.875\n",
      "batch_idx: 23\n",
      "0.875\n",
      "batch_idx: 24\n",
      "0.8733333333333333\n",
      "Training Epoch: 300, total loss: 41.750236\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.8461538461538461\n",
      "batch_idx: 13\n",
      "0.8452380952380952\n",
      "batch_idx: 14\n",
      "0.8444444444444444\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8504901960784313\n",
      "batch_idx: 17\n",
      "0.8518518518518519\n",
      "batch_idx: 18\n",
      "0.8596491228070176\n",
      "batch_idx: 19\n",
      "0.8604166666666667\n",
      "batch_idx: 20\n",
      "0.8630952380952381\n",
      "batch_idx: 21\n",
      "0.865530303030303\n",
      "batch_idx: 22\n",
      "0.8695652173913043\n",
      "batch_idx: 23\n",
      "0.8680555555555556\n",
      "batch_idx: 24\n",
      "0.8683333333333333\n",
      "Training Epoch: 301, total loss: 41.900282\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.8541666666666666\n",
      "batch_idx: 12\n",
      "0.8589743589743589\n",
      "batch_idx: 13\n",
      "0.8601190476190477\n",
      "batch_idx: 14\n",
      "0.8638888888888889\n",
      "batch_idx: 15\n",
      "0.8645833333333334\n",
      "batch_idx: 16\n",
      "0.8651960784313726\n",
      "batch_idx: 17\n",
      "0.8657407407407407\n",
      "batch_idx: 18\n",
      "0.868421052631579\n",
      "batch_idx: 19\n",
      "0.8625\n",
      "batch_idx: 20\n",
      "0.8591269841269841\n",
      "batch_idx: 21\n",
      "0.8636363636363636\n",
      "batch_idx: 22\n",
      "0.8623188405797102\n",
      "batch_idx: 23\n",
      "0.8645833333333334\n",
      "batch_idx: 24\n",
      "0.8666666666666667\n",
      "Training Epoch: 302, total loss: 41.827527\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8802083333333334\n",
      "batch_idx: 8\n",
      "0.8657407407407407\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.8541666666666666\n",
      "batch_idx: 12\n",
      "0.8621794871794872\n",
      "batch_idx: 13\n",
      "0.8690476190476191\n",
      "batch_idx: 14\n",
      "0.8666666666666667\n",
      "batch_idx: 15\n",
      "0.8723958333333334\n",
      "batch_idx: 16\n",
      "0.8725490196078431\n",
      "batch_idx: 17\n",
      "0.8726851851851852\n",
      "batch_idx: 18\n",
      "0.875\n",
      "batch_idx: 19\n",
      "0.8791666666666667\n",
      "batch_idx: 20\n",
      "0.8650793650793651\n",
      "batch_idx: 21\n",
      "0.8636363636363636\n",
      "batch_idx: 22\n",
      "0.8532608695652174\n",
      "batch_idx: 23\n",
      "0.8541666666666666\n",
      "batch_idx: 24\n",
      "0.8566666666666667\n",
      "Training Epoch: 303, total loss: 41.990730\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.9375\n",
      "batch_idx: 2\n",
      "0.9444444444444444\n",
      "batch_idx: 3\n",
      "0.90625\n",
      "batch_idx: 4\n",
      "0.9083333333333333\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "batch_idx: 6\n",
      "0.9166666666666666\n",
      "batch_idx: 7\n",
      "0.8958333333333334\n",
      "batch_idx: 8\n",
      "0.9027777777777778\n",
      "batch_idx: 9\n",
      "0.8916666666666667\n",
      "batch_idx: 10\n",
      "0.8939393939393939\n",
      "batch_idx: 11\n",
      "0.8993055555555556\n",
      "batch_idx: 12\n",
      "0.8910256410256411\n",
      "batch_idx: 13\n",
      "0.8869047619047619\n",
      "batch_idx: 14\n",
      "0.8888888888888888\n",
      "batch_idx: 15\n",
      "0.8802083333333334\n",
      "batch_idx: 16\n",
      "0.8823529411764706\n",
      "batch_idx: 17\n",
      "0.8865740740740741\n",
      "batch_idx: 18\n",
      "0.8881578947368421\n",
      "batch_idx: 19\n",
      "0.8875\n",
      "batch_idx: 20\n",
      "0.8849206349206349\n",
      "batch_idx: 21\n",
      "0.8882575757575758\n",
      "batch_idx: 22\n",
      "0.8840579710144928\n",
      "batch_idx: 23\n",
      "0.8819444444444444\n",
      "batch_idx: 24\n",
      "0.8816666666666667\n",
      "Training Epoch: 304, total loss: 41.784594\n",
      "batch_idx: 0\n",
      "1.0\n",
      "batch_idx: 1\n",
      "0.9583333333333334\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.90625\n",
      "batch_idx: 4\n",
      "0.9083333333333333\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8365384615384616\n",
      "batch_idx: 13\n",
      "0.8452380952380952\n",
      "batch_idx: 14\n",
      "0.85\n",
      "batch_idx: 15\n",
      "0.8541666666666666\n",
      "batch_idx: 16\n",
      "0.8553921568627451\n",
      "batch_idx: 17\n",
      "0.8587962962962963\n",
      "batch_idx: 18\n",
      "0.8618421052631579\n",
      "batch_idx: 19\n",
      "0.8541666666666666\n",
      "batch_idx: 20\n",
      "0.8551587301587301\n",
      "batch_idx: 21\n",
      "0.8598484848484849\n",
      "batch_idx: 22\n",
      "0.8586956521739131\n",
      "batch_idx: 23\n",
      "0.859375\n",
      "batch_idx: 24\n",
      "0.8616666666666667\n",
      "Training Epoch: 305, total loss: 41.948463\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8636363636363636\n",
      "batch_idx: 11\n",
      "0.8541666666666666\n",
      "batch_idx: 12\n",
      "0.8621794871794872\n",
      "batch_idx: 13\n",
      "0.8541666666666666\n",
      "batch_idx: 14\n",
      "0.8555555555555555\n",
      "batch_idx: 15\n",
      "0.8515625\n",
      "batch_idx: 16\n",
      "0.8553921568627451\n",
      "batch_idx: 17\n",
      "0.8564814814814815\n",
      "batch_idx: 18\n",
      "0.8574561403508771\n",
      "batch_idx: 19\n",
      "0.8604166666666667\n",
      "batch_idx: 20\n",
      "0.8611111111111112\n",
      "batch_idx: 21\n",
      "0.8617424242424242\n",
      "batch_idx: 22\n",
      "0.8659420289855072\n",
      "batch_idx: 23\n",
      "0.8645833333333334\n",
      "batch_idx: 24\n",
      "0.865\n",
      "Training Epoch: 306, total loss: 41.980984\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.8802083333333334\n",
      "batch_idx: 8\n",
      "0.8703703703703703\n",
      "batch_idx: 9\n",
      "0.8833333333333333\n",
      "batch_idx: 10\n",
      "0.8939393939393939\n",
      "batch_idx: 11\n",
      "0.8888888888888888\n",
      "batch_idx: 12\n",
      "0.8910256410256411\n",
      "batch_idx: 13\n",
      "0.8928571428571429\n",
      "batch_idx: 14\n",
      "0.8888888888888888\n",
      "batch_idx: 15\n",
      "0.8958333333333334\n",
      "batch_idx: 16\n",
      "0.8995098039215687\n",
      "batch_idx: 17\n",
      "0.8958333333333334\n",
      "batch_idx: 18\n",
      "0.8969298245614035\n",
      "batch_idx: 19\n",
      "0.89375\n",
      "batch_idx: 20\n",
      "0.8849206349206349\n",
      "batch_idx: 21\n",
      "0.8901515151515151\n",
      "batch_idx: 22\n",
      "0.8858695652173914\n",
      "batch_idx: 23\n",
      "0.8802083333333334\n",
      "batch_idx: 24\n",
      "0.8783333333333333\n",
      "Training Epoch: 307, total loss: 41.639128\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.9583333333333334\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.8916666666666667\n",
      "batch_idx: 5\n",
      "0.9027777777777778\n",
      "batch_idx: 6\n",
      "0.8869047619047619\n",
      "batch_idx: 7\n",
      "0.8854166666666666\n",
      "batch_idx: 8\n",
      "0.8842592592592593\n",
      "batch_idx: 9\n",
      "0.8916666666666667\n",
      "batch_idx: 10\n",
      "0.8863636363636364\n",
      "batch_idx: 11\n",
      "0.8888888888888888\n",
      "batch_idx: 12\n",
      "0.8878205128205128\n",
      "batch_idx: 13\n",
      "0.8928571428571429\n",
      "batch_idx: 14\n",
      "0.8916666666666667\n",
      "batch_idx: 15\n",
      "0.8880208333333334\n",
      "batch_idx: 16\n",
      "0.8774509803921569\n",
      "batch_idx: 17\n",
      "0.8773148148148148\n",
      "batch_idx: 18\n",
      "0.8793859649122807\n",
      "batch_idx: 19\n",
      "0.875\n",
      "batch_idx: 20\n",
      "0.873015873015873\n",
      "batch_idx: 21\n",
      "0.8787878787878788\n",
      "batch_idx: 22\n",
      "0.875\n",
      "batch_idx: 23\n",
      "0.8732638888888888\n",
      "batch_idx: 24\n",
      "0.8733333333333333\n",
      "Training Epoch: 308, total loss: 41.977143\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.9375\n",
      "batch_idx: 2\n",
      "0.9305555555555556\n",
      "batch_idx: 3\n",
      "0.9270833333333334\n",
      "batch_idx: 4\n",
      "0.9083333333333333\n",
      "batch_idx: 5\n",
      "0.9027777777777778\n",
      "batch_idx: 6\n",
      "0.9047619047619048\n",
      "batch_idx: 7\n",
      "0.9010416666666666\n",
      "batch_idx: 8\n",
      "0.9120370370370371\n",
      "batch_idx: 9\n",
      "0.9041666666666667\n",
      "batch_idx: 10\n",
      "0.8939393939393939\n",
      "batch_idx: 11\n",
      "0.8993055555555556\n",
      "batch_idx: 12\n",
      "0.8942307692307693\n",
      "batch_idx: 13\n",
      "0.8988095238095238\n",
      "batch_idx: 14\n",
      "0.8861111111111111\n",
      "batch_idx: 15\n",
      "0.8828125\n",
      "batch_idx: 16\n",
      "0.8872549019607843\n",
      "batch_idx: 17\n",
      "0.8865740740740741\n",
      "batch_idx: 18\n",
      "0.8925438596491229\n",
      "batch_idx: 19\n",
      "0.8833333333333333\n",
      "batch_idx: 20\n",
      "0.876984126984127\n",
      "batch_idx: 21\n",
      "0.8806818181818182\n",
      "batch_idx: 22\n",
      "0.8858695652173914\n",
      "batch_idx: 23\n",
      "0.8854166666666666\n",
      "batch_idx: 24\n",
      "0.8833333333333333\n",
      "Training Epoch: 309, total loss: 41.758093\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8541666666666666\n",
      "batch_idx: 12\n",
      "0.8653846153846154\n",
      "batch_idx: 13\n",
      "0.8660714285714286\n",
      "batch_idx: 14\n",
      "0.8722222222222222\n",
      "batch_idx: 15\n",
      "0.8697916666666666\n",
      "batch_idx: 16\n",
      "0.875\n",
      "batch_idx: 17\n",
      "0.8703703703703703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 18\n",
      "0.8771929824561403\n",
      "batch_idx: 19\n",
      "0.86875\n",
      "batch_idx: 20\n",
      "0.871031746031746\n",
      "batch_idx: 21\n",
      "0.875\n",
      "batch_idx: 22\n",
      "0.8695652173913043\n",
      "batch_idx: 23\n",
      "0.8680555555555556\n",
      "batch_idx: 24\n",
      "0.8666666666666667\n",
      "Training Epoch: 310, total loss: 41.827095\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.8958333333333334\n",
      "batch_idx: 6\n",
      "0.8809523809523809\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8636363636363636\n",
      "batch_idx: 11\n",
      "0.8715277777777778\n",
      "batch_idx: 12\n",
      "0.8685897435897436\n",
      "batch_idx: 13\n",
      "0.8630952380952381\n",
      "batch_idx: 14\n",
      "0.8583333333333333\n",
      "batch_idx: 15\n",
      "0.859375\n",
      "batch_idx: 16\n",
      "0.8602941176470589\n",
      "batch_idx: 17\n",
      "0.8680555555555556\n",
      "batch_idx: 18\n",
      "0.8728070175438597\n",
      "batch_idx: 19\n",
      "0.8770833333333333\n",
      "batch_idx: 20\n",
      "0.871031746031746\n",
      "batch_idx: 21\n",
      "0.8731060606060606\n",
      "batch_idx: 22\n",
      "0.875\n",
      "batch_idx: 23\n",
      "0.8680555555555556\n",
      "batch_idx: 24\n",
      "0.8683333333333333\n",
      "Training Epoch: 311, total loss: 41.739051\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.8869047619047619\n",
      "batch_idx: 7\n",
      "0.8802083333333334\n",
      "batch_idx: 8\n",
      "0.8796296296296297\n",
      "batch_idx: 9\n",
      "0.875\n",
      "batch_idx: 10\n",
      "0.875\n",
      "batch_idx: 11\n",
      "0.8715277777777778\n",
      "batch_idx: 12\n",
      "0.875\n",
      "batch_idx: 13\n",
      "0.8779761904761905\n",
      "batch_idx: 14\n",
      "0.8722222222222222\n",
      "batch_idx: 15\n",
      "0.875\n",
      "batch_idx: 16\n",
      "0.8774509803921569\n",
      "batch_idx: 17\n",
      "0.8773148148148148\n",
      "batch_idx: 18\n",
      "0.8771929824561403\n",
      "batch_idx: 19\n",
      "0.8770833333333333\n",
      "batch_idx: 20\n",
      "0.875\n",
      "batch_idx: 21\n",
      "0.8787878787878788\n",
      "batch_idx: 22\n",
      "0.8713768115942029\n",
      "batch_idx: 23\n",
      "0.8611111111111112\n",
      "batch_idx: 24\n",
      "0.8666666666666667\n",
      "Training Epoch: 312, total loss: 41.950913\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8916666666666667\n",
      "batch_idx: 5\n",
      "0.8888888888888888\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8802083333333334\n",
      "batch_idx: 8\n",
      "0.8842592592592593\n",
      "batch_idx: 9\n",
      "0.8791666666666667\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.8461538461538461\n",
      "batch_idx: 13\n",
      "0.8452380952380952\n",
      "batch_idx: 14\n",
      "0.8472222222222222\n",
      "batch_idx: 15\n",
      "0.8489583333333334\n",
      "batch_idx: 16\n",
      "0.8382352941176471\n",
      "batch_idx: 17\n",
      "0.8425925925925926\n",
      "batch_idx: 18\n",
      "0.8486842105263158\n",
      "batch_idx: 19\n",
      "0.85\n",
      "batch_idx: 20\n",
      "0.8531746031746031\n",
      "batch_idx: 21\n",
      "0.8579545454545454\n",
      "batch_idx: 22\n",
      "0.8568840579710145\n",
      "batch_idx: 23\n",
      "0.8559027777777778\n",
      "batch_idx: 24\n",
      "0.8583333333333333\n",
      "Training Epoch: 313, total loss: 42.128122\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8564814814814815\n",
      "batch_idx: 9\n",
      "0.8625\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.8472222222222222\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8422619047619048\n",
      "batch_idx: 14\n",
      "0.8388888888888889\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8333333333333334\n",
      "batch_idx: 17\n",
      "0.8356481481481481\n",
      "batch_idx: 18\n",
      "0.8399122807017544\n",
      "batch_idx: 19\n",
      "0.8479166666666667\n",
      "batch_idx: 20\n",
      "0.8452380952380952\n",
      "batch_idx: 21\n",
      "0.8503787878787878\n",
      "batch_idx: 22\n",
      "0.8496376811594203\n",
      "batch_idx: 23\n",
      "0.8541666666666666\n",
      "batch_idx: 24\n",
      "0.8433333333333334\n",
      "Training Epoch: 314, total loss: 42.303581\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9583333333333334\n",
      "batch_idx: 2\n",
      "0.9305555555555556\n",
      "batch_idx: 3\n",
      "0.9166666666666666\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8888888888888888\n",
      "batch_idx: 6\n",
      "0.8928571428571429\n",
      "batch_idx: 7\n",
      "0.9010416666666666\n",
      "batch_idx: 8\n",
      "0.8935185185185185\n",
      "batch_idx: 9\n",
      "0.8875\n",
      "batch_idx: 10\n",
      "0.8712121212121212\n",
      "batch_idx: 11\n",
      "0.8680555555555556\n",
      "batch_idx: 12\n",
      "0.8589743589743589\n",
      "batch_idx: 13\n",
      "0.8660714285714286\n",
      "batch_idx: 14\n",
      "0.8638888888888889\n",
      "batch_idx: 15\n",
      "0.8619791666666666\n",
      "batch_idx: 16\n",
      "0.8676470588235294\n",
      "batch_idx: 17\n",
      "0.8587962962962963\n",
      "batch_idx: 18\n",
      "0.8508771929824561\n",
      "batch_idx: 19\n",
      "0.8520833333333333\n",
      "batch_idx: 20\n",
      "0.8511904761904762\n",
      "batch_idx: 21\n",
      "0.8522727272727273\n",
      "batch_idx: 22\n",
      "0.8532608695652174\n",
      "batch_idx: 23\n",
      "0.8576388888888888\n",
      "batch_idx: 24\n",
      "0.86\n",
      "Training Epoch: 315, total loss: 41.969002\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8363095238095238\n",
      "batch_idx: 14\n",
      "0.8305555555555556\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8357843137254902\n",
      "batch_idx: 17\n",
      "0.8356481481481481\n",
      "batch_idx: 18\n",
      "0.8333333333333334\n",
      "batch_idx: 19\n",
      "0.8375\n",
      "batch_idx: 20\n",
      "0.8392857142857143\n",
      "batch_idx: 21\n",
      "0.8390151515151515\n",
      "batch_idx: 22\n",
      "0.8387681159420289\n",
      "batch_idx: 23\n",
      "0.8368055555555556\n",
      "batch_idx: 24\n",
      "0.8366666666666667\n",
      "Training Epoch: 316, total loss: 42.449245\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8674242424242424\n",
      "batch_idx: 11\n",
      "0.8715277777777778\n",
      "batch_idx: 12\n",
      "0.8717948717948718\n",
      "batch_idx: 13\n",
      "0.8690476190476191\n",
      "batch_idx: 14\n",
      "0.8666666666666667\n",
      "batch_idx: 15\n",
      "0.8671875\n",
      "batch_idx: 16\n",
      "0.8627450980392157\n",
      "batch_idx: 17\n",
      "0.8680555555555556\n",
      "batch_idx: 18\n",
      "0.8706140350877193\n",
      "batch_idx: 19\n",
      "0.8729166666666667\n",
      "batch_idx: 20\n",
      "0.873015873015873\n",
      "batch_idx: 21\n",
      "0.8712121212121212\n",
      "batch_idx: 22\n",
      "0.8659420289855072\n",
      "batch_idx: 23\n",
      "0.8715277777777778\n",
      "batch_idx: 24\n",
      "0.8716666666666667\n",
      "Training Epoch: 317, total loss: 41.840729\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n",
      "batch_idx: 11\n",
      "0.8541666666666666\n",
      "batch_idx: 12\n",
      "0.8589743589743589\n",
      "batch_idx: 13\n",
      "0.8482142857142857\n",
      "batch_idx: 14\n",
      "0.8555555555555555\n",
      "batch_idx: 15\n",
      "0.8619791666666666\n",
      "batch_idx: 16\n",
      "0.8651960784313726\n",
      "batch_idx: 17\n",
      "0.8564814814814815\n",
      "batch_idx: 18\n",
      "0.8552631578947368\n",
      "batch_idx: 19\n",
      "0.8583333333333333\n",
      "batch_idx: 20\n",
      "0.8571428571428571\n",
      "batch_idx: 21\n",
      "0.8541666666666666\n",
      "batch_idx: 22\n",
      "0.8532608695652174\n",
      "batch_idx: 23\n",
      "0.8489583333333334\n",
      "batch_idx: 24\n",
      "0.85\n",
      "Training Epoch: 318, total loss: 42.111448\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.9305555555555556\n",
      "batch_idx: 3\n",
      "0.9166666666666666\n",
      "batch_idx: 4\n",
      "0.9166666666666666\n",
      "batch_idx: 5\n",
      "0.8958333333333334\n",
      "batch_idx: 6\n",
      "0.8988095238095238\n",
      "batch_idx: 7\n",
      "0.890625\n",
      "batch_idx: 8\n",
      "0.8981481481481481\n",
      "batch_idx: 9\n",
      "0.8958333333333334\n",
      "batch_idx: 10\n",
      "0.8939393939393939\n",
      "batch_idx: 11\n",
      "0.8958333333333334\n",
      "batch_idx: 12\n",
      "0.8974358974358975\n",
      "batch_idx: 13\n",
      "0.8958333333333334\n",
      "batch_idx: 14\n",
      "0.9\n",
      "batch_idx: 15\n",
      "0.8932291666666666\n",
      "batch_idx: 16\n",
      "0.8872549019607843\n",
      "batch_idx: 17\n",
      "0.8888888888888888\n",
      "batch_idx: 18\n",
      "0.8859649122807017\n",
      "batch_idx: 19\n",
      "0.875\n",
      "batch_idx: 20\n",
      "0.876984126984127\n",
      "batch_idx: 21\n",
      "0.875\n",
      "batch_idx: 22\n",
      "0.8731884057971014\n",
      "batch_idx: 23\n",
      "0.8732638888888888\n",
      "batch_idx: 24\n",
      "0.8783333333333333\n",
      "Training Epoch: 319, total loss: 41.597090\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8625\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.8461538461538461\n",
      "batch_idx: 13\n",
      "0.8482142857142857\n",
      "batch_idx: 14\n",
      "0.8555555555555555\n",
      "batch_idx: 15\n",
      "0.8515625\n",
      "batch_idx: 16\n",
      "0.8406862745098039\n",
      "batch_idx: 17\n",
      "0.8425925925925926\n",
      "batch_idx: 18\n",
      "0.8486842105263158\n",
      "batch_idx: 19\n",
      "0.8479166666666667\n",
      "batch_idx: 20\n",
      "0.8511904761904762\n",
      "batch_idx: 21\n",
      "0.8484848484848485\n",
      "batch_idx: 22\n",
      "0.8514492753623188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 23\n",
      "0.8541666666666666\n",
      "batch_idx: 24\n",
      "0.8533333333333334\n",
      "Training Epoch: 320, total loss: 42.058253\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.8809523809523809\n",
      "batch_idx: 7\n",
      "0.8854166666666666\n",
      "batch_idx: 8\n",
      "0.8888888888888888\n",
      "batch_idx: 9\n",
      "0.9\n",
      "batch_idx: 10\n",
      "0.8977272727272727\n",
      "batch_idx: 11\n",
      "0.8993055555555556\n",
      "batch_idx: 12\n",
      "0.9006410256410257\n",
      "batch_idx: 13\n",
      "0.8988095238095238\n",
      "batch_idx: 14\n",
      "0.8916666666666667\n",
      "batch_idx: 15\n",
      "0.890625\n",
      "batch_idx: 16\n",
      "0.8823529411764706\n",
      "batch_idx: 17\n",
      "0.8842592592592593\n",
      "batch_idx: 18\n",
      "0.8771929824561403\n",
      "batch_idx: 19\n",
      "0.8666666666666667\n",
      "batch_idx: 20\n",
      "0.8650793650793651\n",
      "batch_idx: 21\n",
      "0.8636363636363636\n",
      "batch_idx: 22\n",
      "0.8659420289855072\n",
      "batch_idx: 23\n",
      "0.8697916666666666\n",
      "batch_idx: 24\n",
      "0.8683333333333333\n",
      "Training Epoch: 321, total loss: 41.836280\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8703703703703703\n",
      "batch_idx: 9\n",
      "0.8625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-df9ca902e514>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MLP:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnfm_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'dataset/xiaoguan/RF/RF_for_train/train_class_9/model/gene_4000_MLP.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-165f23e5dbad>\u001b[0m in \u001b[0;36mtrain_data\u001b[0;34m(model, data_loader, batch_size, model_path)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m#print(\"y_predict:\",y_predict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m#loss = loss_func(y_predict.view(-1), labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from MLP import MLP\n",
    "model=MLP(4225,1000,100,9)\n",
    "model.cuda()\n",
    "print(\"MLP:\",model)\n",
    "train_data(model,train_loader,nfm_config['batch_size'],'dataset/xiaoguan/RF/RF_for_train/train_class_9/model/gene_4000_MLP.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d82514",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(nfm.state_dict(),'dataset/xiaoguan/RF/RF_for_train/train_class_9/model/gene_4000_MLP.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05851f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row: 0 label: [0]\n",
      "row: 1 label: [0]\n",
      "row: 2 label: [0]\n",
      "row: 3 label: [0]\n",
      "row: 4 label: [0]\n",
      "row: 5 label: [0]\n",
      "row: 6 label: [0]\n",
      "row: 7 label: [0]\n",
      "row: 8 label: [0]\n",
      "row: 9 label: [0]\n",
      "row: 10 label: [0]\n",
      "row: 11 label: [0]\n",
      "row: 12 label: [0]\n",
      "row: 13 label: [0]\n",
      "row: 14 label: [0]\n",
      "row: 15 label: [0]\n",
      "row: 16 label: [0]\n",
      "row: 17 label: [0]\n",
      "row: 18 label: [0]\n",
      "row: 19 label: [0]\n",
      "row: 20 label: [0]\n",
      "row: 21 label: [0]\n",
      "row: 22 label: [0]\n",
      "row: 23 label: [0]\n",
      "row: 24 label: [0]\n",
      "row: 25 label: [0]\n",
      "row: 26 label: [0]\n",
      "row: 27 label: [0]\n",
      "row: 28 label: [0]\n",
      "row: 29 label: [0]\n",
      "row: 30 label: [0]\n",
      "row: 31 label: [0]\n",
      "row: 32 label: [0]\n",
      "row: 33 label: [0]\n",
      "row: 34 label: [0]\n",
      "row: 35 label: [0]\n",
      "row: 36 label: [0]\n",
      "row: 37 label: [0]\n",
      "row: 38 label: [0]\n",
      "row: 39 label: [0]\n",
      "row: 40 label: [0]\n",
      "row: 41 label: [0]\n",
      "row: 42 label: [0]\n",
      "row: 43 label: [1]\n",
      "row: 44 label: [1]\n",
      "row: 45 label: [1]\n",
      "row: 46 label: [1]\n",
      "row: 47 label: [1]\n",
      "row: 48 label: [1]\n",
      "row: 49 label: [1]\n",
      "row: 50 label: [1]\n",
      "row: 51 label: [1]\n",
      "row: 52 label: [1]\n",
      "row: 53 label: [1]\n",
      "row: 54 label: [1]\n",
      "row: 55 label: [1]\n",
      "row: 56 label: [1]\n",
      "row: 57 label: [1]\n",
      "row: 58 label: [1]\n",
      "row: 59 label: [1]\n",
      "row: 60 label: [1]\n",
      "row: 61 label: [1]\n",
      "row: 62 label: [1]\n",
      "row: 63 label: [1]\n",
      "row: 64 label: [1]\n",
      "row: 65 label: [1]\n",
      "row: 66 label: [1]\n",
      "row: 67 label: [1]\n",
      "row: 68 label: [1]\n",
      "row: 69 label: [1]\n",
      "row: 70 label: [1]\n",
      "row: 71 label: [1]\n",
      "row: 72 label: [1]\n",
      "row: 73 label: [1]\n",
      "row: 74 label: [1]\n",
      "row: 75 label: [1]\n",
      "row: 76 label: [1]\n",
      "row: 77 label: [1]\n",
      "row: 78 label: [1]\n",
      "row: 79 label: [1]\n",
      "row: 80 label: [1]\n",
      "row: 81 label: [1]\n",
      "row: 82 label: [1]\n",
      "row: 83 label: [1]\n",
      "row: 84 label: [1]\n",
      "row: 85 label: [1]\n",
      "row: 86 label: [1]\n",
      "row: 87 label: [1]\n",
      "row: 88 label: [1]\n",
      "row: 89 label: [1]\n",
      "row: 90 label: [1]\n",
      "row: 91 label: [1]\n",
      "row: 92 label: [1]\n",
      "row: 93 label: [1]\n",
      "row: 94 label: [1]\n",
      "row: 95 label: [1]\n",
      "row: 96 label: [1]\n",
      "row: 97 label: [1]\n",
      "row: 98 label: [1]\n",
      "row: 99 label: [1]\n",
      "row: 100 label: [1]\n",
      "row: 101 label: [1]\n",
      "row: 102 label: [1]\n",
      "row: 103 label: [1]\n",
      "row: 104 label: [1]\n",
      "row: 105 label: [1]\n",
      "row: 106 label: [1]\n",
      "row: 107 label: [1]\n",
      "row: 108 label: [1]\n",
      "row: 109 label: [1]\n",
      "row: 110 label: [1]\n",
      "row: 111 label: [1]\n",
      "row: 112 label: [1]\n",
      "row: 113 label: [1]\n",
      "row: 114 label: [1]\n",
      "row: 115 label: [1]\n",
      "row: 116 label: [1]\n",
      "row: 117 label: [1]\n",
      "row: 118 label: [1]\n",
      "row: 119 label: [1]\n",
      "row: 120 label: [1]\n",
      "row: 121 label: [1]\n",
      "row: 122 label: [1]\n",
      "row: 123 label: [1]\n",
      "row: 124 label: [1]\n",
      "row: 125 label: [1]\n",
      "row: 126 label: [1]\n",
      "row: 127 label: [1]\n",
      "row: 128 label: [1]\n",
      "row: 129 label: [1]\n",
      "row: 130 label: [1]\n",
      "row: 131 label: [1]\n",
      "row: 132 label: [1]\n",
      "row: 133 label: [1]\n",
      "row: 134 label: [1]\n",
      "row: 135 label: [1]\n",
      "row: 136 label: [1]\n",
      "row: 137 label: [1]\n",
      "row: 138 label: [1]\n",
      "row: 139 label: [1]\n",
      "row: 140 label: [1]\n",
      "row: 141 label: [1]\n",
      "row: 142 label: [1]\n",
      "row: 143 label: [1]\n",
      "row: 144 label: [1]\n",
      "row: 145 label: [1]\n",
      "row: 146 label: [2]\n",
      "row: 147 label: [2]\n",
      "row: 148 label: [2]\n",
      "row: 149 label: [2]\n",
      "row: 150 label: [2]\n",
      "row: 151 label: [2]\n",
      "row: 152 label: [2]\n",
      "row: 153 label: [2]\n",
      "row: 154 label: [2]\n",
      "row: 155 label: [2]\n",
      "row: 156 label: [2]\n",
      "row: 157 label: [2]\n",
      "row: 158 label: [2]\n",
      "row: 159 label: [2]\n",
      "row: 160 label: [2]\n",
      "row: 161 label: [2]\n",
      "row: 162 label: [2]\n",
      "row: 163 label: [2]\n",
      "row: 164 label: [2]\n",
      "row: 165 label: [2]\n",
      "row: 166 label: [2]\n",
      "row: 167 label: [2]\n",
      "row: 168 label: [2]\n",
      "row: 169 label: [2]\n",
      "row: 170 label: [2]\n",
      "row: 171 label: [2]\n",
      "row: 172 label: [2]\n",
      "row: 173 label: [2]\n",
      "row: 174 label: [2]\n",
      "row: 175 label: [2]\n",
      "row: 176 label: [2]\n",
      "row: 177 label: [2]\n",
      "row: 178 label: [2]\n",
      "row: 179 label: [2]\n",
      "row: 180 label: [2]\n",
      "row: 181 label: [2]\n",
      "row: 182 label: [2]\n",
      "row: 183 label: [2]\n",
      "row: 184 label: [2]\n",
      "row: 185 label: [2]\n",
      "row: 186 label: [2]\n",
      "row: 187 label: [2]\n",
      "row: 188 label: [2]\n",
      "row: 189 label: [2]\n",
      "row: 190 label: [2]\n",
      "row: 191 label: [2]\n",
      "row: 192 label: [2]\n",
      "row: 193 label: [2]\n",
      "row: 194 label: [2]\n",
      "row: 195 label: [2]\n",
      "row: 196 label: [2]\n",
      "row: 197 label: [2]\n",
      "row: 198 label: [2]\n",
      "row: 199 label: [2]\n",
      "row: 200 label: [3]\n",
      "row: 201 label: [3]\n",
      "row: 202 label: [3]\n",
      "row: 203 label: [3]\n",
      "row: 204 label: [3]\n",
      "row: 205 label: [3]\n",
      "row: 206 label: [3]\n",
      "row: 207 label: [3]\n",
      "row: 208 label: [3]\n",
      "row: 209 label: [3]\n",
      "row: 210 label: [3]\n",
      "row: 211 label: [3]\n",
      "row: 212 label: [3]\n",
      "row: 213 label: [3]\n",
      "row: 214 label: [3]\n",
      "row: 215 label: [3]\n",
      "row: 216 label: [3]\n",
      "row: 217 label: [3]\n",
      "row: 218 label: [3]\n",
      "row: 219 label: [3]\n",
      "row: 220 label: [3]\n",
      "row: 221 label: [3]\n",
      "row: 222 label: [3]\n",
      "row: 223 label: [3]\n",
      "row: 224 label: [3]\n",
      "row: 225 label: [3]\n",
      "row: 226 label: [3]\n",
      "row: 227 label: [3]\n",
      "row: 228 label: [3]\n",
      "row: 229 label: [3]\n",
      "row: 230 label: [3]\n",
      "row: 231 label: [3]\n",
      "row: 232 label: [3]\n",
      "row: 233 label: [3]\n",
      "row: 234 label: [3]\n",
      "row: 235 label: [3]\n",
      "row: 236 label: [3]\n",
      "row: 237 label: [3]\n",
      "row: 238 label: [3]\n",
      "row: 239 label: [3]\n",
      "row: 240 label: [3]\n",
      "row: 241 label: [3]\n",
      "row: 242 label: [3]\n",
      "row: 243 label: [3]\n",
      "row: 244 label: [3]\n",
      "row: 245 label: [3]\n",
      "row: 246 label: [3]\n",
      "row: 247 label: [3]\n",
      "row: 248 label: [3]\n",
      "row: 249 label: [3]\n",
      "row: 250 label: [3]\n",
      "row: 251 label: [3]\n",
      "row: 252 label: [3]\n",
      "row: 253 label: [3]\n",
      "row: 254 label: [3]\n",
      "row: 255 label: [3]\n",
      "row: 256 label: [3]\n",
      "row: 257 label: [3]\n",
      "row: 258 label: [3]\n",
      "row: 259 label: [3]\n",
      "row: 260 label: [3]\n",
      "row: 261 label: [3]\n",
      "row: 262 label: [3]\n",
      "row: 263 label: [3]\n",
      "row: 264 label: [3]\n",
      "row: 265 label: [3]\n",
      "row: 266 label: [3]\n",
      "row: 267 label: [3]\n",
      "row: 268 label: [3]\n",
      "row: 269 label: [3]\n",
      "row: 270 label: [3]\n",
      "row: 271 label: [3]\n",
      "row: 272 label: [3]\n",
      "row: 273 label: [4]\n",
      "row: 274 label: [4]\n",
      "row: 275 label: [4]\n",
      "row: 276 label: [4]\n",
      "row: 277 label: [4]\n",
      "row: 278 label: [4]\n",
      "row: 279 label: [4]\n",
      "row: 280 label: [4]\n",
      "row: 281 label: [4]\n",
      "row: 282 label: [4]\n",
      "row: 283 label: [4]\n",
      "row: 284 label: [4]\n",
      "row: 285 label: [4]\n",
      "row: 286 label: [4]\n",
      "row: 287 label: [4]\n",
      "row: 288 label: [4]\n",
      "row: 289 label: [4]\n",
      "row: 290 label: [4]\n",
      "row: 291 label: [4]\n",
      "row: 292 label: [4]\n",
      "row: 293 label: [4]\n",
      "row: 294 label: [4]\n",
      "row: 295 label: [4]\n",
      "row: 296 label: [5]\n",
      "row: 297 label: [5]\n",
      "row: 298 label: [5]\n",
      "row: 299 label: [5]\n",
      "row: 300 label: [5]\n",
      "row: 301 label: [5]\n",
      "row: 302 label: [5]\n",
      "row: 303 label: [5]\n",
      "row: 304 label: [5]\n",
      "row: 305 label: [5]\n",
      "row: 306 label: [5]\n",
      "row: 307 label: [5]\n",
      "row: 308 label: [5]\n",
      "row: 309 label: [5]\n",
      "row: 310 label: [5]\n",
      "row: 311 label: [5]\n",
      "row: 312 label: [5]\n",
      "row: 313 label: [5]\n",
      "row: 314 label: [5]\n",
      "row: 315 label: [5]\n",
      "row: 316 label: [5]\n",
      "row: 317 label: [5]\n",
      "row: 318 label: [5]\n",
      "row: 319 label: [5]\n",
      "row: 320 label: [5]\n",
      "row: 321 label: [5]\n",
      "row: 322 label: [5]\n",
      "row: 323 label: [5]\n",
      "row: 324 label: [5]\n",
      "row: 325 label: [5]\n",
      "row: 326 label: [6]\n",
      "row: 327 label: [6]\n",
      "row: 328 label: [6]\n",
      "row: 329 label: [6]\n",
      "row: 330 label: [6]\n",
      "row: 331 label: [6]\n",
      "row: 332 label: [6]\n",
      "row: 333 label: [6]\n",
      "row: 334 label: [6]\n",
      "row: 335 label: [6]\n",
      "row: 336 label: [6]\n",
      "row: 337 label: [6]\n",
      "row: 338 label: [6]\n",
      "row: 339 label: [6]\n",
      "row: 340 label: [6]\n",
      "row: 341 label: [6]\n",
      "row: 342 label: [6]\n",
      "row: 343 label: [6]\n",
      "row: 344 label: [6]\n",
      "row: 345 label: [6]\n",
      "row: 346 label: [6]\n",
      "row: 347 label: [6]\n",
      "row: 348 label: [6]\n",
      "row: 349 label: [6]\n",
      "row: 350 label: [6]\n",
      "row: 351 label: [6]\n",
      "row: 352 label: [6]\n",
      "row: 353 label: [6]\n",
      "row: 354 label: [6]\n",
      "row: 355 label: [6]\n",
      "row: 356 label: [6]\n",
      "row: 357 label: [6]\n",
      "row: 358 label: [6]\n",
      "row: 359 label: [6]\n",
      "row: 360 label: [6]\n",
      "row: 361 label: [6]\n",
      "row: 362 label: [6]\n",
      "row: 363 label: [6]\n",
      "row: 364 label: [6]\n",
      "row: 365 label: [6]\n",
      "row: 366 label: [6]\n",
      "row: 367 label: [6]\n",
      "row: 368 label: [6]\n",
      "row: 369 label: [6]\n",
      "row: 370 label: [6]\n",
      "row: 371 label: [6]\n",
      "row: 372 label: [6]\n",
      "row: 373 label: [6]\n",
      "row: 374 label: [6]\n",
      "row: 375 label: [6]\n",
      "row: 376 label: [6]\n",
      "row: 377 label: [6]\n",
      "row: 378 label: [6]\n",
      "row: 379 label: [6]\n",
      "row: 380 label: [6]\n",
      "row: 381 label: [6]\n",
      "row: 382 label: [6]\n",
      "row: 383 label: [6]\n",
      "row: 384 label: [6]\n",
      "row: 385 label: [6]\n",
      "row: 386 label: [6]\n",
      "row: 387 label: [6]\n",
      "row: 388 label: [6]\n",
      "row: 389 label: [6]\n",
      "row: 390 label: [6]\n",
      "row: 391 label: [6]\n",
      "row: 392 label: [6]\n",
      "row: 393 label: [6]\n",
      "row: 394 label: [6]\n",
      "row: 395 label: [6]\n",
      "row: 396 label: [6]\n",
      "row: 397 label: [6]\n",
      "row: 398 label: [6]\n",
      "row: 399 label: [6]\n",
      "row: 400 label: [6]\n",
      "row: 401 label: [6]\n",
      "row: 402 label: [6]\n",
      "row: 403 label: [6]\n",
      "row: 404 label: [6]\n",
      "row: 405 label: [6]\n",
      "row: 406 label: [6]\n",
      "row: 407 label: [6]\n",
      "row: 408 label: [6]\n",
      "row: 409 label: [6]\n",
      "row: 410 label: [6]\n",
      "row: 411 label: [6]\n",
      "row: 412 label: [6]\n",
      "row: 413 label: [6]\n",
      "row: 414 label: [6]\n",
      "row: 415 label: [6]\n",
      "row: 416 label: [6]\n",
      "row: 417 label: [6]\n",
      "row: 418 label: [6]\n",
      "row: 419 label: [6]\n",
      "row: 420 label: [6]\n",
      "row: 421 label: [6]\n",
      "row: 422 label: [6]\n",
      "row: 423 label: [6]\n",
      "row: 424 label: [6]\n",
      "row: 425 label: [6]\n",
      "row: 426 label: [6]\n",
      "row: 427 label: [6]\n",
      "row: 428 label: [6]\n",
      "row: 429 label: [6]\n",
      "row: 430 label: [6]\n",
      "row: 431 label: [6]\n",
      "row: 432 label: [6]\n",
      "row: 433 label: [6]\n",
      "row: 434 label: [6]\n",
      "row: 435 label: [6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row: 436 label: [6]\n",
      "row: 437 label: [6]\n",
      "row: 438 label: [6]\n",
      "row: 439 label: [6]\n",
      "row: 440 label: [6]\n",
      "row: 441 label: [6]\n",
      "row: 442 label: [6]\n",
      "row: 443 label: [6]\n",
      "row: 444 label: [6]\n",
      "row: 445 label: [6]\n",
      "row: 446 label: [6]\n",
      "row: 447 label: [6]\n",
      "row: 448 label: [6]\n",
      "row: 449 label: [6]\n",
      "row: 450 label: [6]\n",
      "row: 451 label: [6]\n",
      "row: 452 label: [6]\n",
      "row: 453 label: [6]\n",
      "row: 454 label: [6]\n",
      "row: 455 label: [6]\n",
      "row: 456 label: [6]\n",
      "row: 457 label: [6]\n",
      "row: 458 label: [6]\n",
      "row: 459 label: [6]\n",
      "row: 460 label: [6]\n",
      "row: 461 label: [6]\n",
      "row: 462 label: [6]\n",
      "row: 463 label: [6]\n",
      "row: 464 label: [6]\n",
      "row: 465 label: [6]\n",
      "row: 466 label: [6]\n",
      "row: 467 label: [6]\n",
      "row: 468 label: [6]\n",
      "row: 469 label: [6]\n",
      "row: 470 label: [6]\n",
      "row: 471 label: [6]\n",
      "row: 472 label: [6]\n",
      "row: 473 label: [6]\n",
      "row: 474 label: [6]\n",
      "row: 475 label: [6]\n",
      "row: 476 label: [6]\n",
      "row: 477 label: [6]\n",
      "row: 478 label: [6]\n",
      "row: 479 label: [7]\n",
      "row: 480 label: [7]\n",
      "row: 481 label: [7]\n",
      "row: 482 label: [7]\n",
      "row: 483 label: [7]\n",
      "row: 484 label: [7]\n",
      "row: 485 label: [7]\n",
      "row: 486 label: [7]\n",
      "row: 487 label: [7]\n",
      "row: 488 label: [7]\n",
      "row: 489 label: [7]\n",
      "row: 490 label: [7]\n",
      "row: 491 label: [7]\n",
      "row: 492 label: [7]\n",
      "row: 493 label: [7]\n",
      "row: 494 label: [7]\n",
      "row: 495 label: [7]\n",
      "row: 496 label: [7]\n",
      "row: 497 label: [7]\n",
      "row: 498 label: [7]\n",
      "row: 499 label: [7]\n",
      "row: 500 label: [7]\n",
      "row: 501 label: [7]\n",
      "row: 502 label: [7]\n",
      "row: 503 label: [7]\n",
      "row: 504 label: [7]\n",
      "row: 505 label: [7]\n",
      "row: 506 label: [7]\n",
      "row: 507 label: [7]\n",
      "row: 508 label: [7]\n",
      "row: 509 label: [7]\n",
      "row: 510 label: [7]\n",
      "row: 511 label: [7]\n",
      "row: 512 label: [7]\n",
      "row: 513 label: [7]\n",
      "row: 514 label: [7]\n",
      "row: 515 label: [7]\n",
      "row: 516 label: [8]\n",
      "row: 517 label: [8]\n",
      "row: 518 label: [8]\n",
      "row: 519 label: [8]\n",
      "row: 520 label: [8]\n",
      "row: 521 label: [8]\n",
      "row: 522 label: [8]\n",
      "row: 523 label: [8]\n",
      "row: 524 label: [8]\n",
      "row: 525 label: [8]\n",
      "row: 526 label: [8]\n",
      "row: 527 label: [8]\n",
      "row: 528 label: [8]\n",
      "row: 529 label: [8]\n",
      "row: 530 label: [8]\n",
      "row: 531 label: [8]\n",
      "row: 532 label: [8]\n",
      "row: 533 label: [8]\n",
      "row: 534 label: [8]\n",
      "row: 535 label: [8]\n",
      "row: 536 label: [8]\n",
      "row: 537 label: [8]\n",
      "row: 538 label: [8]\n",
      "row: 539 label: [8]\n",
      "row: 540 label: [8]\n",
      "row: 541 label: [8]\n",
      "row: 542 label: [8]\n",
      "row: 543 label: [8]\n",
      "row: 544 label: [8]\n",
      "row: 545 label: [8]\n",
      "row: 546 label: [8]\n",
      "row: 547 label: [8]\n",
      "row: 548 label: [8]\n",
      "row: 549 label: [8]\n",
      "row: 550 label: [8]\n",
      "row: 551 label: [8]\n",
      "row: 552 label: [8]\n",
      "row: 553 label: [8]\n",
      "row: 554 label: [8]\n",
      "row: 555 label: [8]\n",
      "row: 556 label: [8]\n",
      "row: 557 label: [8]\n",
      "row: 558 label: [8]\n",
      "row: 559 label: [8]\n",
      "row: 560 label: [8]\n",
      "row: 561 label: [8]\n",
      "row: 562 label: [8]\n",
      "row: 563 label: [8]\n",
      "row: 564 label: [8]\n",
      "row: 565 label: [8]\n",
      "row: 566 label: [8]\n",
      "row: 567 label: [8]\n",
      "row: 568 label: [8]\n",
      "row: 569 label: [8]\n",
      "row: 570 label: [8]\n",
      "row: 571 label: [8]\n",
      "row: 572 label: [8]\n",
      "row: 573 label: [8]\n",
      "row: 574 label: [8]\n",
      "row: 575 label: [8]\n",
      "row: 576 label: [8]\n",
      "row: 577 label: [8]\n",
      "row: 578 label: [8]\n",
      "row: 579 label: [8]\n",
      "row: 580 label: [8]\n",
      "row: 581 label: [8]\n",
      "row: 582 label: [8]\n",
      "row: 583 label: [8]\n",
      "row: 584 label: [8]\n",
      "row: 585 label: [8]\n",
      "row: 586 label: [8]\n",
      "row: 587 label: [8]\n",
      "row: 588 label: [8]\n",
      "row: 589 label: [8]\n",
      "row: 590 label: [8]\n",
      "row: 591 label: [8]\n",
      "row: 592 label: [8]\n",
      "row: 593 label: [8]\n",
      "row: 594 label: [8]\n",
      "row: 595 label: [8]\n",
      "row: 596 label: [8]\n",
      "row: 597 label: [8]\n",
      "row: 598 label: [8]\n",
      "row: 599 label: [8]\n",
      "row: 600 label: [8]\n",
      "row: 601 label: [8]\n",
      "row: 602 label: [8]\n",
      "label: [[0.82222223 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      " ...\n",
      " [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.82222223]]\n",
      "features: [[  0  35  45 ...  31  27  28]\n",
      " [  1  35  46 ...  30  27  28]\n",
      " [  2  35  41 ...  32  26  29]\n",
      " ...\n",
      " [601  36  38 ...  14  35  32]\n",
      " [602  32  42 ...  10  29  36]\n",
      " [603  37  46 ...  13  27  33]]\n",
      "nfm: NFM(\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (linear_model1): Linear(in_features=4225, out_features=1000, bias=True)\n",
      "  (BN_linear1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear_model2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (BN_linear2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (embedding_layers): Embedding(1001, 100)\n",
      "  (bi_pooling): BiInteractionPooling()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (BN_bi): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dnn_layers): ModuleList(\n",
      "    (0): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (1): Linear(in_features=100, out_features=9, bias=True)\n",
      "  )\n",
      "  (dnn_softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:89: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.09\n",
      "batch_idx: 1\n",
      "0.125\n",
      "batch_idx: 2\n",
      "0.14666666666666667\n",
      "batch_idx: 3\n",
      "0.1625\n",
      "batch_idx: 4\n",
      "0.19\n",
      "batch_idx: 5\n",
      "0.21166666666666667\n",
      "Training Epoch: 0, total loss: 13.144091\n",
      "batch_idx: 0\n",
      "0.25\n",
      "batch_idx: 1\n",
      "0.285\n",
      "batch_idx: 2\n",
      "0.28\n",
      "batch_idx: 3\n",
      "0.3\n",
      "batch_idx: 4\n",
      "0.308\n",
      "batch_idx: 5\n",
      "0.30666666666666664\n",
      "Training Epoch: 1, total loss: 12.977075\n",
      "batch_idx: 0\n",
      "0.38\n",
      "batch_idx: 1\n",
      "0.385\n",
      "batch_idx: 2\n",
      "0.39\n",
      "batch_idx: 3\n",
      "0.3775\n",
      "batch_idx: 4\n",
      "0.374\n",
      "batch_idx: 5\n",
      "0.37666666666666665\n",
      "Training Epoch: 2, total loss: 12.738428\n",
      "batch_idx: 0\n",
      "0.54\n",
      "batch_idx: 1\n",
      "0.41\n",
      "batch_idx: 2\n",
      "0.3933333333333333\n",
      "batch_idx: 3\n",
      "0.4025\n",
      "batch_idx: 4\n",
      "0.394\n",
      "batch_idx: 5\n",
      "0.385\n",
      "Training Epoch: 3, total loss: 12.506386\n",
      "batch_idx: 0\n",
      "0.31\n",
      "batch_idx: 1\n",
      "0.35\n",
      "batch_idx: 2\n",
      "0.38\n",
      "batch_idx: 3\n",
      "0.415\n",
      "batch_idx: 4\n",
      "0.424\n",
      "batch_idx: 5\n",
      "0.4266666666666667\n",
      "Training Epoch: 4, total loss: 12.223554\n",
      "batch_idx: 0\n",
      "0.48\n",
      "batch_idx: 1\n",
      "0.475\n",
      "batch_idx: 2\n",
      "0.47\n",
      "batch_idx: 3\n",
      "0.4825\n",
      "batch_idx: 4\n",
      "0.49\n",
      "batch_idx: 5\n",
      "0.4866666666666667\n",
      "Training Epoch: 5, total loss: 12.004891\n",
      "batch_idx: 0\n",
      "0.61\n",
      "batch_idx: 1\n",
      "0.55\n",
      "batch_idx: 2\n",
      "0.55\n",
      "batch_idx: 3\n",
      "0.555\n",
      "batch_idx: 4\n",
      "0.562\n",
      "batch_idx: 5\n",
      "0.55\n",
      "Training Epoch: 6, total loss: 11.702809\n",
      "batch_idx: 0\n",
      "0.56\n",
      "batch_idx: 1\n",
      "0.57\n",
      "batch_idx: 2\n",
      "0.5633333333333334\n",
      "batch_idx: 3\n",
      "0.58\n",
      "batch_idx: 4\n",
      "0.582\n",
      "batch_idx: 5\n",
      "0.5783333333333334\n",
      "Training Epoch: 7, total loss: 11.490756\n",
      "batch_idx: 0\n",
      "0.61\n",
      "batch_idx: 1\n",
      "0.61\n",
      "batch_idx: 2\n",
      "0.5933333333333334\n",
      "batch_idx: 3\n",
      "0.5825\n",
      "batch_idx: 4\n",
      "0.596\n",
      "batch_idx: 5\n",
      "0.5883333333333334\n",
      "Training Epoch: 8, total loss: 11.349567\n",
      "batch_idx: 0\n",
      "0.61\n",
      "batch_idx: 1\n",
      "0.63\n",
      "batch_idx: 2\n",
      "0.62\n",
      "batch_idx: 3\n",
      "0.58\n",
      "batch_idx: 4\n",
      "0.582\n",
      "batch_idx: 5\n",
      "0.59\n",
      "Training Epoch: 9, total loss: 11.262968\n",
      "batch_idx: 0\n",
      "0.63\n",
      "batch_idx: 1\n",
      "0.605\n",
      "batch_idx: 2\n",
      "0.6033333333333334\n",
      "batch_idx: 3\n",
      "0.605\n",
      "batch_idx: 4\n",
      "0.588\n",
      "batch_idx: 5\n",
      "0.59\n",
      "Training Epoch: 10, total loss: 11.166723\n",
      "batch_idx: 0\n",
      "0.58\n",
      "batch_idx: 1\n",
      "0.59\n",
      "batch_idx: 2\n",
      "0.64\n",
      "batch_idx: 3\n",
      "0.6325\n",
      "batch_idx: 4\n",
      "0.632\n",
      "batch_idx: 5\n",
      "0.6416666666666667\n",
      "Training Epoch: 11, total loss: 11.071270\n",
      "batch_idx: 0\n",
      "0.65\n",
      "batch_idx: 1\n",
      "0.72\n",
      "batch_idx: 2\n",
      "0.7233333333333334\n",
      "batch_idx: 3\n",
      "0.69\n",
      "batch_idx: 4\n",
      "0.688\n",
      "batch_idx: 5\n",
      "0.685\n",
      "Training Epoch: 12, total loss: 11.018391\n",
      "batch_idx: 0\n",
      "0.78\n",
      "batch_idx: 1\n",
      "0.72\n",
      "batch_idx: 2\n",
      "0.71\n",
      "batch_idx: 3\n",
      "0.725\n",
      "batch_idx: 4\n",
      "0.712\n",
      "batch_idx: 5\n",
      "0.7166666666666667\n",
      "Training Epoch: 13, total loss: 10.848578\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.725\n",
      "batch_idx: 2\n",
      "0.7266666666666667\n",
      "batch_idx: 3\n",
      "0.71\n",
      "batch_idx: 4\n",
      "0.716\n",
      "batch_idx: 5\n",
      "0.7183333333333334\n",
      "Training Epoch: 14, total loss: 10.753141\n",
      "batch_idx: 0\n",
      "0.73\n",
      "batch_idx: 1\n",
      "0.73\n",
      "batch_idx: 2\n",
      "0.7533333333333333\n",
      "batch_idx: 3\n",
      "0.73\n",
      "batch_idx: 4\n",
      "0.732\n",
      "batch_idx: 5\n",
      "0.735\n",
      "Training Epoch: 15, total loss: 10.619094\n",
      "batch_idx: 0\n",
      "0.72\n",
      "batch_idx: 1\n",
      "0.725\n",
      "batch_idx: 2\n",
      "0.7133333333333334\n",
      "batch_idx: 3\n",
      "0.7375\n",
      "batch_idx: 4\n",
      "0.742\n",
      "batch_idx: 5\n",
      "0.7483333333333333\n",
      "Training Epoch: 16, total loss: 10.586080\n",
      "batch_idx: 0\n",
      "0.79\n",
      "batch_idx: 1\n",
      "0.815\n",
      "batch_idx: 2\n",
      "0.7666666666666667\n",
      "batch_idx: 3\n",
      "0.76\n",
      "batch_idx: 4\n",
      "0.752\n",
      "batch_idx: 5\n",
      "0.7566666666666667\n",
      "Training Epoch: 17, total loss: 10.548629\n",
      "batch_idx: 0\n",
      "0.72\n",
      "batch_idx: 1\n",
      "0.73\n",
      "batch_idx: 2\n",
      "0.7366666666666667\n",
      "batch_idx: 3\n",
      "0.7375\n",
      "batch_idx: 4\n",
      "0.756\n",
      "batch_idx: 5\n",
      "0.76\n",
      "Training Epoch: 18, total loss: 10.531051\n",
      "batch_idx: 0\n",
      "0.78\n",
      "batch_idx: 1\n",
      "0.78\n",
      "batch_idx: 2\n",
      "0.7866666666666666\n",
      "batch_idx: 3\n",
      "0.79\n",
      "batch_idx: 4\n",
      "0.778\n",
      "batch_idx: 5\n",
      "0.785\n",
      "Training Epoch: 19, total loss: 10.442163\n",
      "batch_idx: 0\n",
      "0.81\n",
      "batch_idx: 1\n",
      "0.83\n",
      "batch_idx: 2\n",
      "0.7866666666666666\n",
      "batch_idx: 3\n",
      "0.785\n",
      "batch_idx: 4\n",
      "0.78\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "Training Epoch: 20, total loss: 10.443403\n",
      "batch_idx: 0\n",
      "0.77\n",
      "batch_idx: 1\n",
      "0.79\n",
      "batch_idx: 2\n",
      "0.8\n",
      "batch_idx: 3\n",
      "0.7925\n",
      "batch_idx: 4\n",
      "0.79\n",
      "batch_idx: 5\n",
      "0.7933333333333333\n",
      "Training Epoch: 21, total loss: 10.423825\n",
      "batch_idx: 0\n",
      "0.86\n",
      "batch_idx: 1\n",
      "0.84\n",
      "batch_idx: 2\n",
      "0.82\n",
      "batch_idx: 3\n",
      "0.81\n",
      "batch_idx: 4\n",
      "0.806\n",
      "batch_idx: 5\n",
      "0.8116666666666666\n",
      "Training Epoch: 22, total loss: 10.312704\n",
      "batch_idx: 0\n",
      "0.87\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.84\n",
      "batch_idx: 3\n",
      "0.8375\n",
      "batch_idx: 4\n",
      "0.826\n",
      "batch_idx: 5\n",
      "0.8116666666666666\n",
      "Training Epoch: 23, total loss: 10.280468\n",
      "batch_idx: 0\n",
      "0.76\n",
      "batch_idx: 1\n",
      "0.77\n",
      "batch_idx: 2\n",
      "0.79\n",
      "batch_idx: 3\n",
      "0.7875\n",
      "batch_idx: 4\n",
      "0.796\n",
      "batch_idx: 5\n",
      "0.795\n",
      "Training Epoch: 24, total loss: 10.348539\n",
      "batch_idx: 0\n",
      "0.82\n",
      "batch_idx: 1\n",
      "0.77\n",
      "batch_idx: 2\n",
      "0.8033333333333333\n",
      "batch_idx: 3\n",
      "0.8175\n",
      "batch_idx: 4\n",
      "0.808\n",
      "batch_idx: 5\n",
      "0.8133333333333334\n",
      "Training Epoch: 25, total loss: 10.273150\n",
      "batch_idx: 0\n",
      "0.8\n",
      "batch_idx: 1\n",
      "0.815\n",
      "batch_idx: 2\n",
      "0.8233333333333334\n",
      "batch_idx: 3\n",
      "0.8075\n",
      "batch_idx: 4\n",
      "0.802\n",
      "batch_idx: 5\n",
      "0.7983333333333333\n",
      "Training Epoch: 26, total loss: 10.271836\n",
      "batch_idx: 0\n",
      "0.85\n",
      "batch_idx: 1\n",
      "0.84\n",
      "batch_idx: 2\n",
      "0.8233333333333334\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.802\n",
      "batch_idx: 5\n",
      "0.8083333333333333\n",
      "Training Epoch: 27, total loss: 10.265012\n",
      "batch_idx: 0\n",
      "0.83\n",
      "batch_idx: 1\n",
      "0.805\n",
      "batch_idx: 2\n",
      "0.8166666666666667\n",
      "batch_idx: 3\n",
      "0.82\n",
      "batch_idx: 4\n",
      "0.824\n",
      "batch_idx: 5\n",
      "0.8233333333333334\n",
      "Training Epoch: 28, total loss: 10.189775\n",
      "batch_idx: 0\n",
      "0.9\n",
      "batch_idx: 1\n",
      "0.89\n",
      "batch_idx: 2\n",
      "0.8566666666666667\n",
      "batch_idx: 3\n",
      "0.85\n",
      "batch_idx: 4\n",
      "0.848\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "Training Epoch: 29, total loss: 10.145954\n",
      "batch_idx: 0\n",
      "0.81\n",
      "batch_idx: 1\n",
      "0.82\n",
      "batch_idx: 2\n",
      "0.8433333333333334\n",
      "batch_idx: 3\n",
      "0.845\n",
      "batch_idx: 4\n",
      "0.826\n",
      "batch_idx: 5\n",
      "0.8233333333333334\n",
      "Training Epoch: 30, total loss: 10.207748\n"
     ]
    }
   ],
   "source": [
    "#增加了mixup数据增强\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "#from utils import \n",
    "\n",
    "#from new_dataset_processed import FMData\n",
    "from dataset_process import FMData\n",
    "\n",
    "#train_dataset = FMData(config.train_libfm,config.train_label,nfm_config['n_class'])\n",
    "train_dataset = FMData(nfm_config['train_data'],nfm_config['train_label'],nfm_config['n_class'])\n",
    "train_loader = data.DataLoader(train_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "#validate_dataset = FMData(config.valid_libfm,config.valid_label)\n",
    "#validate_loader = data.DataLoader(train_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "#test_dataset = FMData(config.test_libfm,config.test_label)\n",
    "#test_loader = data.DataLoader(test_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "if __name__ == \"__main__\":\n",
    "    ####################################################################################\n",
    "    # NFM 模型\n",
    "    ####################################################################################\n",
    "    BATCH_SIZE=100\n",
    "    \"\"\"\n",
    "    training_data, training_label, dense_features_col, sparse_features_col = getTrainData(nfm_config['train_file'], nfm_config['fea_file'])\n",
    "    train_dataset = Data.TensorDataset(torch.tensor(training_data).float(), torch.tensor(training_label).float())\n",
    "\n",
    "    test_data = getTestData(nfm_config['test_file'])\n",
    "    test_dataset = Data.TensorDataset(torch.tensor(test_data).float())\n",
    "    \n",
    "    test_data = getTestData(nfm_config['test_file'])\n",
    "    test_dataset = Data.TensorDataset(torch.tensor(test_data).float())\n",
    "\n",
    "    \"\"\"\n",
    "    #device = torch.device('cuda:0')\n",
    "    #epoch=0\n",
    "    \n",
    "    #model=nn.Linear(10149,16).to(device)\n",
    "    #model=nn.Linear(10149,16)\n",
    "    #model=nn.ReLU(nn.Linear(10149,16))#RuntimeError: all elements of input should be between 0 and 1\n",
    "    #print('model:',model)\n",
    "    nfm = NFM(nfm_config).cuda()#加了device防止出现GPU CPU两种设备的错误提示\n",
    "    print(\"nfm:\",nfm)\n",
    "    #print(nfm)\n",
    "    #nfm.train()\n",
    "    #u=nfm.parameters()\n",
    "    #print(u)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    optimizer = torch.optim.Adam(nfm.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    total = 0\n",
    "    #loss_func = torch.nn.BCELoss()\n",
    "    loss_func=torch.nn.CrossEntropyLoss()\n",
    "    #loss_func=nn.MultiLabelSoftMarginLoss()\n",
    "    #loss_func=torch.nn.LogSoftmax()\n",
    "    \n",
    "    #model=nn.Softmax(nn.Linear(10149,16)).to(device)\n",
    "    # 从DataLoader中获取小批量的id以及数据\n",
    "    for epoch_id in range(1000):\n",
    "        correct=0\n",
    "        total=0\n",
    "        for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "            x = Variable(x)\n",
    "            labels = Variable(labels)\n",
    "            \n",
    "            \n",
    "            #x = torch.tensor(x, dtype=torch.float)\n",
    "            #x=x.clone().detach().requires_grad_(True)\n",
    "            x=torch.tensor(x,dtype=torch.float)\n",
    "            labels=torch.tensor(labels,dtype=torch.float)\n",
    "            x, labels = x.cuda(), labels.cuda()\n",
    "            \n",
    "            #print(\"labels:\",labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_predict = nfm(x)\n",
    "            #print(\"y_predict:\",y_predict)\n",
    "            #loss = loss_func(y_predict.view(-1), labels)\n",
    "            loss = loss_func(y_predict, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss = loss.item()\n",
    "            #loss, predicted = self._train_single_batch(x, labels)\n",
    "\n",
    "            total += loss\n",
    "            \n",
    "            \n",
    "            \n",
    "            predicted = torch.max(y_predict.data,1)\n",
    "            #print(\"predicted:\",predicted)\n",
    "            predicted = torch.max(y_predict.data,1)[1]\n",
    "            \n",
    "            \n",
    "            labels=torch.max(labels,1)\n",
    "            #print(\"labels:\",labels)\n",
    "            labels=labels[1]\n",
    "            \n",
    "            correct += (predicted == labels).sum()\n",
    "            #print(\"correct:\",correct)\n",
    "            #correct=correct[0]\n",
    "            #print(\"new_correct:\",float(correct))\n",
    "            correct=float(correct)   \n",
    "            #if batch_idx % 10 == 0:\n",
    "            print(\"batch_idx:\",batch_idx)\n",
    "            print(correct/(BATCH_SIZE*(batch_idx+1)))\n",
    "            \n",
    "            \"\"\"\n",
    "            #y_predict.detach().numpy()\n",
    "            pred = y_predict\n",
    "            print(\"pred:\",pred.shape)\n",
    "            y=labels.clone().detach().requires_grad_(True)\n",
    "            print(\"y:\",y.shape)\n",
    "            #y=labels.data.cpu().numpy()\n",
    "            #y = labels.detach().numpy()\n",
    "            roc_auc_score(y, pred)\n",
    "            \"\"\"\n",
    "           \n",
    "            # print('[Training Epoch: {}] Batch: {}, Loss: {}'.format(epoch_id, batch_id, loss))\n",
    "        print(\"Training Epoch: %d, total loss: %f\" % (epoch_id, total))\n",
    "        #print(\"auc:\",roc_auc_score)\n",
    "    #功能：保存训练完的网络的各层参数（即weights和bias)\n",
    "    path='dataset/xiaoguan/RF/RF_for_train/train_class_9/model/gene_4000_NFM.pkl'\n",
    "    torch.save(nfm.state_dict(),path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ceeac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f414fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2acfa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#以下两个cell为训练内容，没使用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb9629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "#from utils import \n",
    "\n",
    "#from new_dataset_processed import FMData\n",
    "from dataset_process import FMData\n",
    "\n",
    "#train_dataset = FMData(config.train_libfm,config.train_label,nfm_config['n_class'])\n",
    "train_dataset = FMData(nfm_config['train_data'],nfm_config['train_label'],nfm_config['n_class'])\n",
    "train_loader = data.DataLoader(train_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "#validate_dataset = FMData(config.valid_libfm,config.valid_label)\n",
    "#validate_loader = data.DataLoader(train_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "#test_dataset = FMData(config.test_libfm,config.test_label)\n",
    "#test_loader = data.DataLoader(test_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "if __name__ == \"__main__\":\n",
    "    ####################################################################################\n",
    "    # NFM 模型\n",
    "    ####################################################################################\n",
    "    BATCH_SIZE=100\n",
    "    \"\"\"\n",
    "    training_data, training_label, dense_features_col, sparse_features_col = getTrainData(nfm_config['train_file'], nfm_config['fea_file'])\n",
    "    train_dataset = Data.TensorDataset(torch.tensor(training_data).float(), torch.tensor(training_label).float())\n",
    "\n",
    "    test_data = getTestData(nfm_config['test_file'])\n",
    "    test_dataset = Data.TensorDataset(torch.tensor(test_data).float())\n",
    "    \n",
    "    test_data = getTestData(nfm_config['test_file'])\n",
    "    test_dataset = Data.TensorDataset(torch.tensor(test_data).float())\n",
    "\n",
    "    \"\"\"\n",
    "    #device = torch.device('cuda:0')\n",
    "    #epoch=0\n",
    "    \n",
    "    #model=nn.Linear(10149,16).to(device)\n",
    "    #model=nn.Linear(10149,16)\n",
    "    #model=nn.ReLU(nn.Linear(10149,16))#RuntimeError: all elements of input should be between 0 and 1\n",
    "    #print('model:',model)\n",
    "    nfm = NFM(nfm_config).cuda()#加了device防止出现GPU CPU两种设备的错误提示\n",
    "    print(\"nfm:\",nfm)\n",
    "    #print(nfm)\n",
    "    #nfm.train()\n",
    "    #u=nfm.parameters()\n",
    "    #print(u)\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    optimizer = torch.optim.Adam(nfm.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    total = 0\n",
    "    #loss_func = torch.nn.BCELoss()\n",
    "    loss_func=torch.nn.CrossEntropyLoss()\n",
    "    #loss_func=nn.MultiLabelSoftMarginLoss()\n",
    "    #loss_func=torch.nn.LogSoftmax()\n",
    "    \n",
    "    #model=nn.Softmax(nn.Linear(10149,16)).to(device)\n",
    "    # 从DataLoader中获取小批量的id以及数据\n",
    "    for epoch_id in range(1000):\n",
    "        correct=0\n",
    "        total=0\n",
    "        for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "            x = Variable(x)\n",
    "            labels = Variable(labels)\n",
    "            \n",
    "            \n",
    "            #x = torch.tensor(x, dtype=torch.float)\n",
    "            #x=x.clone().detach().requires_grad_(True)\n",
    "            x=torch.tensor(x,dtype=torch.float)\n",
    "            labels=torch.tensor(labels,dtype=torch.float)\n",
    "            x, labels = x.cuda(), labels.cuda()\n",
    "            \n",
    "            #print(\"labels:\",labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_predict = nfm(x)\n",
    "            #print(\"y_predict:\",y_predict)\n",
    "            #loss = loss_func(y_predict.view(-1), labels)\n",
    "            loss = loss_func(y_predict, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss = loss.item()\n",
    "            #loss, predicted = self._train_single_batch(x, labels)\n",
    "\n",
    "            total += loss\n",
    "            \n",
    "            \n",
    "            \n",
    "            predicted = torch.max(y_predict.data,1)\n",
    "            #print(\"predicted:\",predicted)\n",
    "            predicted = torch.max(y_predict.data,1)[1]\n",
    "            \n",
    "            \n",
    "            labels=torch.max(labels,1)\n",
    "            #print(\"labels:\",labels)\n",
    "            labels=labels[1]\n",
    "            \n",
    "            correct += (predicted == labels).sum()\n",
    "            #print(\"correct:\",correct)\n",
    "            #correct=correct[0]\n",
    "            #print(\"new_correct:\",float(correct))\n",
    "            correct=float(correct)   \n",
    "            #if batch_idx % 10 == 0:\n",
    "            print(\"batch_idx:\",batch_idx)\n",
    "            print(correct/(BATCH_SIZE*(batch_idx+1)))\n",
    "            \n",
    "            \"\"\"\n",
    "            #y_predict.detach().numpy()\n",
    "            pred = y_predict\n",
    "            print(\"pred:\",pred.shape)\n",
    "            y=labels.clone().detach().requires_grad_(True)\n",
    "            print(\"y:\",y.shape)\n",
    "            #y=labels.data.cpu().numpy()\n",
    "            #y = labels.detach().numpy()\n",
    "            roc_auc_score(y, pred)\n",
    "            \"\"\"\n",
    "           \n",
    "            # print('[Training Epoch: {}] Batch: {}, Loss: {}'.format(epoch_id, batch_id, loss))\n",
    "        print(\"Training Epoch: %d, total loss: %f\" % (epoch_id, total))\n",
    "        #print(\"auc:\",roc_auc_score)\n",
    "    #功能：保存训练完的网络的各层参数（即weights和bias)\n",
    "    path='dataset/xiaoguan/RF/RF_for_train/train_class_9/model/gene_4000_NFM.pkl'\n",
    "    torch.save(nfm.state_dict(),path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a27aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68159bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c45ab127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import Trainer\n",
    "from network import NFM\n",
    "import torch.utils.data as Data\n",
    "from Utils.criteo_loader import getTestData, getTrainData\n",
    "\n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':9,\n",
    "    'linear_hidden1':1000,\n",
    "    'linear_hidden':100,#线性模型输出层（隐层个数）\n",
    "    'embed_input_dim':1001,#embed输入维度\n",
    "    'embed_dim': 100, # 用于控制稀疏特征经过Embedding层后的稠密特征大小，embed输出维度\n",
    "    #'dnn_hidden_units': [100,11],#MLP隐层和输出层\n",
    "    \n",
    "    'dnn_hidden_units':[100,9],#MLP隐层\n",
    "    'num_sparse_features_cols':4225,#the number of the gene columns\n",
    "    'num_dense_features': 0,#dense features number\n",
    "    'bi_dropout': 0.5,#Bi-Interaction 的dropout\n",
    "    'num_epoch': 500,#训练epoch次数\n",
    "    'batch_size': 24,#batch_size\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    \n",
    "    #'train_file': '../Data/criteo/processed_data/train_set.csv',\n",
    "    #'fea_file': '../Data/criteo/processed_data/fea_col.npy',\n",
    "    #'validate_file': '../Data/criteo/processed_data/val_set.csv',\n",
    "    #'test_file': '../Data/criteo/processed_data/test_set.csv',\n",
    "    #'model_name': '../TrainedModels/NFM.model'\n",
    "    #'train_file':'data/xiaoqiu_gene_5000/train/final_5000_encode_100x.csv',\n",
    "    'train_data':'dataset/xiaoguan/RF/RF_for_train/train_class_9/train/train_encode_data.csv',\n",
    "    'train_label':'dataset/xiaoguan/RF/RF_for_train/train_class_9/train/train_label.csv',\n",
    "    'test_data':'dataset/xiaoguan/RF/RF_for_train/train_class_9/test/test_encode_data.csv',\n",
    "    'test_label':'dataset/xiaoguan/RF/RF_for_train/train_class_9/test/test_label.csv',\n",
    "    #'title':'dataset/xiaoguan/RF/RF_for_train/train_class_9/test/test_data.csv',\n",
    "    \n",
    "    #'all':''\n",
    "    #'title':'data/xiaoqiu_gene_5000/train/gene_5000_gene_name.csv',\n",
    "    #'all':'data/xiaoqiu_gene_5000/train/gene_5000_label_name.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3c872381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NFM(\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (linear_model1): Linear(in_features=4225, out_features=1000, bias=True)\n",
      "  (BN_linear1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear_model2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (BN_linear2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (embedding_layers): Embedding(1001, 100)\n",
      "  (bi_pooling): BiInteractionPooling()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (BN_bi): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dnn_layers): ModuleList(\n",
      "    (0): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (1): Linear(in_features=100, out_features=9, bias=True)\n",
      "  )\n",
      "  (dnn_softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "##测试集\n",
    "from new_nfm_network import NFM\n",
    "#加载模型\n",
    "path='dataset/xiaoguan/RF/RF_for_train/train_class_9/model/2_gene_4000_NFM.pkl'\n",
    "nfm = NFM(nfm_config).cuda()\n",
    "#net=model.cuda()\n",
    "net=nfm\n",
    "print(net)\n",
    "#net = nn.DataParallel(net)\n",
    "#net = net.to(device)\n",
    "net.load_state_dict(torch.load(path),strict=False)\n",
    "net.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "total = 0\n",
    "#loss_func = torch.nn.BCELoss()\n",
    "loss_func=torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa76357",
   "metadata": {},
   "outputs": [],
   "source": [
    "##测试集\n",
    "#from new_nfm_network import NFM\n",
    "#加载模型\n",
    "path='dataset/xiaoguan/RF/RF_for_train/train_class_9/model/gene_4000_MLP.pkl'\n",
    "#nfm = NFM(nfm_config).cuda()\n",
    "net=model.cuda()\n",
    "#net=nfm\n",
    "print(net)\n",
    "#net = nn.DataParallel(net)\n",
    "#net = net.to(device)\n",
    "net.load_state_dict(torch.load(path),strict=False)\n",
    "net.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "total = 0\n",
    "#loss_func = torch.nn.BCELoss()\n",
    "loss_func=torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9881fc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row: 0 label: [0]\n",
      "row: 1 label: [0]\n",
      "row: 2 label: [0]\n",
      "row: 3 label: [0]\n",
      "row: 4 label: [0]\n",
      "row: 5 label: [0]\n",
      "row: 6 label: [0]\n",
      "row: 7 label: [0]\n",
      "row: 8 label: [0]\n",
      "row: 9 label: [0]\n",
      "row: 10 label: [0]\n",
      "row: 11 label: [0]\n",
      "row: 12 label: [1]\n",
      "row: 13 label: [1]\n",
      "row: 14 label: [1]\n",
      "row: 15 label: [1]\n",
      "row: 16 label: [1]\n",
      "row: 17 label: [1]\n",
      "row: 18 label: [1]\n",
      "row: 19 label: [1]\n",
      "row: 20 label: [1]\n",
      "row: 21 label: [1]\n",
      "row: 22 label: [1]\n",
      "row: 23 label: [1]\n",
      "row: 24 label: [1]\n",
      "row: 25 label: [2]\n",
      "row: 26 label: [2]\n",
      "row: 27 label: [2]\n",
      "row: 28 label: [3]\n",
      "row: 29 label: [3]\n",
      "row: 30 label: [3]\n",
      "row: 31 label: [4]\n",
      "row: 32 label: [4]\n",
      "row: 33 label: [5]\n",
      "row: 34 label: [5]\n",
      "row: 35 label: [6]\n",
      "row: 36 label: [6]\n",
      "row: 37 label: [6]\n",
      "row: 38 label: [6]\n",
      "row: 39 label: [6]\n",
      "row: 40 label: [6]\n",
      "row: 41 label: [6]\n",
      "row: 42 label: [6]\n",
      "row: 43 label: [6]\n",
      "row: 44 label: [6]\n",
      "row: 45 label: [6]\n",
      "row: 46 label: [6]\n",
      "row: 47 label: [6]\n",
      "row: 48 label: [6]\n",
      "row: 49 label: [6]\n",
      "row: 50 label: [6]\n",
      "row: 51 label: [6]\n",
      "row: 52 label: [6]\n",
      "row: 53 label: [6]\n",
      "row: 54 label: [6]\n",
      "row: 55 label: [6]\n",
      "row: 56 label: [6]\n",
      "row: 57 label: [6]\n",
      "row: 58 label: [6]\n",
      "row: 59 label: [6]\n",
      "row: 60 label: [7]\n",
      "row: 61 label: [7]\n",
      "row: 62 label: [7]\n",
      "row: 63 label: [7]\n",
      "row: 64 label: [8]\n",
      "row: 65 label: [8]\n",
      "row: 66 label: [8]\n",
      "row: 67 label: [8]\n",
      "row: 68 label: [8]\n",
      "row: 69 label: [8]\n",
      "row: 70 label: [8]\n",
      "row: 71 label: [8]\n",
      "row: 72 label: [8]\n",
      "row: 73 label: [8]\n",
      "row: 74 label: [8]\n",
      "row: 75 label: [8]\n",
      "row: 76 label: [8]\n",
      "label: [[0.82222223 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.82222223 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.82222223 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.82222223 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.82222223 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.82222223 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.82222223 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.82222223 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.82222223 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.82222223\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.82222223\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.82222223 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.82222223 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.82222223 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.82222223 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]]\n",
      "features: [[ 0 33 48 ... 29 24 27]\n",
      " [ 1 34 42 ... 26 25 27]\n",
      " [ 2 34 42 ... 30 25 26]\n",
      " ...\n",
      " [75 49 43 ...  8 29 30]\n",
      " [76 45 39 ... 13 28 34]\n",
      " [77 45 47 ...  9 29 37]]\n"
     ]
    }
   ],
   "source": [
    "#数据集准备测试\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "from dataset_process import FMData\n",
    "\n",
    "'''  \n",
    "test_dataset = FMData(nfm_config['test_data'],nfm_config['test_label'],nfm_config['n_class'])\n",
    "test_loader = data.DataLoader(test_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "'''\n",
    "test_dataset,test_loader=prepare_dataset(nfm_config['test_data'],nfm_config['test_label'],nfm_config['batch_size'],nfm_config['n_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "38599db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#版权声明：本文为CSDN博主「山阴少年」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。  \n",
    "#原文链接：https://blog.csdn.net/jclian91/article/details/121708431#   \n",
    "from torch.autograd import Variable   \n",
    "from torch.utils.data import DataLoader   \n",
    "from sklearn.metrics import roc_auc_score   \n",
    "from sklearn.metrics import accuracy_score   \n",
    "        \n",
    "        \n",
    "def evaluate_model(test_dl , model):   \n",
    "    predictions,  actuals = [] , []   \n",
    "    for i , (inputs , targets) in enumerate(test_dl):   \n",
    "        # evaluate the model on the test set   \n",
    "        #print(\\ inputs:\\  inputs)   \n",
    "        #print(\\ targets:\\  targets)   \n",
    "        inputs = Variable(inputs)   \n",
    "        targets = Variable(targets)   \n",
    "                    \n",
    "                    \n",
    "        #x = torch.tensor(x  dtype=torch.float)   \n",
    "        #x=x.clone().detach().requires_grad_(True)   \n",
    "        inputs=torch.tensor(inputs ,dtype=torch.float)   \n",
    "        targets=torch.tensor(targets ,dtype=torch.float)   \n",
    "        inputs , targets = inputs.cuda(),  targets.cuda()   \n",
    "        yhat = model(inputs)   \n",
    "        # retrieve numpy array   \n",
    "        #yhat = yhat.detach().numpy()   \n",
    "        yhat = yhat.detach().cpu().numpy()#转换到cpu   \n",
    "        # yhat=yhat.argmax(axis=1)   \n",
    "        #print(' yhat:',  yhat)   \n",
    "        #print('yhat.shape:' ,yhat.shape)   \n",
    "        actual = targets.detach().cpu().numpy()   \n",
    "        actual=actual.round()   \n",
    "        #print('actual:',  actual)   \n",
    "        #print('actual.shape:', actual.shape)   \n",
    "        #actual = actual.reshape(-1  1)   \n",
    "        # round to class values   \n",
    "        yhat = yhat.round()   \n",
    "        # store   \n",
    "        predictions.append(yhat)   \n",
    "        actuals.append(actual)   \n",
    "    #print('prediction:',  predictions)   \n",
    "    #print('actuals:',  actuals)   \n",
    "    predictions , actuals = np.vstack(predictions) , np.vstack(actuals)   \n",
    "    #print('prediction:',  predictions)   \n",
    "    #print('actuals:',  actuals)   \n",
    "    # calculate accuracy   \n",
    "    acc_test = accuracy_score(actuals , predictions)   \n",
    "    return  actuals , predictions ,acc_test \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d9062266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_test 0.4444444444444444\n"
     ]
    }
   ],
   "source": [
    "actuals,predictions,acc_test=evaluate_model(test_loader,net)\n",
    "print(\"acc_test\",acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02c29968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-cd760d46fe5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#print(prec)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#print(rec)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' AUC: %.4f Prec: %.4f Rec: %.4f F1: %.4f'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "#版权声明：本文为CSDN博主「颹蕭蕭」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。  ,\n",
    "#原文链接：https://blog.csdn.net/itnerd/article/details/102105662  ,\n",
    "       \n",
    "import numpy as np  \n",
    "from sklearn.metrics import precision_recall_fscore_support  \n",
    "from sklearn.metrics import roc_auc_score  \n",
    "from sklearn.metrics import precision_recall_curve  \n",
    "from numpy.random import random  \n",
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline  \n",
    "       \n",
    "       \n",
    "prec, rec, f1, _ = precision_recall_fscore_support(actuals, predictions, average=None)  \n",
    "auc = roc_auc_score(actuals, predictions)  \n",
    "#print(prec)  \n",
    "#print(rec)  \n",
    "print(' AUC: %.4f Prec: %.4f Rec: %.4f F1: %.4f' %(auc, prec, rec, f1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7315d5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
