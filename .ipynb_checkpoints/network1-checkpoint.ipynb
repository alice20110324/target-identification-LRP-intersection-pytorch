{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5d0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load network.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from BaseModel.basemodel import BaseModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BiInteractionPooling(nn.Module):\n",
    "    \"\"\"Bi-Interaction Layer used in Neural FM,compress the\n",
    "      pairwise element-wise product of features into one single vector.\n",
    "      Input shape\n",
    "        - A 3D tensor with shape:``(batch_size,field_size,embedding_size)``.\n",
    "      Output shape\n",
    "    http://127.0.0.1:3000/notebooks/NFM-pyorch-master/NFM-pyorch-master/%E6%9C%AA%E5%91%BD%E5%90%8D5.ipynb?kernel_name=python3#    - 3D tensor with shape: ``(batch_size,1,embedding_size)``.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(BiInteractionPooling, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        concated_embeds_value = inputs\n",
    "        square_of_sum = torch.pow(\n",
    "            torch.sum(concated_embeds_value, dim=1, keepdim=True), 2)\n",
    "        sum_of_square = torch.sum(\n",
    "            concated_embeds_value * concated_embeds_value, dim=1, keepdim=True)\n",
    "        cross_term = 0.5 * (square_of_sum - sum_of_square)\n",
    "        return cross_term\n",
    "\n",
    "class NFM(BaseModel):\n",
    "    #def __init__(self, config, dense_features_cols=[], sparse_features_cols,n_class):#=[]为新增\n",
    "    #def __init__(self, config, dense_features_cols, sparse_features_cols):\n",
    "    def __init__(self, config, sparse_features_cols,n_class,dense_features_cols=[]):#=[]为新增\n",
    "        super(NFM, self).__init__(config)\n",
    "        # 稠密和稀疏特征的数量\n",
    "        #self.num_dense_feature = dense_features_cols.__len__()\n",
    "        self.num_dense_feature = 0#修改\n",
    "        self.num_sparse_feature = sparse_features_cols.__len__()\n",
    "        #self.num_sparse_feature = 0##修改\n",
    "        # NFM的线性部分，对应 ∑WiXi\n",
    "        #self.linear_model = nn.Linear(self.num_dense_feature + self.num_sparse_feature, 1)\n",
    "        self.linear_model = nn.Linear(self.num_dense_feature + self.num_sparse_feature, n_class)##修改\n",
    "        # NFM的Embedding层\n",
    "        self.embedding_layers = nn.ModuleList([\n",
    "            nn.Embedding(num_embeddings=feat_dim, embedding_dim=config['embed_dim'])\n",
    "                for feat_dim in sparse_features_cols#将每一行中的每一列中的数据进行嵌入化\n",
    "        ])\n",
    "\n",
    "        # B-Interaction 层\n",
    "        self.bi_pooling = BiInteractionPooling()\n",
    "        self.bi_dropout = config['bi_dropout']\n",
    "        if self.bi_dropout > 0:\n",
    "            self.dropout = nn.Dropout(self.bi_dropout)\n",
    "\n",
    "        # NFM的DNN部分\n",
    "        self.hidden_layers = [self.num_dense_feature + config['embed_dim']] + config['dnn_hidden_units']#是加还是乘\n",
    "        self.dnn_layers = nn.ModuleList([\n",
    "            nn.Linear(in_features=layer[0], out_features=layer[1])\\\n",
    "                for layer in list(zip(self.hidden_layers[:-1], self.hidden_layers[1:]))\n",
    "        ])\n",
    "        #self.dnn_linear = nn.Linear(self.hidden_layers[-1], 1, bias=False)\n",
    "        self.dnn_linear = nn.Linear(self.hidden_layers[-1], n_class, bias=False)\n",
    "        \n",
    "        #增加\n",
    "        self.dnn_softmax=F.softmax(input,dim=0) # 按列SoftMax,列和为1\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 先区分出稀疏特征和稠密特征，这里是按照列来划分的，即所有的行都要进行筛选\n",
    "        dense_input, sparse_inputs = x[:, :self.num_dense_feature], x[:, self.num_dense_feature:]\n",
    "        sparse_inputs = sparse_inputs.long()\n",
    "\n",
    "        # 求出线性部分\n",
    "        linear_output = self.linear_model(x)\n",
    "\n",
    "        # 求出稀疏特征的embedding向量\n",
    "        sparse_embeds = [self.embedding_layers[i](sparse_inputs[:, i]) for i in range(sparse_inputs.shape[1])]\n",
    "        sparse_embeds = torch.cat(sparse_embeds, axis=-1)\n",
    "\n",
    "        # 送入B-Interaction层\n",
    "        fm_input = sparse_embeds.view(-1, self.num_sparse_feature, self._config['embed_dim'])#整理成n行m列\n",
    "        # print(fm_input)\n",
    "        # print(fm_input.shape)\n",
    "\n",
    "        bi_out = self.bi_pooling(fm_input)\n",
    "        if self.bi_dropout:\n",
    "            bi_out = self.dropout(bi_out)\n",
    "\n",
    "        bi_out = bi_out.view(-1, self._config['embed_dim'])\n",
    "        # 将结果聚合起来\n",
    "        dnn_input = torch.cat((dense_input, bi_out), dim=-1)\n",
    "\n",
    "        # DNN 层\n",
    "        dnn_output = dnn_input\n",
    "        for dnn in self.dnn_layers:\n",
    "            dnn_output = dnn(dnn_output)#dnn_output为tensor\n",
    "            # dnn_output = nn.BatchNormalize(dnn_output)\n",
    "            dnn_output = torch.relu(dnn_output)\n",
    "        dnn_output = self.dnn_linear(dnn_output)\n",
    "        dnn_pred=self.dnn_softmax(dnn_output,dim=0)#增加\n",
    "        # Final\n",
    "        output = linear_output + dnn_output#修改\n",
    "        y_pred = self.dnn_softmax(output,dim=0)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3f1ef07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_layers: [10499, 128, 128]\n",
      "[(10499, 128), (128, 128)]\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=10499, out_features=128, bias=True)\n",
      "  (1): Linear(in_features=128, out_features=128, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#from BaseModel.basemodel import BaseModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "hidden_layers = [10499] + [128,128]\n",
    "print('hidden_layers:',hidden_layers)\n",
    "print(list(zip(hidden_layers[:-1], hidden_layers[1:])))\n",
    "dnn_layers = nn.ModuleList([\n",
    "            nn.Linear(in_features=layer[0], out_features=layer[1])\\\n",
    "                for layer in list(zip(hidden_layers[:-1], hidden_layers[1:]))\n",
    "        ])\n",
    "\n",
    "print(dnn_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c29915a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6365afdca4c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdnn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdnn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdnn_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mdnn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;31m# dnn_output = nn.BatchNormalize(dnn_output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdnn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not int"
     ]
    }
   ],
   "source": [
    "dnn_output = 10000\n",
    "for dnn in dnn_layers:\n",
    "        dnn_output = dnn(dnn_output)\n",
    "        # dnn_output = nn.BatchNormalize(dnn_output)\n",
    "        dnn_output = torch.relu(dnn_output)\n",
    "dnn_output = dnn_linear(dnn_output)\n",
    "dnn_pred=dnn_softmax(dnn_output,dim=0)#增加\n",
    "# Final\n",
    "output = linear_output + dnn_output#修改\n",
    "y_pred = dnn_softmax(output,dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e490172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
