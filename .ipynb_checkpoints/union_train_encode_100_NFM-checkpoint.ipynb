{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8098821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embdding length=100  inputs length=1001 ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e81b4dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import Trainer\n",
    "from network import NFM\n",
    "import torch.utils.data as Data\n",
    "from Utils.criteo_loader import getTestData, getTrainData\n",
    "\n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':9,\n",
    "    'linear_hidden1':1000,\n",
    "    'linear_hidden':100,#线性模型输出层（隐层个数）\n",
    "    'embed_input_dim':1001,#embed输入维度\n",
    "    'embed_dim': 100, # 用于控制稀疏特征经过Embedding层后的稠密特征大小，embed输出维度\n",
    "    #'dnn_hidden_units': [100,11],#MLP隐层和输出层\n",
    "    \n",
    "    'dnn_hidden_units':[9],#MLP隐层\n",
    "    'num_sparse_features_cols':10477,#the number of the gene columns\n",
    "    'num_dense_features': 0,#dense features number\n",
    "    'bi_dropout': 0.3,#Bi-Interaction 的dropout\n",
    "    'num_epoch': 500,#训练epoch次数\n",
    "    'batch_size': 24,#batch_size\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    'epoch':1000,\n",
    "    \n",
    "    #'train_file': '../Data/criteo/processed_data/train_set.csv',\n",
    "    #'fea_file': '../Data/criteo/processed_data/fea_col.npy',\n",
    "    #'validate_file': '../Data/criteo/processed_data/val_set.csv',\n",
    "    #'test_file': '../Data/criteo/processed_data/test_set.csv',\n",
    "    #'model_name': '../TrainedModels/NFM.model'\n",
    "    #'train_file':'data/xiaoqiu_gene_5000/train/final_5000_encode_100x.csv',\n",
    "    'train_data':'dataset/qiuguan/non_code/train/train_encode_data_all.csv',\n",
    "    'train_label':'dataset/qiuguan/non_code/train/train_label.csv',\n",
    "    'test_data':'dataset/qiuguan/non_code/test/test_encode_data.csv',\n",
    "    'test_label':'dataset/qiuguan/non_code/test/test_labels.csv',\n",
    "    #'title':'dataset/xiaoguan/RF/RF_for_train/train_class_9/test/test_data.csv',\n",
    "    \n",
    "    #'all':''\n",
    "    #'title':'data/xiaoqiu_gene_5000/train/gene_5000_gene_name.csv',\n",
    "    #'all':'data/xiaoqiu_gene_5000/train/gene_5000_label_name.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aa64108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#准备训练集\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "#from utils import \n",
    "#准备训练集\n",
    "#from new_dataset_processed import FMData\n",
    "from dataset_process import FMData\n",
    "def prepare_dataset(m_data,m_label,batch_size,n_class):\n",
    "    m_dataset=FMData(m_data,m_label,n_class)\n",
    "    m_dataloader=data.DataLoader(m_dataset, drop_last=True,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "    \n",
    "    return m_dataset,m_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f284d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import sys \n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "import torchmetrics\n",
    "def   train_data(model,train_loader,test_loader,batch_size,model_path):\n",
    "    #train_accuracy=torchmetrics.Accuracy()\n",
    "    #test_accuracy=torchmetrics.Accuracy()\n",
    "    BATCH_SIZE=batch_size\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    total = 0\n",
    "    \n",
    "    #loss_func = torch.nn.BCELoss()\n",
    "    loss_func=torch.nn.CrossEntropyLoss()\n",
    "    #loss_func=nn.MultiLabelSoftMarginLoss()\n",
    "    #loss_func=torch.nn.LogSoftmax()\n",
    "    num=1\n",
    "    #model=nn.Softmax(nn.Linear(10149,16)).to(device)\n",
    "    # 从DataLoader中获取小批量的id以及数据\n",
    "    for epoch_id in range(1000):\n",
    "        correct=0\n",
    "        total=0\n",
    "        total_test_acc=0\n",
    "        \n",
    "        for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "            x = Variable(x)\n",
    "            labels = Variable(labels)\n",
    "            \n",
    "            \n",
    "            #x = torch.tensor(x, dtype=torch.float)\n",
    "            #x=x.clone().detach().requires_grad_(True)\n",
    "            x=torch.tensor(x,dtype=torch.float)\n",
    "            labels=torch.tensor(labels,dtype=torch.float)\n",
    "            x, labels = x.cuda(), labels.cuda()\n",
    "            labels_int=labels=torch.max(labels,1)[1]\n",
    "            #labels_int.cuda()\n",
    "            #print(\"labels:\",labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_predict = model(x)\n",
    "            #print(\"y_predict:\",y_predict)\n",
    "            #loss = loss_func(y_predict.view(-1), labels)\n",
    "            loss = loss_func(y_predict, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss = loss.item()\n",
    "            #loss, predicted = self._train_single_batch(x, labels)\n",
    "\n",
    "            total += loss\n",
    "            \n",
    "            \n",
    "            #predicted = torch.max(y_predict.data,1)\n",
    "            #print(\"predicted:\",predicted)\n",
    "            #predicted = torch.max(y_predict.data,1)[1]\n",
    "            #predicted=predicted.detach().cpu().numpy()\n",
    "            \n",
    "            #labels=torch.max(labels,1)\n",
    "            #print(\"labels:\",labels)\n",
    "            #labels=labels[1]\n",
    "            #y_predict=y_predict.cuda()\n",
    "            #labels=torch.max(labels,1)[1].cuda()\n",
    "            #labels=labels.detach().cpu().numpy()\n",
    "            #correct += (predicted == labels).sum()\n",
    "            #print(\"correct:\",correct)\n",
    "            #correct=correct[0]\n",
    "            #print(\"new_correct:\",float(correct))\n",
    "            #correct=float(correct)   \n",
    "            #if batch_idx % 10 == 0:\n",
    "            #print(\"batch_idx:\",batch_idx)\n",
    "            #print(correct/(BATCH_SIZE*(batch_idx+1)))\n",
    "            batch_train_acc=torchmetrics.functional.accuracy(y_predict,labels_int)\n",
    "            #print('batch_train_acc:',batch_train_acc)\n",
    "        #total_train_accuracy=torchmetrics.functional.compute_details()\n",
    "        #print('total_train_accuracy:',total_train_accuracy)\n",
    "        for i , (inputs , targets) in enumerate(test_loader):   \n",
    "            # evaluate the model on the test set   \n",
    "            #print(\\ inputs:\\  inputs)   \n",
    "            #print(\\ targets:\\  targets)   \n",
    "            inputs = Variable(inputs)   \n",
    "            targets = Variable(targets)     \n",
    "            #x = torch.tensor(x  dtype=torch.float)   \n",
    "            #x=x.clone().detach().requires_grad_(True)   \n",
    "            inputs=torch.tensor(inputs ,dtype=torch.float)   \n",
    "            targets=torch.tensor(targets ,dtype=torch.float)   \n",
    "            inputs , targets = inputs.cuda(),  targets.cuda()   \n",
    "            yhat = model(inputs)  \n",
    "            \n",
    "            #yhat = torch.max(yhat.data,1)[1]\n",
    "            #yhat=yhat.detach().cpu().numpy()\n",
    "            #print(\"predicted:\",predicted)\n",
    "            #predicted = torch.max(y_predict.data,1)[1]\n",
    "             #predicted = torch.max(y_predict.data,1)[1]\n",
    "            \n",
    "            \n",
    "            targets=torch.max(targets,1)[1]\n",
    "            #print(\"labels:\",labels)\n",
    "            #labels=labels[1]\n",
    "            #targets=targets.detach().cpu().numpy()\n",
    "            \n",
    "            \n",
    "            \n",
    "            batch_test_acc=torchmetrics.functional.accuracy(yhat,targets)\n",
    "            #print(\"batch_test_acc:\",batch_test_acc)\n",
    "            total_test_acc+=batch_test_acc\n",
    "            #total_test_accuracy=torchmetrics.functional.compute_details()\n",
    "        print('total_test_accuracy:',total_test_acc/(i+1))\n",
    "        \n",
    "                    \n",
    "                    \n",
    "            \n",
    "            #model.evaluate()\n",
    "            #model.eval()\n",
    "            #train_result = evaluate.metrics(model, train_loader)\n",
    "            #valid_result = evaluate.metrics(model, valid_loader)\n",
    "            #est_result = evaluate.metrics(model, test_loader)\n",
    "            #acturals,predictions,acc_test=evaluate_model(test_loader,model)\n",
    "            #print(\"acc_test:  %d  \" %(acc_test))\n",
    "            #print(\"Train_RMSE: {:.3f}, Valid_RMSE: {:.3f}, Test_RMSE: {:.3f}\".format(\n",
    "            #train_result, valid_result, test_result))\n",
    "            # print('[Training Epoch: {}] Batch: {}, Loss: {}'.format(epoch_id, batch_id, loss))\n",
    "        print(\"Training Epoch: %d, total loss: %f\" % (epoch_id, total))\n",
    "        #print(\"auc:\",roc_auc_score)\n",
    "    #功能：保存训练完的网络的各层参数（即weights和bias)\n",
    "    #path='dataset/xiaoguan/RF/RF_for_train/train_class_9/model/gene_4000_NFM.pkl'\n",
    "        if epoch_id %100==0:\n",
    "            num=num+1\n",
    "            path=os.path.join(model_path,'NMF'+str(num)+'.pkl')\n",
    "            torch.save(model.state_dict(),path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bdf605b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMData\n",
      "FMData\n",
      "NFM: NFM(\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (BN_num): BatchNorm1d(10477, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear_model1): Linear(in_features=10477, out_features=1000, bias=True)\n",
      "  (BN_linear1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear_model2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (BN_linear2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (embedding_layers): Embedding(1001, 100)\n",
      "  (bi_pooling): BiInteractionPooling()\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (BN_bi): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dnn_layers): ModuleList(\n",
      "    (0): Linear(in_features=200, out_features=9, bias=True)\n",
      "  )\n",
      "  (dnn_softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_test_accuracy: tensor(0.2083, device='cuda:0')\n",
      "Training Epoch: 0, total loss: 57.016872\n",
      "total_test_accuracy: tensor(0.2083, device='cuda:0')\n",
      "Training Epoch: 1, total loss: 56.603726\n",
      "total_test_accuracy: tensor(0.2500, device='cuda:0')\n",
      "Training Epoch: 2, total loss: 55.920854\n",
      "total_test_accuracy: tensor(0.1667, device='cuda:0')\n",
      "Training Epoch: 3, total loss: 55.100502\n",
      "total_test_accuracy: tensor(0.3750, device='cuda:0')\n",
      "Training Epoch: 4, total loss: 54.447036\n",
      "total_test_accuracy: tensor(0.3750, device='cuda:0')\n",
      "Training Epoch: 5, total loss: 53.556444\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 6, total loss: 52.623930\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 7, total loss: 52.066556\n",
      "total_test_accuracy: tensor(0.4583, device='cuda:0')\n",
      "Training Epoch: 8, total loss: 51.795824\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 9, total loss: 50.853053\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 10, total loss: 50.929974\n",
      "total_test_accuracy: tensor(0.7500, device='cuda:0')\n",
      "Training Epoch: 11, total loss: 50.260877\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 12, total loss: 49.522741\n",
      "total_test_accuracy: tensor(0.7083, device='cuda:0')\n",
      "Training Epoch: 13, total loss: 49.291926\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 14, total loss: 48.966498\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 15, total loss: 48.843394\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 16, total loss: 48.255867\n",
      "total_test_accuracy: tensor(0.6667, device='cuda:0')\n",
      "Training Epoch: 17, total loss: 48.204058\n",
      "total_test_accuracy: tensor(0.6667, device='cuda:0')\n",
      "Training Epoch: 18, total loss: 47.552791\n",
      "total_test_accuracy: tensor(0.6667, device='cuda:0')\n",
      "Training Epoch: 19, total loss: 47.297247\n",
      "total_test_accuracy: tensor(0.6667, device='cuda:0')\n",
      "Training Epoch: 20, total loss: 46.955099\n",
      "total_test_accuracy: tensor(0.7917, device='cuda:0')\n",
      "Training Epoch: 21, total loss: 47.486253\n",
      "total_test_accuracy: tensor(0.7083, device='cuda:0')\n",
      "Training Epoch: 22, total loss: 46.258508\n",
      "total_test_accuracy: tensor(0.8333, device='cuda:0')\n",
      "Training Epoch: 23, total loss: 45.634992\n",
      "total_test_accuracy: tensor(0.6667, device='cuda:0')\n",
      "Training Epoch: 24, total loss: 46.086376\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 25, total loss: 45.805775\n",
      "total_test_accuracy: tensor(0.6667, device='cuda:0')\n",
      "Training Epoch: 26, total loss: 45.918845\n",
      "total_test_accuracy: tensor(0.7500, device='cuda:0')\n",
      "Training Epoch: 27, total loss: 45.381732\n",
      "total_test_accuracy: tensor(0.7083, device='cuda:0')\n",
      "Training Epoch: 28, total loss: 45.189181\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 29, total loss: 44.585038\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 30, total loss: 44.778697\n",
      "total_test_accuracy: tensor(0.7500, device='cuda:0')\n",
      "Training Epoch: 31, total loss: 43.995150\n",
      "total_test_accuracy: tensor(0.7083, device='cuda:0')\n",
      "Training Epoch: 32, total loss: 44.389842\n",
      "total_test_accuracy: tensor(0.7500, device='cuda:0')\n",
      "Training Epoch: 33, total loss: 44.419730\n",
      "total_test_accuracy: tensor(0.6667, device='cuda:0')\n",
      "Training Epoch: 34, total loss: 44.298922\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 35, total loss: 43.989488\n",
      "total_test_accuracy: tensor(0.7917, device='cuda:0')\n",
      "Training Epoch: 36, total loss: 43.282588\n",
      "total_test_accuracy: tensor(0.7917, device='cuda:0')\n",
      "Training Epoch: 37, total loss: 43.300352\n",
      "total_test_accuracy: tensor(0.7917, device='cuda:0')\n",
      "Training Epoch: 38, total loss: 42.774033\n",
      "total_test_accuracy: tensor(0.7083, device='cuda:0')\n",
      "Training Epoch: 39, total loss: 42.750815\n",
      "total_test_accuracy: tensor(0.7500, device='cuda:0')\n",
      "Training Epoch: 40, total loss: 42.362198\n",
      "total_test_accuracy: tensor(0.6667, device='cuda:0')\n",
      "Training Epoch: 41, total loss: 42.479185\n",
      "total_test_accuracy: tensor(0.7083, device='cuda:0')\n",
      "Training Epoch: 42, total loss: 43.025741\n",
      "total_test_accuracy: tensor(0.7917, device='cuda:0')\n",
      "Training Epoch: 43, total loss: 42.309845\n",
      "total_test_accuracy: tensor(0.7083, device='cuda:0')\n",
      "Training Epoch: 44, total loss: 41.906803\n",
      "total_test_accuracy: tensor(0.8333, device='cuda:0')\n",
      "Training Epoch: 45, total loss: 42.102966\n",
      "total_test_accuracy: tensor(0.8333, device='cuda:0')\n",
      "Training Epoch: 46, total loss: 41.957000\n",
      "total_test_accuracy: tensor(0.7500, device='cuda:0')\n",
      "Training Epoch: 47, total loss: 41.622064\n",
      "total_test_accuracy: tensor(0.7917, device='cuda:0')\n",
      "Training Epoch: 48, total loss: 41.763872\n",
      "total_test_accuracy: tensor(0.7500, device='cuda:0')\n",
      "Training Epoch: 49, total loss: 41.668698\n",
      "total_test_accuracy: tensor(0.8333, device='cuda:0')\n",
      "Training Epoch: 50, total loss: 41.312108\n",
      "total_test_accuracy: tensor(0.8333, device='cuda:0')\n",
      "Training Epoch: 51, total loss: 41.012746\n",
      "total_test_accuracy: tensor(0.8750, device='cuda:0')\n",
      "Training Epoch: 52, total loss: 41.236829\n",
      "total_test_accuracy: tensor(0.7500, device='cuda:0')\n",
      "Training Epoch: 53, total loss: 41.044872\n",
      "total_test_accuracy: tensor(0.7917, device='cuda:0')\n",
      "Training Epoch: 54, total loss: 40.610827\n",
      "total_test_accuracy: tensor(0.7917, device='cuda:0')\n",
      "Training Epoch: 55, total loss: 40.774195\n",
      "total_test_accuracy: tensor(0.8750, device='cuda:0')\n",
      "Training Epoch: 56, total loss: 40.558980\n",
      "total_test_accuracy: tensor(0.9583, device='cuda:0')\n",
      "Training Epoch: 57, total loss: 40.351918\n",
      "total_test_accuracy: tensor(0.7917, device='cuda:0')\n",
      "Training Epoch: 58, total loss: 39.927859\n",
      "total_test_accuracy: tensor(0.9167, device='cuda:0')\n",
      "Training Epoch: 59, total loss: 40.117484\n",
      "total_test_accuracy: tensor(0.9167, device='cuda:0')\n",
      "Training Epoch: 60, total loss: 39.993337\n",
      "total_test_accuracy: tensor(0.8750, device='cuda:0')\n",
      "Training Epoch: 61, total loss: 39.937713\n",
      "total_test_accuracy: tensor(0.9583, device='cuda:0')\n",
      "Training Epoch: 62, total loss: 39.880347\n",
      "total_test_accuracy: tensor(0.7083, device='cuda:0')\n",
      "Training Epoch: 63, total loss: 39.747995\n",
      "total_test_accuracy: tensor(0.7500, device='cuda:0')\n",
      "Training Epoch: 64, total loss: 39.572557\n",
      "total_test_accuracy: tensor(0.8333, device='cuda:0')\n",
      "Training Epoch: 65, total loss: 39.460961\n",
      "total_test_accuracy: tensor(0.7500, device='cuda:0')\n",
      "Training Epoch: 66, total loss: 39.356276\n",
      "total_test_accuracy: tensor(0.8333, device='cuda:0')\n",
      "Training Epoch: 67, total loss: 39.307634\n",
      "total_test_accuracy: tensor(0.8750, device='cuda:0')\n",
      "Training Epoch: 68, total loss: 39.189901\n",
      "total_test_accuracy: tensor(0.8750, device='cuda:0')\n",
      "Training Epoch: 69, total loss: 39.467504\n",
      "total_test_accuracy: tensor(0.8333, device='cuda:0')\n",
      "Training Epoch: 70, total loss: 39.143830\n",
      "total_test_accuracy: tensor(0.8333, device='cuda:0')\n",
      "Training Epoch: 71, total loss: 39.381986\n",
      "total_test_accuracy: tensor(0.8333, device='cuda:0')\n",
      "Training Epoch: 72, total loss: 39.260233\n",
      "total_test_accuracy: tensor(0.9167, device='cuda:0')\n",
      "Training Epoch: 73, total loss: 39.164524\n",
      "total_test_accuracy: tensor(0.9167, device='cuda:0')\n",
      "Training Epoch: 74, total loss: 38.548912\n",
      "total_test_accuracy: tensor(0.7917, device='cuda:0')\n",
      "Training Epoch: 75, total loss: 39.085675\n",
      "total_test_accuracy: tensor(0.7917, device='cuda:0')\n",
      "Training Epoch: 76, total loss: 38.564845\n",
      "total_test_accuracy: tensor(0.8750, device='cuda:0')\n",
      "Training Epoch: 77, total loss: 38.705418\n",
      "total_test_accuracy: tensor(0.9167, device='cuda:0')\n",
      "Training Epoch: 78, total loss: 38.380474\n",
      "total_test_accuracy: tensor(0.8750, device='cuda:0')\n",
      "Training Epoch: 79, total loss: 38.647129\n",
      "total_test_accuracy: tensor(0.8333, device='cuda:0')\n",
      "Training Epoch: 80, total loss: 37.863576\n",
      "total_test_accuracy: tensor(0.7917, device='cuda:0')\n",
      "Training Epoch: 81, total loss: 37.745205\n",
      "total_test_accuracy: tensor(0.8333, device='cuda:0')\n",
      "Training Epoch: 82, total loss: 38.137147\n",
      "total_test_accuracy: tensor(0.9583, device='cuda:0')\n",
      "Training Epoch: 83, total loss: 38.185256\n",
      "total_test_accuracy: tensor(0.9583, device='cuda:0')\n",
      "Training Epoch: 84, total loss: 37.733215\n",
      "total_test_accuracy: tensor(0.9583, device='cuda:0')\n",
      "Training Epoch: 85, total loss: 37.654651\n",
      "total_test_accuracy: tensor(0.9167, device='cuda:0')\n",
      "Training Epoch: 86, total loss: 37.567031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_test_accuracy: tensor(0.8750, device='cuda:0')\n",
      "Training Epoch: 87, total loss: 37.669876\n",
      "total_test_accuracy: tensor(0.8750, device='cuda:0')\n",
      "Training Epoch: 88, total loss: 37.290282\n",
      "total_test_accuracy: tensor(0.8750, device='cuda:0')\n",
      "Training Epoch: 89, total loss: 37.459032\n",
      "total_test_accuracy: tensor(0.8333, device='cuda:0')\n",
      "Training Epoch: 90, total loss: 37.273858\n",
      "total_test_accuracy: tensor(1., device='cuda:0')\n",
      "Training Epoch: 91, total loss: 37.735246\n",
      "total_test_accuracy: tensor(0.9167, device='cuda:0')\n",
      "Training Epoch: 92, total loss: 37.351963\n",
      "total_test_accuracy: tensor(0.9167, device='cuda:0')\n",
      "Training Epoch: 93, total loss: 37.262578\n",
      "total_test_accuracy: tensor(0.9583, device='cuda:0')\n",
      "Training Epoch: 94, total loss: 37.259523\n",
      "total_test_accuracy: tensor(0.8333, device='cuda:0')\n",
      "Training Epoch: 95, total loss: 37.659656\n",
      "total_test_accuracy: tensor(0.9583, device='cuda:0')\n",
      "Training Epoch: 96, total loss: 37.727070\n",
      "total_test_accuracy: tensor(0.8750, device='cuda:0')\n",
      "Training Epoch: 97, total loss: 37.427286\n",
      "total_test_accuracy: tensor(0.9167, device='cuda:0')\n",
      "Training Epoch: 98, total loss: 37.179757\n",
      "total_test_accuracy: tensor(0.9167, device='cuda:0')\n",
      "Training Epoch: 99, total loss: 37.226906\n",
      "total_test_accuracy: tensor(0.9583, device='cuda:0')\n",
      "Training Epoch: 100, total loss: 37.497412\n",
      "total_test_accuracy: tensor(0.9583, device='cuda:0')\n",
      "Training Epoch: 101, total loss: 37.342459\n",
      "total_test_accuracy: tensor(1., device='cuda:0')\n",
      "Training Epoch: 102, total loss: 37.293712\n",
      "total_test_accuracy: tensor(0.9583, device='cuda:0')\n",
      "Training Epoch: 103, total loss: 36.957352\n",
      "total_test_accuracy: tensor(0.9167, device='cuda:0')\n",
      "Training Epoch: 104, total loss: 37.193948\n",
      "total_test_accuracy: tensor(0.9583, device='cuda:0')\n",
      "Training Epoch: 105, total loss: 37.327608\n",
      "total_test_accuracy: tensor(0.8333, device='cuda:0')\n",
      "Training Epoch: 106, total loss: 37.228899\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e3ff327faa1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NFM:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnfm_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'dataset/qiuguan/model/NFM_encode_100/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-9ff3c11273ab>\u001b[0m in \u001b[0;36mtrain_data\u001b[0;34m(model, train_loader, test_loader, batch_size, model_path)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;31m#print(\"y_predict:\",y_predict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;31m#loss = loss_func(y_predict.view(-1), labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NFM-pyorch-master/NFM-pyorch-master/new_nfm_network_batch_1.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m#linear_output=self.BN_linear(linear_output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# 求出稀疏特征的embedding向量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0msparse_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0msparse_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NFM-pyorch-master/NFM-pyorch-master/new_nfm_network_batch_1.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m#linear_output=self.BN_linear(linear_output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# 求出稀疏特征的embedding向量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0msparse_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0msparse_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset,train_loader=prepare_dataset(nfm_config['train_data'],nfm_config['train_label'],nfm_config['batch_size'],nfm_config['n_class'])\n",
    "test_dataset,test_loader=prepare_dataset(nfm_config['test_data'],nfm_config['test_label'],nfm_config['batch_size'],nfm_config['n_class'])\n",
    "#from MLP import MLP\n",
    "from new_nfm_network_batch_1 import NFM\n",
    "#model=MLP(4224,1000,100,9)\n",
    "model=NFM(nfm_config)\n",
    "model.cuda()\n",
    "print(\"NFM:\",model)\n",
    "train_data(model,train_loader,test_loader,nfm_config['batch_size'],'dataset/qiuguan/model/NFM_encode_100/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b3ecae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6253c99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入维度为101_embdding  encode length=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "682e1101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import Trainer\n",
    "from network import NFM\n",
    "import torch.utils.data as Data\n",
    "from Utils.criteo_loader import getTestData, getTrainData\n",
    "\n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':9,\n",
    "    'linear_hidden1':1000,\n",
    "    'linear_hidden':100,#线性模型输出层（隐层个数）\n",
    "    'embed_input_dim':101,#embed输入维度\n",
    "    'embed_dim': 20, # 用于控制稀疏特征经过Embedding层后的稠密特征大小，embed输出维度\n",
    "    #'dnn_hidden_units': [100,11],#MLP隐层和输出层\n",
    "    \n",
    "    'dnn_hidden_units':[9],#MLP隐层\n",
    "    'num_sparse_features_cols':10477,#the number of the gene columns\n",
    "    'num_dense_features': 0,#dense features number\n",
    "    'bi_dropout': 0.3,#Bi-Interaction 的dropout\n",
    "    'num_epoch': 500,#训练epoch次数\n",
    "    'batch_size': 24,#batch_size\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    'epoch':1000,\n",
    "    \n",
    "    #'train_file': '../Data/criteo/processed_data/train_set.csv',\n",
    "    #'fea_file': '../Data/criteo/processed_data/fea_col.npy',\n",
    "    #'validate_file': '../Data/criteo/processed_data/val_set.csv',\n",
    "    #'test_file': '../Data/criteo/processed_data/test_set.csv',\n",
    "    #'model_name': '../TrainedModels/NFM.model'\n",
    "    #'train_file':'data/xiaoqiu_gene_5000/train/final_5000_encode_100x.csv',\n",
    "    'train_data':'dataset/qiuguan/non_code/train/train_encode_data_all.csv',\n",
    "    'train_label':'dataset/qiuguan/non_code/train/train_label.csv',\n",
    "    'test_data':'dataset/qiuguan/non_code/test/test_encode_data.csv',\n",
    "    'test_label':'dataset/qiuguan/non_code/test/test_labels.csv',\n",
    "    #'title':'dataset/xiaoguan/RF/RF_for_train/train_class_9/test/test_data.csv',\n",
    "    \n",
    "    #'all':''\n",
    "    #'title':'data/xiaoqiu_gene_5000/train/gene_5000_gene_name.csv',\n",
    "    #'all':'data/xiaoqiu_gene_5000/train/gene_5000_label_name.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c00be52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMData\n",
      "FMData\n",
      "NFM: NFM(\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (BN_num): BatchNorm1d(10477, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear_model1): Linear(in_features=10477, out_features=1000, bias=True)\n",
      "  (BN_linear1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear_model2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (BN_linear2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (embedding_layers): Embedding(101, 20)\n",
      "  (bi_pooling): BiInteractionPooling()\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (BN_bi): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dnn_layers): ModuleList(\n",
      "    (0): Linear(in_features=120, out_features=9, bias=True)\n",
      "  )\n",
      "  (dnn_softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_test_accuracy: tensor(0.1667, device='cuda:0')\n",
      "Training Epoch: 0, total loss: 56.854142\n",
      "total_test_accuracy: tensor(0.1250, device='cuda:0')\n",
      "Training Epoch: 1, total loss: 56.365815\n",
      "total_test_accuracy: tensor(0.2500, device='cuda:0')\n",
      "Training Epoch: 2, total loss: 55.824495\n",
      "total_test_accuracy: tensor(0.3750, device='cuda:0')\n",
      "Training Epoch: 3, total loss: 54.789144\n",
      "total_test_accuracy: tensor(0.3333, device='cuda:0')\n",
      "Training Epoch: 4, total loss: 54.171448\n",
      "total_test_accuracy: tensor(0.4167, device='cuda:0')\n",
      "Training Epoch: 5, total loss: 53.835571\n",
      "total_test_accuracy: tensor(0.4583, device='cuda:0')\n",
      "Training Epoch: 6, total loss: 53.543198\n",
      "total_test_accuracy: tensor(0.4167, device='cuda:0')\n",
      "Training Epoch: 7, total loss: 53.023965\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 8, total loss: 52.284578\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 9, total loss: 51.595329\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 10, total loss: 50.975938\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 11, total loss: 50.806278\n",
      "total_test_accuracy: tensor(0.6667, device='cuda:0')\n",
      "Training Epoch: 12, total loss: 49.752364\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 13, total loss: 49.736658\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 14, total loss: 49.215047\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 15, total loss: 49.065806\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 16, total loss: 48.472873\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 17, total loss: 48.282939\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 18, total loss: 47.821113\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 19, total loss: 47.981835\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 20, total loss: 47.883235\n",
      "total_test_accuracy: tensor(0.7500, device='cuda:0')\n",
      "Training Epoch: 21, total loss: 47.145129\n",
      "total_test_accuracy: tensor(0.7917, device='cuda:0')\n",
      "Training Epoch: 22, total loss: 46.704846\n",
      "total_test_accuracy: tensor(0.7500, device='cuda:0')\n",
      "Training Epoch: 23, total loss: 46.892805\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 24, total loss: 46.512577\n",
      "total_test_accuracy: tensor(0.7083, device='cuda:0')\n",
      "Training Epoch: 25, total loss: 45.977989\n",
      "total_test_accuracy: tensor(0.7500, device='cuda:0')\n",
      "Training Epoch: 26, total loss: 46.145599\n",
      "total_test_accuracy: tensor(0.7500, device='cuda:0')\n",
      "Training Epoch: 27, total loss: 46.057637\n",
      "total_test_accuracy: tensor(0.6667, device='cuda:0')\n",
      "Training Epoch: 28, total loss: 46.096442\n",
      "total_test_accuracy: tensor(0.7500, device='cuda:0')\n",
      "Training Epoch: 29, total loss: 45.141389\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 30, total loss: 44.932578\n",
      "total_test_accuracy: tensor(0.6667, device='cuda:0')\n",
      "Training Epoch: 31, total loss: 44.769755\n",
      "total_test_accuracy: tensor(0.7917, device='cuda:0')\n",
      "Training Epoch: 32, total loss: 44.741171\n",
      "total_test_accuracy: tensor(0.7917, device='cuda:0')\n",
      "Training Epoch: 33, total loss: 44.413950\n",
      "total_test_accuracy: tensor(0.7083, device='cuda:0')\n",
      "Training Epoch: 34, total loss: 43.704018\n",
      "total_test_accuracy: tensor(0.7917, device='cuda:0')\n",
      "Training Epoch: 35, total loss: 43.731423\n",
      "total_test_accuracy: tensor(0.7917, device='cuda:0')\n",
      "Training Epoch: 36, total loss: 43.246641\n",
      "total_test_accuracy: tensor(0.7917, device='cuda:0')\n",
      "Training Epoch: 37, total loss: 43.533985\n",
      "total_test_accuracy: tensor(0.8333, device='cuda:0')\n",
      "Training Epoch: 38, total loss: 42.594652\n",
      "total_test_accuracy: tensor(0.7500, device='cuda:0')\n",
      "Training Epoch: 39, total loss: 42.259506\n",
      "total_test_accuracy: tensor(0.7083, device='cuda:0')\n",
      "Training Epoch: 40, total loss: 42.291304\n",
      "total_test_accuracy: tensor(0.8333, device='cuda:0')\n",
      "Training Epoch: 41, total loss: 42.340382\n",
      "total_test_accuracy: tensor(0.8750, device='cuda:0')\n",
      "Training Epoch: 42, total loss: 42.494752\n",
      "total_test_accuracy: tensor(0.8333, device='cuda:0')\n",
      "Training Epoch: 43, total loss: 42.195586\n",
      "total_test_accuracy: tensor(0.9167, device='cuda:0')\n",
      "Training Epoch: 44, total loss: 41.875486\n",
      "total_test_accuracy: tensor(0.7500, device='cuda:0')\n",
      "Training Epoch: 45, total loss: 41.635473\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 46, total loss: 40.852803\n",
      "total_test_accuracy: tensor(0.8750, device='cuda:0')\n",
      "Training Epoch: 47, total loss: 41.074619\n",
      "total_test_accuracy: tensor(0.7917, device='cuda:0')\n",
      "Training Epoch: 48, total loss: 41.272134\n",
      "total_test_accuracy: tensor(0.8750, device='cuda:0')\n",
      "Training Epoch: 49, total loss: 41.147386\n",
      "total_test_accuracy: tensor(0.9167, device='cuda:0')\n",
      "Training Epoch: 50, total loss: 41.248207\n",
      "total_test_accuracy: tensor(0.7917, device='cuda:0')\n",
      "Training Epoch: 51, total loss: 40.452932\n",
      "total_test_accuracy: tensor(0.9167, device='cuda:0')\n",
      "Training Epoch: 52, total loss: 40.661638\n",
      "total_test_accuracy: tensor(0.7500, device='cuda:0')\n",
      "Training Epoch: 53, total loss: 40.914630\n",
      "total_test_accuracy: tensor(0.9167, device='cuda:0')\n",
      "Training Epoch: 54, total loss: 40.697408\n",
      "total_test_accuracy: tensor(0.7917, device='cuda:0')\n",
      "Training Epoch: 55, total loss: 40.747178\n",
      "total_test_accuracy: tensor(0.8750, device='cuda:0')\n",
      "Training Epoch: 56, total loss: 40.605226\n",
      "total_test_accuracy: tensor(0.8750, device='cuda:0')\n",
      "Training Epoch: 57, total loss: 40.345968\n",
      "total_test_accuracy: tensor(0.9167, device='cuda:0')\n",
      "Training Epoch: 58, total loss: 40.300312\n",
      "total_test_accuracy: tensor(0.8333, device='cuda:0')\n",
      "Training Epoch: 59, total loss: 40.021789\n",
      "total_test_accuracy: tensor(0.7500, device='cuda:0')\n",
      "Training Epoch: 60, total loss: 39.693097\n",
      "total_test_accuracy: tensor(0.9583, device='cuda:0')\n",
      "Training Epoch: 61, total loss: 40.223823\n",
      "total_test_accuracy: tensor(0.8333, device='cuda:0')\n",
      "Training Epoch: 62, total loss: 39.820894\n",
      "total_test_accuracy: tensor(0.7917, device='cuda:0')\n",
      "Training Epoch: 63, total loss: 39.927378\n",
      "total_test_accuracy: tensor(0.7083, device='cuda:0')\n",
      "Training Epoch: 64, total loss: 39.463938\n",
      "total_test_accuracy: tensor(0.8333, device='cuda:0')\n",
      "Training Epoch: 65, total loss: 39.786535\n",
      "total_test_accuracy: tensor(0.7917, device='cuda:0')\n",
      "Training Epoch: 66, total loss: 39.417185\n",
      "total_test_accuracy: tensor(0.8333, device='cuda:0')\n",
      "Training Epoch: 67, total loss: 39.545490\n",
      "total_test_accuracy: tensor(0.8333, device='cuda:0')\n",
      "Training Epoch: 68, total loss: 39.298424\n",
      "total_test_accuracy: tensor(0.8750, device='cuda:0')\n",
      "Training Epoch: 69, total loss: 39.615340\n",
      "total_test_accuracy: tensor(0.7083, device='cuda:0')\n",
      "Training Epoch: 70, total loss: 39.483973\n",
      "total_test_accuracy: tensor(0.7500, device='cuda:0')\n",
      "Training Epoch: 71, total loss: 39.665286\n",
      "total_test_accuracy: tensor(0.8750, device='cuda:0')\n",
      "Training Epoch: 72, total loss: 39.265800\n",
      "total_test_accuracy: tensor(0.8750, device='cuda:0')\n",
      "Training Epoch: 73, total loss: 39.267593\n",
      "total_test_accuracy: tensor(0.8333, device='cuda:0')\n",
      "Training Epoch: 74, total loss: 38.962112\n"
     ]
    }
   ],
   "source": [
    "train_dataset,train_loader=prepare_dataset(nfm_config['train_data'],nfm_config['train_label'],nfm_config['batch_size'],nfm_config['n_class'])\n",
    "test_dataset,test_loader=prepare_dataset(nfm_config['test_data'],nfm_config['test_label'],nfm_config['batch_size'],nfm_config['n_class'])\n",
    "#from MLP import MLP\n",
    "from new_nfm_network_batch_1 import NFM\n",
    "#model=MLP(4224,1000,100,9)\n",
    "model=NFM(nfm_config)\n",
    "model.cuda()\n",
    "print(\"NFM:\",model)\n",
    "train_data(model,train_loader,test_loader,nfm_config['batch_size'],'dataset/qiuguan/model/NFM_encode_100_20/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce80496b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
