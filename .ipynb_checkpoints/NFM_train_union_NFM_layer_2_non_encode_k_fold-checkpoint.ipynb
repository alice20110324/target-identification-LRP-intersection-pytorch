{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f6d7f9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-97712217c0de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m \u001b[0mnfm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNFM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnfm_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0mnfm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[0mnfm_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnfm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \"\"\"\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \"\"\"\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import Trainer\n",
    "#from network import NFM\n",
    "import torch.utils.data as Data\n",
    "from Utils.criteo_loader import getTestData, getTrainData\n",
    "\n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':9,\n",
    "    'linear_hidden1':2000,#线性模型输出层（隐层个数）\n",
    "    'embed_input_dim':20,#embed输入维度\n",
    "    'embed_dim': 10, # 用于控制稀疏特征经过Embedding层后的稠密特征大小，embed输出维度\n",
    "    'dnn_hidden_units': [100,9],#MLP隐层和输出层\n",
    "    'num_sparse_features_cols':10477,#the number of the gene columns\n",
    "    'num_dense_features': 0,#dense features number\n",
    "    'bi_dropout': 0.5,#Bi-Interaction 的dropout\n",
    "    'num_epoch': 500,#训练epoch次数\n",
    "    'batch_size': 24,#batch_size\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    'train_label': 'dataset/gene_247/train/guan_train_label.csv',\n",
    "    'train_data':'dataset/gene_247/train/guan_train_data.csv',\n",
    "    'test_label':'dataset/gene_247/test/qiu_test_label.csv',\n",
    "    'test_data':'dataset/gene_247/test/qiu_test_data.csv',\n",
    "    'gene_name':'dataset/qiuguan/orign/gene_name.csv',\n",
    "    'label_name':'dataset/qiuguan/orign/gene_label.csv'\n",
    "    #'fea_file': '../Data/criteo/processed_data/fea_col.npy',\n",
    "    #'validate_file': '../Data/criteo/processed_data/val_set.csv',\n",
    "    #'test_file': '../Data/criteo/processed_data/test_set.csv',\n",
    "    #'model_name': '../TrainedModels/NFM.model'\n",
    "}\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import torch.nn.functional as F  # 激励函数的库\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "#from utils import \n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import torch.nn.functional as F  # 激励函数的库\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "#from utils import \n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "'''\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "all=pd.read_csv('dataset/qiuguan/orign/union_all_qiuguan.csv',sep=',')\n",
    "all=all.iloc[1:,1:]\n",
    "X=all.iloc[:,:-1]\n",
    "#X=X.values\n",
    "\n",
    "#print(X)\n",
    "\n",
    "y=all.iloc[:,-1]\n",
    "#y=y.values\n",
    "\n",
    "#print(y)\n",
    "\n",
    "\"\"\"\n",
    "y=pd.read_csv('dataset/gene_247/data/guan/guan_label.csv',sep=',')\n",
    "y=y.values\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#y=np.array(y)\n",
    "train_val_data,test_data,train_val_label,test_label=train_test_split(X,y,test_size=0.2,stratify=y,random_state=2)\n",
    "\n",
    "print(train_val_data)\n",
    "train_val_info=pd.concat([train_val_data,train_val_label],axis=1)\n",
    "test_info=pd.concat([test_data,test_label],axis=1)\n",
    "print(train_val_info)\n",
    "train_val_info.to_csv('dataset/qiuguan/orign/train_val_info.csv')\n",
    "test_info.to_csv('dataset/qiuguan/orign/test_info.csv')\n",
    "\n",
    "'''\n",
    "\n",
    "#print(test_data)\n",
    "\n",
    "\n",
    "def one_hot_smoothing(labels, classes, label_smoothing=0.2):\n",
    "    #n = len(labels)\n",
    "    n=labels.shape[0]\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    n = len(labels)\n",
    "    #eoff = label_smoothing / classes\n",
    "    output = np.zeros((n, classes), dtype=np.float32)\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "class KZDatasetTest(data.Dataset):\n",
    "    \"\"\" Construct the FM pytorch dataset. \"\"\"\n",
    "    #def __init__(self, file,label_file, feature_map,n_class=16):\n",
    "    def __init__(self, csv_path):\n",
    "    \n",
    "        '''\n",
    "        txt_path: 所有数据的路径，我的形式为(单张图片路径 类别\\n)\n",
    "        img1.png 0\n",
    "        ...\n",
    "         img100.png 1\n",
    "         ki：当前是第几折,从0开始，范围为[0, K)\n",
    "         K：总的折数\n",
    "         typ：用于区分训练集与验证集\n",
    "         transform：对图片的数据增强\n",
    "         rand：是否随机\n",
    "        '''\n",
    "        '''\n",
    "        self.all_data_info = self.get_img_info(txt_path)\n",
    "        \n",
    "        if rand:\n",
    "            random.seed(1)\n",
    "            random.shuffle(self.all_data_info)\n",
    "        leng = len(self.all_data_info)\n",
    "        every_z_len = leng // K\n",
    "        if typ == 'val':\n",
    "            self.data_info = self.all_data_info[every_z_len * ki : every_z_len * (ki+1)]\n",
    "        elif typ == 'train':\n",
    "            self.data_info = self.all_data_info[: every_z_len * ki] + self.all_data_info[every_z_len * (ki+1) :]\n",
    "            \n",
    "        self.transform = transform\n",
    "        \n",
    "        '''\n",
    "        self.data_info = self.get_data_info(csv_path)\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Dataset读取图片的函数\n",
    "        data, label = self.data_info[index]\n",
    "        #img = Image.open(img_pth).convert('RGB')\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def get_img_info(txt_path):\n",
    "        # 解析输入的txt的函数\n",
    "        # 转为二维list存储，每一维为 [ 图片路径，图片类别]\n",
    "        data_info = []\n",
    "        data = open(txt_path, 'r')\n",
    "        data_lines = data.readlines()\n",
    "        for data_line in data_lines:\n",
    "            data_line = data_line.split()\n",
    "            img_pth = data_line[0]\n",
    "            label = int(data_line[1])\n",
    "            data_info.append((img_pth, label))\n",
    "        return data_info\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def get_data_info(self,csv_path):\n",
    "        #解析路径\n",
    "        #转为一维list存储，每一位为【图片路径，图片类别】\n",
    "        labels=[]\n",
    "        data_info=[]\n",
    "        df=pd.read_csv(csv_path,sep=',',header=None)\n",
    "        df=df.iloc[1:,1:]\n",
    "        \n",
    "        #print(df.iloc[:,-1])\n",
    "        #df=df.applymap(ast.literal_eval)\n",
    "        rows,cols=df.shape\n",
    "        #print(rows,cols)\n",
    "        for i in df.iloc[:,-1]:\n",
    "            #print(i)\n",
    "            labels.append(int(i))\n",
    "        #print('labels:',labels)\n",
    "        labels=np.array(labels)\n",
    "        #print('labels:',labels)\n",
    "        #labels=np.array(labels)\n",
    "        labels=one_hot_smoothing(labels,nfm_config['n_class'])\n",
    "        #print(labels)\n",
    "        for i in range(rows):\n",
    "            data=df.iloc[i,:-1]\n",
    "            data=data.astype(float)#\n",
    "            data=np.array(data)#\n",
    "            \n",
    "            label=labels[i]\n",
    "            #print(data.shape)\n",
    "            #print(label.shape)\n",
    "            #label=label.tolist()\n",
    "            data=torch.from_numpy(data)#\n",
    "            label=torch.from_numpy(label)#\n",
    "            \n",
    "            \n",
    "            data_info.append((data,label))\n",
    "        return data_info\n",
    "    \n",
    "    \n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import *\n",
    "from PIL import Image\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ast\n",
    "import torchvision\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "class KZDataset(Dataset):\n",
    "    def __init__(self, csv_path, K,n_class,ki=0, typ='train', transform=None, rand=False):\n",
    "        '''\n",
    "        txt_path: 所有数据的路径，我的形式为(单张图片路径 类别\\n)\n",
    "        img1.png 0\n",
    "        ...\n",
    "         img100.png 1\n",
    "         ki：当前是第几折,从0开始，范围为[0, K)\n",
    "         K：总的折数\n",
    "         typ：用于区分训练集与验证集\n",
    "         transform：对图片的数据增强\n",
    "         rand：是否随机\n",
    "        '''\n",
    "        '''\n",
    "        self.all_data_info = self.get_img_info(txt_path)\n",
    "        \n",
    "        if rand:\n",
    "            random.seed(1)\n",
    "            random.shuffle(self.all_data_info)\n",
    "        leng = len(self.all_data_info)\n",
    "        every_z_len = leng // K\n",
    "        if typ == 'val':\n",
    "            self.data_info = self.all_data_info[every_z_len * ki : every_z_len * (ki+1)]\n",
    "        elif typ == 'train':\n",
    "            self.data_info = self.all_data_info[: every_z_len * ki] + self.all_data_info[every_z_len * (ki+1) :]\n",
    "            \n",
    "        self.transform = transform\n",
    "        \n",
    "        '''\n",
    "        self.all_data_info = self.get_data_info(csv_path)\n",
    "        \n",
    "        if rand:\n",
    "            random.seed(1)\n",
    "            random.shuffle(self.all_data_info)\n",
    "        leng = len(self.all_data_info)\n",
    "        every_z_len = leng // K\n",
    "        if typ == 'val':\n",
    "            self.data_info = self.all_data_info[every_z_len * ki : every_z_len * (ki+1)]\n",
    "        elif typ == 'train':\n",
    "            self.data_info = self.all_data_info[: every_z_len * ki] + self.all_data_info[every_z_len * (ki+1) :]\n",
    "            \n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Dataset读取图片的函数\n",
    "        data, label = self.data_info[index]\n",
    "        #img = Image.open(img_pth).convert('RGB')\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def get_img_info(txt_path):\n",
    "        # 解析输入的txt的函数\n",
    "        # 转为二维list存储，每一维为 [ 图片路径，图片类别]\n",
    "        data_info = []\n",
    "        data = open(txt_path, 'r')\n",
    "        data_lines = data.readlines()\n",
    "        for data_line in data_lines:\n",
    "            data_line = data_line.split()\n",
    "            img_pth = data_line[0]\n",
    "            label = int(data_line[1])\n",
    "            data_info.append((img_pth, label))\n",
    "        return data_info\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def get_data_info(self,csv_path):\n",
    "        #解析路径\n",
    "        #转为一维list存储，每一位为【图片路径，图片类别】\n",
    "        labels=[]\n",
    "        data_info=[]\n",
    "        df=pd.read_csv(csv_path,sep=',',header=None)\n",
    "        df=df.iloc[1:,1:]\n",
    "        \n",
    "        #print(df.iloc[:,-1])\n",
    "        #df=df.applymap(ast.literal_eval)\n",
    "        rows,cols=df.shape\n",
    "        #print(rows,cols)\n",
    "        for i in df.iloc[:,-1]:\n",
    "            #print(i)\n",
    "            labels.append(int(i))\n",
    "        #print('labels:',labels)\n",
    "        labels=np.array(labels)\n",
    "        #print('labels:',labels)\n",
    "        #labels=np.array(labels)\n",
    "        labels=one_hot_smoothing(labels,nfm_config['n_class'])\n",
    "        #print(labels)\n",
    "        for i in range(rows):\n",
    "            data=df.iloc[i,:-1]\n",
    "            data=data.astype(float)#\n",
    "            data=np.array(data)#\n",
    "            \n",
    "            label=labels[i]\n",
    "            #print(data.shape)\n",
    "            #print(label.shape)\n",
    "            #label=label.tolist()\n",
    "            data=torch.from_numpy(data)#\n",
    "            label=torch.from_numpy(label)#\n",
    "            '''\n",
    "            if i==62:\n",
    "                for num,i in enumerate(data):\n",
    "                    print(num,i)\n",
    "            data=[eval(i) for i in data]\n",
    "            \n",
    "            #data=data.values\n",
    "            #data=data.applymap(ast.literal_eval)\n",
    "            #data=data.applymap(ast.literal_eval)\n",
    "            #data=eval(data)\n",
    "            #print(\"i:\",i)\n",
    "            \n",
    "            \n",
    "            #print(data)\n",
    "            \n",
    "            label=[]\n",
    "            print(df.iloc[i,-1])\n",
    "            label.append(eval(df.iloc[i,-1]))\n",
    "            print(\"label:\",i,label)\n",
    "            #label=[eval(i) for i in label]\n",
    "            \n",
    "            #label=label.applymap(ast.literal_eval)\n",
    "            \n",
    "            #print(label)\n",
    "            label=one_hot_smoothing(np.array(label),nfm_config['n_class'])\n",
    "            label=label.tolist()\n",
    "            \n",
    "            '''\n",
    "            \n",
    "            data_info.append((data,label))\n",
    "        return data_info\n",
    "            \n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "#from utils import \n",
    "#准备训练集\n",
    "#from new_dataset_processed import FMData\n",
    "#from dataset_process import FMData\n",
    "def train_epoch(model,train_loader,batch_size,optimizer,loss_func):\n",
    "    BATCH_SIZE=batch_size\n",
    "    total = 0\n",
    "    correct=0\n",
    "    total_loss=0\n",
    "    #\n",
    "    total_train_accuracy=0  \n",
    "    for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "            \n",
    "        labels = Variable(labels)\n",
    "        x = Variable(x)\n",
    "            \n",
    "            \n",
    "        x=torch.tensor(x,dtype=torch.float)\n",
    "        labels=torch.tensor(labels,dtype=torch.float)\n",
    "        x, labels = x.cuda(), labels.cuda()\n",
    "        labels_int=labels=torch.max(labels,1)[1]\n",
    "            \n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        y_predict = model(x)\n",
    "            \n",
    "        loss = loss_func(y_predict, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        loss = loss.item()\n",
    "           \n",
    "\n",
    "        total_loss += loss\n",
    "            \n",
    "            \n",
    "            \n",
    "        batch_train_acc=torchmetrics.functional.accuracy(y_predict,labels_int)\n",
    "        total_train_accuracy+=batch_train_acc\n",
    "            \n",
    "    total_train_accuracy/=(batch_idx+1)\n",
    "    print('total_train_accuracy:',total_train_accuracy)\n",
    "    print(\"Training Epoch: %d, total loss: %f\" % (epoch_id, total_loss))\n",
    "    return total_loss,total_train_accuracy\n",
    "\n",
    "\n",
    "\n",
    "def val_epoch(model,test_loader,batch_size,optimizer): \n",
    "    batch_size_num=0\n",
    "    total_test_acc=0\n",
    "    for i , (inputs , targets) in enumerate(test_loader):   \n",
    "            print(\"test\")\n",
    "            \n",
    "            inputs = Variable(inputs)   \n",
    "            targets = Variable(targets)     \n",
    "           \n",
    "            inputs=torch.tensor(inputs ,dtype=torch.float)   \n",
    "            targets=torch.tensor(targets ,dtype=torch.float)   \n",
    "            inputs , targets = inputs.cuda(),  targets.cuda()   \n",
    "            yhat = model(inputs)  \n",
    "            \n",
    "            \n",
    "            \n",
    "            targets=torch.max(targets,1)[1]\n",
    "            \n",
    "            \n",
    "            \n",
    "            batch_test_acc=torchmetrics.functional.accuracy(yhat,targets)\n",
    "            \n",
    "            total_test_acc+=batch_test_acc\n",
    "            \n",
    "            batch_size_num=i\n",
    "    total_test_acc/=(batch_size_num+1)\n",
    "        ###print('total_test_accuracy:',total_test_acc/(batch_size+1))\n",
    "    print('total_test_accuracy:',total_test_acc)\n",
    "        \n",
    "                    \n",
    "                    \n",
    "            \n",
    "            \n",
    "    \n",
    "        \n",
    "   \n",
    "    \n",
    "    return total_test_acc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "class BiInteractionPooling(nn.Module):\n",
    "    \"\"\"Bi-Interaction Layer used in Neural FM,compress the\n",
    "      pairwise element-wise product of features into one single vector.\n",
    "      Input shape\n",
    "        - A 3D tensor with shape:``(batch_size,field_size,embedding_size)``.\n",
    "      Output shape\n",
    "    http://127.0.0.1:3000/notebooks/NFM-pyorch-master/NFM-pyorch-master/%E6%9C%AA%E5%91%BD%E5%90%8D5.ipynb?kernel_name=python3#    - 3D tensor with shape: ``(batch_size,1,embedding_size)``.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(BiInteractionPooling, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        concated_embeds_value = inputs\n",
    "        square_of_sum = torch.pow(\n",
    "            torch.sum(concated_embeds_value, dim=1, keepdim=True), 2)\n",
    "        sum_of_square = torch.sum(\n",
    "            concated_embeds_value * concated_embeds_value, dim=1, keepdim=True)\n",
    "        cross_term = 0.5 * (square_of_sum - sum_of_square)\n",
    "        return cross_term\n",
    "    \n",
    "    \n",
    "def embding_process(config,sparse_inputs):\n",
    "    \n",
    "    sparse_inputs=sparse_inputs.long()###\n",
    "    embedding_layers=nn.Embedding(config['embed_input_dim'],config['embed_dim'])\n",
    "        \n",
    "    # B-Interaction 层\n",
    "    bi_pooling = BiInteractionPooling()\n",
    "    bi_dropout = config['bi_dropout']\n",
    "    if bi_dropout > 0:\n",
    "        dropout = nn.Dropout(bi_dropout)\n",
    "            \n",
    "    num_sparse_feature=config['num_sparse_features_cols']\n",
    "    \n",
    "    sparse_embeds = [embedding_layers(sparse_inputs[:, i]) for i in range(sparse_inputs.shape[1])]\n",
    "    sparse_embeds = torch.cat(sparse_embeds, axis=-1)\n",
    "    BN_bi = nn.BatchNorm1d(config['embed_dim'])\n",
    "    # 送入B-Interaction层\n",
    "    fm_input = sparse_embeds.view(-1, num_sparse_feature, config['embed_dim'])#整理成n行m列\n",
    "    # print(fm_input)\n",
    "    # print(fm_input.shape)\n",
    "\n",
    "    bi_out = bi_pooling(fm_input)\n",
    "    if bi_dropout:\n",
    "        bi_out = dropout(bi_out)\n",
    "\n",
    "    bi_out = bi_out.view(-1, config['embed_dim'])\n",
    "    bi_out=BN_bi(bi_out)\n",
    "    \n",
    "    return bi_out\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from basemodel import BaseModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class NFM(BaseModel):\n",
    "    def __init__(self, config, dense_features_cols=[]):#=[]为新增\n",
    "    #def __init__(self, config, dense_features_cols, sparse_features_cols):\n",
    "        super(NFM, self).__init__(config)\n",
    "        # 稠密和稀疏特征的数量\n",
    "        #self.num_dense_feature = dense_features_cols.__len__()\n",
    "        self.num_dense_feature = 0#修改\n",
    "        self.num_sparse_feature = config['num_sparse_features_cols']\n",
    "        #self.num_sparse_feature = 0##修改\n",
    "        self.__config=config\n",
    "        \n",
    "        \n",
    "        self.BN_num=nn.BatchNorm1d(self.num_sparse_feature)\n",
    "        self.linear1=nn.Linear(config['num_sparse_features_cols'],config['linear_hidden1'])\n",
    "        self.bn1=nn.BatchNorm1d(config['linear_hidden1'])\n",
    "        self.drop1=nn.Dropout(0.5)\n",
    "        self.relu1=nn.ReLU()\n",
    "        \n",
    "        self.linear2=nn.Linear(config['linear_hidden1']+config['embed_dim'],config['dnn_hidden_units'][0])\n",
    "        self.bn2=nn.BatchNorm1d(config['dnn_hidden_units'][0])\n",
    "        self.drop2=nn.Dropout(0.5)\n",
    "        self.relu2=nn.ReLU()\n",
    "        \n",
    "        self.linear3=nn.Linear(config['dnn_hidden_units'][0],config['dnn_hidden_units'][1])\n",
    "        self.bn3=nn.BatchNorm1d(config['dnn_hidden_units'][1])\n",
    "        #self.drop3=nn.Dropout(0.3)\n",
    "        self.relu3=nn.ReLU()\n",
    "        \n",
    "        #self.embedding_layers=nn.Embedding(config['embed_input_dim'],config['embed_dim'])\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        #self.dnn_softmax=nn.Softmax(dim=1) # 按列SoftMax,列和为1  #注意nn.softmax的定义和调用\n",
    "        #self.dnn_softmax_=F.softmax(dim=1)\n",
    "        #self.dnn_hidden_units=config['dnn_hidden_units']\n",
    "    def forward(self, x,bi_x):\n",
    "        # 先区分出稀疏特征和稠密特征，这里是按照列来划分的，即所有的行都要进行筛选\n",
    "        #bi_x=bi_x.long()\n",
    "        #print(x.dtype)\n",
    "        # 求出线性部分\n",
    "        #x=F.relu(self.drop(self.BN_linear1(self.linear_model1(self.BN_num(x))))\n",
    "        #x=F.relu(self.drop(self.BN_linear1(self.linear_model1(x))))\n",
    "        x=self.BN_num(x)\n",
    "        #x=self.linear_model1(x)\n",
    "        \n",
    "        #x1=torch.cat((x,bi_x),dim=1)\n",
    "        #print(\"linear_output:\",linear_output)\n",
    "        #linear_output=linear_output.view(-1,self.__config['linear_hidden1'])\n",
    "        #linear_output=self.drop(linear_output)\n",
    "        #linear_output=self.BN_linear(linear_output)\n",
    "        # 求出稀疏特征的embedding向量\n",
    "        \n",
    "        \n",
    "        #print('bi_out.shape:',bi_out.shape)\n",
    "        #print(x.dtype)\n",
    "        #print(bi_out.dtype)\n",
    "        \n",
    "        #input=x,bi_x#不能是list，必须是tensor\n",
    "        #x1,x2=input\n",
    "        y1=self.relu1(self.drop1(self.bn1(self.linear1(x))))\n",
    "        x2=torch.cat((y1,bi_x),dim=1)\n",
    "        #print('y1.shape:',y1.shape)\n",
    "        y2=self.relu2(self.drop2(self.bn2(self.linear2(x2))))\n",
    "        #x3=torch.cat((y2,bi_x),dim=1)\n",
    "        #print('y2.shape:',y2.linear3(y2)))\n",
    "        #print('y3.shape:',y3.shape)\n",
    "        y3=self.relu3(self.bn3(self.linear3(y2)))\n",
    "        y=F.softmax(y3,dim=1)\n",
    "        #x3=torch.cat((y2,bi_x),dim=1)\n",
    "        #x3=torch.cat((y2,bi_x),dim=1)\n",
    "        #print('x3.shape:',x3.shape)\n",
    "        \n",
    "        \n",
    "        y_pred=y\n",
    "        #y_pred=self.dnn_softmax(dnn_output)#增加\n",
    "        #y_pred=F.softmax(dnn_output,dim=0)\n",
    "        # Final\n",
    "        #output = linear_output + y_pred#修改\n",
    "        #y_pred = self.dnn_softmax(output,dim=0)\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "nfm = NFM(nfm_config).cuda()\n",
    "nfm.cuda()\n",
    "nfm_params = list(nfm.named_parameters())\n",
    "#print(nfm_params[1])\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotLoss(loss):\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    x=[i for i in range(201)]\n",
    "    #acc_train=acc_train.cpu()\n",
    "    #acc_test=acc_test.cpu()\n",
    "    plt.plot(x, loss, 'r-', mec='k', label='Logistic Loss', lw=2)\n",
    "    #plt.plot(x,acc_train,'b-',mec='k',label='accuracy Train',lw=2)\n",
    "    #plt.plot(x,acc_test,'g-',mec='k',label='accuracy Test',lw=2)\n",
    "    #plt.plot(x, y_01, 'g-', mec='k', label='0/1 Loss', lw=2)\n",
    "    #plt.plot(x, y_hinge, 'b-',mec='k', label='Hinge Loss', lw=2)\n",
    "    #plt.plot(x, boost, 'm--',mec='k', label='Adaboost Loss',lw=2)\n",
    "    plt.grid(True, ls='--')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('损失函数')\n",
    "    plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b14a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=nfm\n",
    "K=10\n",
    "test_metrics=[]\n",
    "train_loss_total_list=[]\n",
    "for ki in range(K):\n",
    "    trainset = KZDataset(csv_path='dataset/qiuguan/orign/train_val_info.csv',K=K, n_class=nfm_config['n_class'],ki=ki,  typ='train', transform=None, rand=True)\n",
    "    valset = KZDataset(csv_path='dataset/qiuguan/orign/train_val_info.csv', K=K,n_class=nfm_config['n_class'],ki=ki,  typ='val', transform=None, rand=True)\n",
    "    train_loader = data.DataLoader(\n",
    "         dataset=trainset,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         drop_last=True,\n",
    "         batch_size=nfm_config['batch_size'],\n",
    "         shuffle=True)\n",
    "    val_loader = data.DataLoader(\n",
    "         dataset=valset,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         drop_last=True,\n",
    "         batch_size=nfm_config['batch_size']\n",
    "        \n",
    "     )\n",
    "    \n",
    "    model_path='dataset/qiuguan/model_new_K_fold/NFM_layer_2_non_encode/'\n",
    "    #BATCH_SIZE=batch_size\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    #total = 0\n",
    "    \n",
    "    \n",
    "    loss_func=torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    num=0\n",
    "   \n",
    "    \n",
    "    epoches=201\n",
    "    for epoch_id in range(epoches):\n",
    "          \n",
    "        \n",
    "        \n",
    "        train_loss_total,acc_train=train_epoch(model,train_loader,nfm_config['batch_size'],optimizer,loss_func)\n",
    "        train_loss_total_list.append(train_loss_total)#\n",
    "        if epoch_id %20==0:\n",
    "            num=num+1\n",
    "            path=os.path.join(model_path,'MLP'+str(num)+str(K)+'.pkl')\n",
    "            torch.save(model.state_dict(),path)\n",
    "    print(\"the \",ki,\" epoch ends\")\n",
    "    plotLoss(train_loss_total_list)\n",
    "    train_loss_total_list=[]\n",
    "    acc_test=val_epoch(model,val_loader,nfm_config['batch_size'],optimizer)\n",
    "    print(\"acc_test_each_k:\",acc_test)\n",
    "    test_metrics.append(acc_test)\n",
    "\n",
    "print(test_metrics)\n",
    "#test_metrics=test_metrics.tolist()\n",
    "test_metrics=[x.cpu().detach().numpy() for x in test_metrics]\n",
    "print(test_metrics)\n",
    "acc_test_metrics=np.mean(test_metrics) \n",
    "print(\"acc_test_metrics:\",acc_test_metrics)\n",
    "       \n",
    "print(test_metrics)\n",
    "#test_metrics=test_metrics.tolist()\n",
    "test_metrics=[x.cpu().detach().numpy() for x in test_metrics]\n",
    "print(test_metrics)\n",
    "acc_test_metrics=np.mean(test_metrics) \n",
    "print(\"acc_test_metrics:\",acc_test_metrics)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711088bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#功能：加载保存到path中的各层参数到神经网络\n",
    "\n",
    "path='dataset/qiuguan/model_new_K_fold/NFM_layer_2_non_encode/'\n",
    "\n",
    "#nfm=NFM(nfm_config)\n",
    "nfm=NFM(nfm_config)\n",
    "#print(nfm)\n",
    "#net = nn.DataParallel(net)\n",
    "#net = net.to(device)\n",
    "nfm.load_state_dict(torch.load(path),strict=False)\n",
    "nfm.cuda()\n",
    "\n",
    "print(nfm)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nfm_params = list(nfm.named_parameters())\n",
    "#print(nfm_params)\n",
    "net=nfm\n",
    "\n",
    "#———————————————— \n",
    "#版权声明：本文为CSDN博主「山阴少年」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 \n",
    "#原文链接：https://blog.csdn.net/jclian91/article/details/121708431# \n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data import DataLoader \n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import accuracy_score \n",
    " \n",
    "def evaluate_model(test_dl, model): \n",
    "    model.eval()\n",
    "    predictions, actuals = [], [] \n",
    "    for i, (inputs,bi_inputs, targets) in enumerate(test_dl): \n",
    "        # evaluate the model on the test set \n",
    "        #print(\\ inputs:\\ ,inputs) \n",
    "        #print(\\ targets:\\ ,targets) \n",
    "        inputs = Variable(inputs) \n",
    "        targets = Variable(targets) \n",
    "        bi_inputs=Variable(bi_inputs)        \n",
    "                 \n",
    "        #x = torch.tensor(x, dtype=torch.float) \n",
    "        #x=x.clone().detach().requires_grad_(True) \n",
    "        inputs=torch.tensor(inputs,dtype=torch.float) \n",
    "        bi_inputs=torch.tensor(bi_inputs,dtype=torch.float)\n",
    "        targets=torch.tensor(targets,dtype=torch.float) \n",
    "        inputs,bi_inputs, targets = inputs.cuda(),bi_inputs.cuda(), targets.cuda() \n",
    "        yhat = model(inputs,bi_inputs) \n",
    "        # retrieve numpy array \n",
    "        #yhat = yhat.detach().numpy() \n",
    "        yhat = yhat.detach().cpu().numpy()#转换到cpu \n",
    "        # yhat=yhat.argmax(axis=1) \n",
    "        #print(yhat:\\ ,yhat) \n",
    "        #print('yhat.shape:',yhat.shape) \n",
    "        actual = targets.detach().cpu().numpy() \n",
    "        actual=actual.round() \n",
    "        #print(\\ actual:\\ ,actual) \n",
    "        #print('actual.shape:',actual.shape\n",
    "        #predictions.appe) \n",
    "        #actual = actual.reshape(-1, 1) \n",
    "        # round to class values \n",
    "        yhat = yhat.round() \n",
    "        # store nd(yhat) \n",
    "        actuals.append(actual) \n",
    "        predictions.append(yhat)\n",
    "    print(\"prediction:\" ,predictions) \n",
    "    print(\"actuals:\",actuals) \n",
    "    predictions, actuals = np.vstack(predictions), np.vstack(actuals) \n",
    "    print(\"prediction:\" ,predictions) \n",
    "    print(\"actuals:\" ,actuals) \n",
    "    # calculate accuracy \n",
    "    acc_test = accuracy_score(actuals, predictions) \n",
    "    return  actuals, predictions,acc_test \n",
    "\n",
    "\n",
    "import torch.nn.functional as F \n",
    "\n",
    "actuals,predictions,acc_test=evaluate_model(test_loader,net)\n",
    "\n",
    "\n",
    "import torch.nn.functional as F \n",
    "\n",
    "actuals,predictions,acc_test=evaluate_model(test_loader,net)\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score,roc_curve, auc, precision_score, recall_score, f1_score, confusion_matrix, accuracy_score \n",
    "target_list=actuals \n",
    "pred_list=predictions \n",
    "      \n",
    "y_true=target_list \n",
    "y_pred=pred_list \n",
    "      \n",
    "f1=f1_score(y_true=target_list, y_pred=pred_list, average='macro') # 也可以指定micro模式 \n",
    "acc_score=accuracy_score(y_true=target_list, y_pred=pred_list) \n",
    "rec_score=recall_score(y_true=target_list,y_pred=pred_list,average='macro') # 也可以指定micro模式 \n",
    "pre_recall=precision_score(y_true=target_list,y_pred=pred_list,average='macro') \n",
    "print(\"f1_score:  %.4f\" ,f1) \n",
    "print(\"accuracy_score:\" ,acc_score) \n",
    "print(\"recall_score:\",rec_score) \n",
    "print(\"pre_recall:\" ,pre_recall)\n",
    "\n",
    "\n",
    "\n",
    "auc_curve = roc_auc_score(y_true, y_pred, multi_class='ovo')\n",
    "#———————————————— \n",
    "#版权声明：本文为CSDN博主「农民小飞侠」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 \n",
    "#原文链接：https://blog.csdn.net/w5688414/article/details/106595892 \n",
    "PM_y=y_pred \n",
    "true_y=y_true \n",
    "n_classes=PM_y.shape[1] \n",
    "      \n",
    "print(\"n_classes:\",n_classes) \n",
    "fpr = dict() \n",
    "tpr = dict() \n",
    "roc_auc = dict() \n",
    "for i in range(n_classes): \n",
    "    fpr[i], tpr[i], _ = roc_curve(true_y[:, i], PM_y[:, i]) \n",
    "    roc_auc[i] = auc(fpr[i], tpr[i]) \n",
    "    #print(i) \n",
    "    #print(fpr) \n",
    "    #print(tpr) \n",
    "    \n",
    "    \n",
    "    \n",
    "#计算macro auc \n",
    "from scipy import interp \n",
    "# First aggregate all false positive rates \n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)])) \n",
    "       \n",
    "# Then interpolate all ROC curves at this points \n",
    "mean_tpr = np.zeros_like(all_fpr) \n",
    "for i in range(n_classes): \n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i]) \n",
    "       \n",
    "    # Finally average it and compute AUC \n",
    "    mean_tpr /= n_classes \n",
    "       \n",
    "    fpr[\"macro\"] = all_fpr \n",
    "    tpr[\"macro\"] = mean_tpr \n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "#画图 \n",
    "      \n",
    "import matplotlib.pyplot as plt \n",
    "from itertools import cycle \n",
    "from matplotlib.ticker import FuncFormatter \n",
    "lw = 2 \n",
    "# Plot all ROC curves \n",
    "plt.figure() \n",
    "labels=['Con(0)','DN(1)','FSGS(2)','HT(3)','IgA(4)','MCD(5)','MGN(6)','RPGN(7)','SLE(8)'] \n",
    "\n",
    "'''\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"], \n",
    "              label='macro-average ROC curve (area = {0:0.4f})' \n",
    "                    ''.format(roc_auc[\"macro\"]), \n",
    "              color='navy', linestyle=':', linewidth=4) \n",
    "'''\n",
    "\n",
    "       \n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue','blue','yellow','burlywood','cornsilk','darkblue','goldenrod','greenyellow','maroon']) \n",
    "for i, color in zip(range(n_classes), colors): \n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw, \n",
    "                  label=labels[i]+'(area = {0:0.4f})'.format(roc_auc[i])) \n",
    "       \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw) \n",
    "    plt.xlim([0.0, 1.0]) \n",
    "    plt.ylim([0.0, 1.05]) \n",
    "      \n",
    "      \n",
    "    plt.xlabel('1-Specificity (%)') \n",
    "    plt.ylabel('Sensitivity (%)') \n",
    "    plt.title('Multicategory in tubulointerstitium') \n",
    "      \n",
    "\n",
    "        \n",
    "def to_percent(temp, position): \n",
    "    return '%1.0f'%(100*temp) \n",
    "      \n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(to_percent)) \n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(to_percent)) \n",
    "plt.legend(loc=\"lower right\" ) \n",
    "      \n",
    "plt.show() \n",
    "\n",
    "\n",
    "\n",
    "#———————————————— \n",
    "#版权声明：本文为CSDN博主「山阴少年」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 \n",
    "#原文链接：https://blog.csdn.net/jclian91/article/details/103074506/ \n",
    "      \n",
    "from sklearn.metrics import confusion_matrix \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib as mpl \n",
    "      \n",
    "# 支持中文字体显示, 使用于Mac系统 \n",
    "#zhfont=mpl.font_manager.FontProperties(fname=\\ /Library/Fonts/Songti.ttc\\ ) \n",
    "      \n",
    "y_true=actuals# = ['北京', '上海', '成都', '成都', '上海', '北京', '上海', '成都', '北京', '上海'] \n",
    "y_pred=predictions #= ['北京', '上海', '成都', '上海', '成都', '成都', '上海', '成都', '北京', '上海'] \n",
    "      \n",
    "#ValueError: multilabel-indicator is not supported \n",
    "      \n",
    "classes = [0,1,2,3,4,5,6,7,8] \n",
    "#confusion = confusion_matrix(y_true, y_pred)#ValueError: multilabel-indicator is not supported \n",
    "      \n",
    "confusion = confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1)) \n",
    "# 绘制热度图 \n",
    "plt.imshow(confusion, cmap=plt.cm.Greens) \n",
    "indices = range(len(confusion)) \n",
    "#plt.xticks(indices, classes, fontproperties=zhfont) \n",
    "#plt.yticks(indices, classes, fontproperties=zhfont) \n",
    "      \n",
    "plt.xticks(indices, classes) \n",
    "plt.yticks(indices, classes) \n",
    "plt.colorbar() \n",
    "plt.xlabel('y_pred') \n",
    "plt.ylabel('y_true') \n",
    "      \n",
    "# 显示数据 \n",
    "for first_index in range(len(confusion)): \n",
    "    for second_index in range(len(confusion[first_index])): \n",
    "        plt.text(first_index, second_index, confusion[first_index][second_index]) \n",
    "      \n",
    "# 显示图片 \n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
