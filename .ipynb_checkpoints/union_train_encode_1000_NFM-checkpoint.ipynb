{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e049a2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import Trainer\n",
    "from network import NFM\n",
    "import torch.utils.data as Data\n",
    "from Utils.criteo_loader import getTestData, getTrainData\n",
    "\n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':9,\n",
    "    'linear_hidden1':1000,\n",
    "    'linear_hidden':100,#线性模型输出层（隐层个数）\n",
    "    'embed_input_dim':1001,#embed输入维度\n",
    "    'embed_dim': 100, # 用于控制稀疏特征经过Embedding层后的稠密特征大小，embed输出维度\n",
    "    #'dnn_hidden_units': [100,11],#MLP隐层和输出层\n",
    "    \n",
    "    'dnn_hidden_units':[9],#MLP隐层\n",
    "    'num_sparse_features_cols':10477,#the number of the gene columns\n",
    "    'num_dense_features': 0,#dense features number\n",
    "    'bi_dropout': 0.3,#Bi-Interaction 的dropout\n",
    "    'num_epoch': 500,#训练epoch次数\n",
    "    'batch_size': 24,#batch_size\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    'epoch':1000,\n",
    "    \n",
    "    #'train_file': '../Data/criteo/processed_data/train_set.csv',\n",
    "    #'fea_file': '../Data/criteo/processed_data/fea_col.npy',\n",
    "    #'validate_file': '../Data/criteo/processed_data/val_set.csv',\n",
    "    #'test_file': '../Data/criteo/processed_data/test_set.csv',\n",
    "    #'model_name': '../TrainedModels/NFM.model'\n",
    "    #'train_file':'data/xiaoqiu_gene_5000/train/final_5000_encode_100x.csv',\n",
    "    'train_data':'dataset/qiuguan/encode/encode_1000/train/train_encode_data_1000.csv',\n",
    "    'train_label':'dataset/qiuguan/non_code/train/train_label.csv',\n",
    "    #'test_data':'dataset/qiuguan/non_code/test/test_encode_data.csv',\n",
    "    'test_data':'dataset/qiuguan/encode/encode_1000/test/test_encode_data_1000_new.csv',\n",
    "    'test_label':'dataset/qiuguan/non_code/test/test_labels.csv'\n",
    "    #'title':'dataset/xiaoguan/RF/RF_for_train/train_class_9/test/test_data.csv',\n",
    "    \n",
    "    #'all':''\n",
    "    #'title':'data/xiaoqiu_gene_5000/train/gene_5000_gene_name.csv',\n",
    "    #'all':'data/xiaoqiu_gene_5000/train/gene_5000_label_name.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80120470",
   "metadata": {},
   "outputs": [],
   "source": [
    "#准备训练集\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "#from utils import \n",
    "#准备训练集\n",
    "#from new_dataset_processed import FMData\n",
    "from dataset_process import FMData\n",
    "def prepare_dataset(m_data,m_label,batch_size,n_class):\n",
    "    m_dataset=FMData(m_data,m_label,n_class)\n",
    "    m_dataloader=data.DataLoader(m_dataset, drop_last=True,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "    \n",
    "    return m_dataset,m_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f8094ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import sys \n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "import torchmetrics\n",
    "def   train_data(model,train_loader,test_loader,batch_size,model_path):\n",
    "    #train_accuracy=torchmetrics.Accuracy()\n",
    "    #test_accuracy=torchmetrics.Accuracy()\n",
    "    BATCH_SIZE=batch_size\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    total = 0\n",
    "    \n",
    "    #loss_func = torch.nn.BCELoss()\n",
    "    loss_func=torch.nn.CrossEntropyLoss()\n",
    "    #loss_func=nn.MultiLabelSoftMarginLoss()\n",
    "    #loss_func=torch.nn.LogSoftmax()\n",
    "    num=1\n",
    "    #model=nn.Softmax(nn.Linear(10149,16)).to(device)\n",
    "    # 从DataLoader中获取小批量的id以及数据\n",
    "    for epoch_id in range(1000):\n",
    "        correct=0\n",
    "        total=0\n",
    "        total_test_acc=0\n",
    "        \n",
    "        for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "            x = Variable(x)\n",
    "            labels = Variable(labels)\n",
    "            \n",
    "            \n",
    "            #x = torch.tensor(x, dtype=torch.float)\n",
    "            #x=x.clone().detach().requires_grad_(True)\n",
    "            x=torch.tensor(x,dtype=torch.float)\n",
    "            labels=torch.tensor(labels,dtype=torch.float)\n",
    "            x, labels = x.cuda(), labels.cuda()\n",
    "            labels_int=labels=torch.max(labels,1)[1]\n",
    "            #labels_int.cuda()\n",
    "            #print(\"labels:\",labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_predict = model(x)\n",
    "            #print(\"y_predict:\",y_predict)\n",
    "            #loss = loss_func(y_predict.view(-1), labels)\n",
    "            loss = loss_func(y_predict, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss = loss.item()\n",
    "            #loss, predicted = self._train_single_batch(x, labels)\n",
    "\n",
    "            total += loss\n",
    "            \n",
    "            \n",
    "            #predicted = torch.max(y_predict.data,1)\n",
    "            #print(\"predicted:\",predicted)\n",
    "            #predicted = torch.max(y_predict.data,1)[1]\n",
    "            #predicted=predicted.detach().cpu().numpy()\n",
    "            \n",
    "            #labels=torch.max(labels,1)\n",
    "            #print(\"labels:\",labels)\n",
    "            #labels=labels[1]\n",
    "            #y_predict=y_predict.cuda()\n",
    "            #labels=torch.max(labels,1)[1].cuda()\n",
    "            #labels=labels.detach().cpu().numpy()\n",
    "            #correct += (predicted == labels).sum()\n",
    "            #print(\"correct:\",correct)\n",
    "            #correct=correct[0]\n",
    "            #print(\"new_correct:\",float(correct))\n",
    "            #correct=float(correct)   \n",
    "            #if batch_idx % 10 == 0:\n",
    "            #print(\"batch_idx:\",batch_idx)\n",
    "            #print(correct/(BATCH_SIZE*(batch_idx+1)))\n",
    "            batch_train_acc=torchmetrics.functional.accuracy(y_predict,labels_int)\n",
    "            #print('batch_train_acc:',batch_train_acc)\n",
    "        #total_train_accuracy=torchmetrics.functional.compute_details()\n",
    "        #print('total_train_accuracy:',total_train_accuracy)\n",
    "        for i , (inputs , targets) in enumerate(test_loader):   \n",
    "            # evaluate the model on the test set   \n",
    "            #print(\\ inputs:\\  inputs)   \n",
    "            #print(\\ targets:\\  targets)   \n",
    "            inputs = Variable(inputs)   \n",
    "            targets = Variable(targets)     \n",
    "            #x = torch.tensor(x  dtype=torch.float)   \n",
    "            #x=x.clone().detach().requires_grad_(True)   \n",
    "            inputs=torch.tensor(inputs ,dtype=torch.float)   \n",
    "            targets=torch.tensor(targets ,dtype=torch.float)   \n",
    "            inputs , targets = inputs.cuda(),  targets.cuda()   \n",
    "            yhat = model(inputs)  \n",
    "            \n",
    "            #yhat = torch.max(yhat.data,1)[1]\n",
    "            #yhat=yhat.detach().cpu().numpy()\n",
    "            #print(\"predicted:\",predicted)\n",
    "            #predicted = torch.max(y_predict.data,1)[1]\n",
    "             #predicted = torch.max(y_predict.data,1)[1]\n",
    "            \n",
    "            \n",
    "            targets=torch.max(targets,1)[1]\n",
    "            #print(\"labels:\",labels)\n",
    "            #labels=labels[1]\n",
    "            #targets=targets.detach().cpu().numpy()\n",
    "            \n",
    "            \n",
    "            \n",
    "            batch_test_acc=torchmetrics.functional.accuracy(yhat,targets)\n",
    "            #print(\"batch_test_acc:\",batch_test_acc)\n",
    "            total_test_acc+=batch_test_acc\n",
    "            #total_test_accuracy=torchmetrics.functional.compute_details()\n",
    "        print('total_test_accuracy:',total_test_acc/(i+1))\n",
    "        \n",
    "                    \n",
    "                    \n",
    "            \n",
    "            #model.evaluate()\n",
    "            #model.eval()\n",
    "            #train_result = evaluate.metrics(model, train_loader)\n",
    "            #valid_result = evaluate.metrics(model, valid_loader)\n",
    "            #est_result = evaluate.metrics(model, test_loader)\n",
    "            #acturals,predictions,acc_test=evaluate_model(test_loader,model)\n",
    "            #print(\"acc_test:  %d  \" %(acc_test))\n",
    "            #print(\"Train_RMSE: {:.3f}, Valid_RMSE: {:.3f}, Test_RMSE: {:.3f}\".format(\n",
    "            #train_result, valid_result, test_result))\n",
    "            # print('[Training Epoch: {}] Batch: {}, Loss: {}'.format(epoch_id, batch_id, loss))\n",
    "        print(\"Training Epoch: %d, total loss: %f\" % (epoch_id, total))\n",
    "        #print(\"auc:\",roc_auc_score)\n",
    "    #功能：保存训练完的网络的各层参数（即weights和bias)\n",
    "    #path='dataset/xiaoguan/RF/RF_for_train/train_class_9/model/gene_4000_NFM.pkl'\n",
    "        if epoch_id %100==0:\n",
    "            num=num+1\n",
    "            path=os.path.join(model_path,'NMF'+str(num)+'.pkl')\n",
    "            torch.save(model.state_dict(),path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "725dcb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMData\n",
      "FMData\n",
      "NFM: NFM(\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (BN_num): BatchNorm1d(10477, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear_model1): Linear(in_features=10477, out_features=1000, bias=True)\n",
      "  (BN_linear1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear_model2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (BN_linear2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (embedding_layers): Embedding(1001, 100)\n",
      "  (bi_pooling): BiInteractionPooling()\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (BN_bi): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dnn_layers): ModuleList(\n",
      "    (0): Linear(in_features=200, out_features=9, bias=True)\n",
      "  )\n",
      "  (dnn_softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_test_accuracy: tensor(0.1250, device='cuda:0')\n",
      "Training Epoch: 0, total loss: 56.841201\n",
      "total_test_accuracy: tensor(0.0833, device='cuda:0')\n",
      "Training Epoch: 1, total loss: 56.101863\n",
      "total_test_accuracy: tensor(0.1250, device='cuda:0')\n",
      "Training Epoch: 2, total loss: 55.465230\n",
      "total_test_accuracy: tensor(0.1250, device='cuda:0')\n",
      "Training Epoch: 3, total loss: 54.612643\n",
      "total_test_accuracy: tensor(0.0833, device='cuda:0')\n",
      "Training Epoch: 4, total loss: 54.102857\n",
      "total_test_accuracy: tensor(0.1667, device='cuda:0')\n",
      "Training Epoch: 5, total loss: 53.626285\n",
      "total_test_accuracy: tensor(0.1667, device='cuda:0')\n",
      "Training Epoch: 6, total loss: 53.381176\n",
      "total_test_accuracy: tensor(0.2083, device='cuda:0')\n",
      "Training Epoch: 7, total loss: 52.810405\n",
      "total_test_accuracy: tensor(0.1667, device='cuda:0')\n",
      "Training Epoch: 8, total loss: 52.136481\n",
      "total_test_accuracy: tensor(0.1250, device='cuda:0')\n",
      "Training Epoch: 9, total loss: 51.542585\n",
      "total_test_accuracy: tensor(0.1250, device='cuda:0')\n",
      "Training Epoch: 10, total loss: 51.303467\n",
      "total_test_accuracy: tensor(0.1250, device='cuda:0')\n",
      "Training Epoch: 11, total loss: 50.908659\n",
      "total_test_accuracy: tensor(0.0417, device='cuda:0')\n",
      "Training Epoch: 12, total loss: 50.649840\n",
      "total_test_accuracy: tensor(0.0417, device='cuda:0')\n",
      "Training Epoch: 13, total loss: 50.053215\n",
      "total_test_accuracy: tensor(0., device='cuda:0')\n",
      "Training Epoch: 14, total loss: 49.882548\n",
      "total_test_accuracy: tensor(0., device='cuda:0')\n",
      "Training Epoch: 15, total loss: 49.529712\n",
      "total_test_accuracy: tensor(0., device='cuda:0')\n",
      "Training Epoch: 16, total loss: 49.171238\n",
      "total_test_accuracy: tensor(0.1250, device='cuda:0')\n",
      "Training Epoch: 17, total loss: 48.393428\n",
      "total_test_accuracy: tensor(0.0417, device='cuda:0')\n",
      "Training Epoch: 18, total loss: 47.850770\n",
      "total_test_accuracy: tensor(0., device='cuda:0')\n",
      "Training Epoch: 19, total loss: 47.744091\n",
      "total_test_accuracy: tensor(0., device='cuda:0')\n",
      "Training Epoch: 20, total loss: 46.867247\n",
      "total_test_accuracy: tensor(0.0417, device='cuda:0')\n",
      "Training Epoch: 21, total loss: 46.930951\n",
      "total_test_accuracy: tensor(0., device='cuda:0')\n",
      "Training Epoch: 22, total loss: 46.600166\n",
      "total_test_accuracy: tensor(0.0417, device='cuda:0')\n",
      "Training Epoch: 23, total loss: 45.647227\n",
      "total_test_accuracy: tensor(0.0417, device='cuda:0')\n",
      "Training Epoch: 24, total loss: 46.002493\n",
      "total_test_accuracy: tensor(0., device='cuda:0')\n",
      "Training Epoch: 25, total loss: 45.650091\n",
      "total_test_accuracy: tensor(0., device='cuda:0')\n",
      "Training Epoch: 26, total loss: 45.497483\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-88b9ad3bfe0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NFM:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnfm_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'dataset/qiuguan/model/NFM_encode_1000/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-9ff3c11273ab>\u001b[0m in \u001b[0;36mtrain_data\u001b[0;34m(model, train_loader, test_loader, batch_size, model_path)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;31m#loss = loss_func(y_predict.view(-1), labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset,train_loader=prepare_dataset(nfm_config['train_data'],nfm_config['train_label'],nfm_config['batch_size'],nfm_config['n_class'])\n",
    "test_dataset,test_loader=prepare_dataset(nfm_config['test_data'],nfm_config['test_label'],nfm_config['batch_size'],nfm_config['n_class'])\n",
    "#from MLP import MLP\n",
    "from new_nfm_network_batch_1 import NFM\n",
    "#model=MLP(4224,1000,100,9)\n",
    "model=NFM(nfm_config)\n",
    "model.cuda()\n",
    "print(\"NFM:\",model)\n",
    "train_data(model,train_loader,test_loader,nfm_config['batch_size'],'dataset/qiuguan/model/NFM_encode_1000/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63eeed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
