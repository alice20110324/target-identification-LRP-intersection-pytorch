{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe7e925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         RHOA      STX2     CISD1     WDR11     SCYL2   MGC2889     CCDC47  \\\n",
      "0   12.331477  8.843523  9.435926  9.009859  7.107385  5.293768  10.460777   \n",
      "1   12.180184  8.631155  9.379103  8.340756  6.553369  5.594961  10.318911   \n",
      "2   11.981354  8.782034  9.156410  8.503222  7.100889  5.332065  10.114696   \n",
      "3   12.273188  8.327899  9.887579  8.634472  6.890654  5.134499  10.151673   \n",
      "4   12.800277  8.702358  8.718542  9.629215  6.957518  5.043972  10.310961   \n",
      "..        ...       ...       ...       ...       ...       ...        ...   \n",
      "65  12.153250  8.550679  9.548931  8.448184  6.963025  5.560785  10.061710   \n",
      "66  12.500807  8.632233  9.365225  9.203636  7.629996  5.258812  10.790601   \n",
      "67  12.211943  8.845553  8.872259  8.688882  7.165949  5.274345   9.990952   \n",
      "68  12.455192  8.813249  8.660721  8.951007  6.919346  5.590529   9.808096   \n",
      "69  12.447406  8.842976  9.422864  8.981440  6.963361  5.232355  10.180057   \n",
      "\n",
      "        KLF8      CCL1   SLCO3A1  ...     PLAC4     NRBP1    LRRC23     SPHK2  \\\n",
      "0   5.970378  5.456362  8.010686  ...  6.545617  8.789065  6.501364  8.106674   \n",
      "1   6.195327  5.148213  7.918554  ...  6.787745  8.647064  6.287269  8.058970   \n",
      "2   5.949817  5.698111  7.596245  ...  6.718974  8.343611  6.578619  8.220582   \n",
      "3   5.911586  5.422056  7.463196  ...  6.638377  8.646308  6.760575  8.624396   \n",
      "4   5.687737  5.230122  8.119901  ...  6.351381  8.576762  5.930541  7.390278   \n",
      "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
      "65  5.838032  5.435461  7.367615  ...  6.817916  8.516984  6.326146  8.170894   \n",
      "66  6.247450  5.101022  7.697747  ...  6.569032  8.373829  6.509216  7.947656   \n",
      "67  5.962292  5.454457  8.012946  ...  6.687817  8.576306  6.297928  7.978047   \n",
      "68  6.147042  5.336929  7.800522  ...  6.744503  8.897809  6.326779  7.938734   \n",
      "69  6.093205  5.344227  8.107972  ...  6.772365  8.747005  6.168589  8.323465   \n",
      "\n",
      "    KIAA0513    SULT1A1       AMOT       CA1     GPR35  label  \n",
      "0   7.389121  11.585495  12.305785  6.852595  5.892521      2  \n",
      "1   7.682618  11.338129  12.995514  6.768581  5.560515      3  \n",
      "2   6.970959  10.238052  12.650006  7.420241  5.635434      5  \n",
      "3   7.149147  11.138058  13.057925  6.554545  5.726263      1  \n",
      "4   7.699917  10.730794  11.879904  6.327935  5.743072      8  \n",
      "..       ...        ...        ...       ...       ...    ...  \n",
      "65  7.434878  11.706813  12.598518  6.781732  5.566797      4  \n",
      "66  7.205314  10.600006  12.976043  8.518399  5.697897      6  \n",
      "67  7.433020  10.752438  12.603443  6.951718  5.589825      2  \n",
      "68  7.661181  11.715816  12.731128  7.054085  5.736978      7  \n",
      "69  7.353506  11.461577  13.064383  6.790510  5.548161      7  \n",
      "\n",
      "[70 rows x 3301 columns]\n",
      "3301\n",
      "[[12.52935     8.69328509  8.82544101 ...  6.68752395  5.55935423\n",
      "   4.        ]\n",
      " [12.52935     8.69328509  8.82544101 ...  6.68752395  5.55935423\n",
      "   4.        ]\n",
      " [12.52935     8.69328509  8.82544101 ...  6.68752395  5.55935423\n",
      "   4.        ]\n",
      " ...\n",
      " [12.52935     8.69328509  8.82544101 ...  6.68752395  5.55935423\n",
      "   4.        ]\n",
      " [12.52935     8.69328509  8.82544101 ...  6.68752395  5.55935423\n",
      "   4.        ]\n",
      " [12.52935     8.69328509  8.82544101 ...  6.68752395  5.55935423\n",
      "   4.        ]]\n",
      "        0         1         2         3         4         5          6     \\\n",
      "0   12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "1   12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "2   12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "3   12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "4   12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "5   12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "6   12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "7   12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "8   12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "9   12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "10  12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "11  12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "12  12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "13  12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "14  12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "15  12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "16  12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "17  12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "18  12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "19  12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "20  12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "21  12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "22  12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "23  12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "24  12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "25  12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "26  12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "27  12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "28  12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "29  12.52935  8.693285  8.825441  8.934728  6.852871  5.107134  10.393772   \n",
      "\n",
      "        7         8         9     ...      3291      3292      3293      3294  \\\n",
      "0   6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "1   6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "2   6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "3   6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "4   6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "5   6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "6   6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "7   6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "8   6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "9   6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "10  6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "11  6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "12  6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "13  6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "14  6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "15  6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "16  6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "17  6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "18  6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "19  6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "20  6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "21  6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "22  6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "23  6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "24  6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "25  6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "26  6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "27  6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "28  6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "29  6.110756  5.237322  7.739278  ...  6.583158  8.980383  6.222272  8.123986   \n",
      "\n",
      "       3295       3296       3297      3298      3299  3300  \n",
      "0   7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "1   7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "2   7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "3   7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "4   7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "5   7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "6   7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "7   7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "8   7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "9   7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "10  7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "11  7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "12  7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "13  7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "14  7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "15  7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "16  7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "17  7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "18  7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "19  7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "20  7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "21  7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "22  7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "23  7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "24  7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "25  7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "26  7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "27  7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "28  7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "29  7.24484  11.577387  13.084078  6.687524  5.559354   4.0  \n",
      "\n",
      "[30 rows x 3301 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "#为小球生成测试机\n",
    "\n",
    "xiaoguan='dataset/qiuguan/origin_800/xiaoguan/test_info.csv'\n",
    "\n",
    "xiaoqiu='dataset/qiuguan/origin_800/xiaoqiu/test_info.csv'\n",
    "\n",
    "\n",
    "xiaoqiu_df=pd.read_csv(xiaoqiu,sep=',')\n",
    "#print(xiaoguan_df)\n",
    "\n",
    "xiaoqiu_df=xiaoqiu_df.iloc[:,1:]\n",
    "print(xiaoqiu_df)\n",
    "\n",
    "row,col=xiaoqiu_df.shape\n",
    "print(col)\n",
    "\n",
    "xiaoqiu_1=xiaoqiu_df.iloc[37,:]#随机选择一条数据作为测试数据\n",
    "xiaoqiu_1=np.array(xiaoqiu_1).tolist()\n",
    "xiaoqiu_list=[xiaoqiu_1]*30  #20>batch_siza即可\n",
    "\n",
    "xiaoqiu_np=np.array(xiaoqiu_list)\n",
    "\n",
    "print(xiaoqiu_np)\n",
    "\n",
    "\n",
    "xiaoqiu_np=xiaoqiu_np.reshape(30,-1)\n",
    "xiaoqiu_predict_1=pd.DataFrame(xiaoqiu_np)\n",
    "\n",
    "print(xiaoqiu_predict_1)\n",
    "\n",
    "xiaoqiu_predict_1.to_csv('dataset/qiuguan/origin_800/xiaoqiu/xiaoqiu_predict_21.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4061c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         FKBP4      TFAM     ABHD2   NPIPB15     COX15      PTMS      UCHL1  \\\n",
      "0     6.808092  5.624190  5.595234  6.132185  6.530964  5.375144   8.496782   \n",
      "1     6.641582  6.202200  5.603923  5.308529  7.065684  6.484144   8.170323   \n",
      "2     6.821543  6.164184  5.785243  6.037070  7.152982  6.511146   8.360073   \n",
      "3     6.665864  6.193666  5.807002  6.162345  7.123589  5.996546   8.463521   \n",
      "4     6.959879  5.886715  5.742542  5.848354  7.222280  5.857219   8.636164   \n",
      "...        ...       ...       ...       ...       ...       ...        ...   \n",
      "1664  7.184328  6.126787  5.751840  6.324176  7.437429  5.961235  10.629588   \n",
      "1665  7.019760  5.907867  6.000821  5.775343  7.199000  6.452712  10.814609   \n",
      "1666  6.978300  5.772554  5.841500  7.117535  6.885854  6.036706  10.242497   \n",
      "1667  6.731515  5.971825  5.814995  5.731193  7.007962  6.354492  10.938556   \n",
      "1668  7.203295  6.056311  5.886820  6.879557  7.000424  5.970385   9.979768   \n",
      "\n",
      "          CTSG       VDR      ACP6  ...     CERS4  COL4A3BP     PLIN3  \\\n",
      "0     6.054636  7.002407  8.005660  ...  6.004288  6.468106  8.325981   \n",
      "1     5.371616  7.314689  8.242318  ...  6.414594  7.295923  7.841126   \n",
      "2     5.685575  7.575833  8.261513  ...  6.418065  6.999965  7.305420   \n",
      "3     5.290182  7.194940  8.499327  ...  5.949800  7.084352  8.452961   \n",
      "4     5.122571  7.099685  8.284310  ...  6.364236  7.048875  8.068111   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "1664  6.216844  7.659646  9.048781  ...  6.606350  6.834463  7.902296   \n",
      "1665  6.080980  7.114507  9.378890  ...  6.100273  7.222445  8.354266   \n",
      "1666  6.685548  7.196289  8.772293  ...  6.411118  7.740680  7.993595   \n",
      "1667  5.642836  6.208216  8.461981  ...  6.183360  6.850929  8.182654   \n",
      "1668  6.105391  6.372052  8.832374  ...  6.177457  7.593792  8.150405   \n",
      "\n",
      "          GATC    NDUFA13      KLF6  SERPINF1     ABCB7       TYR  label  \n",
      "0     5.142430  10.879327  7.867662  8.674624  6.997677  5.077186      1  \n",
      "1     5.471612  11.260533  7.349496  7.034764  7.795220  4.735406      8  \n",
      "2     5.426000  11.142123  6.992356  6.420298  7.763288  4.974667      8  \n",
      "3     5.487765  11.229367  8.267902  7.151481  7.468115  4.892318      8  \n",
      "4     5.489613  11.272573  8.009211  7.233777  7.625618  4.864379      7  \n",
      "...        ...        ...       ...       ...       ...       ...    ...  \n",
      "1664  5.917893  10.851395  8.138818  6.567540  8.088932  5.483882      1  \n",
      "1665  6.061786  10.506820  8.411821  7.813725  7.935141  5.789583      7  \n",
      "1666  6.285592  10.494058  8.729343  7.947933  7.461682  5.732754      1  \n",
      "1667  5.909734  10.514278  8.036206  6.568491  7.795616  5.572172      4  \n",
      "1668  5.908952  10.123153  8.394462  5.884485  7.431873  5.898274      7  \n",
      "\n",
      "[1669 rows x 91 columns]\n"
     ]
    }
   ],
   "source": [
    "column_df=pd.read_csv('dataset/qiuguan/origin_800/LRP/10/selected_train_val_info1.csv',sep=',')\n",
    "column_df=column_df.iloc[:,1:]\n",
    "print(column_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69f86d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       FKBP4      TFAM     ABHD2   NPIPB15     COX15      PTMS      UCHL1  \\\n",
      "0   7.110070  6.337579  5.705721  6.528007  7.278390  6.035703   9.684451   \n",
      "1   6.797773  6.302931  5.831167  6.687784  7.186308  7.197280  10.569313   \n",
      "2   7.119657  6.133594  5.589718  6.557382  7.415391  6.833058   9.239665   \n",
      "3   6.914884  6.246529  5.658995  6.787315  7.437601  6.410058  10.990221   \n",
      "4   6.953725  6.069555  5.899024  6.102264  7.070516  5.911552   9.820428   \n",
      "..       ...       ...       ...       ...       ...       ...        ...   \n",
      "65  7.564141  6.228094  5.844149  6.688880  7.085689  5.988048  10.175780   \n",
      "66  6.914673  6.340505  5.778528  6.501725  6.946438  6.318769  11.739254   \n",
      "67  6.971611  5.996537  5.836217  6.298996  7.154493  6.449517  10.115445   \n",
      "68  7.173464  5.924999  5.754540  6.470542  6.890282  7.086770   9.847623   \n",
      "69  6.719214  6.186343  5.912917  6.010040  7.066445  5.807227  10.445798   \n",
      "\n",
      "        CTSG       VDR      ACP6  ...     CERS4  COL4A3BP     PLIN3      GATC  \\\n",
      "0   5.932706  6.851948  9.319774  ...  6.402708  6.638037  8.128645  6.100450   \n",
      "1   6.434307  6.700446  9.354278  ...  6.433644  7.224307  7.757857  5.925116   \n",
      "2   6.782693  7.447782  8.929381  ...  6.606591  7.046553  7.645467  5.621696   \n",
      "3   6.221484  7.460157  9.628765  ...  6.638408  6.476203  7.719151  6.446160   \n",
      "4   6.965907  6.265641  8.545858  ...  6.290822  7.400147  8.176679  5.842290   \n",
      "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
      "65  6.434266  7.595398  8.527816  ...  6.268772  7.639723  7.713472  5.949211   \n",
      "66  6.250127  6.998582  8.715050  ...  6.131933  7.910430  8.332066  5.836711   \n",
      "67  6.096747  7.088669  9.123012  ...  6.138635  6.953524  7.960138  5.843396   \n",
      "68  6.221733  6.205325  8.610591  ...  6.505262  6.946532  8.018944  5.797763   \n",
      "69  6.193461  6.697153  9.074114  ...  6.213974  7.249112  7.975034  6.075876   \n",
      "\n",
      "      NDUFA13      KLF6   SERPINF1     ABCB7       TYR  label  \n",
      "0   11.183721  8.759146   6.183483  7.420817  5.508283      2  \n",
      "1   10.766246  8.601619   7.875191  7.387450  5.997590      3  \n",
      "2   10.673001  7.446008   6.352353  7.974823  5.779269      5  \n",
      "3   11.610960  7.801069   7.834025  8.088981  5.592154      1  \n",
      "4   10.659769  8.129258   6.024286  7.726273  5.146571      8  \n",
      "..        ...       ...        ...       ...       ...    ...  \n",
      "65  10.865408  8.291233   7.565509  7.456950  5.920124      4  \n",
      "66  10.628898  8.261688  10.168167  7.707366  5.614699      6  \n",
      "67  10.427410  8.293950   6.007505  7.612996  5.495158      2  \n",
      "68  10.885664  7.611571   6.431923  7.386504  5.456614      7  \n",
      "69  11.224866  8.660324   8.200495  7.581684  5.459156      7  \n",
      "\n",
      "[70 rows x 91 columns]\n"
     ]
    }
   ],
   "source": [
    "xiaoqiu_iga_df=pd.read_csv('dataset/qiuguan/origin_800/xiaoqiu/test_info.csv',sep=',')\n",
    "xiaoqiu_iga_df=xiaoqiu_iga_df.iloc[:,1:]\n",
    "new_xiaoqiu_iga_df=xiaoqiu_iga_df.loc[:,column_df.columns]\n",
    "print(new_xiaoqiu_iga_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e4ef6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       FKBP4      TFAM     ABHD2   NPIPB15     COX15      PTMS     UCHL1  \\\n",
      "0   6.881242  5.876360  5.792805  6.057637  7.212210  6.442146  7.507606   \n",
      "1   6.819875  6.581598  5.678784  5.771402  7.524736  6.676393  7.208212   \n",
      "2   6.825461  6.113709  5.925597  6.005068  7.273506  5.450284  8.738864   \n",
      "3   6.859129  6.038464  5.799086  6.036775  7.376509  6.550323  8.410947   \n",
      "4   6.863772  5.879816  5.800650  5.915405  7.161176  5.579128  8.185107   \n",
      "..       ...       ...       ...       ...       ...       ...       ...   \n",
      "63  6.612051  6.199438  5.841652  5.527375  6.920705  6.412566  8.182697   \n",
      "64  6.486771  5.898276  5.758260  5.850797  6.668784  5.820606  8.061573   \n",
      "65  6.536128  5.816890  5.899146  6.129290  6.803191  5.355849  8.884517   \n",
      "66  6.495212  6.033539  5.762620  5.665052  7.052295  5.772547  8.100537   \n",
      "67  6.814442  5.785833  5.839433  6.151976  6.957745  6.034978  8.363636   \n",
      "\n",
      "        CTSG       VDR      ACP6  ...     CERS4  COL4A3BP     PLIN3      GATC  \\\n",
      "0   5.765572  7.031085  9.053964  ...  6.186815  6.302252  7.521818  5.626470   \n",
      "1   5.299804  7.606546  8.587288  ...  6.322620  6.792881  8.099933  5.224100   \n",
      "2   5.103280  7.436950  8.357921  ...  6.296113  7.092513  7.916476  5.328091   \n",
      "3   4.946225  7.619619  8.388690  ...  6.304612  6.760716  7.830793  5.779973   \n",
      "4   5.225079  7.109112  8.307430  ...  6.462658  6.710912  8.174644  5.399948   \n",
      "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
      "63  5.502628  7.385183  8.529182  ...  6.970369  7.422914  8.262480  5.443624   \n",
      "64  5.393714  6.548465  8.006085  ...  5.880195  6.081732  8.661869  5.367274   \n",
      "65  5.479488  7.097717  8.518766  ...  6.210032  6.881127  8.242759  5.067294   \n",
      "66  5.963075  6.545498  8.335803  ...  5.972160  7.071445  7.996618  5.257948   \n",
      "67  5.282247  7.195456  8.092714  ...  6.216019  6.593389  7.513486  5.367615   \n",
      "\n",
      "      NDUFA13      KLF6  SERPINF1     ABCB7       TYR  label  \n",
      "0   10.783968  6.703100  6.529161  7.870405  5.520970      5  \n",
      "1   11.094085  6.224870  5.360009  7.756287  5.170212      5  \n",
      "2   10.975348  7.346221  8.972016  7.790527  4.981714      8  \n",
      "3   11.082304  7.407574  8.191479  8.110975  4.886192      6  \n",
      "4   11.148620  7.563791  8.055781  7.582556  5.022282      6  \n",
      "..        ...       ...       ...       ...       ...    ...  \n",
      "63  11.083506  7.419883  8.086343  7.898128  4.827507      6  \n",
      "64  11.289033  8.367244  9.598092  7.478645  4.738063      2  \n",
      "65  11.020766  8.354929  8.272012  7.725589  5.149915      7  \n",
      "66  11.188018  8.683612  8.879659  7.393249  4.902857      8  \n",
      "67  10.877267  8.731109  6.389798  7.440256  4.805638      1  \n",
      "\n",
      "[68 rows x 91 columns]\n"
     ]
    }
   ],
   "source": [
    "xiaoguan_iga_df=pd.read_csv('dataset/qiuguan/origin_800/xiaoguan/test_info.csv',sep=',')\n",
    "xiaoguan_iga_df=xiaoguan_iga_df.iloc[:,1:]\n",
    "new_xiaoguan_iga_df=xiaoguan_iga_df.loc[:,column_df.columns]\n",
    "print(new_xiaoguan_iga_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dc15dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "14\n",
      "23\n",
      "28\n",
      "30\n",
      "31\n",
      "41\n",
      "49\n",
      "52\n",
      "55\n",
      "[[6.97430823 6.32138181 5.74226826 ... 8.09121644 4.87862167 4.        ]\n",
      " [6.97430823 6.32138181 5.74226826 ... 8.09121644 4.87862167 4.        ]\n",
      " [6.97430823 6.32138181 5.74226826 ... 8.09121644 4.87862167 4.        ]\n",
      " ...\n",
      " [6.97430823 6.32138181 5.74226826 ... 8.09121644 4.87862167 4.        ]\n",
      " [6.97430823 6.32138181 5.74226826 ... 8.09121644 4.87862167 4.        ]\n",
      " [6.97430823 6.32138181 5.74226826 ... 8.09121644 4.87862167 4.        ]]\n",
      "          0         1         2         3        4         5         6   \\\n",
      "0   6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "1   6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "2   6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "3   6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "4   6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "5   6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "6   6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "7   6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "8   6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "9   6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "10  6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "11  6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "12  6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "13  6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "14  6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "15  6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "16  6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "17  6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "18  6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "19  6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "20  6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "21  6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "22  6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "23  6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "24  6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "25  6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "26  6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "27  6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "28  6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "29  6.974308  6.321382  5.742268  5.823195  7.25712  6.214127  9.069042   \n",
      "\n",
      "          7         8         9   ...        81        82        83        84  \\\n",
      "0   5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "1   5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "2   5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "3   5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "4   5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "5   5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "6   5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "7   5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "8   5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "9   5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "10  5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "11  5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "12  5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "13  5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "14  5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "15  5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "16  5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "17  5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "18  5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "19  5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "20  5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "21  5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "22  5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "23  5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "24  5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "25  5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "26  5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "27  5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "28  5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "29  5.536047  7.246773  8.047216  ...  6.322234  6.282628  8.090128  5.261966   \n",
      "\n",
      "           85        86        87        88        89   90  \n",
      "0   11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "1   11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "2   11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "3   11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "4   11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "5   11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "6   11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "7   11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "8   11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "9   11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "10  11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "11  11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "12  11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "13  11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "14  11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "15  11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "16  11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "17  11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "18  11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "19  11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "20  11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "21  11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "22  11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "23  11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "24  11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "25  11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "26  11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "27  11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "28  11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "29  11.110217  7.621563  7.388004  8.091216  4.878622  4.0  \n",
      "\n",
      "[30 rows x 91 columns]\n"
     ]
    }
   ],
   "source": [
    "rows,cols=new_xiaoguan_iga_df.shape\n",
    "#x=new_xiaoguan_iga_df.iloc[i,-1]\n",
    "#print(x)\n",
    "for i in range(rows):\n",
    "    if int(new_xiaoguan_iga_df.iloc[i,-1])==4:\n",
    "        print(i)\n",
    "        \n",
    "#8\n",
    "#14\n",
    "#23\n",
    "#28\n",
    "#30\n",
    "#31\n",
    "#41\n",
    "#49\n",
    "#52\n",
    "#55        \n",
    "        \n",
    "        \n",
    "#生成小管预测数据集：\n",
    "xiaoguan_1=new_xiaoguan_iga_df.iloc[55,:]#随机选择一条数据作为测试数据\n",
    "xiaoguan_1=np.array(xiaoguan_1).tolist()\n",
    "xiaoguan_list=[xiaoguan_1]*30  #20>batch_siza即可\n",
    "\n",
    "xiaoguan_np=np.array(xiaoguan_list)\n",
    "\n",
    "print(xiaoguan_np)\n",
    "\n",
    "\n",
    "xiaoguan_np=xiaoguan_np.reshape(30,-1)\n",
    "xiaoguan_predict_1=pd.DataFrame(xiaoguan_np)\n",
    "\n",
    "print(xiaoguan_predict_1)\n",
    "\n",
    "xiaoguan_predict_1.to_csv('dataset/qiuguan/origin_800/xiaoguan/xiaoguan_predict_55.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7f6a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25dc1398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (bn0): BatchNorm1d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=90, out_features=1000, bias=True)\n",
      "  (bn1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=100, out_features=9, bias=True)\n",
      "  (bn3): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "(9, 329)\n",
      "bn0.weight : torch.Size([90])\n",
      "bn0.bias : torch.Size([90])\n",
      "fc1.weight : torch.Size([1000, 90])\n",
      "fc1.bias : torch.Size([1000])\n",
      "bn1.weight : torch.Size([1000])\n",
      "bn1.bias : torch.Size([1000])\n",
      "fc2.weight : torch.Size([100, 1000])\n",
      "fc2.bias : torch.Size([100])\n",
      "bn2.weight : torch.Size([100])\n",
      "bn2.bias : torch.Size([100])\n",
      "fc3.weight : torch.Size([9, 100])\n",
      "fc3.bias : torch.Size([9])\n",
      "bn3.weight : torch.Size([9])\n",
      "bn3.bias : torch.Size([9])\n",
      "relevance\n"
     ]
    }
   ],
   "source": [
    "##############  u===2   MLP_encode_100 and MLP_encode_1000  :654    ##################\n",
    "\n",
    "#################model with F10\n",
    "import torch\n",
    "#import Trainer\n",
    "from network import NFM\n",
    "import torch.utils.data as Data\n",
    "from Utils.criteo_loader import getTestData, getTrainData\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import sys \n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from mnist_test import Net, train, test\n",
    "\n",
    "\n",
    "# Network parameters\n",
    "class Params(object):\n",
    "    batch_size = 64\n",
    "    test_batch_size = 20\n",
    "    epochs = 5\n",
    "    lr = 0.01\n",
    "    momentum = 0.5\n",
    "    no_cuda = True\n",
    "    seed = 1\n",
    "    log_interval = 10\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "args = Params()\n",
    "torch.manual_seed(args.seed)\n",
    "#device = torch.device(\"cpu\")\n",
    "device=torch.device('cuda')\n",
    "kwargs = {}\n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':9,\n",
    "    'linear_hidden1':2000,\n",
    "    \n",
    "    \n",
    "    'dnn_hidden_units':[100,8],#MLP隐层\n",
    "    'num_sparse_features_cols':289,#the number of the gene columns\n",
    "    'num_dense_features': 0,#dense features number\n",
    "    'bi_dropout': 0.5,#Bi-Interaction 的dropout\n",
    "    'num_epoch': 500,#训练epoch次数\n",
    "    'batch_size': 16,#batch_size\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    'epoch':1000,\n",
    "    \n",
    "    \n",
    "    'gene_name':'dataset/qiuguan/origin_800/gene_name.csv',\n",
    "    'label_name':'dataset/qiuguan/origin_800/gene_label.csv'\n",
    "    \n",
    "}\n",
    "\n",
    "#model definition\n",
    "import torch.nn as nn\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn0=nn.BatchNorm1d(90)\n",
    "        self.fc1 = nn.Linear(90, 1000)\n",
    "        self.bn1= nn.BatchNorm1d(1000)\n",
    "        self.fc2 = nn.Linear(1000, 100)\n",
    "        self.bn2=nn.BatchNorm1d(100)\n",
    "        self.fc3=nn.Linear(100,9)\n",
    "        self.bn3=nn.BatchNorm1d(9)\n",
    "        \n",
    "        self.drop=nn.Dropout(0.3)\n",
    "    def forward(self, x):\n",
    "        x=self.bn0(x)\n",
    "        x = F.relu(self.drop(self.bn1(self.fc1(x))))\n",
    "        x = F.relu(self.drop(self.bn2(self.fc2(x))))\n",
    "        return F.softmax(self.bn3(self.fc3(x)), dim=1)\n",
    "model = MLP().cuda()\n",
    "print(model)\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import torch.nn.functional as F  # 激励函数的库\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "\n",
    "def one_hot_smoothing(labels, classes, label_smoothing=0.2):\n",
    "    #n = len(labels)\n",
    "    n=labels.shape[0]\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    n = len(labels)\n",
    "    #eoff = label_smoothing / classes\n",
    "    output = np.zeros((n, classes), dtype=np.float32)\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_smoothing(labels, classes, label_smoothing=0.2):\n",
    "    #n = len(labels)\n",
    "    n=labels.shape[0]\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    n = len(labels)\n",
    "    #eoff = label_smoothing / classes\n",
    "    output = np.zeros((n, classes), dtype=np.float32)\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "class KZDatasetTest(data.Dataset):\n",
    "    \"\"\" Construct the FM pytorch dataset. \"\"\"\n",
    "    #def __init__(self, file,label_file, feature_map,n_class=16):\n",
    "    def __init__(self, csv_path):\n",
    "    \n",
    "       \n",
    "        self.data_info = self.get_data_info(csv_path)\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Dataset读取图片的函数\n",
    "        data, label = self.data_info[index]\n",
    "        #img = Image.open(img_pth).convert('RGB')\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_data_info(self,csv_path):\n",
    "        #解析路径\n",
    "        #转为一维list存储，每一位为【图片路径，图片类别】\n",
    "        labels=[]\n",
    "        data_info=[]\n",
    "        df=pd.read_csv(csv_path,sep=',')\n",
    "        #print(\"df:\",df)\n",
    "        df=df.iloc[:,1:]\n",
    "        #print(\"df:\",df)\n",
    "        #print(df.iloc[:,-1])\n",
    "        #df=df.applymap(ast.literal_eval)\n",
    "        rows,cols=df.shape\n",
    "        #print(rows,cols)\n",
    "        for i in df.iloc[:,-1]:\n",
    "            #print(i)\n",
    "            labels.append(int(float(i)))\n",
    "        #print('labels:',labels)\n",
    "        labels=np.array(labels)\n",
    "        #print('labels:',labels)\n",
    "        #labels=np.array(labels)\n",
    "        labels=one_hot_smoothing(labels,nfm_config['n_class'])\n",
    "        #print(labels)\n",
    "        for i in range(rows):\n",
    "            data=df.iloc[i,:-1]\n",
    "            data=data.astype(float)##############\n",
    "            #print(\"i,data:\",i,data)\n",
    "            #data=pd.DataFrame(data,dtype=float)###############\n",
    "            data=np.array(data)##\n",
    "            \n",
    "            label=labels[i]\n",
    "            #print(data.shape)\n",
    "            #print(label.shape)\n",
    "            #label=label.tolist()\n",
    "            data=torch.from_numpy(data)#\n",
    "            label=torch.from_numpy(label)#\n",
    "            \n",
    "            \n",
    "            data_info.append((data,label))\n",
    "        return data_info\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import *\n",
    "from PIL import Image\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ast\n",
    "import torchvision\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "class KZDataset(Dataset):\n",
    "    def __init__(self, csv_path, K,n_class,ki=0, typ='train', transform=None, rand=False):\n",
    "       \n",
    "        self.all_data_info = self.get_data_info(csv_path)\n",
    "        \n",
    "        if rand:\n",
    "            random.seed(1)\n",
    "            random.shuffle(self.all_data_info)\n",
    "        leng = len(self.all_data_info)\n",
    "        every_z_len = leng // K\n",
    "        if typ == 'val':\n",
    "            self.data_info = self.all_data_info[every_z_len * ki : every_z_len * (ki+1)]\n",
    "        elif typ == 'train':\n",
    "            self.data_info = self.all_data_info[: every_z_len * ki] + self.all_data_info[every_z_len * (ki+1) :]\n",
    "            \n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Dataset读取图片的函数\n",
    "        data, label = self.data_info[index]\n",
    "        #img = Image.open(img_pth).convert('RGB')\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "   \n",
    "    \n",
    "    \n",
    "    def get_data_info(self,csv_path):\n",
    "        #解析路径\n",
    "        #转为一维list存储，每一位为【图片路径，图片类别】\n",
    "        labels=[]\n",
    "        data_info=[]\n",
    "        df=pd.read_csv(csv_path,sep=',')\n",
    "        #print(\"df:\",df)\n",
    "        df=df.iloc[:,1:]\n",
    "        #print(\"df:\",df)\n",
    "        print(df.shape)\n",
    "        #print(\"df:\",df)\n",
    "        #print(df.iloc[:,-1])\n",
    "        #df=df.applymap(ast.literal_eval)\n",
    "        rows,cols=df.shape\n",
    "        #print(rows,cols)\n",
    "        for i in df.iloc[:,-1]:\n",
    "            #print(i)\n",
    "            labels.append(int(float(i)))\n",
    "        #print('labels:',i,labels)\n",
    "        labels=np.array(labels)\n",
    "        #print('labels:',labels)\n",
    "        #labels=np.array(labels)\n",
    "        labels=one_hot_smoothing(labels,nfm_config['n_class'])\n",
    "        #print(labels)\n",
    "        for i in range(rows):\n",
    "            data=df.iloc[i,:-1]\n",
    "            data=data.astype(float)##############\n",
    "            #print(\"i,data:\",i,data)\n",
    "            #data=pd.DataFrame(data,dtype=float)###############\n",
    "            data=np.array(data)##\n",
    "            \n",
    "            label=labels[i]\n",
    "            #print(data.shape)\n",
    "            #print(label.shape)\n",
    "            #label=label.tolist()\n",
    "            data=torch.from_numpy(data)#\n",
    "            label=torch.from_numpy(label)#\n",
    "            \n",
    "            \n",
    "            data_info.append((data,label))\n",
    "        return data_info\n",
    "\n",
    "    \n",
    "model_params = list(model.named_parameters())\n",
    "#print(nfm_params)\n",
    "net=model\n",
    "model.cuda()\n",
    "    \n",
    "    \n",
    "loss_df=pd.read_csv('dataset/qiuguan/origin_800/LRP/300/qiu_guan_genes_dis_array_df.csv',sep=',')\n",
    "#print(loss_df)\n",
    "loss_df=loss_df.iloc[:,1:-1]\n",
    "print(loss_df.shape)\n",
    "loss_df=np.array(loss_df,dtype=float)\n",
    "\n",
    "loss_df=torch.from_numpy(loss_df)\n",
    "loss_df=Variable(loss_df)\n",
    "loss_df=loss_df.cuda()\n",
    "#print('loss_df.shape:',loss_df.shape)\n",
    "\n",
    "weight={}\n",
    "for name,parameters in net.named_parameters():\n",
    "    print(name,':',parameters.size())\n",
    "    #names.append(name)\n",
    "    weight[name]=parameters\n",
    "        \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import sys \n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "import torchmetrics\n",
    "#LRP\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline\n",
    "\n",
    "from innvestigator import InnvestigateModel\n",
    "from utils import Flatten\n",
    "\n",
    "\n",
    "inn_model = InnvestigateModel(model, lrp_exponent=2,\n",
    "                              method=\"e-rule\",\n",
    "                              beta=.5)\n",
    "\n",
    "\n",
    "def train_epoch(model,train_loader,batch_size,optimizer,loss_func):\n",
    "    BATCH_SIZE=batch_size\n",
    "    total = 0\n",
    "    correct=0\n",
    "    total_loss=0\n",
    "    #loss_score=torch.tensor([[]]).cuda()\n",
    "    #\n",
    "    #loss_op=0\n",
    "    loss2_list=[]\n",
    "    model.train()\n",
    "    total_train_accuracy=0  \n",
    "    for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "            \n",
    "        labels = Variable(labels)\n",
    "        x = Variable(x)\n",
    "            \n",
    "        x_row,x_col=x.shape   \n",
    "        x=torch.tensor(x,dtype=torch.float)\n",
    "        #print(x.shape)\n",
    "        labels=torch.tensor(labels,dtype=torch.float)\n",
    "        x, labels = x.cuda(), labels.cuda()\n",
    "        labels_int=labels=torch.max(labels,1)[1]#########\n",
    "        #print(labels_int)\n",
    "        #print('labels_int:',labels_int.shape)\n",
    "        #print('labels:',labels) \n",
    "        #print('x:',x.shape)\n",
    "        loss_op=0\n",
    "        '''\n",
    "        for i in range(nfm_config['batch_size']):\n",
    "            l1=torch.mv(weight['fc2.weight'].T,(labels[i]-weight['fc2.bias']))#####\n",
    "            l2=torch.mv(weight['fc1.weight'].T,(l1-weight['fc1.bias']))\n",
    "            #l2=l2[100:]\n",
    "            #l3=torch.mv(weight['fc0.weight'].T,(l2-weight['fc0.bias']))\n",
    "        \n",
    "            l=l2.mul(loss_df[labels_int[i],:])\n",
    "        \n",
    "            #print('l:',l.shape)\n",
    "            #l=torch.sum(l,dim=1)\n",
    "            l=torch.abs(l)\n",
    "            l=torch.sum(l,dim=-1)\n",
    "            l=torch.sum(l,dim=0)\n",
    "            loss_op+=l\n",
    "        '''\n",
    "        \n",
    "        model_prediction, input_relevance_values = inn_model.innvestigate(in_tensor=x, rel_for_class=None,target=None)\n",
    "        #print(\"torch.argmax(labels[batch_idx]):\",torch.argmax(labels[batch_idx]))\n",
    "        input_relevance_values.cuda()\n",
    "        #input_relevance_values=F.softmax(input_relevance_values,dim=1)##############增加激活功能\n",
    "        input_relevance_values=input_relevance_values.exp()###############\n",
    "        #input_relevance_values=F.softmax(input_relevance_values,dim=1)\n",
    "        #xxx=torch.argmax(labels[batch_idx])\n",
    "        #xxx=int(xxx)\n",
    "        #print(\"xxx:\",xxx)\n",
    "        #print('input_relevance:',input_relevance_values.device)\n",
    "        x=torch.mul(x,input_relevance_values)##############注意力\n",
    "        optimizer.zero_grad()\n",
    "        y_predict = model(x)\n",
    "        \n",
    "        loss1 = loss_func(y_predict, labels)\n",
    "        #loss2=u*(1/loss_op)\n",
    "        #print('input_relevance_values:',input_relevance_values.shape)\n",
    "        #print('loss_score:',loss_score.shape)\n",
    "        \n",
    "        #print('input_relevance_values:',input_relevance_values)\n",
    "        loss=loss1\n",
    "        #loss = loss_func(y_predict, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #loss2_list.append(u*loss2)   \n",
    "        loss = loss.item()\n",
    "           \n",
    "\n",
    "        total_loss += loss\n",
    "            \n",
    "            \n",
    "            \n",
    "        batch_train_acc=torchmetrics.functional.accuracy(y_predict,labels_int)\n",
    "        total_train_accuracy+=batch_train_acc\n",
    "    #plotLoss(loss2_list,batch_idx+1)   #################################     \n",
    "    total_train_accuracy/=(batch_idx+1)\n",
    "    print('total_train_accuracy:',total_train_accuracy)\n",
    "    print(\"Training Epoch: %d, total loss: %f\" % (epoch_id, total_loss))\n",
    "    return total_loss,total_train_accuracy\n",
    "\n",
    "def val_epoch(model,test_loader,batch_size,optimizer): \n",
    "    batch_size_num=0\n",
    "    total_test_acc=0\n",
    "    model.eval()\n",
    "    for i , (inputs , targets) in enumerate(test_loader):   \n",
    "            print(\"test\")\n",
    "            \n",
    "            inputs = Variable(inputs)   \n",
    "            targets = Variable(targets)     \n",
    "           \n",
    "            inputs=torch.tensor(inputs ,dtype=torch.float)   \n",
    "            targets=torch.tensor(targets ,dtype=torch.float)   \n",
    "            inputs , targets = inputs.cuda(),  targets.cuda()   \n",
    "            yhat = model(inputs)  \n",
    "            \n",
    "            \n",
    "            \n",
    "            targets=torch.max(targets,1)[1]\n",
    "            \n",
    "            \n",
    "            \n",
    "            batch_test_acc=torchmetrics.functional.accuracy(yhat,targets)\n",
    "            \n",
    "            total_test_acc+=batch_test_acc\n",
    "            \n",
    "            batch_size_num=i\n",
    "    total_test_acc/=(batch_size_num+1)\n",
    "        ###print('total_test_accuracy:',total_test_acc/(batch_size+1))\n",
    "    print('total_test_accuracy:',total_test_acc)\n",
    "        \n",
    "                    \n",
    "                    \n",
    "            \n",
    "            \n",
    "    \n",
    "        \n",
    "   \n",
    "    \n",
    "    return total_test_acc\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotLoss(loss,epoch):\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    x=[i for i in range(epoch)]\n",
    "    #acc_train=acc_train.cpu()\n",
    "    #acc_test=acc_test.cpu()\n",
    "    plt.plot(x, loss, 'r-', mec='k', label='Logistic Loss', lw=2)\n",
    "    #plt.plot(x,acc_train,'b-',mec='k',label='accuracy Train',lw=2)\n",
    "    #plt.plot(x,acc_test,'g-',mec='k',label='accuracy Test',lw=2)\n",
    "    #plt.plot(x, y_01, 'g-', mec='k', label='0/1 Loss', lw=2)\n",
    "    #plt.plot(x, y_hinge, 'b-',mec='k', label='Hinge Loss', lw=2)\n",
    "    #plt.plot(x, boost, 'm--',mec='k', label='Adaboost Loss',lw=2)\n",
    "    plt.grid(True, ls='--')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('损失函数')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc536a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (bn0): BatchNorm1d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=90, out_features=1000, bias=True)\n",
      "  (bn1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=100, out_features=9, bias=True)\n",
      "  (bn3): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "yhat: tensor([[0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "yhat: tensor([[0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100],\n",
      "        [0.0045, 0.0122, 0.0035, 0.0076, 0.3988, 0.0678, 0.0061, 0.1895, 0.3100]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "f1_score:  %.4f 0.1111111111111111\n",
      "accuracy_score: 1.0\n",
      "recall_score: 0.1111111111111111\n",
      "pre_recall: 0.1111111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1496: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n",
      "/home/zhengfang/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/zhengfang/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#################149特征基因运行后的示意图\n",
    "################################################特征基因个数为300\n",
    "\n",
    "##############小球测试\n",
    "import torch\n",
    "\n",
    "#功能：加载保存到path中的各层参数到神经网络\n",
    "\n",
    "#path='dataset/qiuguan/origin_800/LRP/non_encode/20/without_attention/MLP11010.pkl'\n",
    "#path='dataset/qiuguan/origin_800/LRP/non_encode/20/without_attention1/MLP10910.pkl'\n",
    "\n",
    "path='dataset/qiuguan/origin_800/LRP/non_encode/10/without_attention1/MLP1710.pkl'#############model with F10\n",
    "#nfm=NFM(nfm_config)\n",
    "mlp=MLP()\n",
    "#print(nfm)\n",
    "#net = nn.DataParallel(net)\n",
    "#net = net.to(device)\n",
    "mlp.load_state_dict(torch.load(path),strict=False)\n",
    "mlp.cuda()\n",
    "\n",
    "print(mlp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mlp_params = list(mlp.named_parameters())\n",
    "#print(nfm_params)\n",
    "net=mlp\n",
    "\n",
    "\n",
    "testset= KZDatasetTest(csv_path='dataset/qiuguan/origin_800/xiaoguan/xiaoguan_predict_55.csv')\n",
    "   \n",
    "test_loader = data.DataLoader(\n",
    "         dataset=testset,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         drop_last=True,\n",
    "         batch_size=nfm_config['batch_size']\n",
    "        \n",
    "     )\n",
    "#———————————————— \n",
    "#版权声明：本文为CSDN博主「山阴少年」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 \n",
    "#原文链接：https://blog.csdn.net/jclian91/article/details/121708431# \n",
    "from torch.autograd import Variable \n",
    "from torch.utils.data import DataLoader \n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import accuracy_score \n",
    " \n",
    "def evaluate_model(test_dl, model): \n",
    "    model.eval()#测试数据稳定\n",
    "    predictions, actuals = [], [] \n",
    "    for i, (inputs, targets) in enumerate(test_dl): \n",
    "        # evaluate the model on the test set \n",
    "        #print(\\ inputs:\\ ,inputs) \n",
    "        #print(\\ targets:\\ ,targets) \n",
    "        inputs = Variable(inputs) \n",
    "        targets = Variable(targets) \n",
    "                 \n",
    "                 \n",
    "        #x = torch.tensor(x, dtype=torch.float) \n",
    "        #x=x.clone().detach().requires_grad_(True) \n",
    "        inputs=torch.tensor(inputs,dtype=torch.float) \n",
    "        \n",
    "        targets=torch.tensor(targets,dtype=torch.float) \n",
    "        inputs, targets = inputs.cuda(), targets.cuda() \n",
    "        yhat = model(inputs) \n",
    "        \n",
    "        print('yhat:',yhat)################\n",
    "        yhat=(yhat==torch.max(yhat,1,keepdim=True)[0]).to(dtype=torch.int32)\n",
    "        # retrieve numpy array \n",
    "        #yhat = yhat.detach().numpy() \n",
    "        yhat = yhat.detach().cpu().numpy()#转换到cpu \n",
    "        #print(\"yhat:\",yhat)\n",
    "        # yhat=yhat.argmax(axis=1) \n",
    "        #print(yhat:\\ ,yhat) \n",
    "        #print('yhat.shape:',yhat.shape) \n",
    "        actual = targets.detach().cpu().numpy() \n",
    "        actual=actual.round() \n",
    "        #print(\"actual:\",actual)\n",
    "        #print(\\ actual:\\ ,actual) \n",
    "        #print('actual.shape:',actual.shape\n",
    "        #predictions.appe) \n",
    "        #actual = actual.reshape(-1, 1) \n",
    "        # round to class values \n",
    "        yhat = yhat.round() \n",
    "        # store nd(yhat) \n",
    "        actuals.append(actual) \n",
    "        predictions.append(yhat)\n",
    "        \n",
    "    \n",
    "    #print(\"prediction:\" ,predictions) \n",
    "    #print(\"actuals:\",actuals) \n",
    "    predictions, actuals = np.vstack(predictions), np.vstack(actuals) \n",
    "    #print(\"prediction:\" ,predictions) \n",
    "    #print(\"actuals:\" ,actuals) \n",
    "    # calculate accuracy \n",
    "    \n",
    "    #print('actuals.shape:',actuals.shape)\n",
    "    #print('predictions.shape:',predictions.shape)\n",
    "    acc_test = accuracy_score(actuals, predictions) \n",
    "    return  actuals, predictions,acc_test \n",
    "\n",
    "import torch.nn.functional as F \n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score,roc_curve, auc, precision_score, recall_score, f1_score, confusion_matrix, accuracy_score \n",
    "\n",
    "import torch.nn.functional as F \n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score,roc_curve, auc, precision_score, recall_score, f1_score, confusion_matrix, accuracy_score \n",
    "\n",
    "import torch.nn.functional as F \n",
    "\n",
    "actuals,predictions,acc_test=evaluate_model(test_loader,net)\n",
    "\n",
    "\n",
    "import torch.nn.functional as F \n",
    "\n",
    "actuals,predictions,acc_test=evaluate_model(test_loader,net)\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score,roc_curve, auc, precision_score, recall_score, f1_score, confusion_matrix, accuracy_score \n",
    "target_list=actuals \n",
    "pred_list=predictions \n",
    "      \n",
    "y_true=target_list \n",
    "y_pred=pred_list \n",
    "      \n",
    "f1=f1_score(y_true=target_list, y_pred=pred_list, average='macro') # 也可以指定micro模式 \n",
    "acc_score=accuracy_score(y_true=target_list, y_pred=pred_list) \n",
    "rec_score=recall_score(y_true=target_list,y_pred=pred_list,average='macro') # 也可以指定micro模式 \n",
    "pre_recall=precision_score(y_true=target_list,y_pred=pred_list,average='macro') \n",
    "print(\"f1_score:  %.4f\" ,f1) \n",
    "print(\"accuracy_score:\" ,acc_score) \n",
    "print(\"recall_score:\",rec_score) \n",
    "print(\"pre_recall:\" ,pre_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "724b3315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (bn0): BatchNorm1d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=90, out_features=1000, bias=True)\n",
      "  (bn1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=100, out_features=9, bias=True)\n",
      "  (bn3): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "relevance\n",
      "[0 1 2 3 4 5 6 7 8]\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "prediction_score: 22.889645931399343\n",
      "true\n",
      "不合格****************: tensor(5)\n",
      "prediction_score: 16.208067186562833\n",
      "true\n",
      "不合格****************: tensor(7)\n",
      "prediction_score: 19.96386298347468\n",
      "true\n",
      "合格： tensor(4)\n",
      "prediction_score: 19.61623997174043\n",
      "true\n",
      "不合格****************: tensor(5)\n",
      "prediction_score: 17.047617188649586\n",
      "true\n",
      "不合格****************: tensor(7)\n",
      "prediction_score: 16.208067186562833\n",
      "true\n",
      "不合格****************: tensor(7)\n",
      "prediction_score: 16.208067186562833\n",
      "true\n",
      "不合格****************: tensor(7)\n",
      "prediction_score: 17.827995806447344\n",
      "true\n",
      "不合格****************: tensor(5)\n",
      "prediction_score: 21.31063502793447\n",
      "true\n",
      "不合格****************: tensor(8)\n",
      "prediction_score: 21.31063502793447\n",
      "true\n",
      "不合格****************: tensor(8)\n",
      "prediction_score: 16.208067186562833\n",
      "true\n",
      "不合格****************: tensor(7)\n",
      "prediction_score: 21.386512427260833\n",
      "prediction_score: 13.118747767577737\n",
      "true\n",
      "不合格****************: tensor(2)\n",
      "prediction_score: 21.31063502793447\n",
      "true\n",
      "不合格****************: tensor(8)\n",
      "prediction_score: 19.847200906836417\n",
      "true\n",
      "不合格****************: tensor(8)\n",
      "prediction_score: 21.31063502793447\n",
      "true\n",
      "不合格****************: tensor(8)\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(2, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(0, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(7, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(3, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(4, device='cuda:0')\n",
      "torch.argmax(predict): tensor(8, device='cuda:0')\n",
      "torch.argmax(predict): tensor(5, device='cuda:0')\n",
      "prediction_score: 13.284176386567276\n",
      "prediction_score: 17.31871941508927\n",
      "true\n",
      "不合格****************: tensor(7)\n",
      "prediction_score: 18.41609962704078\n",
      "prediction_score: 19.754442548472248\n",
      "true\n",
      "不合格****************: tensor(8)\n",
      "prediction_score: 23.83513220155882\n",
      "true\n",
      "不合格****************: tensor(3)\n",
      "prediction_score: 13.284176386567276\n",
      "prediction_score: 21.863717919869092\n",
      "true\n",
      "不合格****************: tensor(8)\n",
      "prediction_score: 13.284176386567276\n",
      "prediction_score: 23.83513220155882\n",
      "true\n",
      "不合格****************: tensor(3)\n",
      "prediction_score: 21.863717919869092\n",
      "true\n",
      "不合格****************: tensor(8)\n",
      "prediction_score: 19.754442548472248\n",
      "true\n",
      "不合格****************: tensor(8)\n",
      "prediction_score: 18.41609962704078\n",
      "prediction_score: 21.863717919869092\n",
      "true\n",
      "不合格****************: tensor(8)\n",
      "prediction_score: 22.974623555972794\n",
      "true\n",
      "不合格****************: tensor(5)\n",
      "qiu_____genes_features.shape: [[0], [1], [2], [3], [4, 19.96386298347468, tensor([59, 53, 50, 49, 48, 47, 46, 51, 57, 58, 56, 60, 61, 62, 63, 64, 65, 66,\n",
      "        67, 55])], [5], [6], [7], [8]]\n"
     ]
    }
   ],
   "source": [
    "#######找特征基因#############从3301中找40个基因\n",
    "#########################################################本次测试的目的是看200个基因的分类效果\n",
    "##########测试步骤：从3301个基因中提取350个\n",
    "############用200个构建新的分类模型\n",
    "#################特征基因\n",
    "######################为小球，根据上边的测试的基因个数，350最大\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from mnist_test import Net, train, test\n",
    "\n",
    "\n",
    "# Network parameters\n",
    "class Params(object):\n",
    "    batch_size = 64\n",
    "    test_batch_size = 20\n",
    "    epochs = 5\n",
    "    lr = 0.01\n",
    "    momentum = 0.5\n",
    "    no_cuda = True\n",
    "    seed = 1\n",
    "    log_interval = 10\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "args = Params()\n",
    "torch.manual_seed(args.seed)\n",
    "#device = torch.device(\"cpu\")\n",
    "device=torch.device('cuda')\n",
    "kwargs = {}\n",
    "\n",
    "\n",
    "\n",
    "##############数据准备\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#功能：加载保存到path中的各层参数到神经网络\n",
    "\n",
    "#path='dataset/qiuguan/origin_800/LRP/non_encode/20/without_attention/MLP11010.pkl'\n",
    "#path='dataset/qiuguan/origin_800/LRP/non_encode/20/without_attention1/MLP10910.pkl'\n",
    "\n",
    "#path='dataset/qiuguan/origin_800/non_encode_aug/para0.4_0.6_0.2_0.1/MLP10610.pkl'\n",
    "path='dataset/qiuguan/origin_800/LRP/non_encode/10/without_attention1/MLP1710.pkl'\n",
    "#nfm=NFM(nfm_config)\n",
    "mlp=MLP()\n",
    "#print(nfm)\n",
    "#net = nn.DataParallel(net)\n",
    "#net = net.to(device)\n",
    "mlp.load_state_dict(torch.load(path),strict=False)\n",
    "mlp.cuda()\n",
    "\n",
    "print(mlp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mlp_params = list(mlp.named_parameters())\n",
    "#print(nfm_params)\n",
    "net=mlp\n",
    "model=net###########\n",
    "'''\n",
    "testset = KZDatasetTest(csv_path='../NFM-pyorch-master/dataset/qiuguan/orign/')\n",
    "   \n",
    "test_loader = DataLoader(\n",
    "         dataset=testset,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         \n",
    "         batch_size=nfm_config['batch_size']\n",
    "        \n",
    "     )\n",
    "'''\n",
    "\n",
    "testset_xiaoqiu  = KZDatasetTest(csv_path='dataset/qiuguan/origin_800/xiaoguan/xiaoguan_predict_55.csv')#样本收集特征数据集，和测试数据集不同，这里边可能还包含训练集\n",
    "   \n",
    "test_loader_xiaoqiu = DataLoader(\n",
    "         dataset=testset_xiaoqiu,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         \n",
    "         batch_size=nfm_config['batch_size'],\n",
    "         shuffle=True\n",
    "        \n",
    "     )\n",
    "\n",
    "testset_xiaoguan  = KZDatasetTest(csv_path='dataset/qiuguan/origin_800/xiaoguan/xiaoguan_predict_55.csv')\n",
    "   \n",
    "test_loader_xiaoguan = DataLoader(\n",
    "         dataset=testset_xiaoguan,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         \n",
    "         batch_size=nfm_config['batch_size'],\n",
    "         shuffle=True\n",
    "        \n",
    "     )\n",
    "\n",
    "################小球\n",
    "\n",
    "#LRP\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline\n",
    "\n",
    "from innvestigator import InnvestigateModel\n",
    "from utils import Flatten\n",
    "\n",
    "\n",
    "inn_model = InnvestigateModel(model, lrp_exponent=2,\n",
    "                              method=\"e-rule\",\n",
    "                              beta=.5)\n",
    "\n",
    "genes_features=np.array([i for i  in range(9)])\n",
    "\n",
    "print(genes_features)\n",
    "genes_features=genes_features.reshape(9,1).tolist()\n",
    "model.double()\n",
    "for data, target in test_loader_xiaoguan:############小球\n",
    "\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    #targets=torch.max(targets,1)[1]###################\n",
    "    #print('data:',data.shape)\n",
    "    batch_size = int(data.size()[0])\n",
    "    #print('batch_size:',batch_size)#=20\n",
    "    evidence_for_class = []\n",
    "    #print(\"target:\",target.shape)\n",
    "    #print('target:',target[3])\n",
    "    # Overlay with noise \n",
    "    # data[0] += 0.25 * data[0].max() * torch.Tensor(np.random.randn(28*28).reshape(1, 28, 28))\n",
    "    #model_prediction, true_relevance = inn_model.innvestigate(in_tensor=data)\n",
    "\n",
    "    for i in range(9):#10类\n",
    "    # Unfortunately, we had some issue with freeing pytorch memory, therefore \n",
    "    # we need to reevaluate the model separately for every class.\n",
    "        model_prediction, input_relevance_values = inn_model.innvestigate2(in_tensor=data, rel_for_class=i,target=target)\n",
    "        evidence_for_class.append(input_relevance_values)\n",
    "    #print('input_relevance_values:',input_relevance_values.shape)\n",
    "    #print('evidence_for_class:',len(evidence_for_class))\n",
    "    evidence_for_class = np.array([elt.numpy() for elt in evidence_for_class])\n",
    "    #print('evidence_for_class:',evidence_for_class.shape)#[10,20,784]\n",
    "    for idx, example in enumerate(data):#batch 中的每一个样本\n",
    "        #print('example:',example.shape)\n",
    "        prediction = np.argmax(model_prediction.cpu().detach(), axis=1)#\n",
    "        #print('prediction[idx]:',prediction[idx])\n",
    "        #print(evidence_for_class[prediction[idx]][idx])\n",
    "        #fig, axes = plt.subplots(3, 5)\n",
    "        '''\n",
    "        fig.suptitle(\"Prediction of model: \" + str(prediction[idx]) + \"({0:.2f})\".format(\n",
    "            100*float(model_prediction[idx][model_prediction[idx].argmax()].exp()/model_prediction[idx].exp().sum())))\n",
    "        '''\n",
    "        prediction_value=prediction[idx]\n",
    "        p_x=model_prediction[idx][model_prediction[idx].argmax()].exp()\n",
    "        p_sum=model_prediction[idx].exp().sum()\n",
    "        prediction_score=100*float(model_prediction[idx][model_prediction[idx].argmax()].exp()/model_prediction[idx].exp().sum())\n",
    "        #print('prediction_value:',prediction_value)\n",
    "        print('prediction_score:',prediction_score)\n",
    "        #print('分子:',p_x)\n",
    "        #print('分母：',p_sum)\n",
    "        #uu=pr\n",
    "        #print(\"torch.argmax:\",torch.argmax(target[idx]))\n",
    "        if len(genes_features[prediction_value])==1:#有值，但还没有添加预测分数和特征值，只有标签#prediction_value代表第几种疾病\n",
    "            print('true')\n",
    "            if prediction_value!=torch.argmax(target[idx]).cpu().detach()  :\n",
    "                print('不合格****************:',prediction_value)\n",
    "            if prediction_value==torch.argmax(target[idx]).cpu().detach()  :#预测正确\n",
    "                genes_features[prediction_value].append(prediction_score)\n",
    "                print('合格：',prediction_value)\n",
    "                relevance_score_for_every_pixel=evidence_for_class[prediction[idx]][idx]\n",
    "                #print('relevance_score_for_every_pixel.shape:',relevance_score_for_every_pixel.tolist())\n",
    "                relevance_score_for_every_pixel=torch.from_numpy(relevance_score_for_every_pixel)\n",
    "                index=torch.topk(relevance_score_for_every_pixel,20,largest=True)#基因个数50#####150\n",
    "                #print('pixel_sorted:',index)\n",
    "                genes_features[prediction_value].append(index.indices)#添加前50基因特征\n",
    "        \n",
    "        \n",
    "        \n",
    "print('qiu_____genes_features.shape:',genes_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a27d629e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([59, 53, 50, 49, 48, 47, 46, 51, 57, 58, 56, 60, 61, 62, 63, 64, 65, 66,\n",
      "        67, 55])\n"
     ]
    }
   ],
   "source": [
    "guan_single_sample=genes_features[4][2]\n",
    "print(guan_single_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df825dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59, 53, 50, 49, 48, 47, 46, 51, 57, 58, 56, 60, 61, 62, 63, 64, 65, 66, 67, 55]\n"
     ]
    }
   ],
   "source": [
    "guan_single_sample_list=guan_single_sample.numpy().tolist()\n",
    "print(guan_single_sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80b06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns=pd.read_csv('dataset/qiuguan/origin_800/LRP/10/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
