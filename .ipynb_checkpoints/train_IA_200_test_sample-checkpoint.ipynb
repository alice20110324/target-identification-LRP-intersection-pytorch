{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85f88a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (bn0): BatchNorm1d(253, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=253, out_features=1000, bias=True)\n",
      "  (bn1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=100, out_features=9, bias=True)\n",
      "  (bn3): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "relevance\n"
     ]
    }
   ],
   "source": [
    "#############  u===2   MLP_encode_100 and MLP_encode_1000  :654    ##################\n",
    "import torch\n",
    "#import Trainer\n",
    "from network import NFM\n",
    "import torch.utils.data as Data\n",
    "from Utils.criteo_loader import getTestData, getTrainData\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import sys \n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from mnist_test import Net, train, test\n",
    "\n",
    "input_num=253\n",
    "# Network parameters\n",
    "class Params(object):\n",
    "    batch_size = 64\n",
    "    test_batch_size = 20\n",
    "    epochs = 5\n",
    "    lr = 0.01\n",
    "    momentum = 0.5\n",
    "    no_cuda = True\n",
    "    seed = 1\n",
    "    log_interval = 10\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "args = Params()\n",
    "torch.manual_seed(args.seed)\n",
    "#device = torch.device(\"cpu\")\n",
    "device=torch.device('cuda')\n",
    "kwargs = {}\n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':9,\n",
    "    \n",
    "    'num_epoch': 500,#训练epoch次数\n",
    "    'batch_size': 16,#batch_size\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    'epoch':1000,\n",
    "    \n",
    "   \n",
    "    'gene_name':'dataset/qiuguan/origin_800/gene_name.csv',\n",
    "    'label_name':'dataset/qiuguan/origin_800/gene_label.csv'\n",
    "    \n",
    "}\n",
    "\n",
    "#model definition\n",
    "import torch.nn as nn\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn0=nn.BatchNorm1d(input_num)\n",
    "        self.fc1 = nn.Linear(input_num, 1000)\n",
    "        self.bn1= nn.BatchNorm1d(1000)\n",
    "        self.fc2 = nn.Linear(1000, 100)\n",
    "        self.bn2=nn.BatchNorm1d(100)\n",
    "        self.fc3=nn.Linear(100,9)\n",
    "        self.bn3=nn.BatchNorm1d(9)\n",
    "        \n",
    "        self.drop=nn.Dropout(0.3)\n",
    "    def forward(self, x):\n",
    "        #c=torch.add(x,cc)\n",
    "        x=self.bn0(x)\n",
    "        x = F.relu(self.drop(self.bn1(self.fc1(x))))\n",
    "        x = F.relu(self.drop(self.bn2(self.fc2(x))))\n",
    "        return F.softmax(self.bn3(self.fc3(x)), dim=1)\n",
    "model = MLP().cuda()\n",
    "print(model)\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import torch.nn.functional as F  # 激励函数的库\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "\n",
    "def one_hot_smoothing(labels, classes, label_smoothing=0.2):\n",
    "    #n = len(labels)\n",
    "    n=labels.shape[0]\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    n = len(labels)\n",
    "    #eoff = label_smoothing / classes\n",
    "    output = np.zeros((n, classes), dtype=np.float32)\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_smoothing(labels, classes, label_smoothing=0.2):\n",
    "    #n = len(labels)\n",
    "    n=labels.shape[0]\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    n = len(labels)\n",
    "    #eoff = label_smoothing / classes\n",
    "    output = np.zeros((n, classes), dtype=np.float32)\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "class KZDatasetTest(data.Dataset):\n",
    "    \"\"\" Construct the FM pytorch dataset. \"\"\"\n",
    "    #def __init__(self, file,label_file, feature_map,n_class=16):\n",
    "    def __init__(self, csv_path):\n",
    "    \n",
    "       \n",
    "        self.data_info = self.get_data_info(csv_path)\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Dataset读取图片的函数\n",
    "        data, label = self.data_info[index]\n",
    "        #img = Image.open(img_pth).convert('RGB')\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_data_info(self,csv_path):\n",
    "        #解析路径\n",
    "        #转为一维list存储，每一位为【图片路径，图片类别】\n",
    "        labels=[]\n",
    "        data_info=[]\n",
    "        df=pd.read_csv(csv_path,sep=',',header=None)\n",
    "        #print(\"df:\",df)\n",
    "        df=df.iloc[2:,1:]\n",
    "        \n",
    "        rows,cols=df.shape\n",
    "        #print(rows,cols)\n",
    "        for i in df.iloc[:,-1]:\n",
    "            #print(i)\n",
    "            labels.append(int(float(i)))\n",
    "        #print('labels:',labels)\n",
    "        labels=np.array(labels)\n",
    "        \n",
    "        labels=one_hot_smoothing(labels,nfm_config['n_class'])\n",
    "        #print(labels)\n",
    "        for i in range(rows):\n",
    "            data=df.iloc[i,:-1]\n",
    "            data=data.astype(float)##############\n",
    "            \n",
    "            data=np.array(data)##\n",
    "            \n",
    "            label=labels[i]\n",
    "            \n",
    "            data=torch.from_numpy(data)#\n",
    "            label=torch.from_numpy(label)#\n",
    "           \n",
    "            \n",
    "            data_info.append((data,label))\n",
    "        return data_info\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import *\n",
    "from PIL import Image\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ast\n",
    "import torchvision\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "class KZDataset(Dataset):\n",
    "    def __init__(self, csv_path, K,n_class,ki=0, typ='train', transform=None, rand=False):\n",
    "        \n",
    "        self.all_data_info = self.get_data_info(csv_path)\n",
    "        \n",
    "        if rand:\n",
    "            random.seed(1)\n",
    "            random.shuffle(self.all_data_info)\n",
    "        leng = len(self.all_data_info)\n",
    "        every_z_len = leng // K\n",
    "        if typ == 'val':\n",
    "            self.data_info = self.all_data_info[every_z_len * ki : every_z_len * (ki+1)]\n",
    "        elif typ == 'train':\n",
    "            self.data_info = self.all_data_info[: every_z_len * ki] + self.all_data_info[every_z_len * (ki+1) :]\n",
    "            \n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Dataset读取图片的函数\n",
    "        data, label = self.data_info[index]\n",
    "        #img = Image.open(img_pth).convert('RGB')\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "       \n",
    "    \n",
    "    def get_data_info(self,csv_path):\n",
    "        #解析路径\n",
    "        #转为一维list存储，每一位为【图片路径，图片类别】\n",
    "        labels=[]\n",
    "        data_info=[]\n",
    "        df=pd.read_csv(csv_path,sep=',',header=None)\n",
    "        #print(\"df:\",df)\n",
    "        df=df.iloc[2:,1:]\n",
    "        #print(\"df:\",df)\n",
    "        print(df.shape)\n",
    "        #print(\"df:\",df)\n",
    "        #print(df.iloc[:,-1])\n",
    "        #df=df.applymap(ast.literal_eval)\n",
    "        rows,cols=df.shape\n",
    "        #print(rows,cols)\n",
    "        for i in df.iloc[:,-1]:\n",
    "            #print(i)\n",
    "            labels.append(int(float(i)))\n",
    "        #print('labels:',i,labels)\n",
    "        labels=np.array(labels)\n",
    "        #print('labels:',labels)\n",
    "        #labels=np.array(labels)\n",
    "        labels=one_hot_smoothing(labels,nfm_config['n_class'])\n",
    "        #print(labels)\n",
    "        for i in range(rows):\n",
    "            data=df.iloc[i,:-1]\n",
    "            data=data.astype(float)##############\n",
    "            #print(\"i,data:\",i,data)\n",
    "            #data=pd.DataFrame(data,dtype=float)###############\n",
    "            data=np.array(data)##\n",
    "            \n",
    "            label=labels[i]\n",
    "            #print(data.shape)\n",
    "            #print(label.shape)\n",
    "            #label=label.tolist()\n",
    "            data=torch.from_numpy(data)#\n",
    "            label=torch.from_numpy(label)#\n",
    "           \n",
    "            \n",
    "            data_info.append((data,label))\n",
    "        return data_info\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import sys \n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "import torchmetrics\n",
    "#LRP\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline\n",
    "\n",
    "from innvestigator import InnvestigateModel\n",
    "from utils import Flatten\n",
    "\n",
    "\n",
    "inn_model = InnvestigateModel(model, lrp_exponent=2,\n",
    "                              method=\"e-rule\",\n",
    "                              beta=.5)\n",
    "\n",
    "\n",
    "def train_epoch(model,train_loader,batch_size,optimizer,loss_func):\n",
    "    BATCH_SIZE=batch_size\n",
    "    total = 0\n",
    "    correct=0\n",
    "    total_loss=0\n",
    "    #loss_score=torch.tensor([[]]).cuda()\n",
    "    #\n",
    "    #loss_op=0\n",
    "    loss2_list=[]\n",
    "    model.train()\n",
    "    total_train_accuracy=0  \n",
    "    dd=[[0]*input_num]*batch_size\n",
    "    dd=torch.tensor(dd)\n",
    "        \n",
    "    dd=dd.cuda()\n",
    "    \n",
    "    for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "            \n",
    "        labels = Variable(labels)\n",
    "        x = Variable(x)\n",
    "            \n",
    "        x_row,x_col=x.shape   \n",
    "        x=torch.tensor(x,dtype=torch.float)\n",
    "        #print(x.shape)\n",
    "        labels=torch.tensor(labels,dtype=torch.float)\n",
    "        x, labels = x.cuda(), labels.cuda()\n",
    "        labels_int=labels=torch.max(labels,1)[1]#########\n",
    "        #print(labels_int)\n",
    "        #print('labels_int:',labels_int.shape)\n",
    "        #print('labels:',labels) \n",
    "        #print('x:',x.shape)\n",
    "        loss_op=0\n",
    "       \n",
    "        loss1=0\n",
    "        \n",
    "        \n",
    "        pre_tar=[[0]*9]*batch_size\n",
    "        pre_target=torch.tensor(pre_tar,dtype=torch.float)#dtype非常重要，否则就四舍五入为0，因为上边定义的0是整型#torch.float,not float\n",
    "        #print(pre_target)\n",
    "        pre_target=pre_target.cuda()\n",
    "        \n",
    "        cc=[[0]*input_num]*batch_size\n",
    "        cc=torch.tensor(cc,dtype=torch.float)\n",
    "        \n",
    "        cc=cc.cuda()\n",
    "        inn_model.evaluate(in_tensor=x)\n",
    "        \n",
    "        model_prediction, only_max_score,org_shape = inn_model.innvestigatex()\n",
    "        model_prediction.cuda()\n",
    "        #print('only_max_score:',only_max_score)\n",
    "        #only_max_score1=only_max_score.detach().clone()\n",
    "        #print(labels.shape,labels)   \n",
    "        \n",
    "        #print(\"torch.argmax(labels[batch_idx]):\",torch.argmax(labels[batch_idx]))\n",
    "        #print(model_prediction.shape, model_prediction)\n",
    "        #model_prediction.cuda()\n",
    "        #print('input_relevance_values:')\n",
    "        rel_for_class_list=labels\n",
    "        para_list=[]\n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            max_pre=torch.argmax(model_prediction[i])\n",
    "            if max_pre!=labels[i]:\n",
    "                #print('only_max_score1:',i,only_max_score)\n",
    "                #only_max_score=only_max_score1\n",
    "                #input_relevance_values=inn_model.compute_relevance_score(only_max_score,labels[i],org_shape,para=0.1)\n",
    "                #rel_for_class=labels[i]\n",
    "                para=0.1\n",
    "                para_list.append(para)\n",
    "                #print('device:',input_relevance_values.device)\n",
    "                #input_relevance_values[labels[i]]=input_relevance_values[labels[i]].cuda()\n",
    "                #input_relevance_values[labels[i]]=input_relevance_values[labels[i]].exp()\n",
    "                #print('input_relevance_values:',i, input_relevance_values)\n",
    "                #print('input_relevance_values[labels[i]]:',i,batch_idx,input_relevance_values[labels[i]])\n",
    "                #cc[i]=torch.mul(x[i],input_relevance_values[labels[i]])+torch.tensor(0.1,dtype=torch.float)##############注意力\n",
    "                #print('pre_target:',pre_target[i])\n",
    "                #print('cc',i,cc[i].shape)\n",
    "            else:\n",
    "                #print('only_max_score2:',i,only_max_score)\n",
    "                #only_max_score=only_max_score1\n",
    "                #input_relevance_values=inn_model.compute_relevance_score(only_max_score,labels[i],org_shape,para=0)\n",
    "                #input_relevance_values[labels[i]]=input_relevance_values[labels[i]].exp()\n",
    "                #print('input_relevance_values:',i, input_relevance_values)\n",
    "                #print('input_relevance_values[i]:',i,batch_idx,input_relevance_values[labels[i]])\n",
    "                #cc[i]=torch.mul(x[i],input_relevance_values[labels[i]])##############注意力\n",
    "                #print('cc',i,cc[i].shape)\n",
    "                para=0\n",
    "                para_list.append(para)\n",
    "        input_relevance_values=inn_model.compute_relevance_scorex(only_max_score,rel_for_class_list,org_shape,para_list)\n",
    "        cc=torch.mul(x,input_relevance_values)##############注意力        \n",
    "        #print('x:',x)\n",
    "        #print('cc:',cc)\n",
    "        x=torch.add(x,cc)   \n",
    "        #print('new_x:',x)\n",
    "        optimizer.zero_grad()\n",
    "        y_predict=model(x)\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        loss1 = loss_func(y_predict, labels)\n",
    "        #loss2=u*(1/loss_op)\n",
    "        #print('input_relevance_values:',input_relevance_values.shape)\n",
    "        #print('loss_score:',loss_score.shape)\n",
    "        \n",
    "        #print('input_relevance_values:',input_relevance_values)\n",
    "        loss=loss1\n",
    "        #loss = loss_func(y_predict, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #loss2_list.append(u*loss2)   \n",
    "        loss = loss.item()\n",
    "           \n",
    "\n",
    "        total_loss += loss\n",
    "            \n",
    "            \n",
    "            \n",
    "        batch_train_acc=torchmetrics.functional.accuracy(y_predict,labels_int)\n",
    "        total_train_accuracy+=batch_train_acc\n",
    "    #plotLoss(loss2_list,batch_idx+1)   #################################     \n",
    "    total_train_accuracy/=(batch_idx+1)\n",
    "    print('total_train_accuracy:',total_train_accuracy)\n",
    "    print(\"Training Epoch: %d, total loss: %f\" % (epoch_id, total_loss))\n",
    "    return total_loss,total_train_accuracy\n",
    "\n",
    "def val_epoch(model,test_loader,batch_size,optimizer): \n",
    "    batch_size_num=0\n",
    "    total_test_acc=0\n",
    "    model.eval()\n",
    "    for i , (inputs , targets) in enumerate(test_loader):   \n",
    "        print(\"val\")\n",
    "            \n",
    "            \n",
    "        inputs = Variable(inputs)   \n",
    "        targets = Variable(targets)     \n",
    "           \n",
    "        inputs=torch.tensor(inputs ,dtype=torch.float)   \n",
    "        targets=torch.tensor(targets ,dtype=torch.float)   \n",
    "        inputs , targets = inputs.cuda(),  targets.cuda()\n",
    "        targets=torch.max(targets,1)[1]\n",
    "        '''\n",
    "            \n",
    "            \n",
    "            \n",
    "        '''\n",
    "        cc=[[0]*input_num]*batch_size\n",
    "        cc=torch.tensor(cc,dtype=torch.float)\n",
    "        \n",
    "        cc=cc.cuda()\n",
    "        inn_model.evaluate(in_tensor=inputs)\n",
    "        \n",
    "        model_prediction, only_max_score,org_shape = inn_model.innvestigatex()\n",
    "        model_prediction.cuda()\n",
    "        #print('only_max_score:',only_max_score)\n",
    "        #only_max_score1=only_max_score.detach().clone()\n",
    "        #print(labels.shape,labels)   \n",
    "        \n",
    "        #print(\"torch.argmax(labels[batch_idx]):\",torch.argmax(labels[batch_idx]))\n",
    "        #print(model_prediction.shape, model_prediction)\n",
    "        #model_prediction.cuda()\n",
    "        #print('input_relevance_values:')\n",
    "        max_tar=torch.argmax(targets)\n",
    "        rel_for_class_list=targets\n",
    "        para_list=[]\n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            max_pre=torch.argmax(model_prediction[i])\n",
    "            if max_pre!=targets[i]:\n",
    "                #print('only_max_score1:',i,only_max_score)\n",
    "                #only_max_score=only_max_score1\n",
    "                #input_relevance_values=inn_model.compute_relevance_score(only_max_score,labels[i],org_shape,para=0.1)\n",
    "                #rel_for_class=labels[i]\n",
    "                para=0.1\n",
    "                para_list.append(para)\n",
    "                #print('device:',input_relevance_values.device)\n",
    "                #input_relevance_values[labels[i]]=input_relevance_values[labels[i]].cuda()\n",
    "                #input_relevance_values[labels[i]]=input_relevance_values[labels[i]].exp()\n",
    "                #print('input_relevance_values:',i, input_relevance_values)\n",
    "                #print('input_relevance_values[labels[i]]:',i,batch_idx,input_relevance_values[labels[i]])\n",
    "                #cc[i]=torch.mul(x[i],input_relevance_values[labels[i]])+torch.tensor(0.1,dtype=torch.float)##############注意力\n",
    "                #print('pre_target:',pre_target[i])\n",
    "                #print('cc',i,cc[i].shape)\n",
    "            else:\n",
    "                #print('only_max_score2:',i,only_max_score)\n",
    "                #only_max_score=only_max_score1\n",
    "                #input_relevance_values=inn_model.compute_relevance_score(only_max_score,labels[i],org_shape,para=0)\n",
    "                #input_relevance_values[labels[i]]=input_relevance_values[labels[i]].exp()\n",
    "                #print('input_relevance_values:',i, input_relevance_values)\n",
    "                #print('input_relevance_values[i]:',i,batch_idx,input_relevance_values[labels[i]])\n",
    "                #cc[i]=torch.mul(x[i],input_relevance_values[labels[i]])##############注意力\n",
    "                #print('cc',i,cc[i].shape)\n",
    "                para=0\n",
    "                para_list.append(para)\n",
    "        input_relevance_values=inn_model.compute_relevance_scorex(only_max_score,rel_for_class_list,org_shape,para_list)\n",
    "        cc=torch.mul(inputs,input_relevance_values)##############注意力        \n",
    "        #print('x:',x)\n",
    "        #print('cc:',cc)\n",
    "        inputs=torch.add(inputs,cc)   \n",
    "        optimizer.zero_grad()\n",
    "        yhat = model(inputs)  \n",
    "            \n",
    "            \n",
    "            \n",
    "        #targets=torch.max(targets,1)[1]\n",
    "            \n",
    "            \n",
    "            \n",
    "        batch_test_acc=torchmetrics.functional.accuracy(yhat,targets)\n",
    "            \n",
    "        total_test_acc+=batch_test_acc\n",
    "            \n",
    "        batch_size_num=i\n",
    "    total_test_acc/=(batch_size_num+1)\n",
    "        ###print('total_test_accuracy:',total_test_acc/(batch_size+1))\n",
    "    print('total_test_accuracy:',total_test_acc)\n",
    "        \n",
    "                    \n",
    "                    \n",
    "            \n",
    "            \n",
    "    \n",
    "        \n",
    "   \n",
    "    \n",
    "    return total_test_acc\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotLoss(loss,epoch):\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    x=[i for i in range(epoch)]\n",
    "    #acc_train=acc_train.cpu()\n",
    "    #acc_test=acc_test.cpu()\n",
    "    plt.plot(x, loss, 'r-', mec='k', label='Logistic Loss', lw=2)\n",
    "    #plt.plot(x,acc_train,'b-',mec='k',label='accuracy Train',lw=2)\n",
    "    #plt.plot(x,acc_test,'g-',mec='k',label='accuracy Test',lw=2)\n",
    "    #plt.plot(x, y_01, 'g-', mec='k', label='0/1 Loss', lw=2)\n",
    "    #plt.plot(x, y_hinge, 'b-',mec='k', label='Hinge Loss', lw=2)\n",
    "    #plt.plot(x, boost, 'm--',mec='k', label='Adaboost Loss',lw=2)\n",
    "    plt.grid(True, ls='--')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('损失函数')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46d06daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "L1=[1,2,3]\n",
    "\n",
    "L2=[3,2,4]\n",
    "L3=[L1,L2]\n",
    "print(L3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "702cc715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(545, 254)\n",
      "(545, 254)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:383: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:385: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_train_accuracy: tensor(0.2812, device='cuda:0')\n",
      "Training Epoch: 0, total loss: 63.802810\n",
      "total_train_accuracy: tensor(0.6104, device='cuda:0')\n",
      "Training Epoch: 1, total loss: 58.926460\n",
      "total_train_accuracy: tensor(0.7354, device='cuda:0')\n",
      "Training Epoch: 2, total loss: 56.711014\n",
      "total_train_accuracy: tensor(0.7667, device='cuda:0')\n",
      "Training Epoch: 3, total loss: 55.160303\n",
      "total_train_accuracy: tensor(0.8458, device='cuda:0')\n",
      "Training Epoch: 4, total loss: 54.270033\n",
      "total_train_accuracy: tensor(0.8146, device='cuda:0')\n",
      "Training Epoch: 5, total loss: 54.232640\n",
      "total_train_accuracy: tensor(0.8479, device='cuda:0')\n",
      "Training Epoch: 6, total loss: 53.178523\n",
      "total_train_accuracy: tensor(0.8667, device='cuda:0')\n",
      "Training Epoch: 7, total loss: 52.342121\n",
      "total_train_accuracy: tensor(0.9021, device='cuda:0')\n",
      "Training Epoch: 8, total loss: 51.791575\n",
      "total_train_accuracy: tensor(0.8688, device='cuda:0')\n",
      "Training Epoch: 9, total loss: 51.647621\n",
      "total_train_accuracy: tensor(0.9083, device='cuda:0')\n",
      "Training Epoch: 10, total loss: 50.691178\n",
      "total_train_accuracy: tensor(0.8792, device='cuda:0')\n",
      "Training Epoch: 11, total loss: 50.923808\n",
      "total_train_accuracy: tensor(0.8917, device='cuda:0')\n",
      "Training Epoch: 12, total loss: 50.947274\n",
      "total_train_accuracy: tensor(0.8938, device='cuda:0')\n",
      "Training Epoch: 13, total loss: 50.505370\n",
      "total_train_accuracy: tensor(0.9104, device='cuda:0')\n",
      "Training Epoch: 14, total loss: 49.837749\n",
      "total_train_accuracy: tensor(0.9229, device='cuda:0')\n",
      "Training Epoch: 15, total loss: 49.222879\n",
      "total_train_accuracy: tensor(0.9188, device='cuda:0')\n",
      "Training Epoch: 16, total loss: 49.588721\n",
      "total_train_accuracy: tensor(0.9146, device='cuda:0')\n",
      "Training Epoch: 17, total loss: 49.505432\n",
      "total_train_accuracy: tensor(0.9375, device='cuda:0')\n",
      "Training Epoch: 18, total loss: 48.751031\n",
      "total_train_accuracy: tensor(0.9188, device='cuda:0')\n",
      "Training Epoch: 19, total loss: 48.933724\n",
      "total_train_accuracy: tensor(0.9021, device='cuda:0')\n",
      "Training Epoch: 20, total loss: 49.244304\n",
      "total_train_accuracy: tensor(0.9375, device='cuda:0')\n",
      "Training Epoch: 21, total loss: 48.462344\n",
      "total_train_accuracy: tensor(0.9125, device='cuda:0')\n",
      "Training Epoch: 22, total loss: 48.817154\n",
      "total_train_accuracy: tensor(0.9542, device='cuda:0')\n",
      "Training Epoch: 23, total loss: 47.926170\n",
      "total_train_accuracy: tensor(0.9583, device='cuda:0')\n",
      "Training Epoch: 24, total loss: 47.358030\n",
      "total_train_accuracy: tensor(0.9500, device='cuda:0')\n",
      "Training Epoch: 25, total loss: 47.907881\n",
      "total_train_accuracy: tensor(0.9229, device='cuda:0')\n",
      "Training Epoch: 26, total loss: 47.752919\n",
      "total_train_accuracy: tensor(0.9417, device='cuda:0')\n",
      "Training Epoch: 27, total loss: 47.608649\n",
      "total_train_accuracy: tensor(0.9333, device='cuda:0')\n",
      "Training Epoch: 28, total loss: 47.444687\n",
      "total_train_accuracy: tensor(0.9375, device='cuda:0')\n",
      "Training Epoch: 29, total loss: 47.269816\n",
      "total_train_accuracy: tensor(0.9542, device='cuda:0')\n",
      "Training Epoch: 30, total loss: 46.808937\n",
      "total_train_accuracy: tensor(0.9583, device='cuda:0')\n",
      "Training Epoch: 31, total loss: 46.756886\n",
      "total_train_accuracy: tensor(0.9479, device='cuda:0')\n",
      "Training Epoch: 32, total loss: 46.844941\n",
      "total_train_accuracy: tensor(0.9396, device='cuda:0')\n",
      "Training Epoch: 33, total loss: 47.236841\n",
      "total_train_accuracy: tensor(0.9417, device='cuda:0')\n",
      "Training Epoch: 34, total loss: 46.980783\n",
      "total_train_accuracy: tensor(0.9479, device='cuda:0')\n",
      "Training Epoch: 35, total loss: 46.432470\n",
      "total_train_accuracy: tensor(0.9688, device='cuda:0')\n",
      "Training Epoch: 36, total loss: 46.154237\n",
      "total_train_accuracy: tensor(0.9625, device='cuda:0')\n",
      "Training Epoch: 37, total loss: 46.312351\n",
      "total_train_accuracy: tensor(0.9458, device='cuda:0')\n",
      "Training Epoch: 38, total loss: 46.169015\n",
      "total_train_accuracy: tensor(0.9583, device='cuda:0')\n",
      "Training Epoch: 39, total loss: 45.867089\n",
      "total_train_accuracy: tensor(0.9521, device='cuda:0')\n",
      "Training Epoch: 40, total loss: 46.572869\n",
      "total_train_accuracy: tensor(0.9583, device='cuda:0')\n",
      "Training Epoch: 41, total loss: 45.882128\n",
      "total_train_accuracy: tensor(0.9646, device='cuda:0')\n",
      "Training Epoch: 42, total loss: 46.172927\n",
      "total_train_accuracy: tensor(0.9667, device='cuda:0')\n",
      "Training Epoch: 43, total loss: 45.584724\n",
      "total_train_accuracy: tensor(0.9646, device='cuda:0')\n",
      "Training Epoch: 44, total loss: 45.835671\n",
      "total_train_accuracy: tensor(0.9688, device='cuda:0')\n",
      "Training Epoch: 45, total loss: 45.526198\n",
      "total_train_accuracy: tensor(0.9646, device='cuda:0')\n",
      "Training Epoch: 46, total loss: 45.616758\n",
      "total_train_accuracy: tensor(0.9792, device='cuda:0')\n",
      "Training Epoch: 47, total loss: 45.256062\n",
      "total_train_accuracy: tensor(0.9604, device='cuda:0')\n",
      "Training Epoch: 48, total loss: 46.029434\n",
      "total_train_accuracy: tensor(0.9688, device='cuda:0')\n",
      "Training Epoch: 49, total loss: 45.560851\n",
      "total_train_accuracy: tensor(0.9750, device='cuda:0')\n",
      "Training Epoch: 50, total loss: 45.142387\n",
      "total_train_accuracy: tensor(0.9750, device='cuda:0')\n",
      "Training Epoch: 51, total loss: 45.085800\n",
      "total_train_accuracy: tensor(0.9875, device='cuda:0')\n",
      "Training Epoch: 52, total loss: 44.669323\n",
      "total_train_accuracy: tensor(0.9583, device='cuda:0')\n",
      "Training Epoch: 53, total loss: 45.499745\n",
      "total_train_accuracy: tensor(0.9667, device='cuda:0')\n",
      "Training Epoch: 54, total loss: 44.823267\n",
      "total_train_accuracy: tensor(0.9688, device='cuda:0')\n",
      "Training Epoch: 55, total loss: 45.113242\n",
      "total_train_accuracy: tensor(0.9708, device='cuda:0')\n",
      "Training Epoch: 56, total loss: 44.886435\n",
      "total_train_accuracy: tensor(0.9854, device='cuda:0')\n",
      "Training Epoch: 57, total loss: 44.144122\n",
      "total_train_accuracy: tensor(0.9750, device='cuda:0')\n",
      "Training Epoch: 58, total loss: 44.862868\n",
      "total_train_accuracy: tensor(0.9667, device='cuda:0')\n",
      "Training Epoch: 59, total loss: 45.164445\n",
      "total_train_accuracy: tensor(0.9771, device='cuda:0')\n",
      "Training Epoch: 60, total loss: 44.405620\n",
      "total_train_accuracy: tensor(0.9646, device='cuda:0')\n",
      "Training Epoch: 61, total loss: 45.023399\n",
      "total_train_accuracy: tensor(0.9750, device='cuda:0')\n",
      "Training Epoch: 62, total loss: 44.810730\n",
      "total_train_accuracy: tensor(0.9813, device='cuda:0')\n",
      "Training Epoch: 63, total loss: 44.434344\n",
      "total_train_accuracy: tensor(0.9771, device='cuda:0')\n",
      "Training Epoch: 64, total loss: 44.567670\n",
      "total_train_accuracy: tensor(0.9688, device='cuda:0')\n",
      "Training Epoch: 65, total loss: 44.625807\n",
      "total_train_accuracy: tensor(0.9792, device='cuda:0')\n",
      "Training Epoch: 66, total loss: 44.325919\n",
      "total_train_accuracy: tensor(0.9833, device='cuda:0')\n",
      "Training Epoch: 67, total loss: 44.223953\n",
      "total_train_accuracy: tensor(0.9875, device='cuda:0')\n",
      "Training Epoch: 68, total loss: 43.927631\n",
      "total_train_accuracy: tensor(0.9688, device='cuda:0')\n",
      "Training Epoch: 69, total loss: 44.575664\n",
      "total_train_accuracy: tensor(0.9604, device='cuda:0')\n",
      "Training Epoch: 70, total loss: 44.570621\n",
      "total_train_accuracy: tensor(0.9833, device='cuda:0')\n",
      "Training Epoch: 71, total loss: 44.021565\n",
      "total_train_accuracy: tensor(0.9875, device='cuda:0')\n",
      "Training Epoch: 72, total loss: 43.628111\n",
      "total_train_accuracy: tensor(0.9708, device='cuda:0')\n",
      "Training Epoch: 73, total loss: 44.008518\n",
      "total_train_accuracy: tensor(0.9854, device='cuda:0')\n",
      "Training Epoch: 74, total loss: 44.030960\n",
      "total_train_accuracy: tensor(0.9688, device='cuda:0')\n",
      "Training Epoch: 75, total loss: 44.368464\n",
      "total_train_accuracy: tensor(0.9833, device='cuda:0')\n",
      "Training Epoch: 76, total loss: 44.086818\n",
      "total_train_accuracy: tensor(0.9833, device='cuda:0')\n",
      "Training Epoch: 77, total loss: 43.594079\n",
      "total_train_accuracy: tensor(0.9854, device='cuda:0')\n",
      "Training Epoch: 78, total loss: 43.494067\n",
      "total_train_accuracy: tensor(0.9771, device='cuda:0')\n",
      "Training Epoch: 79, total loss: 44.292869\n",
      "total_train_accuracy: tensor(0.9729, device='cuda:0')\n",
      "Training Epoch: 80, total loss: 44.318484\n",
      "total_train_accuracy: tensor(0.9813, device='cuda:0')\n",
      "Training Epoch: 81, total loss: 43.876905\n",
      "total_train_accuracy: tensor(0.9667, device='cuda:0')\n",
      "Training Epoch: 82, total loss: 44.394377\n",
      "total_train_accuracy: tensor(0.9750, device='cuda:0')\n",
      "Training Epoch: 83, total loss: 43.964587\n",
      "total_train_accuracy: tensor(0.9854, device='cuda:0')\n",
      "Training Epoch: 84, total loss: 43.534912\n",
      "total_train_accuracy: tensor(0.9875, device='cuda:0')\n",
      "Training Epoch: 85, total loss: 43.489488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_train_accuracy: tensor(0.9938, device='cuda:0')\n",
      "Training Epoch: 86, total loss: 43.459385\n",
      "total_train_accuracy: tensor(0.9938, device='cuda:0')\n",
      "Training Epoch: 87, total loss: 43.401893\n",
      "total_train_accuracy: tensor(0.9854, device='cuda:0')\n",
      "Training Epoch: 88, total loss: 43.224934\n",
      "total_train_accuracy: tensor(0.9854, device='cuda:0')\n",
      "Training Epoch: 89, total loss: 43.363191\n",
      "total_train_accuracy: tensor(0.9917, device='cuda:0')\n",
      "Training Epoch: 90, total loss: 43.242919\n",
      "total_train_accuracy: tensor(0.9854, device='cuda:0')\n",
      "Training Epoch: 91, total loss: 43.635668\n",
      "total_train_accuracy: tensor(0.9750, device='cuda:0')\n",
      "Training Epoch: 92, total loss: 43.725819\n",
      "total_train_accuracy: tensor(0.9813, device='cuda:0')\n",
      "Training Epoch: 93, total loss: 43.259551\n",
      "total_train_accuracy: tensor(0.9833, device='cuda:0')\n",
      "Training Epoch: 94, total loss: 43.491547\n",
      "total_train_accuracy: tensor(0.9896, device='cuda:0')\n",
      "Training Epoch: 95, total loss: 43.362850\n",
      "total_train_accuracy: tensor(0.9771, device='cuda:0')\n",
      "Training Epoch: 96, total loss: 43.328615\n",
      "total_train_accuracy: tensor(0.9813, device='cuda:0')\n",
      "Training Epoch: 97, total loss: 43.418165\n",
      "total_train_accuracy: tensor(0.9958, device='cuda:0')\n",
      "Training Epoch: 98, total loss: 42.840567\n",
      "total_train_accuracy: tensor(0.9667, device='cuda:0')\n",
      "Training Epoch: 99, total loss: 43.907688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n",
      "/home/zhengfang/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 25439 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/zhengfang/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 22833 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/zhengfang/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 20989 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/zhengfang/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 25968 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_train_accuracy: tensor(0.9979, device='cuda:0')\n",
      "Training Epoch: 100, total loss: 42.782987\n",
      "the  0  epoch ends\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 25439 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/zhengfang/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 22833 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/zhengfang/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 20989 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/zhengfang/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 25968 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:498: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:499: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-547632c1005e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mtrain_loss_total_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0macc_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnfm_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"acc_test_each_k:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mtest_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-e59f804d3888>\u001b[0m in \u001b[0;36mval_epoch\u001b[0;34m(model, test_loader, batch_size, optimizer)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             \u001b[0mmodel_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_relevance_values\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0morg_shape\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0minn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minnvestigate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_for_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m             \u001b[0;31m#print(\"torch.argmax(labels[batch_idx]):\",torch.argmax(labels[batch_idx]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0;31m#print(model_prediction.shape, model_prediction)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "K=10\n",
    "test_metrics=[]\n",
    "train_loss_total_list=[]\n",
    "for ki in range(K):\n",
    "    trainset = KZDataset(csv_path='dataset/qiuguan/origin_800/LRP/200/selected_train_val_info.csv',K=K, n_class=nfm_config['n_class'],ki=ki,  typ='train', transform=None, rand=True)\n",
    "    valset = KZDataset(csv_path='dataset/qiuguan/origin_800/LRP/200/selected_train_val_info.csv', K=K,n_class=nfm_config['n_class'],ki=ki,  typ='val', transform=None, rand=True)\n",
    "    train_loader = data.DataLoader(\n",
    "         dataset=trainset,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         drop_last=True,\n",
    "         batch_size=nfm_config['batch_size'],\n",
    "         shuffle=True)\n",
    "    val_loader = data.DataLoader(\n",
    "         dataset=valset,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         drop_last=True,\n",
    "         batch_size=nfm_config['batch_size']\n",
    "        \n",
    "     )\n",
    "    \n",
    "    model_path='dataset/qiuguan/origin_800/LRP/non_encode/200/attention0.1/'\n",
    "    #BATCH_SIZE=batch_size\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    #total = 0\n",
    "    \n",
    "    \n",
    "    loss_func=torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    num=0\n",
    "   \n",
    "    \n",
    "    epoches=101\n",
    "    for epoch_id in range(epoches):\n",
    "          \n",
    "        \n",
    "        \n",
    "        train_loss_total,acc_train=train_epoch(model,train_loader,nfm_config['batch_size'],optimizer,loss_func)\n",
    "        train_loss_total_list.append(train_loss_total)#\n",
    "        if epoch_id %10==0:\n",
    "            num=num+1\n",
    "            path=os.path.join(model_path,'MLP'+str(num)+str(K)+'.pkl')\n",
    "            torch.save(model.state_dict(),path)\n",
    "    print(\"the \",ki,\" epoch ends\")\n",
    "    plotLoss(train_loss_total_list,epoches)\n",
    "    \n",
    "    train_loss_total_list=[]\n",
    "    acc_test=val_epoch(model,val_loader,nfm_config['batch_size'],optimizer)\n",
    "    print(\"acc_test_each_k:\",acc_test)\n",
    "    test_metrics.append(acc_test)\n",
    "\n",
    "print(test_metrics)\n",
    "#test_metrics=test_metrics.tolist()\n",
    "test_metrics=[x.cpu().detach().numpy() for x in test_metrics]\n",
    "print(test_metrics)\n",
    "acc_test_metrics=np.mean(test_metrics) \n",
    "print(\"acc_test_metrics:\",acc_test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cae6830",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################149特征基因运行后的示意图\n",
    "################################################特征基因个数为300\n",
    "\n",
    "##############小球测试\n",
    "import torch\n",
    "\n",
    "#功能：加载保存到path中的各层参数到神经网络\n",
    "\n",
    "path='dataset/qiuguan/origin_800/LRP/non_encode/200/attention0.2/MLP1010.pkl'\n",
    "\n",
    "#nfm=NFM(nfm_config)\n",
    "mlp=MLP()\n",
    "#print(nfm)\n",
    "#net = nn.DataParallel(net)\n",
    "#net = net.to(device)\n",
    "mlp.load_state_dict(torch.load(path),strict=False)\n",
    "mlp.cuda()\n",
    "\n",
    "print(mlp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mlp_params = list(mlp.named_parameters())\n",
    "#print(nfm_params)\n",
    "net=mlp\n",
    "\n",
    "\n",
    "testset_guan = KZDatasetTest(csv_path='dataset/qiuguan/origin_800/LRP/200/selected_xiaoguan_test_info.csv')\n",
    "   \n",
    "test_loader_guan = data.DataLoader(\n",
    "         dataset=testset_guan,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         drop_last=True,\n",
    "         batch_size=nfm_config['batch_size']\n",
    "        \n",
    "     )\n",
    "\n",
    "testset_qiu = KZDatasetTest(csv_path='dataset/qiuguan/origin_800/LRP/200/selected_xiaoqiu_test_info.csv')\n",
    "   \n",
    "test_loader_qiu = data.DataLoader(\n",
    "         dataset=testset_qiu,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         drop_last=True,\n",
    "         batch_size=nfm_config['batch_size']\n",
    "        \n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4e61b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PlotCurves import plotGraph,plotLoss,plotMatrix\n",
    "from train_val_test import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6cfcff98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:  %.4f 0.9578002244668912\n",
      "accuracy_score: 0.96875\n",
      "recall_score: 0.9592592592592593\n",
      "pre_recall: 0.963888888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/NFM-pyorch-master/NFM-pyorch-master/train_val_test.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs=torch.tensor(inputs,dtype=torch.float)\n",
      "/home/zhengfang/NFM-pyorch-master/NFM-pyorch-master/train_val_test.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets=torch.tensor(targets,dtype=torch.float)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'roc_auc_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-76281ad91ca6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mactuals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader_guan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplotGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactuals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/NFM-pyorch-master/NFM-pyorch-master/PlotCurves.py\u001b[0m in \u001b[0;36mplotGraph\u001b[0;34m(actuals, predictions, acc_test)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mauc_curve\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ovo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;31m#————————————————\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m#版权声明：本文为CSDN博主「农民小飞侠」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'roc_auc_score' is not defined"
     ]
    }
   ],
   "source": [
    "actuals,predictions,acc_test=evaluate_model(test_loader_guan,net,input_num)\n",
    "plotGraph(actuals,predictions,acc_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa0dd4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score:  %.4f 0.9172047270086485\n",
      "accuracy_score: 0.921875\n",
      "recall_score: 0.9342592592592592\n",
      "pre_recall: 0.9182539682539682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/NFM-pyorch-master/NFM-pyorch-master/train_val_test.py:374: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs=torch.tensor(inputs,dtype=torch.float)\n",
      "/home/zhengfang/NFM-pyorch-master/NFM-pyorch-master/train_val_test.py:376: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets=torch.tensor(targets,dtype=torch.float)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'roc_auc_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-27ba3e3dcbf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mactuals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader_qiu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplotGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactuals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/NFM-pyorch-master/NFM-pyorch-master/PlotCurves.py\u001b[0m in \u001b[0;36mplotGraph\u001b[0;34m(actuals, predictions, acc_test)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mauc_curve\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ovo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;31m#————————————————\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m#版权声明：本文为CSDN博主「农民小飞侠」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'roc_auc_score' is not defined"
     ]
    }
   ],
   "source": [
    "actuals,predictions,acc_test=evaluate_model(test_loader_qiu,net,input_num)\n",
    "plotGraph(actuals,predictions,acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4851f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
