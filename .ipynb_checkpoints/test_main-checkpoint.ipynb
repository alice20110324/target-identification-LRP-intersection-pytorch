{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56b35574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "\n",
    "import config\n",
    "\n",
    "\n",
    "def read_features(file, features):\n",
    "\t\"\"\" Read features from the given file. \"\"\"\n",
    "\ti = len(features)\n",
    "\twith open(file, 'r') as fd:\n",
    "\t\tline = fd.readline()\n",
    "\t\twhile line:\n",
    "\t\t\titems = line.strip().split()\n",
    "\t\t\tfor item in items[1:]:\n",
    "\t\t\t\titem = item.split(':')[0]\n",
    "\t\t\t\tif item not in features:\n",
    "\t\t\t\t\tfeatures[item] = i\n",
    "\t\t\t\t\ti += 1\n",
    "\t\t\tline = fd.readline()\n",
    "\treturn features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0dda6259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "\n",
    "import config\n",
    "\n",
    "\n",
    "def read_features(file, features):\n",
    "    \"\"\" Read features from the given file. \"\"\"\n",
    "    i = len(features)\n",
    "    with open(file, 'r') as fd:\n",
    "        line = fd.readline()\n",
    "        line =fd.readline()\n",
    "        #print(line)\n",
    "        while line:\n",
    "            items = line.strip().split()\n",
    "            print(items)\n",
    "            for item in items:\n",
    "                #print(item)\n",
    "                item = item.split(',')\n",
    "                #print(item)\n",
    "                if item not in features:\n",
    "                    features[item] = i\n",
    "                    i += 1\n",
    "            line = fd.readline()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86ad94dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "import config\n",
    "\n",
    "\n",
    "def read_features(file, features):\n",
    "    \"\"\" Read features from the given file. \"\"\"\n",
    "    i = len(features)\n",
    "    fd=pd.read_csv(file,sep=',')\n",
    "    nrow=fd.shape[0]\n",
    "    ncol=fd.shape[1]\n",
    "    lists=[[] for i in range(nrow)]\n",
    "    for j,col_value in range(ncol):\n",
    "        lists[i].append(col_value)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4528f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3b43b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[190 220 490 ... 360 370 420]\n",
      " [110 220 500 ... 340 260 420]\n",
      " [100 300 500 ... 330 350 390]\n",
      " ...\n",
      " [450 360 500 ... 430 360 500]\n",
      " [420 350 530 ... 440 370 500]\n",
      " [420 360 540 ... 430 370 450]]\n",
      "{190: 0, 220: 1, 490: 2, 450: 3, 380: 4, 160: 5, 140: 6, 410: 7, 520: 8, 470: 9, 340: 10, 150: 11, 260: 12, 500: 13, 320: 14, 200: 15, 170: 16, 290: 17, 280: 18, 130: 19, 230: 20, 350: 21, 440: 22, 580: 23, 240: 24, 390: 25, 370: 26, 250: 27, 100: 28, 560: 29, 300: 30, 430: 31, 270: 32, 330: 33, 110: 34, 210: 35, 570: 36, 590: 37, 530: 38, 180: 39, 710: 40, 620: 41, 510: 42, 120: 43, 550: 44, 360: 45, 420: 46, 400: 47, 70: 48, 310: 49, 460: 50, 480: 51, 760: 52, 90: 53, 680: 54, 540: 55, 700: 56, 670: 57, 630: 58, 640: 59, 660: 60, 600: 61, 690: 62, 610: 63, 650: 64, 730: 65, 770: 66, 60: 67, 790: 68, 80: 69, 810: 70, 720: 71, 740: 72, 750: 73, 780: 74, 800: 75, 50: 76, 40: 77, 20: 78, 30: 79, 10: 80, 1: 81, 830: 82, 840: 83, 890: 84, 910: 85, 820: 86, 860: 87, 880: 88, 850: 89, 870: 90, 900: 91, 920: 92, 930: 93, 940: 94, 950: 95, 960: 96, 970: 97, 1000: 98, 980: 99}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "import config\n",
    "\n",
    "features={}\n",
    "file=\"data/frappe/c_df_v_fff2000.csv\"\n",
    "num = len(features)\n",
    "fd=pd.read_csv(file,sep=',')\n",
    "nrow=fd.shape[0]\n",
    "ncol=fd.shape[1]\n",
    "n_fd=np.array(fd)\n",
    "#print(n_fd)\n",
    "n_fd=n_fd[:,2:]\n",
    "print(n_fd)\n",
    "#features={}\n",
    "\n",
    "for i, row_list in enumerate(n_fd):\n",
    "    for j, col_value in enumerate(row_list):\n",
    "        #print(col_value)\n",
    "        if col_value not in features:\n",
    "            features[col_value]=num\n",
    "            num=num+1\n",
    "\n",
    "print(features)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e94b02c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.train_libfm=\"data/frappe/frappe.train.libfm\"\n",
    "config.valid_libfm=\"data/frappe/frappe.validation.libfm\"\n",
    "config.test_libfm=\"data/frappe/frappe.test.libfm\"\n",
    "csv_data=\"data/frappe/c_df_v_fff2000.csv\"\n",
    "label_file=\"data/frappe/label.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83e3493c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0,0,190,220,490,450,380,160,140,160,410,520,470,340,220,340,150,470,260,490,500,320,260,200,170,290,280,130,230,150,350,470,230,170,140,440,450,520,580,340,470,240,160,150,450,390,140,260,370,220,350,440,230,250,370,440,100,220,370,340,560,470,410,370,300,430,520,260,270,240,330,130,450,490,240,110,110,210,240,240,520,570,390,560,590,200,530,180,130,300,300,370,710,410,450,300,620,510,510,120,450,550,350,220,360,420,230,390,330,530,270,150,400,500,570,380,70,400,230,310,580,430,460,460,480,190,140,570,470,520,240,330,480,760,550,90,510,370,580,120,450,360,400,100,350,260,360,350,420,170,160,480,390,320,400,260,450,360,410,510,440,370,230,210,390,370,400,360,680,310,420,520,560,70,190,170,480,180,180,510,170,260,100,230,180,240,310,360,430,300,360,250,370,120,110,430,540,440,500,520,310,550,230,330,230,410,700,370,560,290,360,530,190,200,450,280,420,250,440,340,170,390,180,240,530,550,350,300,500,360,500,470,230,160,250,410,170,190,670,400,490,220,220,590,150,460,180,520,470,180,200,420,140,330,370,340,220,270,260,450,390,220,140,480,190,250,180,120,270,340,630,140,640,220,110,110,280,540,150,240,180,310,390,160,210,340,380,310,210,150,460,120,470,430,570,250,170,170,450,170,220,210,230,150,310,240,670,500,220,360,390,350,280,120,640,280,150,230,470,230,420,160,390,410,470,460,380,400,510,520,200,550,340,500,280,460,240,180,500,470,410,340,330,660,140,390,230,540,370,390,150,170,390,130,530,360,340,380,520,180,380,150,450,430,420,330,140,560,240,420,530,410,440,560,420,450,570,470,550,310,460,290,420,380,490,140,260,250,160,120,290,220,310,440,270,500,270,540,430,160,560,340,360,440,490,430,210,130,460,330,320,170,420,360,490,400,320,420,430,640,320,570,370,480,320,420,350,430,340,290,370,320,300,130,430,460,180,590,170,490,430,540,510,600,690,150,400,160,420,510,140,330,150,320,350,380,140,390,520,200,620,90,440,230,220,240,300,420,480,180,510,470,450,320,380,540,120,360,250,410,510,630,390,120,150,200,140,230,170,310,340,510,190,430,450,110,460,330,540,390,300,480,120,170,210,470,180,400,400,270,530,560,390,190,300,440,130,340,660,180,420,190,590,200,320,120,200,480,380,210,300,370,250,390,340,560,160,130,350,370,520,550,560,290,140,150,400,220,160,220,440,250,120,360,350,200,420,540,330,370,120,270,300,160,410,280,420,390,430,550,320,480,340,420,500,180,220,130,580,580,530,470,230,410,250,90,290,140,180,460,150,610,170,330,310,190,240,430,180,350,100,490,460,480,470,370,220,140,150,450,540,230,180,200,210,470,210,280,420,180,340,110,220,300,180,460,150,160,190,120,170,540,290,160,570,320,310,400,500,260,460,190,170,370,200,530,290,500,340,100,240,150,460,360,500,100,280,430,450,330,610,520,120,480,490,170,650,140,200,460,120,230,490,540,490,380,330,310,350,610,410,560,520,520,630,590,540,180,570,210,360,280,550,360,370,510,540,420,430,620,350,530,570,210,400,520,120,450,240,210,620,120,130,150,170,140,320,310,410,220,610,250,420,440,320,460,370,390,310,170,190,160,140,590,320,230,430,270,230,320,140,490,400,390,560,460,350,200,470,410,490,400,290,120,210,380,280,170,360,380,290,360,130,190,290,200,560,450,430,100,380,160,150,230,170,290,370,390,470,390,460,420,220,510,280,150,160,250,410,300,120,300,420,140,370,100,130,110,180,210,290,580,490,250,210,530,130,370,120,210,400,180,340,280,250,190,330,520,250,420,390,150,190,350,410,230,130,190,490,170,160,370,310,200,420,120,300,490,180,420,140,230,160,350,240,190,320,230,400,130,460,120,260,430,200,360,220,200,380,400,360,270,260,520,250,440,200,380,520,230,290,410,410,330,260,310,400,200,170,150,150,480,280,130,180,490,210,440,330,340,290,580,450,380,360,280,100,150,460,170,290,380,110,360,150,270,360,410,200,170,660,390,150,230,220,410,150,490,450,150,410,550,280,390,110,150,550,400,270,420,480,420,690,370,110,170,360,130,480,550,520,320,540,150,160,120,250,190,210,150,280,230,120,190,110,250,290,140,250,170,160,270,420,330,420,120,560,490,430,390,240,110,500,430,410,420,400,380,430,490,400,330,520,120,230,360,150,410,140,320,390,170,150,510,200,260,170,260,150,390,430,510,410,540,150,120,440,90,280,400,250,500,370,380,480,120,630,520,210,440,290,310,450,540,620,430,480,400,420,140,610,520,430,620,170,600,260,100,100,160,130,270,140,400,210,190,320,130,390,220,520,300,300,90,160,130,320,180,370,270,320,200,260,240,440,550,260,160,130,290,260,450,280,170,240,620,170,220,290,330,210,220,190,180,170,140,120,170,370,260,200,160,160,130,180,230,100,480,250,460,140,180,160,200,210,210,190,170,140,330,470,200,550,320,350,140,460,320,590,390,470,230,710,700,410,510,440,230,700,550,660,530,760,730,700,310,400,460,470,240,180,120,100,240,240,150,130,470,290,420,460,420,500,150,540,260,210,450,430,100,410,170,380,340,390,370,160,490,480,500,500,410,480,260,130,120,290,590,170,120,380,220,430,420,370,160,130,180,100,260,340,490,260,490,480,290,130,460,140,320,370,110,200,200,410,180,760,140,210,140,110,250,400,190,120,400,190,180,90,420,610,400,530,260,290,110,150,130,130,220,130,160,120,430,460,460,280,360,440,490,200,630,130,130,170,360,340,430,190,450,240,300,90,520,390,180,130,280,190,160,670,430,400,320,410,200,110,340,170,130,130,520,400,150,280,390,430,150,100,560,150,430,310,530,190,100,200,190,170,700,580,220,180,170,650,110,380,410,220,260,490,170,420,470,180,130,210,770,290,300,390,290,620,120,140,150,420,120,130,270,300,470,110,210,210,450,100,490,140,130,130,140,360,370,200,350,410,710,370,450,250,130,150,230,200,380,590,170,350,150,600,110,460,450,140,410,690,430,280,450,160,290,230,230,450,610,490,390,360,340,180,450,150,590,310,320,490,570,210,330,280,500,410,410,150,430,580,160,470,290,630,150,580,350,310,550,550,510,520,520,360,490,340,170,510,650,150,150,140,140,730,390,210,230,380,240,380,310,280,190,210,160,140,450,260,300,190,380,430,340,440,300,450,560,390,490,560,360,580,140,160,490,590,450,150,230,360,230,480,160,150,190,450,150,200,180,340,60,290,120,270,170,310,180,180,140,360,270,210,380,320,490,410,150,460,460,290,250,180,190,140,430,160,160,240,480,560,560,210,640,310,180,360,650,450,210,190,480,150,110,150,150,280,250,430,310,250,120,340,140,420,110,170,270,150,580,130,330,520,180,140,250,200,340,180,330,360,290,200,550,140,120,390,350,310,340,130,260,280,430,390,150,450,140,240,140,320,340,400,230,150,370,400,650,670,240,200,200,320,140,280,340,200,170,220,160,530,380,260,170,620,110,440,210,630,350,200,140,190,370,320,280,160,500,180,250,140,250,400,440,300,190,440,110,170,570,540,200,390,410,390,290,190,200,290,370,660,440,640,520,270,310,180,450,120,460,130,100,450,410,380,230,330,550,260,230,430,290,260,290,300,160,300,420,150,350,260,460,360,270,350,370,160,330,560,520,140,140,520,400,470,170,100,230,660,470,400,270,560,380,510,540,450,480,330,130,500,530,200,130,260,140,400,120,120,450,170,340,210,540,540,350,410,390,200,230,590,470,390,380,450,450,400,440,370,150,230,180,420,280,310,380,300,540,400,270,540,610,170,300,480,160,200,360,410,650,580,510,520,310,440,240,220,340,160,300,120,370,400,200,200,260,130,580,470,420,450,120,270,260,160,430,380,480,390,110,140,330,160,360,150,220,210,210,270,160,230,600,440,200,400,120,310,130,790,300,630,670,190,130,190,330,110,180,610,490,560,410,480,340,230,500,160,340,500,370,260,510,170,170,270,400,70,230,120,540,620,140,150,160,370,110,360,440,360,250,80,130,430,350,210,120,320,160,190,400,350,360,280,580,550,210,230,360,270,430,540,390,200,280,360,270,200,230,150,120,200,80,270,170,410,230,220,210,560,300,310,130,420,370,190,110,230,510,390,130,280,210,260,430,450,260,250,170,190,270,170,350,270,560,160,400,360,620,370,500,190,370,300,400,570,400,430,460,160,130,350,170,450,220,310,280,430,320,680,180,540,480,160,370,320,520,320,600,470,390,440,410,340,320,620,380,300,340,470,380,450,490,300,450,250,160,320,470,230,150,240,770,200,270,440,260,180,320,320,460,320,440,120,410,250,490,490,550,350,270,500,490,540,520,460,410,260,520,420,400,180,250,150,340,570,180,170,250,440,180,140,290,620,380,450,380,440,110,300,370,130,160,420,440,360,400,140,500,220,150,480,340,220,220,530,150,550,390,400,490,330,430,190,530,170,350,340,370,220,130,560,200,250,360,440,310,340,420,360,380,490,160,100,390,390,150,420,390,160,360,370,390,170,490,200,370,320,430,480,190,350,530,460,430,420,280,390,390,400,280,320,470,300,450,160,190,450,360,390,440,450,350,200,310,390,110,230,220,420,220,370,270,190,190,120,150,390,380,410,250,200,240,330,320,350,340,500,410,190,380,460,410,350,470,400,370,240,180,240,350,250,260,290,360,410,240,470,180,320,370,360,200,500,400,400,430,350,460,430,390,280,250,220,510,190,290,410,260,360,370,500,410,210,480,160,460,400,380,450,350,610,130,330,220,170,340,330,260,270,450,470,190,240,500,160,450,290,240,530,310,350,340,220,470,290,300,220,340,140,410,360,460,280,510,450,320,460,460,270,640,370,410,430,480,350,380,320,190,310,350,200,350,470,370,330,530,470,310,380,410,290,420,420,450,490,440,160,410,280,190,210,320,410,170,450,480,340,220,440,500,540,440,190,370,450,260,430,310,460,240,480,350,380,360,330,270,470,230,310,270,400,360,340,200,250,620,350,560,390,450,220,320,360,390,530,520,320,380,430,410,440,360,270,120,430,290,240,270,370,510,450,420,270,260,380,560,180,180,400,170,360,360,360,200,610,480,190,300,180,430,360,160,300,470,540,240,180,470,210,350,210,420,330,330,170,440,510,560,420,510,630,370,500,330,640,640,350,330,460,220,340,210,500,410,260,170,340,100,500,140,350,340,350,460,250,340,450,410,80,250,100,370,500,340,300,350,190,540,170,180,180,490,320,130,170,180,270,160,130,320,210,160,320,440,290,200,120,520,220,340,550,230,400,190,160,420,130,440,130,240,640,150,320,360,530,470,540,410,140,360,410,520,360,330,300,160,450,140,560,370,360,220,360,240,160,100,140,430,230,180,340,100,150,150,420,250,470,330,300,190,300,270,450,390,430,170,140,420,150,320,440,220,540,180,250,320,290,450,380,190,380,530,680,160,500,410,530,160,250,380,490,250,400,270,100,390,520,650,190,180,300,130,130,280,130,380,280,350,420,310,300,190,420,350,130,550,170,130,400,130,440,280,230,150,570,120,180,150,380,410,210,810,120,170,510,220,460,190,160,320,250,180,540,120,360,440,150,270,360,160,400,300,440,320,270,200,490,110,270,290,390,400,220,380,380,120,150,160,160,210,160,190,180,350,170,140,260,130,80,160,120,280,220,240,160,140,280,130,130,160,420,430,340,270,430,170,220,420,560,160,450,320,190,170,410,470,260,220,190,450,420,230,280,300,400,480,510,170,230,370,430,440,340,170,370,380,480,450,210,230,160,130,470,200,390,450,560,300,470,450,460,420,240,420,400,290,190,360,120,390,430,140,360,380,320,310,400,150,210,300,430,370,320,420,190,470,120,220,400,280,370,490,140,160,470,400,410,170,450,140,360,490,630,410,560,370,430,280,570,350,210,300,140,230,420,370,710,190,550,320,130,410,490,440,390,410,160,440,350,200,580,550,450,320,270,460,510,450,440,360,500,330,360,450,370,270,280,130,190,320,380,310,450,150,300,340,380,130,130,190,130,400,220,220,460,410,490,450,190,490,390,380,540,590,160,220,410,480,250,470,160,260,250,390,260,330,460,420,390,590,380,440,360,360,280,140,230,330,110,220,530,460,300,140,410,170,140,230,340,140,450,340,350,160,360,250,100,110,380,400,350,240,240,290,200,570,410,110,300,330,250,390,140,180,240,170,190,140,400,170,530,200,240,420,640,300,420,170,100,720,100,140,410,180,460,210,200,160,150,380,310,470,360,190,170,310,420,560,150,250,340,110,410,570,170,120,150,300,310,480,170,300,160,150,140,220,370,560,360,320,180,170,300,350,380,320,590,540,470,480,320,100,130,140,80,160,170,470,200,130,410,450,200,200,330,540,150,210,220,150,440,160,460,590,210,120,140,160,180,260,210,520,190,130,290,110,130,490,250,320,140,360,340,220,440,520,260,490,220,180,150,170,320,170,170,180,150,150,410,320,180,300,350,250,330,410,560,160,590,270,130,160,380,330,190,200,260,110,160,290,470,140,190,130,300,430,100,380,140,140,250,110,170,260,430,490,150,440,510,640,130,130,460,140,260,200,320,310,200,640,210,430,160,130,570,270,230,240,530,430,420,400,90,160,140,150,180,220,550,190,180,400,550,200,260,210,520,170,270,240,450,190,360,240,470,160,130,440,370,240,280,300,570,170,270,460,250,390,180,180,380,500,550,380,240,410,490,510,350,210,450,110,410,470,130,160,270,130,110,160,410,160,280,210,100,400,110,190,380,260,200,130,270,280,150,470,520,340,320,440,120,550,370,370,510,490,100,680,470,420,560,300,300,210,650,290,400,380,420,500,140,120,250,260,320,490,490,130,110,150,430,190,520,200,440,580,380,280,390,330,450,370,550,180,310,410,200,150,350,380,480,190,140,470,550,150,190,210,660,390,160,180,240,230,430,340,130,240,150,290,170,610,330,420,290,510,520,620,470,150,170,190,130,200,210,520,320,550,250,240,310,210,90,210,190,170,270,210,580,210,150,120,320,200,140,150,500,100,410,130,180,130,180,290,160,170,120,110,190,180,260,160,300,260,90,290,160,240,150,240,380,190,230,530,120,210,260,470,220,460,490,550,360,310,770,560,520,140,380,480,330,350,440,360,130,90,270,120,70,280,240,180,480,570,620,510,350,510,350,100,380,270,350,100,680,570,660,460,550,320,160,410,150,210,370,610,550,220,260,730,170,240,250,140,170,180,160,230,170,410,240,460,470,380,470,530,450,420,420,460,450,410,400,470,270,470,560,500,410,480,440,510,650,320,130,270,270,200,170,380,220,230,410,420,160,180,150,160,340,130,200,140,190,160,130,110,320,620,130,150,160,130,510,160,490,450,330,420,560,500,740,90,410,550,550,230,550,260,430,500,300,470,330,440,430,460,420,300,420,320,180,320,410,470,420,280,500,260,170,300,270,160,260,220,420,160,160,270,160,300,140,140,480,350,90,310,170,550,460,230,250,370,390,330,400,300,270,300,290,480,180,390,210,250,160,130,350,320,370,370,340,440,310,440,550,400,370,240,150,100,190,610,200,310,110,160,200,340,390,130,330,110,430,400,370,510,180,460,260,600,280,240,570,410,190,330,400,320,210,340,190,120,300,360,400,160,520,180,430,360,190,150,150,110,90,230,230,330,230,450,530,610,650,610,590,120,100,140,590,150,190,200,430,160,350,150,210,240,210,200,150,390,410,520,100,380,540,560,190,550,560,340,260,630,140,410,90,180,190,160,180,280,170,320,150,430,360,290,320,120,210,280,400,220,220,510,140,550,270,540,670,350,530,340,280,140,350,600,120,320,410,450,530,490,370,400,220,310,130,540,390,530,300,390,190,640,210,160,430,510,370,190,210,320,300,190,290,410,220,500,390,460,470,270,570,130,650,470,230,150,270,190,480,440,150,560,260,190,490,270,610,530,580,460,550,640,120,670,490,570,220,130,190,190,360,220,280,370,420,310,440,370,150,130,140,140,520,270,230,290,410,370,320,300,310,240,400,170,170,430,210,340,410,180,120,120,340,310,260,730,470,450,230,390,430,250,190,170,390,470,340,190,530,260,420,140,440,360,480,380,650,80,200,440,180,570,330,130,140,590,360,300,220,440,370,340,630,280,310,560,700,350,400,670,170,450,390,750,180,490,510,250,660,360,310,100,80,200,120,190,100,130,130,120,190,140,130,250,140,120,140,400,160,230,410,80,250,130,310,280,290,260,100,130,110,280,380,330,280,670,280,240,460,550,550,500,500,550,340,460,250,180,630,290,420,170,450,330,270,370,370,100,160,80,130,90,110,400,90,490,310,130,100,410,470,290,140,140,370,330,400,370,170,250,180,620,450,230,170,290,430,580,530,370,370,690,340,490,170,650,470,530,560,170,110,170,110,160,370,520,310,130,130,400,230,140,140,160,100,180,170,150,140,280,130,130,160,270,380,570,240,90,210,600,90,160,390,130,480,190,130,160,150,170,160,460,120,620,560,230,310,260,180,130,400,390,630,410,130,540,360,620,260,430,180,270,100,150,410,130,120,170,160,320,320,180,260,290,280,130,390,350,220,160,180,480,230,620,170,310,260,580,320,120,470,290,150,270,220,280,270,350,160,220,340,140,390,170,190,190,270,160,160,120,480,130,210,460,390,220,450,410,230,500,150,140,140,570,210,120,530,440,380,190,320,400,470,210,450,150,130,280,200,540,290,370,350,440,490,400,620,220,170,110,100,140,500,150,120,140,380,280,280,170,450,480,90,170,240,160,180,100,200,410,190,450,260,160,360,120,210,140,170,410,260,470,120,670,590,400,240,150,200,190,510,140,340,250,130,360,140,210,140,390,200,390,140,560,420,260,130,550,630,170,100,180,150,350,220,250,390,500,170,320,460,190,140,120,630,140,360,170,200,100,280,330,510,330,440,500,250,520,500,400,130,180,130,200,190,120,110,460,180,220,150,90,260,180,170,670,300,200,400,460,420,140,90,120,180,220,180,250,130,110,170,610,100,180,200,580,300,220,410,240,150,210,410,210,220,180,80,170,200,120,160,100,300,200,280,500,420,190,470,580,180,510,580,280,550,500,210,410,170,150,150,240,330,250,140,500,730,130,200,260,110,160,430,140,510,690,410,480,210,430,400,120,340,220,480,290,140,260,400,160,470,210,190,380,160,310,150,400,390,190,200,390,560,400,580,330,230,130,260,270,210,400,160,190,440,360,250,270,130,130,380,260,170,410,580,220,270,680,160,190,400,250,400,530,120,270,450,310,250,420,330,230,230,220,470,360,340,120,400,310,440,440,80,190,150,200,290,120,260,150,300,300,590,500,580,280,300,370,150,320,430,400,460,340,320,200,290,320,570,220,210,330,150,120,130,170,360,100,180,110,110,260,670,360,540,330,350,460,340,410,570,520,210,300,160,410,450,140,400,480,130,200,610,170,120,170,630,300,220,370,120,490,260,230,140,120,320,170,130,160,220,110,110,400,180,330,340,280,250,240,620,120,140,200,600,180,570,640,290,450,320,210,220,410,530,370,420,290,360,260,170,340,310,360,260,380,390,420,330,160,240,110,120,360,520,460,500,170,380,350,330,180,260,200,220,200,330,140,360,370,420,480,600,640,240,600,180,160,720,90,160,460,330,350,120,500,120,300,350,190,200,120,320,400,550,180,230,360,310,370,160,690,140,290,200,490,110,150,140,180,230,150,150,380,330,210,210,100,640,240,230,440,140,290,120,350,530,150,170,150,220,460,380,460,170,210,520,160,180,290,500,160,270,210,110,280,380,240,150,280,120,120,140,520,380,170,340,410,90,340,180,720,90,540,210,570,140,260,380,660,700,270,620,780,470,370,470,420,540,370,460,420,180,330,140,430,540,550,140,200,150,170,150,320,110,170,310,280,360,210,200,340,130,180,150,150,170,140,220,340,300,350,420,120,180,140,310,150,130,140,150,150,140,440,430,270,350,120,190,380,250,360,720,430,130,330,320,370,320,590,380,410,270,390,130,170,170,440,310,200,390,240,310,440,400,240,570,500,390,110,530,430,170,440,440,140,460,220,160,390,660,110,140,710,660,710,440,330,470,650,610,570,200,600,570,680,700,750,460,520,330,590,590,640,560,430,530,350,570,610,590,590,630,470,530,440,250,110,110,210,200,220,750,380,170,260,130,470,500,220,700,110,210,520,160,390,140,430,200,150,600,550,490,450,450,430,290,410,280,540,230,240,190,310,380,650,370,430,300,140,120,140,130,730,380,510,190,460,250,460,410,140,200,150,210,370,170,410,320,180,630,510,300,450,180,140,210,380,410,460,300,190,340,250,180,330,160,160,390,410,350,250,400,150,140,150,600,140,250,300,580,190,180,310,160,550,150,140,350,190,460,310,250,460,250,280,240,440,160,160,310,160,280,170,200,300,160,190,350,130,180,560,310,390,390,420,170,350,290,220,530,240,220,510,620,440,150,170,440,590,170,340,210,160,580,430,310,240,150,480,210,450,220,430,130,190,120,160,200,180,140,170,130,90,610,510,70,520,120,120,560,160,160,350,610,420,530,210,140,120,460,130,220,180,160,340,490,510,140,710,120,320,230,120,510,270,500,160,350,110,410,330,140,610,130,250,320,270,230,620,190,130,510,180,160,120,590,420,140,440,270,190,200,270,120,420,510,270,120,490,150,380,390,290,130,420,120,150,160,120,170,320,460,160,480,430,320,180,140,150,100,170,110,150,100,120,140,270,540,110,180,220,180,120,230,480,500,490,430,260,420,660,550,240,160,660,230,200,110,590,520,430,210,560,520,210,450,450,500,380,390,530,220,260,500,380,540,560,400,400,650,460,340,500,120,460,510,490,280,160,640,100,190,380,630,560,280,610,350,440,440,420,530,390,430,260,420,660,500,540,530,200,640,400,200,420,420,540,310,500,320,490,480,490,320,270,420,340,410,540,520,410,550,250,520,590,150,540,210,320,420,290,450,480,400,650,480,550,150,420,470,130,240,160,250,310,540,170,250,330,420,280,150,500,520,300,250,250,200,640,280,700,310,380,520,500,440,380,340,320,180,320,140,260,530,360,180,160,190,610,130,520,140,150,410,400,400,330,470,310,140,220,110,120,650,130,400,330,420,160,180,330,450,160,360,440,210,120,280,160,410,190,450,440,180,400,260,90,210,420,200,350,340,270,330,200,360,340,170,410,450,180,350,370,150,440,310,510,470,530,130,470,160,230,110,460,240,180,310,470,310,360,450,90,100,250,370,400,490,420,230,200,380,360,540,280,270,660,500,440,330,130,290,220,290,580,180,270,550,430,450,360,190,410,240,430,520,340,180,160,150,150,230,350,220,430,460,140,140,400,270,150,150,260,350,610,130,250,470,420,350,490,130,380,460,330,380,580,460,210,250,360,190,120,430,290,400,570,430,540,170,140,170,280,360,160,180,370,390,290,210,250,430,510,140,150,160,320,220,170,430,300,410,450,130,480,470,470,240,470,360,390,380,230,460,340,410,600,300,550,200,370,560,500,130,380,550,500,420,610,340,420,280,450,580,520,480,610,400,420,500,330,290,440,130,530,520,260,150,510,500,280,590,340,440,570,220,420,200,440,150,200,390,470,450,350,440,450,240,180,500,470,550,130,480,270,290,480,90,240,390,300,530,310,290,140,500,310,260,620,420,100,260,460,210,240,100,200,370,690,390,330,360,200,420,470,430,140,200,290,160,600,410,570,370,660,220,510,260,180,450,380,300,270,550,420,140,410,180,410,530,610,120,450,400,150,180,570,250,610,530,650,120,360,240,330,340,510,590,170,320,610,630,120,300,120,490,630,460,130,120,160,500,110,140,310,390,570,270,510,600,310,330,470,700,440,420,290,250,220,490,280,480,340,410,500,190,130,370,470,150,240,150,430,450,390,200,450,320,130,190,220,200,380,370,380,240,220,460,140,260,220,340,200,450,430,380,450,140,120,330,190,200,430,380,310,250,650,260,500,100,520,120,200,200,210,540,320,600,400,100,480,520,520,480,550,220,300,230,250,450,140,390,140,190,170,200,540,210,520,340,110,460,290,340,150,400,350,220,180,120,380,150,140,120,270,450,140,370,330,210,460,290,340,190,190,120,190,130,110,180,370,300,470,210,490,510,400,430,460,190,200,140,270,320,190,430,250,120,300,290,480,490,460,120,120,350,330,360,250,210,260,510,200,190,130,430,580,480,390,500,520,650,170,130,460,420,270,240,150,170,120,250,450,450,270,290,190,210,320,400,460,400,100,220,380,490,230,150,390,230,200,490,340,460,290,380,490,330,380,440,130,230,220,140,420,360,480,200,160,190,250,600,190,250,430,150,410,480,130,200,340,180,310,340,360,410,250,160,260,220,510,230,440,260,320,160,460,450,300,460,430,140,140,600,460,460,370,550,370,360,150,280,320,430,300,320,280,360,160,100,300,390,350,340,150,210,360,200,390,290,390,160,180,170,170,320,730,460,520,110,200,410,390,210,470,630,230,430,180,520,420,180,360,360,330,320,280,570,560,370,600,240,400,490,260,90,280,380,310,290,130,390,420,400,260,370,340,410,270,150,330,320,340,590,160,140,450,130,410,240,420,420,160,470,300,330,570,310,380,370,410,320,380,350,510,360,390,180,240,250,200,460,140,300,440,250,110,250,280,430,370,200,490,550,400,240,290,470,400,160,340,310,520,410,470,400,240,260,370,360,400,140,490,290,240,160,330,360,340,310,310,210,390,140,510,250,380,410,430,370,500,590,200,360,320,250,650,490,540,280,380,370,490,270,160,320,570,450,380,320,350,570,500,330,430,230,300,430,460,420,260,230,370,270,470,260,300,170,280,600,340,560,160,90,210,540,250,400,290,400,290,130,320,110,120,310,520,360,190,310,350,240,310,340,310,340,260,510,340,400,330,280,210,290,470,340,160,340,320,90,160,330,320,140,360,350,350,390,350,340,230,150,310,430,530,300,170,400,440,420,370,310,530,540,300,370,370,470,430,420,320,570,240,520,520,320,200,170,420,570,400,330,200,300,570,360,470,370,340,360,110,330,350,400,390,330,330,390,170,240,160,530,530,220,410,310,190,320,370,180,300,120,200,200,630,360,360,350,430,260,400,390,300,460,290,590,490,110,220,380,350,370,350,410,590,430,250,500,360,410,290,270,150,430,460,460,410,290,270,230,370,140,260,360,250,130,390,280,330,200,250,430,470,430,120,410,140,360,410,430,170,340,490,460,280,350,230,430,230,460,470,460,580,380,340,320,400,460,90,310,460,380,300,390,420,160,270,500,190,240,120,440,340,480,700,490,500,300,460,430,150,110,200,200,340,130,220,90,90,230,340,520,430,270,370,470,270,140,690,240,360,130,180,110,100,160,330,300,160,510,130,190,220,490,310,130,500,210,180,230,140,270,340,380,140,440,190,470,440,300,320,390,410,350,340,410,320,320,260,440,450,270,350,330,460,380,340,350,470,330,440,410,280,550,360,460,400,300,370,420,590,400,170,490,170,270,120,310,500,530,400,420,260,490,440,400,410,400,350,330,140,450,250,410,320,240,380,250,330,220,270,420,430,490,270,390,350,360,380,530,200,340,400,260,380,160,180,150,390,480,680,160,250,330,500,500,300,320,360,480,240,350,630,270,460,470,500,500,310,320,360,370,500,420,430,150,480,340,410,370,230,380,260,300,440,400,360,250,580,190,250,260,380,350,190,250,360,230,80,350,350,230,190,260,370,160,440,400,140,150,380,470,120,220,440,510,460,290,480,470,450,130,460,310,380,460,210,420,390,400,350,490,400,430,580,530,480,340,420,440,490,520,560,380,460,140,130,110,160,500,190,440,410,120,370,390,300,390,280,510,290,510,280,400,400,270,230,310,100,210,650,520,460,330,380,370,180,310,450,320,610,420,380,350,270,390,140,120,80,430,330,340,160,410,350,380,190,380,290,330,430,560,160,300,80,60,330,100,80,150,110,140,130,150,150,120,130,120,190,530,280,120,140,450,220,340,400,140,370,470,140,400,400,370,170,130,480,500,110,510,280,660,140,240,240,370,130,450,440,160,150,380,400,360,260,680,220,190,210,120,440,240,180,260,480,450,310,460,130,210,170,170,160,660,150,350,410,180,250,480,350,440,380,300,370,270,160,170,140,410,170,550,100,160,540,70,700,180,130,150,210,500,250,80,300,410,490,520,190,290,590,300,520,560,660,410,420,420,540,600,280,120,490,190,560,230,530,310,260,90,340,260,300,590,560,490,140,320,170,670,170,540,370,300,260,290,520,250,180,130,610,380,600,140,550,500,320,390,620,420,540,180,600,100,280,370,390,170,240,150,460,180,200,480,330,400,120,500,590,480,280,270,200,580,530,190,520,380,360,490,120,370,530,450,180,560,460,140,190,160,470,530,500,130,490,310,430,150,590,150,320,280,170,330,370,170,380,370,350,350,260,170,350,190,350,200,210,200,420,250,510,320,520,440,600,390,390,380,470,480,140,140,390,250,130,470,360,360,170,480,430,210,270,410,510,310,290,430,160,460,160,360,240,220,390,620,440,190,340,330,410,200,500,300,430,100,600,540,330,180,310,390,220,170,360,220,400,130,190,170,120,360,280,130,470,210,480,150,250,560,460,110,510,190,120,370,330,110,160,230,120,390,510,660,160,370,140,460,400,340,290,340,470,160,410,280,560,180,280,260,320,360,240,210,150,430,230,130,200,440,350,350,130,650,300,440,320,340,130,250,520,340,420,480,380,410,230,370,210,140,380,390,170,580,500,400,130,150,270,280,160,320,370,270,350,450,540,190,120,470,440,450,170,160,280,300,150,460,280,180,430,400,150,310,210,470,210,190,240,410,480,130,460,460,520,160,180,240,250,230,110,330,100,500,170,320,400,520,390,180,180,580,320,200,380,510,500,510,410,600,360,370,190,380,390,370,310,200,270,450,440,430,350,400,240,230,90,360,270,150,560,140,440,420,260,360,520,120,130,170,570,660,530,240,360,290,340,450,410,400,240,220,170,400,530,370,610,600,240,180,290,420,570,380,460,100,400,310,470,190,130,440,380,430,450,440,250,210,490,340,350,80,380,470,460,240,190,320,350,250,300,680,430,220,260,430,410,190,410,590,420,440,430,330,490,480,190,110,440,420,130,380,490,440,100,330,320,400,170,410,500,460,360,130,130,460,580,490,170,360,210,380,330,330,460,240,150,440,240,340,400,390,440,490,110,420,380,460,300,250,290,330,340,330,120,260,230,300,330,180,170,130,260,300,110,570,190,260,280,80,210,170,120,230,200,120,140,350,490,160,370,90,200,500,330,430,210,110,210,140,290,370,150,390,380,490,160,460,500,180,200,390,370,570,120,480,250,460,360,150,160,410,410,420,210,440,450,400,240,420,490,440,360,160,310,250,300,260,410,490,220,400,120,230,150,330,220,440,170,160,340,230,130,360,390,510,200,120,150,450,730,740,240,460,420,710,550,760,220,490,360,620,600,430,240,650,340,290,430,410,340,780,280,450,360,700,700,390,210,130,150,450,480,470,190,340,200,180,490,570,240,170,440,370,130,410,160,120,140,160,570,440,550,130,320,160,500,290,310,600,610,140,280,160,220,500,450,450,170,540,230,500,150,480,600,280,360,320,260,340,490,420,110,90,430,250,180,110,170,130,140,210,220,410,150,140,170,360,140,520,400,250,540,610,160,270,110,470,750,160,240,200,100,210,170,240,150,160,230,190,310,410,140,230,120,270,180,230,200,120,120,390,210,570,310,170,520,610,470,440,390,410,230,460,380,310,280,400,390,360,200,660,320,120,320,220,410,570,480,130,220,310,260,420,460,170,270,170,260,180,540,140,130,130,230,180,300,200,320,180,490,100,190,180,190,270,130,320,410,180,300,160,160,110,390,130,470,650,510,260,480,180,120,120,430,90,100,170,180,270,420,130,300,370,320,470,570,170,420,370,240,250,450,180,410,210,310,230,330,130,390,170,300,430,130,420,350,170,480,400,130,310,480,340,150,300,370,370,150,330,270,130,180,230,420,220,120,380,310,340,200,400,330,470,390,390,130,130,250,390,230,410,170,200,150,190,320,510,210,400,170,240,140,310,190,310,410,290,170,540,280,290,250,450,570,420,210,150,120,360,110,340,390,490,340,510,210,430,300,340,330,140,480,120,150,220,240,410,120,110,350,330,290,450,160,350,250,170,90,160,580,370,190,470,160,320,520,500,370,310,400,140,310,380,420,350,320,390,280,230,310,240,380,270,370,170,410,190,490,450,230,350,410,170,410,290,210,190,450,230,250,90,300,240,360,420,730,330,300,160,270,250,290,280,150,350,310,240,220,180,140,350,610,370,360,230,180,410,320,150,130,580,530,400,400,520,290,240,200,510,480,130,510,390,190,180,500,230,340,190,200,200,480,410,250,360,210,270,340,200,370,310,200,160,300,530,180,510,560,340,620,180,140,370,690,220,570,240,310,230,500,380,330,440,500,420,160,140,120,200,140,110,170,350,140,520,220,130,180,360,130,470,230,290,160,140,290,330,420,180,490,150,110,420,150,380,580,200,250,630,160,520,230,380,490,600,660,300,130,140,260,320,280,350,450,150,290,300,230,630,370,500,200,380,180,450,420,420,480,450,420,410,480,470,250,350,260,360,290,170,540,590,510,250,210,130,180,490,450,370,260,380,330,280,530,260,500,330,300,440,200,250,140,330,280,450,340,130,220,200,240,230,400,510,390,540,350,390,430,360,580,480,600,370,340,510,450,430,460,700,330,610,460,390,440,410,140,160,130,360,200,330,190,120,240,280,480,310,440,350,290,380,270,420,240,80,440,180,400,200,210,390,640,210,450,130,90,240,760,500,230,170,580,160,120,490,160,590,210,320,230,150,150,550,120,170,190,390,410,480,660,610,310,480,510,400,360,160,290,590,440,550,620,570,650,450,150,410,170,360,110,140,180,80,110,450,480,400,150,510,120,470,370,540,110,320,420,440,90,430,470,420,430,90,220,330,390,410,410,250,330,160,240,160,230,350,150,510,440,370,440,420,460,240,550,620,170,590,140,600,410,410,470,360,130,540,380,310,180,420,420,180,480,100,160,220,210,270,180,610,120,190,180,160,150,140,420,350,130,430,170,280,410,410,310,360,260,360,510,340,620,410,420,550,420,120,630,390,360,220,510,280,370,90,150,180,280,370,340,430,130,300,420,340,120,210,230,210,250,160,280,420,490,280,170,390,450,130,340,440,470,330,320,510,160,600,240,130,130,80,530,130,510,430,610,110,110,690,420,290,220,290,220,280,680,170,290,290,420,380,200,500,160,370,360,310,150,280,380,480,370,480,520,310,140,160,220,500,110,460,220,320,300,300,200,210,170,370,150,390,380,260,280,240,410,180,260,170,530,150,210,410,150,290,320,400,400,620,440,370,470,180,150,380,270,450,190,570,140,260,270,390,500,150,200,250,330,360,130,420,590,270,190,420,300,610,270,120,500,380,190,490,280,590,460,270,350,350,220,170,220,240,470,220,180,310,150,530,180,380,370,130,150,140,300,230,210,160,150,480,170,460,530,160,490,310,330,370,360,410,160,130,430,370,480,280,380,300,450,140,420,570,110,430,460,210,480,160,360,370,730,190,260,340,240,430,240,110,160,130,360,410,200,110,260,120,130,140,140,120,130,430,460,470,330,130,560,90,110,430,420,290,650,510,560,380,220,190,180,260,330,370,440,210,230,670,550,560,320,150,130,450,410,370,120,140,140,110,150,680,390,260,400,140,510,230,400,280,150,740,260,540,420,100,540,400,430,430,340,530,430,510,390,290,330,370,460,370,450,420,460,470,450,420,160,130,280,110,300,210,470,630,570,200,100,450,740,380,220,350,440,350,240,110,190,330,570,620,500,280,450,400,210,340,360,370,240,380,300,120,210,350,430,530,410,500,560,440,400,460,280,160,440,610,450,120,200,160,380,190,300,150,300,250,420,170,240,340,230,220,260,390,240,220,180,90,160,120,210,180,180,170,210,180,270,300,500,190,430,110,290,430,460,640,470,320,210,340,230,580,510,370,580,630,540,320,220,470,380,200,530,370,340,180,200,160,360,240,210,260,550,210,410,370,290,450,330,130,380,260,100,380,390,210,460,120,180,230,190,310,650,150,420,180,190,300,140,190,160,250,100,320,330,170,90,330,120,340,170,240,150,260,300,300,280,300,260,490,280,360,450,470,270,640,140,440,290,360,300,320,280,330,320,180,150,290,180,160,460,270,260,180,490,320,350,150,450,380,190,160,190,280,190,310,140,260,310,120,150,400,140,270,200,290,290,430,510,190,350,420,630,380,510,180,450,210,380,190,350,260,330,170,590,210,230,220,340,180,160,220,300,400,390,620,480,630,430,150,510,190,350,380,240,240,470,390,180,210,220,530,460,420,590,210,420,180,480,470,190,540,370,360,280,160,440,300,200,330,410,260,190,410,630,310,450,100,430,120,280,490,460,510,320,340,420,290,160,400,370,250,200,370,400,340,300,430,190,160,450,310,550,470,240,240,480,470,260,360,700,420,290,150,200,360,260,490,410,480,200,250,420,400,310,340,260,320,260,170,250,380,400,280,430,180,420,400,320,480,200,440,650,110,390,330,140,310,130,140,260,150,430,180,200,320,230,240,140,360,170,330,110,310,160,340,240,200,270,490,410,250,410,380,490,270,440,350,430,330,210,370,480,260,390,330,520,460,250,160,410,460,230,310,340,290,430,380,200,230,330,150,460,450,390,380,520,190,210,300,300,410,390,360,320,420,500,360,280,280,350,360,420,320,400,200,280,400,300,190,410,470,140,160,220,180,330,410,350,290,450,470,360,300,160,250,210,430,340,400,180,160,390,310,280,150,150,330,400,150,190,170,220,350,360,510,220,350,220,330,300,540,590,260,500,410,110,330,150,270,120,260,150,250,160,460,300,290,180,570,160,140,220,380,120,370,500,190,290,240,210,240,120,310,420,340,390,410,300,440,150,210,440,140,200,180,380,320,200,140,220,300,320,340,140,400,300,330,120,390,390,180,220,230,480,420,230,270,490,230,370,330,440,340,370,210,510,300,230,180,180,380,300,100,270,140,370,440,310,250,300,230,250,170,270,180,400,190,110,530,160,390,190,440,280,400,400,110,390,210,290,270,370,360,240,170,260,290,170,240,410,130,230,300,240,550,230,270,290,100,100,290,170,370,300,260,290,380,210,280,430,290,190,200,260,180,410,340,610,190,360,110,490,100,360,130,120,170,240,280,320,530,440,480,230,290,370,210,440,290,210,300,300,330,350,380,370,350,460,490,470,440,320,480,450,210,320,430,460,130,560,270,360,420,170,410,480,230,210,350,140,190,140,390,460,130,570,380,370,700,560,310,520,330,370,290,520,170,240,190,420,300,200,220,150,340,400,250,410,260,250,450,540,90,420,200,540,110,200,200,140,490,210,490,400,300,370,270,560,140,190,210,260,240,320,130,230,150,250,520,400,200,300,380,310,530,190,320,300,430,490,330,330,440,110,120,440,560,390,430,530,160,150,150,380,590,600,480,150,130,130,190,450,380,130,690,180,150,210,550,140,170,430,280,160,350,490,340,140,290,160,260,160,520,460,400,460,210,150,390,470,160,540,330,270,440,420,230,150,370,170,230,130,410,170,420,490,340,310,470,580,360,510,300,140,270,280,350,230,140,150,220,280,350,400,410,410,140,110,260,250,260,380,130,110,110,400,150,140,290,400,220,120,130,220,360,260,340,160,380,410,470,380,450,350,310,210,450,510,430,150,160,230,430,480,190,150,400,520,180,670,340,190,340,140,530,410,430,330,490,520,360,350,440,490,650,330,290,200,150,330,130,210,360,240,190,370,550,590,230,520,510,330,170,420,440,110,460,240,420,370,320,150,250,180,250,220,280,610,590,110,130,140,260,150,300,310,200,210,330,210,160,470,280,150,320,350,310,460,370,390,440,360,380,500,290,180,110,200,520,290,280,260,110,470,150,390,160,130,540,160,420,160,280,410,260,150,380,150,320,530,140,480,490,170,470,260,390,370,240,540,220,390,350,320,450,480,430,370,300,600,140,560,410,370,290,690,280,210,530,470,440,360,320,110,300,230,180,180,170,320,190,150,320,110,450,290,180,320,240,430,150,340,150,470,450,490,510,130,400,480,360,410,230,200,220,280,310,300,330,570,260,440,400,280,510,420,500,110,650,430,190,110,390,220,430,160,420,420,500,180,310,630,360,310,310,190,270,130,180,220,560,250,300,220,280,380,180,410,350,340,440,260,460,190,340,420,160,430,290,540,430,430,120,200,470,360,130,440,220,160,520,420,240,100,300,290,280,300,360,270,440,130,240,480,200,360,160,210,290,510,420,150,210,290,430,230,340,460,290,430,190,420,170,540,470,200,340,260,290,420,430,420,370,500,500,380,280,260,180,340,270,410,320,300,290,200,330,210,170,100,440,260,130,150,380,300,210,410,180,300,560,170,160,160,250,480,310,170,170,120,310,120,190,310,510,170,250,150,150,190,580,450,290,120,470,380,230,190,200,650,390,620,520,260,550,440,570,510,660,570,440,320,680,410,680,350,160,330,410,340,420,120,150,100,180,150,320,210,440,380,140,250,370,310,340,270,300,180,130,380,140,240,140,260,140,220,250,370,710,370,480,580,470,410,230,470,410,290,360,420,150,160,450,350,120,180,220,600,130,130,150,170,100,250,540,210,400,690,340,560,390,330,490,460,480,280,420,130,540,90,150,300,200,220,200,300,310,270,110,90,420,100,310,510,680,530,470,410,170,150,170,150,190,450,490,320,490,120,440,270,170,230,140,100,520,400,500,190,500,100,350,160,620,490,290,240,310,280,350,150,460,190,140,400,300,130,220,340,250,100,430,350,330,360,180,180,300,140,520,380,130,500,160,500,470,500,440,360,280,480,600,330,480,400,380,440,440,410,550,540,410,410,430,170,180,210,270,270,210,300,380,180,250,130,230,450,430,300,360,170,260,170,460,440,470,310,230,430,210,270,210,450,480,390,260,540,130,270,560,320,170,290,120,290,370,240,310,240,130,140,180,480,450,670,160,220,370,460,130,210,180,400,250,370,530,130,530,430,210,400,210,280,420,420,180,220,160,130,190,180,430,120,180,470,560,150,430,120,280,410,270,100,460,410,130,500,460,520,390,460,190,240,160,510,450,310,300,230,150,490,470,190,630,220,590,430,390,330,430,260,190,220,430,590,420,120,350,230,270,580,230,360,270,300,140,110,160,110,340,330,420,340,180,330,250,440,460,190,410,150,470,480,540,380,170,440,390,350,270,330,430,570,160,110,110,260,250,680,450,440,150,420,280,330,440,140,560,620,140,180,150,440,270,260,300,110,310,220,130,510,300,330,360,120,230,100,280,490,140,180,530,230,130,140,410,480,610,670,170,220,260,220,450,450,140,360,140,340,290,340,240,280,290,480,500,550,270,400,150,340,360,440,150,530,210,460,240,140,460,380,450,230,270,150,380,160,150,380,170,150,560,500,440,490,390,160,370,340,250,160,170,490,320,260,300,160,460,450,190,380,190,180,440,460,140,110,150,540,200,270,250,500,150,450,280,270,490,300,240,220,250,330,420,140,290,420,570,260,320,550,460,230,160,490,120,210,120,250,260,160,380,340,110,210,150,350,170,200,560,540,210,110,520,470,390,370,540,440,430,360,410,160,470,200,240,110,560,150,380,460,130,600,480,240,340,140,640,120,340,550,520,480,140,670,220,510,380,400,160,230,310,640,400,330,630,170,110,500,350,470,620,670,710,140,270,660,160,220,210,190,380,400,420,300,380,170,230,140,270,190,140,170,90,200,150,490,380,200,390,490,210,460,480,260,400,500,190,460,140,330,220,300,190,200,160,170,290,160,360,140,380,150,390,440,710,480,110,320,200,200,300,310,450,290,310,420,130,220,150,370,330,230,270,170,390,160,230,380,490,360,360,450,440,190,110,110,170,480,240,240,340,310,340,450,190,90,210,260,120,520,460,250,130,190,240,250,370,520,340,380,170,480,470,410,390,190,490,500,450,560,500,640,320,180,350,560,350,440,440,240,260,450,300,460,390,140,240,190,720,300,490,460,610,380,500,230,410,280,330,200,270,620,470,120,460,180,440,270,280,570,470,550,140,400,670,410,80,260,330,230,290,250,590,260,450,180,270,380,240,460,340,510,380,360,230,160,350,110,150,220,140,120,230,230,330,380,130,400,410,150,260,380,250,520,210,290,460,260,450,270,330,470,280,470,490,250,340,280,260,200,560,580,360,410,420,530,480,650,490,450,390,390,390,270,270,360,340,250,570,350,270,350,160,470,450,260,160,340,470,130,250,170,320,460,540,270,250,300,590,150,150,430,200,160,250,690,220,140,550,190,430,510,180,260,470,160,190,440,240,300,180,690,280,200,120,510,410,600,240,160,240,240,220,350,630,200,330,490,140,220,470,430,470,390,380,300,240,190,380,350,350,140,430,410,420,220,390,390,130,310,440,580,600,390,260,280,290,220,390,200,390,390,290,470,300,190,150,320,320,400,440,140,120,330,400,360,470,190,170,200,260,350,300,390,260,350,140,420,360,430,380,300,550,330,510,390,380,420,500,390,430,210,410,410,500,410,250,180,110,190,230,340,490,160,190,190,250,220,280,230,490,280,170,330,150,430,260,290,420,330,370,230,460,400,360,350,280,440,460,140,340,150,440,190,260,340,140,430,320,150,190,430,410,130,410,200,510,210,130,460,180,210,120,340,440,470,430,260,390,450,310,320,430,450,400,360,350,320,540,370,160,300,230,330,390,340,490,500,250,360,370,420']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-c053e0af6e03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#features = read_features(config.train_libfm, features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-35e28d70daeb>\u001b[0m in \u001b[0;36mread_features\u001b[0;34m(file, features)\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;31m#print(item)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                     \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "features={}\n",
    "#features = read_features(config.train_libfm, features)\n",
    "features = read_features(csv_data, features)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83878353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label\n",
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "..     ...\n",
      "939     15\n",
      "940     15\n",
      "941     15\n",
      "942     15\n",
      "943     15\n",
      "\n",
      "[944 rows x 1 columns]\n",
      "[[ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [ 9]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [10]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [11]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [12]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [13]\n",
      " [14]\n",
      " [14]\n",
      " [14]\n",
      " [14]\n",
      " [14]\n",
      " [14]\n",
      " [14]\n",
      " [14]\n",
      " [14]\n",
      " [14]\n",
      " [14]\n",
      " [14]\n",
      " [14]\n",
      " [14]\n",
      " [14]\n",
      " [15]\n",
      " [15]\n",
      " [15]\n",
      " [15]\n",
      " [15]\n",
      " [15]\n",
      " [15]\n",
      " [15]\n",
      " [15]\n",
      " [15]\n",
      " [15]\n",
      " [15]\n",
      " [15]\n",
      " [15]\n",
      " [15]\n",
      " [15]\n",
      " [15]\n",
      " [15]\n",
      " [15]\n",
      " [15]\n",
      " [15]\n",
      " [15]\n",
      " [15]\n",
      " [15]\n",
      " [15]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#label=[]\n",
    "#features = read_features(config.train_libfm, features)\n",
    "label_fd=pd.read_csv(label_file,sep=',')\n",
    "#print(features)\n",
    "#print(label_fd)\n",
    "label=np.array(label_fd)\n",
    "#print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0539cce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "import config\n",
    "\n",
    "features={}\n",
    "file=\"data/frappe/c_df_v_fff2000.csv\"\n",
    "num = len(features)\n",
    "fd=pd.read_csv(file,sep=',')\n",
    "nrow=fd.shape[0]\n",
    "ncol=fd.shape[1]\n",
    "n_fd=np.array(fd)\n",
    "#print(n_fd)\n",
    "n_fd=n_fd[:,2:]\n",
    "print(n_fd)\n",
    "#features={}\n",
    "\n",
    "for i, row_list in enumerate(n_fd):\n",
    "    for j, col_value in enumerate(row_list):\n",
    "        #print(col_value)\n",
    "        if col_value not in features:\n",
    "            features[col_value]=num\n",
    "            num=num+1\n",
    "\n",
    "print(features)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36995d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "import config\n",
    "\n",
    "\n",
    "def read_features(file, features):\n",
    "    \"\"\" Read features from the given file. \"\"\"\n",
    "    file=\"data/frappe/c_df_v_fff2000.csv\"\n",
    "    num = len(features)\n",
    "    fd=pd.read_csv(file,sep=',')\n",
    "    nrow=fd.shape[0]\n",
    "    ncol=fd.shape[1]\n",
    "    n_fd=np.array(fd)\n",
    "    #print(n_fd)\n",
    "    n_fd=n_fd[:,2:]\n",
    "    print('n_fd.shape:')\n",
    "    print(n_fd.shape)\n",
    "    #print(n_fd)\n",
    "    #features={}\n",
    "\n",
    "    for i, row_list in enumerate(n_fd):\n",
    "        for j, col_value in enumerate(row_list):\n",
    "            #print(col_value)\n",
    "            if col_value not in features:\n",
    "                features[col_value]=num\n",
    "                num=num+1\n",
    "\n",
    "    #print(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8475af02",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.train_libfm=\"data/frappe/frappe.train.libfm\"\n",
    "config.valid_libfm=\"data/frappe/frappe.validation.libfm\"\n",
    "config.test_libfm=\"data/frappe/frappe.test.libfm\"\n",
    "csv_data=\"data/frappe/c_df_v_fff2000.csv\"\n",
    "label_file=\"data/frappe/label.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35b2be33",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.train_libfm=\"data/frappe/frappe.train.libfm\"\n",
    "config.valid_libfm=\"data/frappe/frappe.validation.libfm\"\n",
    "config.test_libfm=\"data/frappe/frappe.test.libfm\"\n",
    "csv_data=\"data/frappe/c_df_v_fff2000.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7358e285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_fd.shape:\n",
      "(944, 10150)\n",
      "{190: 0, 220: 1, 490: 2, 450: 3, 380: 4, 160: 5, 140: 6, 410: 7, 520: 8, 470: 9, 340: 10, 150: 11, 260: 12, 500: 13, 320: 14, 200: 15, 170: 16, 290: 17, 280: 18, 130: 19, 230: 20, 350: 21, 440: 22, 580: 23, 240: 24, 390: 25, 370: 26, 250: 27, 100: 28, 560: 29, 300: 30, 430: 31, 270: 32, 330: 33, 110: 34, 210: 35, 570: 36, 590: 37, 530: 38, 180: 39, 710: 40, 620: 41, 510: 42, 120: 43, 550: 44, 360: 45, 420: 46, 400: 47, 70: 48, 310: 49, 460: 50, 480: 51, 760: 52, 90: 53, 680: 54, 540: 55, 700: 56, 670: 57, 630: 58, 640: 59, 660: 60, 600: 61, 690: 62, 610: 63, 650: 64, 730: 65, 770: 66, 60: 67, 790: 68, 80: 69, 810: 70, 720: 71, 740: 72, 750: 73, 780: 74, 800: 75, 50: 76, 40: 77, 20: 78, 30: 79, 10: 80, 1: 81, 830: 82, 840: 83, 890: 84, 910: 85, 820: 86, 860: 87, 880: 88, 850: 89, 870: 90, 900: 91, 920: 92, 930: 93, 940: 94, 950: 95, 960: 96, 970: 97, 1000: 98, 980: 99}\n"
     ]
    }
   ],
   "source": [
    "features={}\n",
    "#features = read_features(config.train_libfm, features)\n",
    "features = read_features(csv_data, features)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bce817da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_features():\n",
    "\t\"\"\" Get the number of existing features in all the three files. \"\"\"\n",
    "\tfeatures = {}\n",
    "\tfeatures = read_features(csv_data, features)\n",
    "\t#features = read_features(config.valid_libfm, features)\n",
    "\t#features = read_features(config.test_libfm, features)\n",
    "\tprint(\"number of features: {}\".format(len(features)))\n",
    "\treturn features, len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d0e5fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " ...\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "nrow=944\n",
    "ncol=10150\n",
    "feature_v=[]\n",
    "\"\"\"\n",
    "feature_v=[1 for i in range(ncol)]\n",
    "#print(feature_v)\n",
    "#feature_values=[feature_v for j in range(nrow)]\n",
    "\n",
    "for item in range(nrow):\n",
    "    feature_values.append(feature_v)\n",
    "\"\"\"\n",
    "\n",
    "feature_v=[[1 for i in range(ncol)] for i in range(nrow)]\n",
    "feature_value=np.array(feature_v)\n",
    "print(feature_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "941886f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3]]\n"
     ]
    }
   ],
   "source": [
    "f=[1,2,3]\n",
    "v=[]\n",
    "for i in range(10):\n",
    "    v.append(f)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc5a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMData(data.Dataset):\n",
    "\t\"\"\" Construct the FM pytorch dataset. \"\"\"\n",
    "\tdef __init__(self, file, feature_map):\n",
    "\t\tsuper(FMData, self).__init__()\n",
    "\t\tself.label = []\n",
    "\t\tself.features = []\n",
    "\t\tself.feature_values = []\n",
    "\n",
    "\t\twith open(file, 'r') as fd:\n",
    "\t\t\tline = fd.readline()\n",
    "\n",
    "\t\t\twhile line:\n",
    "\t\t\t\titems = line.strip().split()\n",
    "\n",
    "\t\t\t\t# convert features\n",
    "\t\t\t\traw = [item.split(':')[0] for item in items[1:]]\n",
    "\t\t\t\tself.features.append(\n",
    "\t\t\t\t\tnp.array([feature_map[item] for item in raw]))\n",
    "\t\t\t\tself.feature_values.append(np.array(\n",
    "\t\t\t\t\t[item.split(':')[1] for item in items[1:]], dtype=np.float32))\n",
    "\n",
    "\t\t\t\t# convert labels\n",
    "\t\t\t\tif config.loss_type == 'square_loss':\n",
    "\t\t\t\t\tself.label.append(np.float32(items[0]))\n",
    "\t\t\t\telse: # log_loss\n",
    "\t\t\t\t\tlabel = 1 if float(items[0]) > 0 else 0\n",
    "\t\t\t\t\tself.label.append(label)\n",
    "\n",
    "\t\t\t\tline = fd.readline()\n",
    "\n",
    "\t\tassert all(len(item) == len(self.features[0]\n",
    "\t\t\t) for item in self.features), 'features are of different length'\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.label)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tlabel = self.label[idx]\n",
    "\t\tfeatures = self.features[idx]\n",
    "\t\tfeature_values = self.feature_values[idx]\n",
    "\t\treturn features, feature_values, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "edd804e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([125, 128, 127])\n"
     ]
    }
   ],
   "source": [
    "ff={}\n",
    "ff={125:15,128:16,127:9}\n",
    "print(ff.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17065b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_fd.shape:\n",
      "(944, 10150)\n",
      "number of features: 100\n",
      "[[ 0  1  2 ... 45 26 46]\n",
      " [34  1 13 ... 10 12 46]\n",
      " [28 30 13 ... 33 21 25]\n",
      " ...\n",
      " [ 3 45 13 ... 31 45 13]\n",
      " [46 21 38 ... 22 26 13]\n",
      " [46 45 55 ... 31 26  3]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "features=[]\n",
    "file=\"data/frappe/c_df_v_fff2000.csv\"\n",
    "num = len(features)\n",
    "fd=pd.read_csv(file,sep=',')\n",
    "nrow=fd.shape[0]\n",
    "ncol=fd.shape[1]\n",
    "n_fd=np.array(fd)\n",
    "#print(n_fd)\n",
    "n_fd=n_fd[:,2:]\n",
    "feature_map,lenth=map_features()\n",
    "#raw = [item for item in  enumerate(n_fd)]\n",
    "#print(raw)\n",
    "#raw=raw.tolist()\n",
    "for i, item in enumerate(n_fd):\n",
    "    u=[feature_map[x] for x in item]\n",
    "    features.append(u)\n",
    "features=np.array(features)\n",
    "#print(features)\n",
    "#3features=features.tolist()\n",
    "print(features)\n",
    "#t=[ for item in u]\n",
    "#print(t)\n",
    "#features.append([feature_map[item] for item in raw])\n",
    "                          \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99826e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "\n",
    "import config\n",
    "\n",
    "config.train_libfm=\"data/frappe/train_data.csv\"\n",
    "config.valid_libfm=\"data/frappe/validate_data.csv\"\n",
    "config.test_libfm=\"data/frappe/test_data.csv\"\n",
    "\n",
    "\n",
    "\n",
    "config.train_label=\"data/frappe/train_label.csv\"\n",
    "config.valid_label=\"data/frappe/validate_label.csv\"\n",
    "config.test_label=\"data/frappe/test_label.csv\"\n",
    "\n",
    "csv_data=\"data/frappe/c_df_v_fff2000.csv\"\n",
    "label_file=\"data/frappe/label.csv\"\n",
    "#config.train_libfm=\"data/frappe/frappe.train.libfm\"\n",
    "#config.valid_libfm=\"data/frappe/frappe.validation.libfm\"\n",
    "#config.test_libfm=\"data/frappe/frappe.test.libfm\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "839d1fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "import config\n",
    "\n",
    "\n",
    "def read_features(file, features):\n",
    "    \"\"\" Read features from the given file. \"\"\"\n",
    "    file=\"data/frappe/c_df_v_fff2000.csv\"\n",
    "    num = len(features)\n",
    "    fd=pd.read_csv(file,sep=',')\n",
    "    nrow=fd.shape[0]\n",
    "    ncol=fd.shape[1]\n",
    "    n_fd=np.array(fd)\n",
    "    #print(n_fd)\n",
    "    n_fd=n_fd[:,2:]\n",
    "    print('n_fd.shape:')\n",
    "    print(n_fd.shape)\n",
    "    #print(n_fd)\n",
    "    #features={}\n",
    "\n",
    "    for i, row_list in enumerate(n_fd):\n",
    "        for j, col_value in enumerate(row_list):\n",
    "            #print(col_value)\n",
    "            if col_value not in features:\n",
    "                features[col_value]=num\n",
    "                num=num+1\n",
    "\n",
    "    #print(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f754adb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_features():\n",
    "    \"\"\" Get the number of existing features in all the three files. \"\"\"\n",
    "    features = {}\n",
    "    #features = read_features(csv_data, features)\n",
    "    \n",
    "    features = read_features(config.train_libfm, features)\n",
    "    features = read_features(config.valid_libfm, features)\n",
    "    features = read_features(config.test_libfm, features)\n",
    "    \n",
    "    \n",
    "    print(\"number of features: {}\".format(len(features)))\n",
    "    return features, len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42813afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "ff_label=pd.read_csv(label_file,sep=',')\n",
    "#nrow=fd.shape[0]\n",
    "#ncol=fd.shape[1]\n",
    "n_ff_label=np.array(ff_label)\n",
    "#print(n_fd)\n",
    "#n_ff_label=n_ff_label[:,2:]\n",
    "train_label=n_ff_label[0:744,:]\n",
    "train_label=pd.DataFrame(train_label)\n",
    "train_label.to_csv('train_label.csv')\n",
    "    \n",
    "    \n",
    "validate_label=n_ff_label[744:844,:]\n",
    "validate_label=pd.DataFrame(validate_label)\n",
    "validate_label.to_csv('validate_label.csv')\n",
    "    \n",
    "test_label=n_ff_label[844:944,:]\n",
    "test_label=pd.DataFrame(test_label)\n",
    "test_label.to_csv('test_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80625add",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff=pd.read_csv(csv_data,sep=',')\n",
    "#nrow=fd.shape[0]\n",
    "#ncol=fd.shape[1]\n",
    "n_ff=np.array(ff)\n",
    "#print(n_fd)\n",
    "n_ff=n_ff[:,2:]\n",
    "train_data=n_ff[0:744,:]\n",
    "train_data=pd.DataFrame(train_data)\n",
    "train_data.to_csv('train_data.csv')\n",
    "    \n",
    "    \n",
    "validate_data=n_ff[744:844,:]\n",
    "validate_data=pd.DataFrame(validate_data)\n",
    "validate_data.to_csv('validate_data.csv')\n",
    "    \n",
    "test_data=n_ff[844:944,:]\n",
    "test_data=pd.DataFrame(test_data)\n",
    "test_data.to_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11de7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "def seg_data(file):\n",
    "    ff=pd.read_csv(file,sep=',')\n",
    "    #nrow=fd.shape[0]\n",
    "    #ncol=fd.shape[1]\n",
    "    n_ff=np.array(ff)\n",
    "    #print(n_fd)\n",
    "    n_ff=n_ff[:,2:]\n",
    "    train_data=n_ff[0:744,:]\n",
    "    train_data=pd.DataFrame(train_data)\n",
    "    train_data.to_csv('train_data.csv')\n",
    "    \n",
    "    \n",
    "    validate_data=n_ff[744:844,:]\n",
    "    validate_data=pd.DataFrame(validate_data)\n",
    "    validate_data.to_csv('validate_data.csv')\n",
    "    \n",
    "    test_data=n_ff[844:944,:]\n",
    "    test_data=pd.DataFrame(test_data)\n",
    "    test_data.to_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcd392d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " ...\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "nrow=944\n",
    "ncol=10150\n",
    "feature_v=[]\n",
    "\"\"\"\n",
    "feature_v=[1 for i in range(ncol)]\n",
    "#print(feature_v)\n",
    "#feature_values=[feature_v for j in range(nrow)]\n",
    "\n",
    "for item in range(nrow):\n",
    "    feature_values.append(feature_v)\n",
    "\"\"\"\n",
    "\n",
    "feature_v=[[1 for i in range(ncol)] for i in range(nrow)]\n",
    "feature_value=np.array(feature_v)\n",
    "print(feature_value)\n",
    "\n",
    "\n",
    "class FMData(data.Dataset):\n",
    "    \"\"\" Construct the FM pytorch dataset. \"\"\"\n",
    "    def __init__(self, file,label_file, feature_map):\n",
    "        super(FMData, self).__init__()\n",
    "        self.label = []\n",
    "        self.features = []\n",
    "        self.feature_values = []\n",
    "        \n",
    "        features=[]\n",
    "        #feature_map.keys()\n",
    "        #self.features=np.array(feature_map)\n",
    "        #feature_map\n",
    "        #file=\"data/frappe/c_df_v_fff2000.csv\"\n",
    "        #num = len(features)\n",
    "        fd=pd.read_csv(file,sep=',')\n",
    "        #nrow=fd.shape[0]\n",
    "        #ncol=fd.shape[1]\n",
    "        n_fd=np.array(fd)\n",
    "        #print(n_fd)\n",
    "        n_fd=n_fd[:,2:]\n",
    "        for i, item in enumerate(n_fd):\n",
    "            u=[feature_map[x] for x in item]\n",
    "            features.append(u)\n",
    "        self.features=np.array(features)\n",
    "        #self.features=features.tolist()\n",
    "        \n",
    "        \n",
    "        nrow,ncol=n_fd.shape\n",
    "        #ncol=10150\n",
    "        feature_v=[]\n",
    "        \"\"\"\n",
    "            feature_v=[1 for i in range(ncol)]\n",
    "            #print(feature_v)\n",
    "            #feature_values=[feature_v for j in range(nrow)]\n",
    "\n",
    "            for item in range(nrow):\n",
    "            feature_values.append(feature_v)\n",
    "        \"\"\"\n",
    "\n",
    "        feature_v=[[1 for i in range(ncol)] for i in range(nrow)]\n",
    "        self.feature_values=np.array(feature_v)\n",
    "        #print(feature_value)\n",
    "        #self.feature_values=feature_value.tolist()\n",
    "        #feature_map,lenth=map_features()\n",
    "        #raw = [item for item in  enumerate(n_fd)]\n",
    "        #print(raw)\n",
    "        #raw=raw.tolist()\n",
    "        \n",
    "        \n",
    "        label_fd=pd.read_csv(label_file,sep=',')\n",
    "        #print(features)\n",
    "        #print(label_fd)\n",
    "        self.label=np.array(label_fd)\n",
    "        #print(label)\n",
    "        # convert labels\n",
    "        \"\"\"if config.loss_type == 'square_loss':\n",
    "            self.label.append(np.float32(items[0]))\n",
    "        else: # log_loss\n",
    "            label = 1 if float(items[0]) > 0 else 0\n",
    "            self.label.append(label)\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        assert all(len(item) == len(self.features[0]\n",
    "            ) for item in self.features), 'features are of different length'\n",
    "        \"\"\"\n",
    "        print(len(self.features))\n",
    "        print(len(self.feature_values))\n",
    "        print(len(self.label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.label[idx]\n",
    "        features = self.features[idx]\n",
    "        feature_values = self.feature_values[idx]\n",
    "        return features, feature_values, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ff78793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_fd.shape:\n",
      "(944, 10150)\n",
      "n_fd.shape:\n",
      "(944, 10150)\n",
      "n_fd.shape:\n",
      "(944, 10150)\n",
      "number of features: 100\n",
      "744\n",
      "744\n",
      "744\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "features_map,num_features=map_features()\n",
    "#config.train_libfm=\"data/frappe/frappe.train.libfm\"\n",
    "#config.valid_libfm=\"data/frappe/frappe.validation.libfm\"\n",
    "#config.test_libfm=\"data/frappe/frappe.test.libfm\"\n",
    "\n",
    "config.train_libfm=\"data/frappe/train_data.csv\"\n",
    "config.valid_libfm=\"data/frappe/validate_data.csv\"\n",
    "config.test_libfm=\"data/frappe/test_data.csv\"\n",
    "\n",
    "config.train_label=\"data/frappe/train_label.csv\"\n",
    "config.valid_label=\"data/frappe/validate_label.csv\"\n",
    "config.test_label=\"data/frappe/test_label.csv\"\n",
    "\n",
    "csv_data=\"data/frappe/c_df_v_fff2000.csv\"\n",
    "label_file=\"data/frappe/label.csv\"\n",
    "#features_map=list(features.keys())\n",
    "train_dataset = FMData(config.train_libfm,config.train_label,features_map)\n",
    "train_loader = data.DataLoader(train_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "validate_dataset = FMData(config.valid_libfm,config.valid_label,features_map)\n",
    "validate_loader = data.DataLoader(train_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "test_dataset = FMData(config.test_libfm,config.test_label,features_map)\n",
    "test_loader = data.DataLoader(test_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "743d4943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 10150])\n",
      "torch.Size([100, 10150])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 10150])\n",
      "torch.Size([100, 10150])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 10150])\n",
      "torch.Size([100, 10150])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 10150])\n",
      "torch.Size([100, 10150])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 10150])\n",
      "torch.Size([100, 10150])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 10150])\n",
      "torch.Size([100, 10150])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 10150])\n",
      "torch.Size([100, 10150])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 10150])\n",
      "torch.Size([100, 10150])\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 10150])\n",
      "torch.Size([100, 10150])\n",
      "torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "for features, feature_values, label in train_loader:\n",
    "    features = features.cuda()\n",
    "    feature_values = feature_values.cuda()\n",
    "    label = label.cuda()\n",
    "        \n",
    "    print(features.shape)\n",
    "    print(feature_values.shape)\n",
    "    print(label.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0939d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import model\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--lr\", \n",
    "    type=float, \n",
    "    default=0.05, \n",
    "    help=\"learning rate\")\n",
    "\n",
    "\n",
    "parser.add_argument(\"--dropout\", \n",
    "    default='[0.9, 0.7]',  \n",
    "    help=\"dropout rate for FM and MLP\")\n",
    "parser.add_argument(\"--batch_size\", \n",
    "    type=int, \n",
    "    default=128, \n",
    "    help=\"batch size for training\")\n",
    "\n",
    "parser.add_argument(\"--epochs\", \n",
    "    type=int,\n",
    "    default=100, \n",
    "    help=\"training epochs\")\n",
    "\n",
    "parser.add_argument(\"--hidden_factor\", \n",
    "    type=int,\n",
    "    default=64, #被修改，原始值为64\n",
    "    help=\"predictive factors numbers in the model\")\n",
    "\n",
    "parser.add_argument(\"--layers\", \n",
    "    default='[64]', \n",
    "    help=\"size of layers in MLP model, '[]' is NFM-0\")\n",
    "\n",
    "parser.add_argument(\"--lamda\", \n",
    "    type=float, \n",
    "    default=0.0, \n",
    "    help=\"regularizer for bilinear layers\")\n",
    "\n",
    "parser.add_argument(\"--batch_norm\", \n",
    "    default=True, \n",
    "    help=\"use batch_norm or not\")\n",
    "\n",
    "parser.add_argument(\"--pre_train\", \n",
    "    action='store_true', \n",
    "    default=False, \n",
    "    help=\"whether use the pre-train or not\")\n",
    "\n",
    "parser.add_argument(\"--out\", \n",
    "    default=True, \n",
    "    help=\"save model or not\")\n",
    "\n",
    "parser.add_argument(\"--gpu\", \n",
    "    type=str,\n",
    "    default=\"0\",  \n",
    "    help=\"gpu card ID\")\n",
    "\n",
    "args = parser.parse_args(args=[])#[]\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6a79bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NFM(nn.Module):\n",
    "    def __init__(self, num_features, num_factors, \n",
    "        act_function, layers, batch_norm, drop_prob, pretrain_FM):\n",
    "        super(NFM, self).__init__()\n",
    "        \"\"\"\n",
    "        num_features: number of features,\n",
    "        num_factors: number of hidden factors,\n",
    "        act_function: activation function for MLP layer,\n",
    "        layers: list of dimension of deep layers,\n",
    "        batch_norm: bool type, whether to use batch norm or not,\n",
    "        drop_prob: list of the dropout rate for FM and MLP,\n",
    "        pretrain_FM: the pre-trained FM weights.\n",
    "        \"\"\"\n",
    "        self.num_features = num_features\n",
    "        self.num_factors = num_factors\n",
    "        self.act_function = act_function\n",
    "        self.layers = layers\n",
    "        self.batch_norm = batch_norm\n",
    "        self.drop_prob = drop_prob\n",
    "        self.pretrain_FM = pretrain_FM\n",
    "\n",
    "        self.embeddings = nn.Embedding(num_features, num_factors)\n",
    "        self.biases = nn.Embedding(num_features, 1)\n",
    "        self.bias_ = nn.Parameter(torch.tensor([0.0]))\n",
    "\n",
    "        FM_modules = []\n",
    "        if self.batch_norm:\n",
    "            FM_modules.append(nn.BatchNorm1d(num_factors))\n",
    "        FM_modules.append(nn.Dropout(drop_prob[0]))\n",
    "        self.FM_layers = nn.Sequential(*FM_modules)\n",
    "\n",
    "        MLP_module = []\n",
    "        in_dim = num_factors\n",
    "        for dim in self.layers:\n",
    "            out_dim = dim\n",
    "            MLP_module.append(nn.Linear(in_dim, out_dim))\n",
    "            in_dim = out_dim\n",
    "\n",
    "            if self.batch_norm:\n",
    "                MLP_module.append(nn.BatchNorm1d(out_dim))\n",
    "            if self.act_function == 'relu':\n",
    "                MLP_module.append(nn.ReLU())\n",
    "            elif self.act_function == 'sigmoid':\n",
    "                MLP_module.append(nn.Sigmoid())\n",
    "            elif self.act_function == 'tanh':\n",
    "                MLP_module.append(nn.Tanh())\n",
    "            elif self.act_function == 'softmax':\n",
    "                MLP_module.append(F.softmax())#修改\n",
    "\n",
    "            MLP_module.append(nn.Dropout(drop_prob[-1]))\n",
    "        self.deep_layers = nn.Sequential(*MLP_module)\n",
    "\n",
    "        predict_size = layers[-1] if layers else num_factors\n",
    "        self.prediction = nn.Linear(predict_size, 1, bias=False)\n",
    "\n",
    "        self._init_weight_()\n",
    "\n",
    "    def _init_weight_(self):\n",
    "        \"\"\" Try to mimic the original weight initialization. \"\"\"\n",
    "        if self.pretrain_FM:\n",
    "            self.embeddings.weight.data.copy_(\n",
    "                            self.pretrain_FM.embeddings.weight)\n",
    "            self.biases.weight.data.copy_(\n",
    "                            self.pretrain_FM.biases.weight)\n",
    "            self.bias_.data.copy_(self.pretrain_FM.bias_)\n",
    "        else:\n",
    "            nn.init.normal_(self.embeddings.weight, std=0.01)\n",
    "            nn.init.constant_(self.biases.weight, 0.0)\n",
    "\n",
    "        # for deep layers\n",
    "        if len(self.layers) > 0:\n",
    "            for m in self.deep_layers:\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_normal_(m.weight)\n",
    "            nn.init.xavier_normal_(self.prediction.weight)\n",
    "        else:\n",
    "            nn.init.constant_(self.prediction.weight, 1.0)\n",
    "\n",
    "    def forward(self, features, feature_values):\n",
    "        nonzero_embed = self.embeddings(features)\n",
    "        feature_values = feature_values.unsqueeze(dim=-1)\n",
    "        nonzero_embed = nonzero_embed * feature_values\n",
    "\n",
    "        # Bi-Interaction layer\n",
    "        sum_square_embed = nonzero_embed.sum(dim=1).pow(2)\n",
    "        square_sum_embed = (nonzero_embed.pow(2)).sum(dim=1)\n",
    "\n",
    "        # FM model\n",
    "        FM = 0.5 * (sum_square_embed - square_sum_embed)\n",
    "        FM = self.FM_layers(FM)\n",
    "        if self.layers: # have deep layers\n",
    "            FM = self.deep_layers(FM)\n",
    "        FM = self.prediction(FM)\n",
    "\n",
    "        # bias addition\n",
    "        feature_bias = self.biases(features)\n",
    "        feature_bias = (feature_bias * feature_values).sum(dim=1)\n",
    "        FM = FM + feature_bias + self.bias_\n",
    "        return FM.view(-1)\n",
    "\n",
    "\n",
    "class FM(nn.Module):\n",
    "    def __init__(self, num_features, num_factors, batch_norm, drop_prob):\n",
    "        super(FM, self).__init__()\n",
    "        \"\"\"\n",
    "        num_features: number of features,\n",
    "        num_factors: number of hidden factors,\n",
    "        batch_norm: bool type, whether to use batch norm or not,\n",
    "        drop_prob: list of the dropout rate for FM and MLP,\n",
    "        \"\"\"\n",
    "        self.num_features = num_features\n",
    "        self.num_factors = num_factors\n",
    "        self.batch_norm = batch_norm\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.embeddings = nn.Embedding(num_features, num_factors)\n",
    "        self.biases = nn.Embedding(num_features, 1)\n",
    "        self.bias_ = nn.Parameter(torch.tensor([0.0]))\n",
    "\n",
    "        FM_modules = []\n",
    "        if self.batch_norm:\n",
    "            FM_modules.append(nn.BatchNorm1d(num_factors))\t\t\n",
    "        FM_modules.append(nn.Dropout(drop_prob[0]))\n",
    "        self.FM_layers = nn.Sequential(*FM_modules)\n",
    "\n",
    "        nn.init.normal_(self.embeddings.weight, std=0.01)\n",
    "        nn.init.constant_(self.biases.weight, 0.0)\n",
    "\n",
    "\n",
    "    def forward(self, features, feature_values):\n",
    "        nonzero_embed = self.embeddings(features)\n",
    "        feature_values = feature_values.unsqueeze(dim=-1)\n",
    "        nonzero_embed = nonzero_embed * feature_values\n",
    "\n",
    "        # Bi-Interaction layer\n",
    "        sum_square_embed = nonzero_embed.sum(dim=1).pow(2)\n",
    "        square_sum_embed = (nonzero_embed.pow(2)).sum(dim=1)\n",
    "\n",
    "        # FM model\n",
    "        FM = 0.5 * (sum_square_embed - square_sum_embed)\n",
    "        FM = self.FM_layers(FM).sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # bias addition\n",
    "        feature_bias = self.biases(features)\n",
    "        feature_bias = (feature_bias * feature_values).sum(dim=1)\n",
    "        FM = FM + feature_bias + self.bias_\n",
    "        return FM.view(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c73a60df",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "softmax() missing 1 required positional argument: 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-be2bccdf851e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         args.batch_norm, eval(args.dropout), FM_model)\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Adagrad'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NFM-pyorch-master/NFM-pyorch-master/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_features, num_factors, act_function, layers, batch_norm, drop_prob, pretrain_FM)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mMLP_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_function\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'softmax'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mMLP_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#修改\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mMLP_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: softmax() missing 1 required positional argument: 'input'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "if args.pre_train:\n",
    "    assert os.path.exists(config.FM_model_path), 'lack of FM model'\n",
    "    assert config.model == 'NFM', 'only support NFM for now'\n",
    "    FM_model = torch.load(config.FM_model_path)\n",
    "else:\n",
    "    FM_model = None\n",
    "\n",
    "if config.model == 'FM':\n",
    "    model = model.FM(num_features, args.hidden_factor,\n",
    "                    args.batch_norm, eval(args.dropout))\n",
    "else:\n",
    "    model = model.NFM(\n",
    "        num_features, args.hidden_factor, \n",
    "        config.activation_function, eval(args.layers), \n",
    "        args.batch_norm, eval(args.dropout), FM_model)\n",
    "model.cuda()\n",
    "if config.optimizer == 'Adagrad':\n",
    "    optimizer = optim.Adagrad(\n",
    "        model.parameters(), lr=args.lr, initial_accumulator_value=1e-8)\n",
    "elif config.optimizer == 'Adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "elif config.optimizer == 'SGD':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "elif config.optimizer == 'Momentum':\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.95)\n",
    "\n",
    "    \n",
    "#config.activation_function='softmax'\n",
    "    \n",
    "if config.loss_type == 'square_loss':\n",
    "    criterion = nn.MSELoss(reduction='sum')\n",
    "elif config.loss_type=='cross_entropy_loss':#被修改，增加了交叉熵损失\n",
    "    criterion=nn.CrossEntropyLoss(reduction='mean')\n",
    "else: # log_loss\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "762b2257",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-ab2b0097837c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_rmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Enable dropout and batch_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import model\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "\n",
    "arg.pre_train=False\n",
    "#model = model.FM(10150, 100, eval(args.dropout=0.5))\n",
    "config.loss_type=='cross_entropy_loss'\n",
    "count, best_rmse = 0, 100\n",
    "criterion=nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "config.optimizer == 'Momentum'\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.95)\n",
    "model = model.NFM(\n",
    "        num_features, args.hidden_factor, \n",
    "        config.activation_function, eval(args.layers), \n",
    "        args.batch_norm, eval(args.dropout), FM_model)\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "if args.pre_train:\n",
    "    assert os.path.exists(config.FM_model_path), 'lack of FM model'\n",
    "    assert config.model == 'NFM', 'only support NFM for now'\n",
    "    FM_model = torch.load(config.FM_model_path)\n",
    "else:\n",
    "    FM_model = None\n",
    "    \n",
    "    \n",
    "    \n",
    "for epoch in range(args.epochs):\n",
    "    model.train() # Enable dropout and batch_norm\n",
    "    start_time = time.time()\n",
    "\n",
    "    for features, feature_values, label in train_loader:\n",
    "        features = features.cuda()\n",
    "        feature_values = feature_values.cuda()\n",
    "        label = label.cuda()\n",
    "\n",
    "        model.zero_grad()\n",
    "        prediction = model(features, feature_values,act_function='softmax')\n",
    "        loss = criterion(prediction, label) \n",
    "        loss += args.lamda * model.embeddings.weight.norm()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # writer.add_scalar('data/loss', loss.item(), count)\n",
    "        count += 1\n",
    "        \n",
    "    model.eval()\n",
    "    train_result = evaluate.metrics(model, train_loader)\n",
    "    valid_result = evaluate.metrics(model, valid_loader)\n",
    "    test_result = evaluate.metrics(model, test_loader)\n",
    "\n",
    "    print(\"Runing Epoch {:03d} \".format(epoch) + \"costs \" + time.strftime(\n",
    "                        \"%H: %M: %S\", time.gmtime(time.time()-start_time)))\n",
    "    print(\"Train_RMSE: {:.3f}, Valid_RMSE: {:.3f}, Test_RMSE: {:.3f}\".format(\n",
    "                        train_result, valid_result, test_result))\n",
    "\n",
    "    if test_result < best_rmse:\n",
    "        best_rmse, best_epoch = test_result, epoch\n",
    "        if args.out:\n",
    "            if not os.path.exists(config.model_path):\n",
    "                os.mkdir(config.model_path)\n",
    "            torch.save(model, \n",
    "                '{}{}.pth'.format(config.model_path, config.model))\n",
    "\n",
    "print(\"End. Best epoch {:03d}: Test_RMSE is {:.3f}\".format(best_epoch, best_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1afd92c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
