{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72e7a311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import Trainer\n",
    "#from network import NFM\n",
    "import torch.utils.data as Data\n",
    "from Utils.criteo_loader import getTestData, getTrainData\n",
    "\n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':9,\n",
    "    'linear_hidden1':2000,#线性模型输出层（隐层个数）\n",
    "    'embed_input_dim':20,#embed输入维度\n",
    "    'embed_dim': 10, # 用于控制稀疏特征经过Embedding层后的稠密特征大小，embed输出维度\n",
    "    'dnn_hidden_units': [100,9],#MLP隐层和输出层\n",
    "    'num_sparse_features_cols':10477,#the number of the gene columns\n",
    "    'num_dense_features': 0,#dense features number\n",
    "    'bi_dropout': 0.5,#Bi-Interaction 的dropout\n",
    "    'num_epoch': 500,#训练epoch次数\n",
    "    'batch_size': 24,#batch_size\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    'train_label': 'dataset/gene_247/train/guan_train_label.csv',\n",
    "    'train_data':'dataset/gene_247/train/guan_train_data.csv',\n",
    "    'test_label':'dataset/gene_247/test/qiu_test_label.csv',\n",
    "    'test_data':'dataset/gene_247/test/qiu_test_data.csv',\n",
    "    'gene_name':'dataset/qiuguan/orign/gene_name.csv',\n",
    "    'label_name':'dataset/qiuguan/orign/gene_label.csv'\n",
    "    #'fea_file': '../Data/criteo/processed_data/fea_col.npy',\n",
    "    #'validate_file': '../Data/criteo/processed_data/val_set.csv',\n",
    "    #'test_file': '../Data/criteo/processed_data/test_set.csv',\n",
    "    #'model_name': '../TrainedModels/NFM.model'\n",
    "}\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import torch.nn.functional as F  # 激励函数的库\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "#from utils import \n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import torch.nn.functional as F  # 激励函数的库\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "#from utils import \n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "'''\n",
    "\n",
    "all=pd.read_csv('dataset/qiuguan/orign/qiuguan_encode_all_100.csv',sep=',')\n",
    "all=all.iloc[1:,1:]\n",
    "X=all.iloc[:,:-1]\n",
    "#X=X.values\n",
    "\n",
    "#print(X)\n",
    "\n",
    "y=all.iloc[:,-1]\n",
    "#y=y.values\n",
    "\n",
    "#print(y)\n",
    "\n",
    "\n",
    "\n",
    "#y=np.array(y)\n",
    "train_val_data,test_data,train_val_label,test_label=train_test_split(X,y,test_size=0.2,stratify=y,random_state=2)\n",
    "\n",
    "print(train_val_data)\n",
    "train_val_info=pd.concat([train_val_data,train_val_label],axis=1)\n",
    "test_info=pd.concat([test_data,test_label],axis=1)\n",
    "print(train_val_info)\n",
    "train_val_info.to_csv('dataset/qiuguan/orign/train_val_info_encode_100.csv')\n",
    "test_info.to_csv('dataset/qiuguan/orign/test_info_encode_100.csv')\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "all=pd.read_csv('dataset/qiuguan/orign/qiuguan_encode_all_100.csv',sep=',')\n",
    "all=all.iloc[1:,1:]\n",
    "X=all.iloc[:,:-1]\n",
    "X=X.values\n",
    "\n",
    "print(X)\n",
    "\n",
    "y=all.iloc[:,-1]\n",
    "y=y.values\n",
    "print(y)\n",
    "'''\n",
    "\n",
    "'''\n",
    "print(y)\n",
    "row_num=[]\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    for j ,value,in enumerate(X[i]):\n",
    "        if value<0:\n",
    "            row_num.append(i)\n",
    "print(row_num)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "print(\"max:,min:,\",np.max(X),np.min(X))\n",
    "\"\"\"\n",
    "y=pd.read_csv('dataset/gene_247/data/guan/guan_label.csv',sep=',')\n",
    "y=y.values\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#y=np.array(y)\n",
    "train_data,test_data,train_label,test_label=train_test_split(X,y,test_size=0.3,stratify=y,random_state=2)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_label.shape)\n",
    "print(test_label.shape)\n",
    "#print(train_data)\n",
    "#print(test_data)\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_smoothing(labels, classes, label_smoothing=0.2):\n",
    "    #n = len(labels)\n",
    "    n=labels.shape[0]\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    n = len(labels)\n",
    "    #eoff = label_smoothing / classes\n",
    "    output = np.zeros((n, classes), dtype=np.float32)\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "class KZDatasetTest(data.Dataset):\n",
    "    \"\"\" Construct the FM pytorch dataset. \"\"\"\n",
    "    #def __init__(self, file,label_file, feature_map,n_class=16):\n",
    "    def __init__(self, csv_path):\n",
    "    \n",
    "        '''\n",
    "        txt_path: 所有数据的路径，我的形式为(单张图片路径 类别\\n)\n",
    "        img1.png 0\n",
    "        ...\n",
    "         img100.png 1\n",
    "         ki：当前是第几折,从0开始，范围为[0, K)\n",
    "         K：总的折数\n",
    "         typ：用于区分训练集与验证集\n",
    "         transform：对图片的数据增强\n",
    "         rand：是否随机\n",
    "        '''\n",
    "        '''\n",
    "        self.all_data_info = self.get_img_info(txt_path)\n",
    "        \n",
    "        if rand:\n",
    "            random.seed(1)\n",
    "            random.shuffle(self.all_data_info)\n",
    "        leng = len(self.all_data_info)\n",
    "        every_z_len = leng // K\n",
    "        if typ == 'val':\n",
    "            self.data_info = self.all_data_info[every_z_len * ki : every_z_len * (ki+1)]\n",
    "        elif typ == 'train':\n",
    "            self.data_info = self.all_data_info[: every_z_len * ki] + self.all_data_info[every_z_len * (ki+1) :]\n",
    "            \n",
    "        self.transform = transform\n",
    "        \n",
    "        '''\n",
    "        self.data_info = self.get_data_info(csv_path)\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Dataset读取图片的函数\n",
    "        data, bi_data,label = self.data_info[index]\n",
    "        #img = Image.open(img_pth).convert('RGB')\n",
    "        \n",
    "        return data, bi_data,label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def get_img_info(txt_path):\n",
    "        # 解析输入的txt的函数\n",
    "        # 转为二维list存储，每一维为 [ 图片路径，图片类别]\n",
    "        data_info = []\n",
    "        data = open(txt_path, 'r')\n",
    "        data_lines = data.readlines()\n",
    "        for data_line in data_lines:\n",
    "            data_line = data_line.split()\n",
    "            img_pth = data_line[0]\n",
    "            label = int(data_line[1])\n",
    "            data_info.append((img_pth, label))\n",
    "        return data_info\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def get_data_info(self,csv_path):\n",
    "        #解析路径\n",
    "        #转为一维list存储，每一位为【图片路径，图片类别】\n",
    "        labels=[]\n",
    "        data_info=[]\n",
    "        df=pd.read_csv(csv_path,sep=',',header=None)\n",
    "        df=df.iloc[1:,1:]\n",
    "        \n",
    "        #print(df.iloc[:,-1])\n",
    "        #df=df.applymap(ast.literal_eval)\n",
    "        rows,cols=df.shape\n",
    "        #print(rows,cols)\n",
    "        for i in df.iloc[:,-1]:\n",
    "            #print(i)\n",
    "            labels.append(int(i))\n",
    "        #print('labels:',labels)\n",
    "        labels=np.array(labels)\n",
    "        #print('labels:',labels)\n",
    "        #labels=np.array(labels)\n",
    "        labels=one_hot_smoothing(labels,nfm_config['n_class'])\n",
    "        #print(labels)\n",
    "        for i in range(rows):\n",
    "            data=df.iloc[i,:-1]#data\n",
    "            data=data.astype(float)#\n",
    "            data=np.array(data)#\n",
    "            \n",
    "            \n",
    "            #data=torch.from_numpy(data)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            label=labels[i]#label\n",
    "            #print(data.shape)\n",
    "            #print(label.shape)\n",
    "            #label=label.tolist()\n",
    "            data=torch.from_numpy(data)#\n",
    "            #print(\"data.shape:\",data.shape)\n",
    "            label=torch.from_numpy(label)#\n",
    "            bi_data=embding_process(nfm_config,data)\n",
    "            #print(\"bi_data.shape:\",bi_data.shape)\n",
    "            '''\n",
    "            if i==62:\n",
    "                for num,i in enumerate(data):\n",
    "                    print(num,i)\n",
    "            data=[eval(i) for i in data]\n",
    "            \n",
    "            #data=data.values\n",
    "            #data=data.applymap(ast.literal_eval)\n",
    "            #data=data.applymap(ast.literal_eval)\n",
    "            #data=eval(data)\n",
    "            #print(\"i:\",i)\n",
    "            \n",
    "            \n",
    "            #print(data)\n",
    "            \n",
    "            label=[]\n",
    "            print(df.iloc[i,-1])\n",
    "            label.append(eval(df.iloc[i,-1]))\n",
    "            print(\"label:\",i,label)\n",
    "            #label=[eval(i) for i in label]\n",
    "            \n",
    "            #label=label.applymap(ast.literal_eval)\n",
    "            \n",
    "            #print(label)\n",
    "            label=one_hot_smoothing(np.array(label),nfm_config['n_class'])\n",
    "            label=label.tolist()\n",
    "            \n",
    "            '''\n",
    "            \n",
    "            data_info.append((data,bi_data,label))\n",
    "        return data_info\n",
    "    \n",
    "    \n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import *\n",
    "from PIL import Image\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ast\n",
    "import torchvision\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "class KZDataset(Dataset):\n",
    "    def __init__(self, csv_path, K,n_class,ki=0, typ='train', transform=None, rand=False):\n",
    "        '''\n",
    "        txt_path: 所有数据的路径，我的形式为(单张图片路径 类别\\n)\n",
    "        img1.png 0\n",
    "        ...\n",
    "         img100.png 1\n",
    "         ki：当前是第几折,从0开始，范围为[0, K)\n",
    "         K：总的折数\n",
    "         typ：用于区分训练集与验证集\n",
    "         transform：对图片的数据增强\n",
    "         rand：是否随机\n",
    "        '''\n",
    "        '''\n",
    "        self.all_data_info = self.get_img_info(txt_path)\n",
    "        \n",
    "        if rand:\n",
    "            random.seed(1)\n",
    "            random.shuffle(self.all_data_info)\n",
    "        leng = len(self.all_data_info)\n",
    "        every_z_len = leng // K\n",
    "        if typ == 'val':\n",
    "            self.data_info = self.all_data_info[every_z_len * ki : every_z_len * (ki+1)]\n",
    "        elif typ == 'train':\n",
    "            self.data_info = self.all_data_info[: every_z_len * ki] + self.all_data_info[every_z_len * (ki+1) :]\n",
    "            \n",
    "        self.transform = transform\n",
    "        \n",
    "        '''\n",
    "        self.all_data_info = self.get_data_info(csv_path)\n",
    "        \n",
    "        if rand:\n",
    "            random.seed(1)\n",
    "            random.shuffle(self.all_data_info)\n",
    "        leng = len(self.all_data_info)\n",
    "        every_z_len = leng // K\n",
    "        if typ == 'val':\n",
    "            self.data_info = self.all_data_info[every_z_len * ki : every_z_len * (ki+1)]\n",
    "        elif typ == 'train':\n",
    "            self.data_info = self.all_data_info[: every_z_len * ki] + self.all_data_info[every_z_len * (ki+1) :]\n",
    "            \n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Dataset读取图片的函数\n",
    "        data, bi_data,label = self.data_info[index]\n",
    "        #img = Image.open(img_pth).convert('RGB')\n",
    "        \n",
    "        return data, bi_data,label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def get_img_info(txt_path):\n",
    "        # 解析输入的txt的函数\n",
    "        # 转为二维list存储，每一维为 [ 图片路径，图片类别]\n",
    "        data_info = []\n",
    "        data = open(txt_path, 'r')\n",
    "        data_lines = data.readlines()\n",
    "        for data_line in data_lines:\n",
    "            data_line = data_line.split()\n",
    "            img_pth = data_line[0]\n",
    "            label = int(data_line[1])\n",
    "            data_info.append((img_pth, label))\n",
    "        return data_info\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def get_data_info(self,csv_path):\n",
    "        #解析路径\n",
    "        #转为一维list存储，每一位为【图片路径，图片类别】\n",
    "        labels=[]\n",
    "        data_info=[]\n",
    "        df=pd.read_csv(csv_path,sep=',',header=None)\n",
    "        df=df.iloc[1:,1:]\n",
    "        \n",
    "        #print(df.iloc[:,-1])\n",
    "        #df=df.applymap(ast.literal_eval)\n",
    "        rows,cols=df.shape\n",
    "        #print(rows,cols)\n",
    "        for i in df.iloc[:,-1]:\n",
    "            #print(i)\n",
    "            labels.append(int(i))\n",
    "        #print('labels:',labels)\n",
    "        labels=np.array(labels)\n",
    "        #print('labels:',labels)\n",
    "        #labels=np.array(labels)\n",
    "        labels=one_hot_smoothing(labels,nfm_config['n_class'])\n",
    "        #print(labels)\n",
    "        for i in range(rows):\n",
    "            data=df.iloc[i,:-1]#data\n",
    "            data=data.astype(float)#\n",
    "            data=np.array(data)#\n",
    "            \n",
    "            \n",
    "            #data=torch.from_numpy(data)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            label=labels[i]#label\n",
    "            #print(data.shape)\n",
    "            #print(label.shape)\n",
    "            #label=label.tolist()\n",
    "            data=torch.from_numpy(data)#\n",
    "            #print(\"data.shape:\",data.shape)\n",
    "            label=torch.from_numpy(label)#\n",
    "            bi_data=embding_process(nfm_config,data)\n",
    "            #print(\"bi_data.shape:\",bi_data.shape)\n",
    "            '''\n",
    "            if i==62:\n",
    "                for num,i in enumerate(data):\n",
    "                    print(num,i)\n",
    "            data=[eval(i) for i in data]\n",
    "            \n",
    "            #data=data.values\n",
    "            #data=data.applymap(ast.literal_eval)\n",
    "            #data=data.applymap(ast.literal_eval)\n",
    "            #data=eval(data)\n",
    "            #print(\"i:\",i)\n",
    "            \n",
    "            \n",
    "            #print(data)\n",
    "            \n",
    "            label=[]\n",
    "            print(df.iloc[i,-1])\n",
    "            label.append(eval(df.iloc[i,-1]))\n",
    "            print(\"label:\",i,label)\n",
    "            #label=[eval(i) for i in label]\n",
    "            \n",
    "            #label=label.applymap(ast.literal_eval)\n",
    "            \n",
    "            #print(label)\n",
    "            label=one_hot_smoothing(np.array(label),nfm_config['n_class'])\n",
    "            label=label.tolist()\n",
    "            \n",
    "            '''\n",
    "            \n",
    "            data_info.append((data,bi_data,label))\n",
    "        return data_info\n",
    "            \n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import sys \n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "import torchmetrics\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "#from utils import \n",
    "#准备训练集\n",
    "#from new_dataset_processed import FMData\n",
    "#from dataset_process import FMData\n",
    "def train_epoch(model,train_loader,batch_size,optimizer,loss_func):\n",
    "    BATCH_SIZE=batch_size\n",
    "    total = 0\n",
    "    correct=0\n",
    "    total_loss=0\n",
    "    #\n",
    "    total_train_accuracy=0  \n",
    "    model.train()\n",
    "    for batch_idx, (x, bi_x,labels) in enumerate(train_loader):\n",
    "            \n",
    "        x = Variable(x)\n",
    "        bi_x=Variable(bi_x)\n",
    "        labels = Variable(labels)\n",
    "            \n",
    "            \n",
    "        #x = torch.tensor(x, dtype=torch.float)\n",
    "        #x=x.clone().detach().requires_grad_(True)\n",
    "        x=torch.tensor(x,dtype=torch.float)\n",
    "        bi_x=torch.tensor(bi_x,dtype=torch.long)\n",
    "        labels=torch.tensor(labels,dtype=torch.float)\n",
    "        x, bi_x,labels = x.cuda(), bi_x.cuda(),labels.cuda()\n",
    "        labels_int=labels=torch.max(labels,1)[1]\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        y_predict = model(x,bi_x)\n",
    "            \n",
    "        loss = loss_func(y_predict, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        loss = loss.item()\n",
    "           \n",
    "\n",
    "        total_loss += loss\n",
    "            \n",
    "            \n",
    "            \n",
    "        batch_train_acc=torchmetrics.functional.accuracy(y_predict,labels_int)\n",
    "        total_train_accuracy+=batch_train_acc\n",
    "            \n",
    "    total_train_accuracy/=(batch_idx+1)\n",
    "    print('total_train_accuracy:',total_train_accuracy)\n",
    "    print(\"Training Epoch: %d, total loss: %f\" % (epoch_id, total_loss))\n",
    "    return total_loss,total_train_accuracy\n",
    "\n",
    "\n",
    "\n",
    "def val_epoch(model,test_loader,batch_size,optimizer): \n",
    "    batch_size_num=0\n",
    "    total_test_acc=0\n",
    "    model.eval()\n",
    "    for i , (inputs ,bi_inputs, targets) in enumerate(test_loader):   \n",
    "            print(\"test\")\n",
    "            # evaluate the model on the test set   \n",
    "            #print(\\ inputs:\\  inputs)   \n",
    "            #print(\\ targets:\\  targets)   \n",
    "            inputs = Variable(inputs)   \n",
    "            bi_inputs=Variable(bi_inputs)\n",
    "            targets = Variable(targets)     \n",
    "            #x = torch.tensor(x  dtype=torch.float)   \n",
    "            #x=x.clone().detach().requires_grad_(True)   \n",
    "            inputs=torch.tensor(inputs ,dtype=torch.float)  \n",
    "            bi_inputs=torch.tensor(bi_inputs,dtype=torch.float)\n",
    "            targets=torch.tensor(targets ,dtype=torch.float)   \n",
    "            inputs ,bi_inputs,targets = inputs.cuda(),bi_inputs.cuda(), targets.cuda()   \n",
    "            yhat = model(inputs,bi_inputs)   \n",
    "            \n",
    "            \n",
    "            \n",
    "            targets=torch.max(targets,1)[1]\n",
    "            \n",
    "            \n",
    "            \n",
    "            batch_test_acc=torchmetrics.functional.accuracy(yhat,targets)\n",
    "            \n",
    "            total_test_acc+=batch_test_acc\n",
    "            \n",
    "            batch_size_num=i\n",
    "    total_test_acc/=(batch_size_num+1)\n",
    "        ###print('total_test_accuracy:',total_test_acc/(batch_size+1))\n",
    "    print('total_test_accuracy:',total_test_acc)\n",
    "        \n",
    "    return total_test_acc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "class BiInteractionPooling(nn.Module):\n",
    "    \"\"\"Bi-Interaction Layer used in Neural FM,compress the\n",
    "      pairwise element-wise product of features into one single vector.\n",
    "      Input shape\n",
    "        - A 3D tensor with shape:``(batch_size,field_size,embedding_size)``.\n",
    "      Output shape\n",
    "    http://127.0.0.1:3000/notebooks/NFM-pyorch-master/NFM-pyorch-master/%E6%9C%AA%E5%91%BD%E5%90%8D5.ipynb?kernel_name=python3#    - 3D tensor with shape: ``(batch_size,1,embedding_size)``.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(BiInteractionPooling, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        concated_embeds_value = inputs\n",
    "        square_of_sum = torch.pow(\n",
    "            torch.sum(concated_embeds_value, dim=1, keepdim=True), 2)\n",
    "        sum_of_square = torch.sum(\n",
    "            concated_embeds_value * concated_embeds_value, dim=1, keepdim=True)\n",
    "        cross_term = 0.5 * (square_of_sum - sum_of_square)\n",
    "        return cross_term\n",
    "\n",
    "import torch.nn as nn\n",
    "class BiInteractionPooling(nn.Module):\n",
    "    \"\"\"Bi-Interaction Layer used in Neural FM,compress the\n",
    "      pairwise element-wise product of features into one single vector.\n",
    "      Input shape\n",
    "        - A 3D tensor with shape:``(batch_size,field_size,embedding_size)``.\n",
    "      Output shape\n",
    "    http://127.0.0.1:3000/notebooks/NFM-pyorch-master/NFM-pyorch-master/%E6%9C%AA%E5%91%BD%E5%90%8D5.ipynb?kernel_name=python3#    - 3D tensor with shape: ``(batch_size,1,embedding_size)``.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(BiInteractionPooling, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        concated_embeds_value = inputs\n",
    "        #print(\"bi_interaction compute:\")\n",
    "        #print(\"inputs:\",inputs.shape)\n",
    "        square_of_sum = torch.pow(\n",
    "            torch.sum(concated_embeds_value, dim=1, keepdim=True), 2)\n",
    "        #print(\"square_of_sum：\",square_of_sum.shape)\n",
    "        sum_of_square = torch.sum(\n",
    "            concated_embeds_value * concated_embeds_value, dim=1, keepdim=True)\n",
    "        #print(\"sum_of_square:\",sum_of_square.shape)\n",
    "        cross_term = 0.5 * (square_of_sum - sum_of_square)\n",
    "        #print(\"cross_term:\",cross_term.shape)\n",
    "        return cross_term\n",
    "    \n",
    "    \n",
    "def embding_process(config,sparse_inputs):\n",
    "    #sparse_inputs=torch.tensor(sparse_inputs)\n",
    "    #sparse_inputs=torch.tensor(sparse_inputs,dtype=torch.int8)\n",
    "    sparse_inputs=sparse_inputs.int()\n",
    "    embedding_layers=nn.Embedding(config['embed_input_dim'],config['embed_dim'])\n",
    "    #print(\"sparse_inputs:\",sparse_inputs.shape)   \n",
    "    # B-Interaction 层\n",
    "    bi_pooling = BiInteractionPooling()\n",
    "    bi_dropout = config['bi_dropout']\n",
    "    if bi_dropout > 0:\n",
    "        dropout = nn.Dropout(bi_dropout)\n",
    "            \n",
    "    num_sparse_feature=config['num_sparse_features_cols']\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    #测试错误：IndexError: tuple index out of range\n",
    "    sparse_embeds=[]\n",
    "    for i in range(10):\n",
    "        #print(\"i,sparse_inputs[i]:\",i,sparse_inputs[i])\n",
    "        \n",
    "        sparse_embed=embedding_layers(sparse_inputs[i])\n",
    "        #print(\"sparse_embed:\",sparse_embed)\n",
    "        #sparse_embeds=sparse_embeds.append(sparse_embeds)\n",
    "        sparse_embeds.append(sparse_embed)\n",
    "    \n",
    "    \n",
    "    print(\"sparse_embeds:\",sparse_embeds)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    sparse_embeds = embedding_layers(sparse_inputs) \n",
    "    #sparse_embeds=np.array(sparse_embeds)\n",
    "    #print('sparse_embeds.shape:',sparse_embeds.shape)\n",
    "    sparse_embeds=sparse_embeds.detach().numpy()\n",
    "    sparse_embeds=sparse_embeds.T\n",
    "    sparse_embeds=torch.from_numpy(sparse_embeds)\n",
    "    #sparse_embeds =[ torch.cat((sparse_embeds[i]), axis=0) for i  in range(sparse_embeds.shape[0])]\n",
    "    #print(\"sparse_embeds:\",sparse_embeds)\n",
    "    #print(\"sparse_numpy.T.shape:\",sparse_embeds.shape)\n",
    "    \n",
    "    #BN_bi = nn.BatchNorm1d(config['embed_dim'])\n",
    "    # 送入B-Interaction层\n",
    "    #fm_input = sparse_embeds.view(-1, num_sparse_feature, config['embed_dim'])#整理成n行m列\n",
    "    fm_input=sparse_embeds\n",
    "    # print(fm_input)\n",
    "    # print(fm_input.shape)\n",
    "    #print(\"fm_input:\",fm_input)\n",
    "\n",
    "    bi_out = bi_pooling(fm_input)\n",
    "    \n",
    "    #print(\"bi_out.shape:\",bi_out.shape)\n",
    "    bi_out=bi_out.detach().numpy()\n",
    "    bi_out=bi_out.T\n",
    "    bi_out=torch.from_numpy(bi_out)\n",
    "    #print(\"bi_out.T.shape:\",bi_out.shape)\n",
    "    \n",
    "    bi_out = bi_out.view(-1)###\n",
    "    if bi_dropout:\n",
    "        bi_out = dropout(bi_out)\n",
    "    #bi_out = bi_out.unsqueeze(0)\n",
    "    #\n",
    "    #print(\"bi_out_view.shape:\",bi_out.shape)\n",
    "    #bi_out=BN_bi(bi_out)########\n",
    "    \n",
    "    \n",
    "    return bi_out\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from basemodel import BaseModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class NFM(BaseModel):\n",
    "    def __init__(self, config, dense_features_cols=[]):#=[]为新增\n",
    "    #def __init__(self, config, dense_features_cols, sparse_features_cols):\n",
    "        super(NFM, self).__init__(config)\n",
    "        # 稠密和稀疏特征的数量\n",
    "        #self.num_dense_feature = dense_features_cols.__len__()\n",
    "        self.num_dense_feature = 0#修改\n",
    "        self.num_sparse_feature = config['num_sparse_features_cols']\n",
    "        #self.num_sparse_feature = 0##修改\n",
    "        self.__config=config\n",
    "        \n",
    "        self.bi_BN=nn.BatchNorm1d(nfm_config['embed_dim'])\n",
    "        \n",
    "        self.BN_num=nn.BatchNorm1d(self.num_sparse_feature)\n",
    "        self.linear1=nn.Linear(config['num_sparse_features_cols'],config['linear_hidden1'])\n",
    "        self.bn1=nn.BatchNorm1d(config['linear_hidden1'])\n",
    "        self.drop1=nn.Dropout(0.5)\n",
    "        self.relu1=nn.ReLU()\n",
    "        \n",
    "        self.linear2=nn.Linear(config['linear_hidden1']+config['embed_dim'],config['dnn_hidden_units'][0])\n",
    "        self.bn2=nn.BatchNorm1d(config['dnn_hidden_units'][0])\n",
    "        self.drop2=nn.Dropout(0.5)\n",
    "        self.relu2=nn.ReLU()\n",
    "        \n",
    "        self.linear3=nn.Linear(config['dnn_hidden_units'][0],config['dnn_hidden_units'][1])\n",
    "        self.bn3=nn.BatchNorm1d(config['dnn_hidden_units'][1])\n",
    "        #self.drop3=nn.Dropout(0.3)\n",
    "        self.relu3=nn.ReLU()\n",
    "        \n",
    "        #self.embedding_layers=nn.Embedding(config['embed_input_dim'],config['embed_dim'])\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        #self.dnn_softmax=nn.Softmax(dim=1) # 按列SoftMax,列和为1  #注意nn.softmax的定义和调用\n",
    "        #self.dnn_softmax_=F.softmax(dim=1)\n",
    "        #self.dnn_hidden_units=config['dnn_hidden_units']\n",
    "    def forward(self, x,bi_x):\n",
    "        # 先区分出稀疏特征和稠密特征，这里是按照列来划分的，即所有的行都要进行筛选\n",
    "        #bi_x=bi_x.long()\n",
    "        #print(x.dtype)\n",
    "        # 求出线性部分\n",
    "        #x=F.relu(self.drop(self.BN_linear1(self.linear_model1(self.BN_num(x))))\n",
    "        #x=F.relu(self.drop(self.BN_linear1(self.linear_model1(x))))\n",
    "        #print(\"x,,bi_x\",x.shape,bi_x.shape)\n",
    "        x=self.BN_num(x)#####\n",
    "        #print(\"x_BN:\",bi_x.shape)\n",
    "        #x=self.linear_model1(x)\n",
    "        \n",
    "        #x1=torch.cat((x,bi_x),dim=1)\n",
    "        #print(\"linear_output:\",linear_output)\n",
    "        #linear_output=linear_output.view(-1,self.__config['linear_hidden1'])\n",
    "        #linear_output=self.drop(linear_output)\n",
    "        #linear_output=self.BN_linear(linear_output)\n",
    "        # 求出稀疏特征的embedding向量\n",
    "        \n",
    "        \n",
    "        #print('bi_out.shape:',bi_out.shape)\n",
    "        #print(x.dtype)\n",
    "        #print(bi_out.dtype)\n",
    "        \n",
    "        #input=x,bi_x#不能是list，必须是tensor\n",
    "        #x1,x2=input\n",
    "        y1=self.relu1(self.drop1(self.bn1(self.linear1(x))))\n",
    "        bi_x=torch.tensor(bi_x,dtype=torch.float)#######\n",
    "        bi_x=self.bi_BN(bi_x)####\n",
    "        x2=torch.cat((y1,bi_x),dim=1)\n",
    "        #print('y1.shape:',y1.shape)\n",
    "        y2=self.relu2(self.drop2(self.bn2(self.linear2(x2))))\n",
    "        #x3=torch.cat((y2,bi_x),dim=1)\n",
    "        #print('y2.shape:',y2.linear3(y2)))\n",
    "        #print('y3.shape:',y3.shape)\n",
    "        y3=self.relu3(self.bn3(self.linear3(y2)))\n",
    "        y=F.softmax(y3,dim=1)\n",
    "        #x3=torch.cat((y2,bi_x),dim=1)\n",
    "        #x3=torch.cat((y2,bi_x),dim=1)\n",
    "        #print('x3.shape:',x3.shape)\n",
    "        \n",
    "        \n",
    "        y_pred=y\n",
    "        #y_pred=self.dnn_softmax(dnn_output)#增加\n",
    "        #y_pred=F.softmax(dnn_output,dim=0)\n",
    "        # Final\n",
    "        #output = linear_output + y_pred#修改\n",
    "        #y_pred = self.dnn_softmax(output,dim=0)\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "nfm = NFM(nfm_config).cuda()\n",
    "nfm.cuda()\n",
    "nfm_params = list(nfm.named_parameters())\n",
    "#print(nfm_params[1])\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotLoss(loss,epoch):\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    x=[i for i in range(epoch)]\n",
    "    #acc_train=acc_train.cpu()\n",
    "    #acc_test=acc_test.cpu()\n",
    "    plt.plot(x, loss, 'r-', mec='k', label='Logistic Loss', lw=2)\n",
    "    #plt.plot(x,acc_train,'b-',mec='k',label='accuracy Train',lw=2)\n",
    "    #plt.plot(x,acc_test,'g-',mec='k',label='accuracy Test',lw=2)\n",
    "    #plt.plot(x, y_01, 'g-', mec='k', label='0/1 Loss', lw=2)\n",
    "    #plt.plot(x, y_hinge, 'b-',mec='k', label='Hinge Loss', lw=2)\n",
    "    #plt.plot(x, boost, 'm--',mec='k', label='Adaboost Loss',lw=2)\n",
    "    plt.grid(True, ls='--')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('损失函数')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "173a3a02",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-515bf248814c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_loss_total_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mki\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrainset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKZDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dataset/qiuguan/orign/train_val_info.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnfm_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mki\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mki\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mvalset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKZDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dataset/qiuguan/orign/train_val_info.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnfm_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mki\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mki\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     train_loader = data.DataLoader(\n",
      "\u001b[0;32m<ipython-input-3-1a9f5efac0ce>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, csv_path, K, n_class, ki, typ, transform, rand)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         '''\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_data_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrand\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-1a9f5efac0ce>\u001b[0m in \u001b[0;36mget_data_info\u001b[0;34m(self, csv_path)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0;31m#print(\"data.shape:\",data.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0mbi_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membding_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnfm_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m             \u001b[0;31m#print(\"bi_data.shape:\",bi_data.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             '''\n",
      "\u001b[0;32m<ipython-input-3-1a9f5efac0ce>\u001b[0m in \u001b[0;36membding_process\u001b[0;34m(config, sparse_inputs)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m     \u001b[0msparse_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m     \u001b[0;31m#sparse_embeds=np.array(sparse_embeds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0;31m#print('sparse_embeds.shape:',sparse_embeds.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model=nfm\n",
    "K=10\n",
    "test_metrics=[]\n",
    "train_loss_total_list=[]\n",
    "for ki in range(K):\n",
    "    trainset = KZDataset(csv_path='dataset/qiuguan/orign/train_val_info.csv',K=K, n_class=nfm_config['n_class'],ki=ki,  typ='train', transform=None, rand=True)\n",
    "    valset = KZDataset(csv_path='dataset/qiuguan/orign/train_val_info.csv', K=K,n_class=nfm_config['n_class'],ki=ki,  typ='val', transform=None, rand=True)\n",
    "    train_loader = data.DataLoader(\n",
    "         dataset=trainset,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         drop_last=True,\n",
    "         batch_size=nfm_config['batch_size'],\n",
    "         shuffle=True)\n",
    "    val_loader = data.DataLoader(\n",
    "         dataset=valset,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         drop_last=True,\n",
    "         batch_size=nfm_config['batch_size']\n",
    "        \n",
    "     )\n",
    "    \n",
    "    model_path='dataset/qiuguan/model_new_K_fold/NFM_layer_2_non_encode/'\n",
    "    #BATCH_SIZE=batch_size\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    #total = 0\n",
    "    \n",
    "    \n",
    "    loss_func=torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    num=0\n",
    "   \n",
    "    \n",
    "    epoches=101\n",
    "    for epoch_id in range(epoches):\n",
    "          \n",
    "        \n",
    "        \n",
    "        train_loss_total,acc_train=train_epoch(model,train_loader,nfm_config['batch_size'],optimizer,loss_func)\n",
    "        train_loss_total_list.append(train_loss_total)#\n",
    "        if epoch_id %20==0:\n",
    "            num=num+1\n",
    "            path=os.path.join(model_path,'MLP'+str(num)+str(K)+'.pkl')\n",
    "            torch.save(model.state_dict(),path)\n",
    "    print(\"the \",ki,\" epoch ends\")\n",
    "    plotLoss(train_loss_total_list,epoches)\n",
    "    train_loss_total_list=[]\n",
    "    acc_test=val_epoch(model,val_loader,nfm_config['batch_size'],optimizer)\n",
    "    print(\"acc_test_each_k:\",acc_test)\n",
    "    test_metrics.append(acc_test)\n",
    "\n",
    "print(test_metrics)\n",
    "#test_metrics=test_metrics.tolist()\n",
    "test_metrics=[x.cpu().detach().numpy() for x in test_metrics]\n",
    "print(test_metrics)\n",
    "acc_test_metrics=np.mean(test_metrics) \n",
    "print(\"acc_test_metrics:\",acc_test_metrics)\n",
    "       \n",
    "print(test_metrics)\n",
    "#test_metrics=test_metrics.tolist()\n",
    "test_metrics=[x.cpu().detach().numpy() for x in test_metrics]\n",
    "print(test_metrics)\n",
    "acc_test_metrics=np.mean(test_metrics) \n",
    "print(\"acc_test_metrics:\",acc_test_metrics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a8bb01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
