{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa003039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from basemodel import BaseModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BiInteractionPooling(nn.Module):\n",
    "    \"\"\"Bi-Interaction Layer used in Neural FM,compress the\n",
    "      pairwise element-wise product of features into one single vector.\n",
    "      Input shape\n",
    "        - A 3D tensor with shape:``(batch_size,field_size,embedding_size)``.\n",
    "      Output shape\n",
    "    http://127.0.0.1:3000/notebooks/NFM-pyorch-master/NFM-pyorch-master/%E6%9C%AA%E5%91%BD%E5%90%8D5.ipynb?kernel_name=python3#    - 3D tensor with shape: ``(batch_size,1,embedding_size)``.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(BiInteractionPooling, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        concated_embeds_value = inputs\n",
    "        square_of_sum = torch.pow(\n",
    "            torch.sum(concated_embeds_value, dim=1, keepdim=True), 2)\n",
    "        sum_of_square = torch.sum(\n",
    "            concated_embeds_value * concated_embeds_value, dim=1, keepdim=True)\n",
    "        cross_term = 0.5 * (square_of_sum - sum_of_square)\n",
    "        return cross_term\n",
    "\n",
    "class NFM(BaseModel):\n",
    "    def __init__(self, config, dense_features_cols=[]):#=[]为新增\n",
    "    #def __init__(self, config, dense_features_cols, sparse_features_cols):\n",
    "        super(NFM, self).__init__(config)\n",
    "        # 稠密和稀疏特征的数量\n",
    "        #self.num_dense_feature = dense_features_cols.__len__()\n",
    "        self.num_dense_feature = 0#修改\n",
    "        self.num_sparse_feature = config['num_sparse_features_cols']\n",
    "        #self.num_sparse_feature = 0##修改\n",
    "        self.__config=config\n",
    "        self.drop=nn.Dropout(0.5)\n",
    "        # NFM的线性部分，对应 ∑WiXi\n",
    "        self.linear_model =nn.Linear(self.num_dense_feature + self.num_sparse_feature, config['linear_hidden'])\n",
    "            \n",
    "        self.BN_linear = nn.BatchNorm1d(config['linear_hidden'])\n",
    "        #self.linear_model = nn.Linear(self.num_dense_feature + self.num_sparse_feature, n_class)##修改\n",
    "        # NFM的Embedding层\n",
    "        \n",
    "        \"\"\"\n",
    "        self.dnn_layers = nn.ModuleList([\n",
    "            nn.Linear(in_features=layer[0], out_features=layer[1])\\\n",
    "                for layer in list(zip(self.hidden_layers[:-1], self.hidden_layers[1:])) \n",
    "        ])\n",
    "        \"\"\"\n",
    "        self.embedding_layers=nn.Embedding(config['embed_input_dim'],config['embed_dim'])\n",
    "        \n",
    "        # B-Interaction 层\n",
    "        self.bi_pooling = BiInteractionPooling()\n",
    "        self.bi_dropout = config['bi_dropout']\n",
    "        if self.bi_dropout > 0:\n",
    "            self.dropout = nn.Dropout(self.bi_dropout)\n",
    "            \n",
    "            \n",
    "        self.BN_bi = nn.BatchNorm1d(config['embed_dim'])\n",
    "        \n",
    "        \n",
    "        # NFM的DNN部分\n",
    "        self.hidden_layers = [self.num_dense_feature+config['linear_hidden'] + config['embed_dim']] + config['dnn_hidden_units']#是加还是乘\n",
    "        self.dnn_layers = nn.ModuleList([\n",
    "            nn.Linear(in_features=layer[0], out_features=layer[1])\\\n",
    "                for layer in list(zip(self.hidden_layers[:-1], self.hidden_layers[1:])) \n",
    "        ])\n",
    "        #self.dnn_linear = nn.Linear(self.hidden_layers[-1], 1, bias=False)\n",
    "        #self.dnn_linear = nn.Linear(self.hidden_layers[-1], n_class, bias=False)\n",
    "        \n",
    "        #增加\n",
    "        self.dnn_softmax=nn.Softmax(dim=1) # 按列SoftMax,列和为1  #注意nn.softmax的定义和调用\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 先区分出稀疏特征和稠密特征，这里是按照列来划分的，即所有的行都要进行筛选\n",
    "        dense_input, sparse_inputs = x[:, :self.num_dense_feature], x[:, self.num_dense_feature:]\n",
    "        sparse_inputs = sparse_inputs.long()\n",
    "\n",
    "        # 求出线性部分\n",
    "        linear_output = self.linear_model(x)\n",
    "        \n",
    "        print(\"linear_output:\",linear_output)\n",
    "        linear_output=linear_output.view(-1,self.__config['linear_hidden'])\n",
    "        linear_output=self.drop(linear_output)\n",
    "        linear_output=self.BN_linear(linear_output)\n",
    "        # 求出稀疏特征的embedding向量\n",
    "        sparse_embeds = [self.embedding_layers(sparse_inputs[:, i]) for i in range(sparse_inputs.shape[1])]\n",
    "        sparse_embeds = torch.cat(sparse_embeds, axis=-1)\n",
    "\n",
    "        # 送入B-Interaction层\n",
    "        fm_input = sparse_embeds.view(-1, self.num_sparse_feature, self.__config['embed_dim'])#整理成n行m列\n",
    "        # print(fm_input)\n",
    "        # print(fm_input.shape)\n",
    "\n",
    "        bi_out = self.bi_pooling(fm_input)\n",
    "        if self.bi_dropout:\n",
    "            bi_out = self.dropout(bi_out)\n",
    "\n",
    "        bi_out = bi_out.view(-1, self.__config['embed_dim'])\n",
    "        \n",
    "        bi_out=self.BN_bi(bi_out)\n",
    "        \n",
    "        # 将结果聚合起来\n",
    "        dnn_input = torch.cat((dense_input, bi_out,linear_output), dim=-1)\n",
    "\n",
    "        # DNN 层\n",
    "        dnn_output = dnn_input\n",
    "        for dnn in self.dnn_layers:\n",
    "            dnn_output = dnn(dnn_output)#dnn_output为tensor\n",
    "            # dnn_output = nn.BatchNormalize(dnn_output)\n",
    "            dnn_output = torch.relu(dnn_output)\n",
    "        #dnn_output = self.dnn_linear(dnn_output)\n",
    "        \n",
    "        print(\"dnn_softmax_output:\",dnn_output.shape)\n",
    "        y_pred=self.dnn_softmax(dnn_output)#增加\n",
    "        \n",
    "        # Final\n",
    "        #output = linear_output + y_pred#修改\n",
    "        #y_pred = self.dnn_softmax(output,dim=0)\n",
    "\n",
    "        return y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
