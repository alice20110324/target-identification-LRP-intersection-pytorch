{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8ada6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bn0.weight\n",
      "bn0.bias\n",
      "bn0.running_mean\n",
      "bn0.running_var\n",
      "bn0.num_batches_tracked\n",
      "fc1.weight\n",
      "fc1.bias\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "bn1.running_mean\n",
      "bn1.running_var\n",
      "bn1.num_batches_tracked\n",
      "fc2.weight\n",
      "fc2.bias\n",
      "bn2.weight\n",
      "bn2.bias\n",
      "bn2.running_mean\n",
      "bn2.running_var\n",
      "bn2.num_batches_tracked\n",
      "fc3.weight\n",
      "fc3.bias\n",
      "bn3.weight\n",
      "bn3.bias\n",
      "bn3.running_mean\n",
      "bn3.running_var\n",
      "bn3.num_batches_tracked\n",
      "MLP(\n",
      "  (bn0): BatchNorm1d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=208, out_features=1000, bias=True)\n",
      "  (bn1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=100, out_features=9, bias=True)\n",
      "  (bn3): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "relevance\n"
     ]
    }
   ],
   "source": [
    "#############  u===2   MLP_encode_100 and MLP_encode_1000  :654    ##################\n",
    "import torch\n",
    "#import Trainer\n",
    "from network import NFM\n",
    "import torch.utils.data as Data\n",
    "from Utils.criteo_loader import getTestData, getTrainData\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import sys \n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "import torchmetrics\n",
    "\n",
    "from innvestigator import InnvestigateModel\n",
    "from utils import Flatten\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from mnist_test import Net, train, test\n",
    "\n",
    "input_num=208\n",
    "# Network parameters\n",
    "class Params(object):\n",
    "    batch_size = 64\n",
    "    test_batch_size = 20\n",
    "    epochs = 5\n",
    "    lr = 0.01\n",
    "    momentum = 0.5\n",
    "    no_cuda = True\n",
    "    seed = 1\n",
    "    log_interval = 10\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "args = Params()\n",
    "torch.manual_seed(args.seed)\n",
    "#device = torch.device(\"cpu\")\n",
    "device=torch.device('cuda')\n",
    "kwargs = {}\n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':9,\n",
    "    \n",
    "    'num_epoch': 500,#训练epoch次数\n",
    "    'batch_size': 16,#batch_size\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    'epoch':1000,\n",
    "    \n",
    "   \n",
    "    'gene_name':'dataset/qiuguan/origin_800/gene_name.csv',\n",
    "    'label_name':'dataset/qiuguan/origin_800/gene_label.csv'\n",
    "    \n",
    "}\n",
    "\n",
    "#model definition\n",
    "import torch.nn as nn\n",
    "    \n",
    "    \n",
    "\n",
    "#model1 = MLP().cuda()\n",
    "#print(model1)\n",
    "\n",
    "class MLP_P(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn0=nn.BatchNorm1d(input_num)\n",
    "        self.fc1 = nn.Linear(input_num, 1000)\n",
    "        self.bn1= nn.BatchNorm1d(1000)\n",
    "        self.fc2 = nn.Linear(1000, 100)\n",
    "        self.bn2=nn.BatchNorm1d(100)\n",
    "        self.fc3=nn.Linear(100,9)\n",
    "        self.bn3=nn.BatchNorm1d(9)\n",
    "        \n",
    "        self.drop=nn.Dropout(0.3)    \n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn0=nn.BatchNorm1d(input_num)\n",
    "        self.fc1 = nn.Linear(input_num, 1000)\n",
    "        self.bn1= nn.BatchNorm1d(1000)\n",
    "        self.fc2 = nn.Linear(1000, 100)\n",
    "        self.bn2=nn.BatchNorm1d(100)\n",
    "        self.fc3=nn.Linear(100,9)\n",
    "        self.bn3=nn.BatchNorm1d(9)\n",
    "        \n",
    "        self.drop=nn.Dropout(0.3) \n",
    "        \n",
    "        #self.model1=MLP1().cuda() \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        y1=self.bn0(x)\n",
    "        y1 = F.relu(self.drop(self.bn1(self.fc1(y1))))\n",
    "        y1= F.relu(self.drop(self.bn2(self.fc2(y1))))\n",
    "        return F.softmax(self.bn3(self.fc3(y1)), dim=1)\n",
    "        \n",
    "           \n",
    "        \n",
    "        \n",
    "        \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "model = MLP().cuda()\n",
    "\n",
    "for i in model.state_dict():\n",
    "    print(i)\n",
    "print(model)\n",
    "mlp_paras=list(model.named_parameters())\n",
    "#print(mlp_paras)\n",
    "inn_model = InnvestigateModel(model, lrp_exponent=2,\n",
    "                              method=\"e-rule\",\n",
    "                              beta=.5)\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import torch.nn.functional as F  # 激励函数的库\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "\n",
    "def one_hot_smoothing(labels, classes, label_smoothing=0.2):\n",
    "    #n = len(labels)\n",
    "    n=labels.shape[0]\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    n = len(labels)\n",
    "    #eoff = label_smoothing / classes\n",
    "    output = np.zeros((n, classes), dtype=np.float32)\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_smoothing(labels, classes, label_smoothing=0.2):\n",
    "    #n = len(labels)\n",
    "    n=labels.shape[0]\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    n = len(labels)\n",
    "    #eoff = label_smoothing / classes\n",
    "    output = np.zeros((n, classes), dtype=np.float32)\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "class KZDatasetTest(data.Dataset):\n",
    "    \"\"\" Construct the FM pytorch dataset. \"\"\"\n",
    "    #def __init__(self, file,label_file, feature_map,n_class=16):\n",
    "    def __init__(self, csv_path):\n",
    "    \n",
    "       \n",
    "        self.data_info = self.get_data_info(csv_path)\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Dataset读取图片的函数\n",
    "        data, label = self.data_info[index]\n",
    "        #img = Image.open(img_pth).convert('RGB')\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_data_info(self,csv_path):\n",
    "        #解析路径\n",
    "        #转为一维list存储，每一位为【图片路径，图片类别】\n",
    "        labels=[]\n",
    "        data_info=[]\n",
    "        df=pd.read_csv(csv_path,sep=',',header=None)\n",
    "        #print(\"df:\",df)\n",
    "        df=df.iloc[2:,1:]\n",
    "        \n",
    "        rows,cols=df.shape\n",
    "        #print(rows,cols)\n",
    "        for i in df.iloc[:,-1]:\n",
    "            #print(i)\n",
    "            labels.append(int(float(i)))\n",
    "        #print('labels:',labels)\n",
    "        labels=np.array(labels)\n",
    "        \n",
    "        labels=one_hot_smoothing(labels,nfm_config['n_class'])\n",
    "        #print(labels)\n",
    "        for i in range(rows):\n",
    "            data=df.iloc[i,:-1]\n",
    "            data=data.astype(float)##############\n",
    "            \n",
    "            data=np.array(data)##\n",
    "            \n",
    "            label=labels[i]\n",
    "            \n",
    "            data=torch.from_numpy(data)#\n",
    "            label=torch.from_numpy(label)#\n",
    "           \n",
    "            \n",
    "            data_info.append((data,label))\n",
    "        return data_info\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import *\n",
    "from PIL import Image\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ast\n",
    "import torchvision\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "class KZDataset(Dataset):\n",
    "    def __init__(self, csv_path, K,n_class,ki=0, typ='train', transform=None, rand=False):\n",
    "        \n",
    "        self.all_data_info = self.get_data_info(csv_path)\n",
    "        \n",
    "        if rand:\n",
    "            random.seed(1)\n",
    "            random.shuffle(self.all_data_info)\n",
    "        leng = len(self.all_data_info)\n",
    "        every_z_len = leng // K\n",
    "        if typ == 'val':\n",
    "            self.data_info = self.all_data_info[every_z_len * ki : every_z_len * (ki+1)]\n",
    "        elif typ == 'train':\n",
    "            self.data_info = self.all_data_info[: every_z_len * ki] + self.all_data_info[every_z_len * (ki+1) :]\n",
    "            \n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Dataset读取图片的函数\n",
    "        data, label = self.data_info[index]\n",
    "        #img = Image.open(img_pth).convert('RGB')\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "       \n",
    "    \n",
    "    def get_data_info(self,csv_path):\n",
    "        #解析路径\n",
    "        #转为一维list存储，每一位为【图片路径，图片类别】\n",
    "        labels=[]\n",
    "        data_info=[]\n",
    "        df=pd.read_csv(csv_path,sep=',',header=None)\n",
    "        #print(\"df:\",df)\n",
    "        df=df.iloc[2:,1:]\n",
    "        #print(\"df:\",df)\n",
    "        print(df.shape)\n",
    "        #print(\"df:\",df)\n",
    "        #print(df.iloc[:,-1])\n",
    "        #df=df.applymap(ast.literal_eval)\n",
    "        rows,cols=df.shape\n",
    "        #print(rows,cols)\n",
    "        for i in df.iloc[:,-1]:\n",
    "            #print(i)\n",
    "            labels.append(int(float(i)))\n",
    "        #print('labels:',i,labels)\n",
    "        labels=np.array(labels)\n",
    "        #print('labels:',labels)\n",
    "        #labels=np.array(labels)\n",
    "        labels=one_hot_smoothing(labels,nfm_config['n_class'])\n",
    "        #print(labels)\n",
    "        for i in range(rows):\n",
    "            data=df.iloc[i,:-1]\n",
    "            data=data.astype(float)##############\n",
    "            #print(\"i,data:\",i,data)\n",
    "            #data=pd.DataFrame(data,dtype=float)###############\n",
    "            data=np.array(data)##\n",
    "            \n",
    "            label=labels[i]\n",
    "            #print(data.shape)\n",
    "            #print(label.shape)\n",
    "            #label=label.tolist()\n",
    "            data=torch.from_numpy(data)#\n",
    "            label=torch.from_numpy(label)#\n",
    "           \n",
    "            \n",
    "            data_info.append((data,label))\n",
    "        return data_info\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import sys \n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "import torchmetrics\n",
    "#LRP\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline\n",
    "\n",
    "from innvestigator import InnvestigateModel\n",
    "from utils import Flatten\n",
    "def standN(x):\n",
    "    row,col=x.shape\n",
    "    max_x=torch.max(x,1)\n",
    "    min_x=torch.min(x,1)\n",
    "    #print(max_x)\n",
    "    #print(min_x)\n",
    "    for i in range(row):\n",
    "        x[i]=(x[i]-min_x.values[i])/max_x.values[i]\n",
    "    return x\n",
    "\n",
    "def standNorm(x):\n",
    "    row,col=x.shape\n",
    "    mean=torch.mean(x,1)\n",
    "    std=torch.std(x,1)\n",
    "    #print(mean)\n",
    "    #print(std)\n",
    "    for i in range(row):\n",
    "        x[i]=(x[i]-mean[i])/std[i]\n",
    "    return x\n",
    "\n",
    "def augment_epoch(kii,train_loader,batch_size):\n",
    "    BATCH_SIZE=batch_size\n",
    "    total = 0\n",
    "    correct=0\n",
    "    total_loss=0\n",
    "    #loss_score=torch.tensor([[]]).cuda()\n",
    "    #\n",
    "    #loss_op=0\n",
    "    loss2_list=[]\n",
    "    model.train()\n",
    "    total_train_accuracy=0  \n",
    "    #BL=nn.BatchNorm1d(input_num)\n",
    "    #BL=BL.cuda()\n",
    "    y=[[0]*input_num]*batch_size\n",
    "    y=torch.tensor(y,dtype=torch.float)\n",
    "    y=y.cuda()\n",
    "    \n",
    "    for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "            \n",
    "        labels = Variable(labels)\n",
    "        x = Variable(x)\n",
    "            \n",
    "        x_row,x_col=x.shape   \n",
    "        x=torch.tensor(x,dtype=torch.float)\n",
    "        #print(x.shape)\n",
    "        labels=torch.tensor(labels,dtype=torch.float)\n",
    "        x, labels = x.cuda(), labels.cuda()\n",
    "        labels_int=labels=torch.max(labels,1)[1]#########\n",
    "        #print(labels_int)\n",
    "        #print('labels_int:',labels_int.shape)\n",
    "        #print('labels:',labels) \n",
    "        #print('x:',x.shape)\n",
    "        ll=labels.view(batch_size,-1)\n",
    "        #ll.dtype=torch.int\n",
    "        #print('ll:',ll)\n",
    "        if kii>=0 : \n",
    "            \n",
    "            inn_model.evaluate(in_tensor=x)\n",
    "        \n",
    "            model_prediction, only_max_score,org_shape = inn_model.innvestigatex()\n",
    "            model_prediction.cuda()\n",
    "            #print('only_max_score:',only_max_score)\n",
    "            #only_max_score1=only_max_score.detach().clone()\n",
    "            #print(labels.shape,labels)   \n",
    "        \n",
    "            #print(\"torch.argmax(labels[batch_idx]):\",torch.argmax(labels[batch_idx]))\n",
    "            #print(model_prediction.shape, model_prediction)\n",
    "            #model_prediction.cuda()\n",
    "            #print('input_relevance_values:')\n",
    "            rel_for_class_list=torch.argmax(model_prediction,1)\n",
    "            #print(rel_for_class_list)\n",
    "            para_list=[]\n",
    "            \n",
    "            '''  \n",
    "            rand=torch.linspace(0, 1, steps=input_num)\n",
    "            rand=rand.detach().numpy().tolist()\n",
    "            \n",
    "            para_list=[round(i,4) for i in rand]\n",
    "            '''\n",
    "            \n",
    "            #print(para_list)\n",
    "            \n",
    "            \n",
    "            for i in range(batch_size):\n",
    "            \n",
    "                max_pre=torch.argmax(model_prediction[i])\n",
    "                if max_pre!=labels[i]:\n",
    "                    #print('only_max_score1:',i,only_max_score)\n",
    "                    #only_max_score=only_max_score1\n",
    "                    #input_relevance_values=inn_model.compute_relevance_score(only_max_score,labels[i],org_shape,para=0.1)\n",
    "                    #rel_for_class=labels[i]\n",
    "                    #para=random.gauss(0.6,0.4)#########0.6，0.4：98  96\n",
    "                    \n",
    "                    #para=random.gauss(0.6,0.7)############0.6  0.7  96  95\n",
    "                    \n",
    "                    #para=random.gauss(0.7,0.4)###############\n",
    "                    #para=random.gauss(0.7,0.3)#############9110  96  98，0.2，0.1\n",
    "                    #para=random.gauss(0.6,0.5)########96  98  0.1  0.1  9110\n",
    "                    #para=random.gauss(0.6,0.2)#######0.1 0.1  96  96\n",
    "                    para=random.gauss(0.6,0.4)############7510pkl   96  96  0.6  0.4  0.1  0.1\n",
    "                    para=round(para,4)\n",
    "                    para_list.append(para)\n",
    "                    #print('device:',input_relevance_values.device)\n",
    "                    #input_relevance_values[labels[i]]=input_relevance_values[labels[i]].cuda()\n",
    "                    #input_relevance_values[labels[i]]=input_relevance_values[labels[i]].exp()\n",
    "                    #print('input_relevance_values:',i, input_relevance_values)\n",
    "                    #print('input_relevance_values[labels[i]]:',i,batch_idx,input_relevance_values[labels[i]])\n",
    "                    #cc[i]=torch.mul(x[i],input_relevance_values[labels[i]])+torch.tensor(0.1,dtype=torch.float)##############注意力\n",
    "                    #print('pre_target:',pre_target[i])\n",
    "                    #print('cc',i,cc[i].shape)\n",
    "                else:\n",
    "                    #print('only_max_score2:',i,only_max_score)\n",
    "                    #only_max_score=only_max_score1\n",
    "                    #input_relevance_values=inn_model.compute_relevance_score(only_max_score,labels[i],org_shape,para=0)\n",
    "                    #input_relevance_values[labels[i]]=input_relevance_values[labels[i]].exp()\n",
    "                    #print('input_relevance_values:',i, input_relevance_values)\n",
    "                    #print('input_relevance_values[i]:',i,batch_idx,input_relevance_values[labels[i]])\n",
    "                    #cc[i]=torch.mul(x[i],input_relevance_values[labels[i]])##############注意力\n",
    "                    #print('cc',i,cc[i].shape)\n",
    "                    para=random.gauss(0.1,0.2)#######11010pkl   96  96  0.6  0.4  0.1  0.2\n",
    "                    para=round(para,4)\n",
    "                    para_list.append(para)\n",
    "            \n",
    "            \n",
    "            \n",
    "            input_relevance_values,layers=inn_model.compute_relevance_scorex(only_max_score,rel_for_class_list,org_shape,para_list)\n",
    "            input_len=len(input_relevance_values)\n",
    "            \n",
    "            ee=[[0]*input_num]*batch_size\n",
    "            ee=torch.tensor(ee,dtype=torch.float)\n",
    "        \n",
    "            ee=ee.cuda()\n",
    "            input_relevance_values[-1]=input_relevance_values[-1].exp()############\n",
    "            \n",
    "            \n",
    "            #sum_input_relevance_values=torch.sum(input_relevance_values,dim=0)\n",
    "            #input_relevance_values=F.softmax(input_relevance_values/sum_input_relevance_values,dim=1)\n",
    "            input_relevance_values[-1]=F.softmax(input_relevance_values[-1],dim=1)################\n",
    "        \n",
    "            '''\n",
    "            for i in range(batch_size):\n",
    "                        max_pre=torch.argmax(model_prediction[i])######\n",
    "                        if max_pre!=labels[i]: #and i==min_predict_mis:###\n",
    "                            \n",
    "                                \n",
    "            \n",
    "                                ee[i]=torch.mul(x[i],input_relevance_values[-1][i])##############注意力33333333333333333weight\n",
    "                                #dd=torch.mul(bias,input_relevance_values[-1][i])############bias\n",
    "                                #print('cc:',i,cc)\n",
    "                                #ee[i]=torch.mul(drop,input_relevance_values[i])\n",
    "                                ################\n",
    "            '''                    \n",
    "            ee=torch.mul(x,input_relevance_values[-1])\n",
    "            \n",
    "            \n",
    "            x1=torch.subtract(x,ee)\n",
    "            x2=torch.add(x,ee)\n",
    "            #print(ll.shape)\n",
    "            #print(x.shape)\n",
    "            #x1=torch.cat((x1,ll),dim=1)\n",
    "            #x2=torch.cat((x2,ll),dim=1)\n",
    "            \n",
    "            l1=ll\n",
    "            l2=ll\n",
    "            if batch_idx==0:\n",
    "                \n",
    "                y=torch.cat((x1,x2),dim=0)\n",
    "                l=torch.cat((l1,l2),dim=0)\n",
    "            y=torch.cat((y,x1),dim=0)\n",
    "            y=torch.cat((y,x2),dim=0)\n",
    "            \n",
    "            l=torch.cat((l,l1),dim=0)\n",
    "            l=torch.cat((l,l2),dim=0)\n",
    "    \n",
    "    \n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "                    \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "              \n",
    "            \n",
    "            \n",
    "                \n",
    "                \n",
    "                \n",
    "                        \n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "    '''  \n",
    "        #print('x_new:',x)\n",
    "        #print('new_x:',x)\n",
    "        optimizer.zero_grad()\n",
    "        y_predict=model(x)\n",
    "        #y_predict=model(x,labels)\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        loss1 = loss_func(y_predict, labels)\n",
    "        #print('loss1:',loss1)\n",
    "        #loss2=loss_func(y_predict1,labels)\n",
    "        #loss2=u*(1/loss_op)\n",
    "        #print('input_relevance_values:',input_relevance_values.shape)\n",
    "        #print('loss_score:',loss_score.shape)\n",
    "        #cc=1.0/torch.abs(torch.sum(cc))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #loss2=u*cc\n",
    "        #print('input_relevance_values:',input_relevance_values)\n",
    "        \n",
    "        #print('loss1:',loss1)\n",
    "        #print('loss2:',loss2)\n",
    "        #loss=loss1+loss2\n",
    "        loss=loss1\n",
    "        #loss = loss_func(y_predict, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #loss2_list.append(u*loss2)   \n",
    "        loss = loss.item()\n",
    "           \n",
    "\n",
    "        total_loss += loss\n",
    "            \n",
    "            \n",
    "            \n",
    "        batch_train_acc=torchmetrics.functional.accuracy(y_predict,labels_int)\n",
    "        total_train_accuracy+=batch_train_acc\n",
    "    #plotLoss(loss2_list,batch_idx+1)   #################################     \n",
    "    total_train_accuracy/=(batch_idx+1)\n",
    "    print('total_train_accuracy:',total_train_accuracy)\n",
    "    print(\"Training Epoch: %d, total loss: %f\" % (epoch_id, total_loss))\n",
    "    return total_loss,total_train_accuracy\n",
    "    \n",
    "    ''' \n",
    "    return y,l\n",
    "\n",
    "def val_epoch(test_loader,batch_size,optimizer): \n",
    "    batch_size_num=0\n",
    "    total_test_acc=0\n",
    "    model.eval()\n",
    "    for i , (inputs , targets) in enumerate(test_loader):   \n",
    "        print(\"val\")\n",
    "            \n",
    "            \n",
    "        inputs = Variable(inputs)   \n",
    "        targets = Variable(targets)     \n",
    "           \n",
    "        inputs=torch.tensor(inputs ,dtype=torch.float)   \n",
    "        targets=torch.tensor(targets ,dtype=torch.float)   \n",
    "        inputs , targets = inputs.cuda(),  targets.cuda()\n",
    "        targets=torch.max(targets,1)[1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #yhat = model1(inputs,targets)\n",
    "        yhat=model(inputs)\n",
    "            \n",
    "            \n",
    "            \n",
    "        #targets=torch.max(targets,1)[1]\n",
    "            \n",
    "            \n",
    "            \n",
    "        batch_test_acc=torchmetrics.functional.accuracy(yhat,targets)\n",
    "            \n",
    "        total_test_acc+=batch_test_acc\n",
    "            \n",
    "        batch_size_num=i\n",
    "    total_test_acc/=(batch_size_num+1)\n",
    "        ###print('total_test_accuracy:',total_test_acc/(batch_size+1))\n",
    "    print('total_test_accuracy:',total_test_acc)\n",
    "        \n",
    "                    \n",
    "                    \n",
    "            \n",
    "            \n",
    "    \n",
    "        \n",
    "   \n",
    "    \n",
    "    return total_test_acc\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotLoss(loss,epoch):\n",
    "    plt.rcParams['font.sans-serif']=['SimHei']\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    x=[i for i in range(epoch)]\n",
    "    #acc_train=acc_train.cpu()\n",
    "    #acc_test=acc_test.cpu()\n",
    "    plt.plot(x, loss, 'r-', mec='k', label='Logistic Loss', lw=2)\n",
    "    #plt.plot(x,acc_train,'b-',mec='k',label='accuracy Train',lw=2)\n",
    "    #plt.plot(x,acc_test,'g-',mec='k',label='accuracy Test',lw=2)\n",
    "    #plt.plot(x, y_01, 'g-', mec='k', label='0/1 Loss', lw=2)\n",
    "    #plt.plot(x, y_hinge, 'b-',mec='k', label='Hinge Loss', lw=2)\n",
    "    #plt.plot(x, boost, 'm--',mec='k', label='Adaboost Loss',lw=2)\n",
    "    plt.grid(True, ls='--')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('损失函数')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b312a636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (bn0): BatchNorm1d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=208, out_features=1000, bias=True)\n",
      "  (bn1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=100, out_features=9, bias=True)\n",
      "  (bn3): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "relevance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:440: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:442: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1          2         3         4         5          6    \\\n",
      "0     4.709579  4.716463   7.777922  6.882918  5.101910  8.764083   9.613808   \n",
      "1     4.868883  4.651964   5.547213  6.857551  5.260423  8.786485   9.445354   \n",
      "2     5.117973  4.718863   5.005919  7.255142  4.495315  8.349341   9.278605   \n",
      "3     5.200118  5.144049   9.352578  8.063399  5.411821  8.901824   9.407613   \n",
      "4     5.089912  4.631964   8.808535  6.953736  5.038565  8.811357   9.607741   \n",
      "...        ...       ...        ...       ...       ...       ...        ...   \n",
      "1115  5.553076  4.966623  11.248826  7.629302  5.666522  8.771038   9.297793   \n",
      "1116  5.771052  5.235774  10.515899  6.782695  6.003650  8.080636   8.961834   \n",
      "1117  5.453845  5.431081   9.703544  8.007027  5.384284  7.520735  10.316896   \n",
      "1118  5.318877  5.108947  10.911935  6.798035  5.473487  9.176603   9.771338   \n",
      "1119  6.104203  5.345085  14.274673  7.173561  5.114929  8.312332   9.282376   \n",
      "\n",
      "           7         8         9    ...        199       200       201  \\\n",
      "0     5.525864  6.667677  6.101399  ...   8.330604  4.882448  7.844787   \n",
      "1     5.283682  6.614093  5.855824  ...   8.969608  4.570182  7.753607   \n",
      "2     4.957799  6.790067  5.548050  ...   7.550879  5.974465  6.831539   \n",
      "3     5.302907  6.672465  5.817746  ...   8.736396  5.651922  7.131129   \n",
      "4     5.646970  6.618763  6.124764  ...   8.465464  4.896180  7.212823   \n",
      "...        ...       ...       ...  ...        ...       ...       ...   \n",
      "1115  5.300544  7.089303  5.992670  ...   9.557904  7.525370  7.489691   \n",
      "1116  5.512223  6.881691  6.350294  ...   9.633316  7.303010  7.512509   \n",
      "1117  5.353253  6.336235  6.216756  ...  10.057063  6.989415  7.057801   \n",
      "1118  6.083480  7.267985  5.944916  ...   9.796268  7.981633  7.989257   \n",
      "1119  5.816428  7.434035  6.187270  ...   9.237068  8.331946  8.104911   \n",
      "\n",
      "           202        203       204       205       206       207  0    \n",
      "0     4.704659   9.679579  4.276991  6.917216  5.910201  7.487748    2  \n",
      "1     4.621443   9.966607  4.588909  8.114681  6.026882  7.882585    4  \n",
      "2     5.616941   9.219448  4.348893  8.351503  5.438474  7.008321    1  \n",
      "3     5.050356   9.345680  4.380085  7.515898  5.794964  7.409737    8  \n",
      "4     4.957150   9.769796  4.604753  7.156324  6.136122  7.303727    8  \n",
      "...        ...        ...       ...       ...       ...       ...  ...  \n",
      "1115  5.240892   9.876747  4.971641  9.093557  5.866214  7.606283    1  \n",
      "1116  5.049038  10.116394  5.417959  6.192524  5.996686  7.799855    4  \n",
      "1117  5.268650  10.601934  5.279703  7.762892  6.019433  8.072816    8  \n",
      "1118  5.140606  10.293953  4.894806  6.655146  6.785099  7.585711    6  \n",
      "1119  5.348380  10.484610  4.837410  6.704679  5.798682  7.844321    5  \n",
      "\n",
      "[1120 rows x 209 columns]\n"
     ]
    }
   ],
   "source": [
    "#_,model=MLPA().cuda\n",
    "import torch\n",
    "\n",
    "#功能：加载保存到path中的各层参数到神经网络\n",
    "\n",
    "path='dataset/qiuguan/origin_800/LRP/non_encode/50/without_attention/MLP1010.pkl'\n",
    "\n",
    "#nfm=NFM(nfm_config)\n",
    "mlp=MLP()\n",
    "#print(nfm)\n",
    "#net = nn.DataParallel(net)\n",
    "#net = net.to(device)\n",
    "mlp.load_state_dict(torch.load(path),strict=False)\n",
    "mlp.cuda()\n",
    "\n",
    "print(mlp)\n",
    "\n",
    "\n",
    "#print(model.state_dict())\n",
    "\n",
    "mlp_params = list(mlp.named_parameters())\n",
    "#print(mlp_params)\n",
    "model=mlp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inn_model = InnvestigateModel(model, lrp_exponent=2,\n",
    "                              method=\"e-rule\",\n",
    "                              beta=.5)\n",
    "\n",
    "\n",
    "import itertools\n",
    "\n",
    "K=10\n",
    "test_metrics=[]\n",
    "train_loss_total_list=[]\n",
    "\n",
    "testset= KZDatasetTest(csv_path='dataset/qiuguan/origin_800/LRP/50/selected_train_val_info.csv')\n",
    "   \n",
    "test_loader = data.DataLoader(\n",
    "         dataset=testset,\n",
    "         #transform=torchvision.transforms.ToTensor(),\n",
    "         drop_last=True,\n",
    "         batch_size=nfm_config['batch_size']\n",
    "        \n",
    "     )\n",
    "\n",
    "\n",
    "#model_path='dataset/qiuguan/origin_800/LRP/non_encode/200/attention0.01/'\n",
    "#BATCH_SIZE=batch_size\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "#total = 0\n",
    "    \n",
    "    \n",
    "#loss_func=torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "num=0\n",
    "   \n",
    "model_named_parameters=[j for j in model.named_parameters()]\n",
    "#print('model_parameters:',model_named_parameters)\n",
    "#print('model.state.dict:',model.state_dict())\n",
    "epoches=1\n",
    "for epoch_id in range(epoches):\n",
    "          \n",
    "        \n",
    "        \n",
    "        y,l=augment_epoch(0,test_loader,nfm_config['batch_size'])\n",
    "        #train_loss_total_list.append(train_loss_total)#\n",
    "        \n",
    "        '''  \n",
    "        if epoch_id %10==0:\n",
    "            num=num+1\n",
    "            path=os.path.join(model_path,'MLP'+str(num)+str(K)+'.pkl')\n",
    "            torch.save(model.state_dict(),path)\n",
    "            \n",
    "         '''  \n",
    "y=y.detach().cpu().numpy()\n",
    "y=pd.DataFrame(y)\n",
    "\n",
    "l=l.detach().cpu().numpy()\n",
    "l=pd.DataFrame(l)\n",
    "#print(y)\n",
    "#print(l)\n",
    "\n",
    "\n",
    "aug=pd.concat((y,l),axis=1)\n",
    "aug.to_csv('dataset/qiuguan/origin_800/LRP/50/aug_train_val_info.csv')\n",
    "print(aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2504a6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            TYR        PPM1E                REN               ABCC3  \\\n",
      "0        4.70958      4.71646            7.77792             6.88292   \n",
      "1        4.86888      4.65196            5.54721             6.85755   \n",
      "2        5.11797      4.71886            5.00592             7.25514   \n",
      "3        5.20012      5.14405            9.35258              8.0634   \n",
      "4        5.08991      4.63196            8.80853             6.95374   \n",
      "..           ...          ...                ...                 ...   \n",
      "541  5.743529064  5.210875345        10.46588395  6.7502614670000005   \n",
      "542  5.427847751  5.405209976  9.657287377000001         7.968845876   \n",
      "543  5.293333995  5.084620506        10.85997562   6.765459452999999   \n",
      "544   6.07443638  5.319655345        14.20590899         7.139406749   \n",
      "545   5.19511412  4.927471871         6.83466967  7.9497790739999985   \n",
      "\n",
      "0           LRP5       DNAJB1              ABCD3        NUBPL        FKBP4  \\\n",
      "0        5.10191      8.76408            9.61381      5.52586      6.66768   \n",
      "1        5.26042      8.78648            9.44535      5.28368      6.61409   \n",
      "2        4.49531      8.34934            9.27861       4.9578      6.79007   \n",
      "3        5.41182      8.90182            9.40761      5.30291      6.67247   \n",
      "4        5.03857      8.81136            9.60774      5.64697      6.61876   \n",
      "..           ...          ...                ...          ...          ...   \n",
      "541  5.973083531  8.041910695  8.918939803999999  5.485931233  6.848810165   \n",
      "542  5.358618036   7.48444839        10.26745748  5.327675867  6.305334465   \n",
      "543   5.44740957  9.132807376  9.724481127999999  6.054224798  7.233185653   \n",
      "544  5.089893268  8.272700125        9.238210411   5.78875476  7.398263494   \n",
      "545  5.215868235  8.668916388         8.96872861  5.524441201  7.009632536   \n",
      "\n",
      "0           TFAM  ...     ATP6V0E2               IGF1             ZNF593  \\\n",
      "0         6.1014  ...       8.3306            4.88245            7.84479   \n",
      "1        5.85582  ...      8.96961            4.57018            7.75361   \n",
      "2        5.54805  ...      7.55088            5.97447            6.83154   \n",
      "3        5.81775  ...       8.7364            5.65192            7.13113   \n",
      "4        6.12476  ...      8.46546            4.89618            7.21282   \n",
      "..           ...  ...          ...                ...                ...   \n",
      "541  6.317953585  ...  9.587394448        7.268261777         7.47670395   \n",
      "542  6.187009694  ...  10.00895899         6.95610894  7.023943157000001   \n",
      "543  5.916318053  ...  9.749359931  7.943620007000001        7.951179894   \n",
      "544  6.157667994  ...  9.193050093        8.292288181        8.066300433   \n",
      "545  6.132822777  ...  9.092804957  7.676555197000001         7.38484989   \n",
      "\n",
      "0                 AADAC      ATP5IF1        MUC16     SERPINF1  \\\n",
      "0               4.70466      9.67958      4.27699      6.91722   \n",
      "1               4.62144      9.96661      4.58891      8.11468   \n",
      "2               5.61694      9.21945      4.34889       8.3515   \n",
      "3               5.05036      9.34568      4.38008       7.5159   \n",
      "4               4.95715       9.7698      4.60475      7.15632   \n",
      "..                  ...          ...          ...          ...   \n",
      "541         5.024974264  10.06828642  5.392151009   6.16271403   \n",
      "542         5.243536465  10.55134956  5.254541275   7.72590708   \n",
      "543         5.116125795  10.24490888  4.871486101  6.623344071   \n",
      "544  5.3228251680000005  10.43459007  4.814384327  6.672702295   \n",
      "545         5.092608074  9.684586981  4.789443717  7.532901706   \n",
      "\n",
      "0               MRPL44              ABCB7 label  \n",
      "0               5.9102            7.48775     2  \n",
      "1              6.02688            7.88258     4  \n",
      "2              5.43847            7.00832     1  \n",
      "3              5.79496            7.40974     8  \n",
      "4              6.13612            7.30373     8  \n",
      "..                 ...                ...   ...  \n",
      "541  5.968105592000001        7.762717611     4  \n",
      "542  5.990752387000001        8.034228317     8  \n",
      "543        6.751462625        7.549562876     6  \n",
      "544  5.770955197999999  7.806921347999999     5  \n",
      "545        5.715177177        7.185852079     7  \n",
      "\n",
      "[1665 rows x 209 columns]\n"
     ]
    }
   ],
   "source": [
    "x=pd.read_csv('dataset/qiuguan/origin_800/LRP/50/selected_train_val_info.csv',sep=',')\n",
    "#print(x)\n",
    "\n",
    "#x=x.iloc[:,1:]\n",
    "\n",
    "#print(x)\n",
    "\n",
    "column=x.iloc[0,1:]#列名\n",
    "#print(column)\n",
    "\n",
    "x_new=x.iloc[1:,1:]\n",
    "x_new.columns=column\n",
    "#print(x_new)\n",
    "aug.columns=column\n",
    "#print(aug)\n",
    "\n",
    "all=pd.concat((aug,x_new),axis=0)\n",
    "print(all)\n",
    "\n",
    "all.to_csv('dataset/qiuguan/origin_800/LRP/50/all_train_val_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea10446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
