{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48637339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import Trainer\n",
    "from network import NFM\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from Utils.criteo_loader import getTestData, getTrainData\n",
    "\n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':9,\n",
    "    'linear_hidden1':1000,\n",
    "    #'linear_hidden':100,#线性模型输出层（隐层个数）\n",
    "    'embed_input_dim':1001,#embed输入维度\n",
    "    'embed_dim': 100, # 用于控制稀疏特征经过Embedding层后的稠密特征大小，embed输出维度\n",
    "    #'dnn_hidden_units': [100,11],#MLP隐层和输出层\n",
    "    'linear1_drop':0.5,\n",
    "    'dnn_hidden_units':[100,9],#MLP隐层\n",
    "    'dnn_layer_units':[100,9],\n",
    "    'num_sparse_features_cols':10477,#the number of the gene columns\n",
    "    'num_dense_features': 0,#dense features number\n",
    "    'bi_dropout': 0.3,#Bi-Interaction 的dropout\n",
    "    'num_epoch': 500,#训练epoch次数\n",
    "    'batch_size': 24,#batch_size\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    'epoch':1000,\n",
    "    \n",
    "    #'train_file': '../Data/criteo/processed_data/train_set.csv',\n",
    "    #'fea_file': '../Data/criteo/processed_data/fea_col.npy',\n",
    "    #'validate_file': '../Data/criteo/processed_data/val_set.csv',\n",
    "    #'test_file': '../Data/criteo/processed_data/test_set.csv',\n",
    "    #'model_name': '../TrainedModels/NFM.model'\n",
    "    #'train_file':'data/xiaoqiu_gene_5000/train/final_5000_encode_100x.csv',\n",
    "    'train_data':'dataset/qiuguan/encode/encode_1000/train/train_encode_data_1000_new.csv',\n",
    "    'train_label':'dataset/qiuguan/non_code/train/train_label.csv',\n",
    "    #'test_data':'dataset/qiuguan/non_code/test/test_encode_data.csv',\n",
    "    'test_data':'dataset/qiuguan/encode/encode_1000/test/test_encode_data_1000_new.csv',\n",
    "    'test_label':'dataset/qiuguan/non_code/test/test_labels.csv'\n",
    "    #'title':'dataset/xiaoguan/RF/RF_for_train/train_class_9/test/test_data.csv',\n",
    "    \n",
    "    #'all':''\n",
    "    #'title':'data/xiaoqiu_gene_5000/train/gene_5000_gene_name.csv',\n",
    "    #'all':'data/xiaoqiu_gene_5000/train/gene_5000_label_name.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24b6f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiInteractionPooling(nn.Module):\n",
    "    \"\"\"Bi-Interaction Layer used in Neural FM,compress the\n",
    "      pairwise element-wise product of features into one single vector.\n",
    "      Input shape\n",
    "        - A 3D tensor with shape:``(batch_size,field_size,embedding_size)``.\n",
    "      Output shape\n",
    "    http://127.0.0.1:3000/notebooks/NFM-pyorch-master/NFM-pyorch-master/%E6%9C%AA%E5%91%BD%E5%90%8D5.ipynb?kernel_name=python3#    - 3D tensor with shape: ``(batch_size,1,embedding_size)``.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(BiInteractionPooling, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        concated_embeds_value = inputs\n",
    "        square_of_sum = torch.pow(\n",
    "            torch.sum(concated_embeds_value, dim=1, keepdim=True), 2)\n",
    "        sum_of_square = torch.sum(\n",
    "            concated_embeds_value * concated_embeds_value, dim=1, keepdim=True)\n",
    "        cross_term = 0.5 * (square_of_sum - sum_of_square)\n",
    "        return cross_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63a52264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embding_process(config,sparse_inputs):\n",
    "    \n",
    "    \n",
    "    embedding_layers=nn.Embedding(config['embed_input_dim'],config['embed_dim'])\n",
    "        \n",
    "    # B-Interaction 层\n",
    "    bi_pooling = BiInteractionPooling()\n",
    "    bi_dropout = config['bi_dropout']\n",
    "    if bi_dropout > 0:\n",
    "        dropout = nn.Dropout(bi_dropout)\n",
    "            \n",
    "    num_sparse_feature=config['num_sparse_features_cols']\n",
    "    \n",
    "    sparse_embeds = [embedding_layers(sparse_inputs[:, i]) for i in range(sparse_inputs.shape[1])]\n",
    "    sparse_embeds = torch.cat(sparse_embeds, axis=-1)\n",
    "\n",
    "    # 送入B-Interaction层\n",
    "    fm_input = sparse_embeds.view(-1, num_sparse_feature, config['embed_dim'])#整理成n行m列\n",
    "    # print(fm_input)\n",
    "    # print(fm_input.shape)\n",
    "\n",
    "    bi_out = bi_pooling(fm_input)\n",
    "    if bi_dropout:\n",
    "        bi_out = dropout(bi_out)\n",
    "\n",
    "    bi_out = bi_out.view(-1, config['embed_dim'])\n",
    "    \n",
    "    return bi_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5e1a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "\n",
    "def one_hot_smoothing(labels, classes, label_smoothing=0.2):\n",
    "    n = len(labels)\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "def one_hot(labels, classes):\n",
    "    n = len(labels)\n",
    "    #eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32)\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1\n",
    "        #print(\"row:\",row,\"label:\",label)\n",
    "    return output\n",
    "\n",
    "class FMData(data.Dataset):\n",
    "    \"\"\" Construct the FM pytorch dataset. \"\"\"\n",
    "    #def __init__(self, file,label_file, feature_map,n_class=16):\n",
    "    def __init__(self, file,label_file, n_class):\n",
    "        super(FMData, self).__init__()\n",
    "        self.label =torch.Tensor([])\n",
    "        self.features = torch.Tensor([])\n",
    "        self.bi_features=torch.Tensor([])\n",
    "        \n",
    "        \n",
    "        fd=pd.read_csv(file,sep=',')\n",
    "        \n",
    "        n_fd=np.array(fd)\n",
    "        \n",
    "        \n",
    "        self.features=np.array(n_fd)\n",
    "        self.bi_features=torch.from_numpy(self.features)\n",
    "        self.bi_features=embding_process(nfm_config,self.bi_features)\n",
    "        \n",
    "        self.features=torch.from_numpy(self.features)\n",
    "        nrow,ncol=n_fd.shape\n",
    "        \n",
    "\n",
    "        \n",
    "        label_fd=pd.read_csv(label_file,sep=',')\n",
    "        #print(features)\n",
    "        #print(label_fd)\n",
    "        label=np.array(label_fd)\n",
    "        #label=label[:,1:]\n",
    "        label=one_hot_smoothing(label,n_class)\n",
    "        self.label=label\n",
    "        self.label=torch.from_numpy(self.label)\n",
    "        \n",
    "        print(self.features)\n",
    "        print(self.bi_features)\n",
    "        print(self.label)\n",
    "        #print(\"label:\",label)\n",
    "        #print(\"features:\",self.features)\n",
    "        #print(label)\n",
    "        # convert labels\n",
    "        \"\"\"if config.loss_type == 'square_loss':\n",
    "            self.label.append(np.float32(items[0]))\n",
    "        else: # log_loss\n",
    "            label = 1 if float(items[0]) > 0 else 0\n",
    "            self.label.append(label)\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        assert all(len(item) == len(self.features[0]\n",
    "            ) for item in self.features), 'features are of different length'\n",
    "        \"\"\"\n",
    "        #print(len(self.features))\n",
    "        #print(len(self.feature_values))\n",
    "        #print(len(self.label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.label[idx]\n",
    "        features = self.features[idx]\n",
    "        bi_features=self.bi_features[idx]\n",
    "        #feature_values = self.feature_values[idx]\n",
    "        #return features, feature_values, label\n",
    "        return features, bi_features, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7cd50592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#准备训练集\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "#from utils import \n",
    "#准备训练集\n",
    "#from new_dataset_processed import FMData\n",
    "#from dataset_process import FMData\n",
    "def prepare_dataset(m_data,m_label,batch_size,n_class):\n",
    "    m_dataset=FMData(m_data,m_label,n_class)\n",
    "    m_dataloader=data.DataLoader(m_dataset, drop_last=True,batch_size=batch_size,shuffle=True)\n",
    "    \n",
    "    return m_dataset,m_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0f704d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import sys \n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "import torchmetrics\n",
    "def   train_data(model,train_loader,test_loader,batch_size,model_path):\n",
    "    #train_accuracy=torchmetrics.Accuracy()\n",
    "    #test_accuracy=torchmetrics.Accuracy()\n",
    "    BATCH_SIZE=batch_size\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    total = 0\n",
    "    \n",
    "    #loss_func = torch.nn.BCELoss()\n",
    "    loss_func=torch.nn.CrossEntropyLoss()\n",
    "    #loss_func=nn.MultiLabelSoftMarginLoss()\n",
    "    #loss_func=torch.nn.LogSoftmax()\n",
    "    num=2000\n",
    "    #model=nn.Softmax(nn.Linear(10149,16)).to(device)\n",
    "    # 从DataLoader中获取小批量的id以及数据\n",
    "    \n",
    "    batch_size=0\n",
    "    for epoch_id in range(1000):\n",
    "        correct=0\n",
    "        total=0\n",
    "        total_test_acc=0\n",
    "        total_train_accuracy=0\n",
    "        for batch_idx, (x, bi_x,labels) in enumerate(train_loader):\n",
    "            x = Variable(x)\n",
    "            bi_x=Variable(bi_x)\n",
    "            labels = Variable(labels)\n",
    "            \n",
    "            \n",
    "            #x = torch.tensor(x, dtype=torch.float)\n",
    "            #x=x.clone().detach().requires_grad_(True)\n",
    "            x=torch.tensor(x,dtype=torch.float)\n",
    "            bi_x=torch.tensor(bi_x,dtype=torch.long)\n",
    "            labels=torch.tensor(labels,dtype=torch.float)\n",
    "            x, bi_x,labels = x.cuda(),bi_x.cuda(), labels.cuda()\n",
    "            print(x)\n",
    "            print(bi_x)\n",
    "            print(labels)\n",
    "            labels_int=labels=torch.max(labels,1)[1]\n",
    "            #labels_int.cuda()\n",
    "            #print(\"labels:\",labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_predict = model(x,bi_x)\n",
    "            #print(\"y_predict:\",y_predict)\n",
    "            #loss = loss_func(y_predict.view(-1), labels)\n",
    "            loss = loss_func(y_predict, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss = loss.item()\n",
    "            #loss, predicted = self._train_single_batch(x, labels)\n",
    "\n",
    "            total += loss\n",
    "            \n",
    "            \n",
    "            #predicted = torch.max(y_predict.data,1)\n",
    "            #print(\"predicted:\",predicted)\n",
    "            #predicted = torch.max(y_predict.data,1)[1]\n",
    "            #predicted=predicted.detach().cpu().numpy()\n",
    "            \n",
    "            #labels=torch.max(labels,1)\n",
    "            #print(\"labels:\",labels)\n",
    "            #labels=labels[1]\n",
    "            #y_predict=y_predict.cuda()\n",
    "            #labels=torch.max(labels,1)[1].cuda()\n",
    "            #labels=labels.detach().cpu().numpy()\n",
    "            #correct += (predicted == labels).sum()\n",
    "            #print(\"correct:\",correct)\n",
    "            #correct=correct[0]\n",
    "            #print(\"new_correct:\",float(correct))\n",
    "            #correct=float(correct)   \n",
    "            #if batch_idx % 10 == 0:\n",
    "            #print(\"batch_idx:\",batch_idx)\n",
    "            #print(correct/(BATCH_SIZE*(batch_idx+1)))\n",
    "            batch_train_acc=torchmetrics.functional.accuracy(y_predict,labels_int)\n",
    "            #print('batch_train_acc:',batch_train_acc)\n",
    "            total_train_accuracy+=batch_train_acc\n",
    "        #total_train_accuracy=torchmetrics.functional.compute_details()\n",
    "        total_train_accuracy/=(batch_idx+1)\n",
    "        print('total_train_accuracy:',total_train_accuracy)\n",
    "        #print('total_train_accuracy:',total_train_accuracy)\n",
    "        for i , (inputs ,bi_inputs, targets) in enumerate(test_loader):   \n",
    "            print(\"test\")\n",
    "            # evaluate the model on the test set   \n",
    "            #print(\\ inputs:\\  inputs)   \n",
    "            #print(\\ targets:\\  targets)   \n",
    "            inputs = Variable(inputs)   \n",
    "            bi_inputs=Variable(bi_inputs)\n",
    "            targets = Variable(targets)     \n",
    "            #x = torch.tensor(x  dtype=torch.float)   \n",
    "            #x=x.clone().detach().requires_grad_(True)   \n",
    "            inputs=torch.tensor(inputs ,dtype=torch.float) \n",
    "            bi_inputs=torch.tensor(inputs,dtype=torch.float)\n",
    "            targets=torch.tensor(targets ,dtype=torch.float)   \n",
    "            inputs ,bi_inputs, targets = inputs.cuda(),bi_inputs.cuda(),  targets.cuda()   \n",
    "            yhat = model(inputs,bi_inputs)  \n",
    "            \n",
    "            #yhat = torch.max(yhat.data,1)[1]\n",
    "            #yhat=yhat.detach().cpu().numpy()\n",
    "            #print(\"predicted:\",predicted)\n",
    "            #predicted = torch.max(y_predict.data,1)[1]\n",
    "             #predicted = torch.max(y_predict.data,1)[1]\n",
    "            \n",
    "            \n",
    "            targets=torch.max(targets,1)[1]\n",
    "            #print(\"labels:\",labels)\n",
    "            #labels=labels[1]\n",
    "            #targets=targets.detach().cpu().numpy()\n",
    "            \n",
    "            \n",
    "            \n",
    "            batch_test_acc=torchmetrics.functional.accuracy(yhat,targets)\n",
    "            #print(\"batch_test_acc:\",batch_test_acc)\n",
    "            total_test_acc+=batch_test_acc\n",
    "            #total_test_accuracy=torchmetrics.functional.compute_details()\n",
    "            batch_size=i\n",
    "        print('total_test_accuracy:',total_test_acc/(batch_size+1))\n",
    "        \n",
    "                    \n",
    "                    \n",
    "            \n",
    "            #model.evaluate()\n",
    "            #model.eval()\n",
    "            #train_result = evaluate.metrics(model, train_loader)\n",
    "            #valid_result = evaluate.metrics(model, valid_loader)\n",
    "            #est_result = evaluate.metrics(model, test_loader)\n",
    "            #acturals,predictions,acc_test=evaluate_model(test_loader,model)\n",
    "            #print(\"acc_test:  %d  \" %(acc_test))\n",
    "            #print(\"Train_RMSE: {:.3f}, Valid_RMSE: {:.3f}, Test_RMSE: {:.3f}\".format(\n",
    "            #train_result, valid_result, test_result))\n",
    "            # print('[Training Epoch: {}] Batch: {}, Loss: {}'.format(epoch_id, batch_id, loss))\n",
    "        print(\"Training Epoch: %d, total loss: %f\" % (epoch_id, total))\n",
    "        #print(\"auc:\",roc_auc_score)\n",
    "    #功能：保存训练完的网络的各层参数（即weights和bias)\n",
    "    #path='dataset/xiaoguan/RF/RF_for_train/train_class_9/model/gene_4000_NFM.pkl'\n",
    "        if epoch_id %100==0:\n",
    "            num=num+1\n",
    "            path=os.path.join(model_path,'NMF'+str(num)+'.pkl')\n",
    "            torch.save(model.state_dict(),path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "43f0e38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from basemodel import BaseModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class NFM(BaseModel):\n",
    "    def __init__(self, config, dense_features_cols=[]):#=[]为新增\n",
    "    #def __init__(self, config, dense_features_cols, sparse_features_cols):\n",
    "        super(NFM, self).__init__(config)\n",
    "        # 稠密和稀疏特征的数量\n",
    "        #self.num_dense_feature = dense_features_cols.__len__()\n",
    "        self.num_dense_feature = 0#修改\n",
    "        self.num_sparse_feature = config['num_sparse_features_cols']\n",
    "        #self.num_sparse_feature = 0##修改\n",
    "        self.__config=config\n",
    "        \n",
    "        \n",
    "        self.BN_num=nn.BatchNorm1d(self.num_sparse_feature)\n",
    "        self.linear1=nn.Linear(config['num_sparse_features_cols'],config['linear_hidden1'])\n",
    "        self.bn1=nn.BatchNorm1d(config['linear_hidden1'])\n",
    "        self.drop1=nn.Dropout(0.5)\n",
    "        self.relu1=nn.ReLU()\n",
    "        \n",
    "        self.linear2=nn.Linear(config['linear_hidden1'],config['dnn_hidden_units'][0])\n",
    "        self.bn2=nn.BatchNorm1d(config['dnn_hidden_units'][0])\n",
    "        self.drop2=nn.Dropout(0.5)\n",
    "        self.relu2=nn.ReLU()\n",
    "        \n",
    "        self.linear3=nn.Linear(config['dnn_hidden_units'][0]+config['embed_dim'],config['dnn_hidden_units'][1])\n",
    "        self.bn3=nn.BatchNorm1d(config['dnn_hidden_units'][1])\n",
    "        #self.drop3=nn.Dropout(0.3)\n",
    "        self.relu3=nn.ReLU()\n",
    "        \n",
    "        #self.embedding_layers=nn.Embedding(config['embed_input_dim'],config['embed_dim'])\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "        self.BN_bi = nn.BatchNorm1d(config['embed_dim'])\n",
    "        \n",
    "        \n",
    "        #self.dnn_softmax=nn.Softmax(dim=1) # 按列SoftMax,列和为1  #注意nn.softmax的定义和调用\n",
    "        #self.dnn_softmax_=F.softmax(dim=1)\n",
    "        #self.dnn_hidden_units=config['dnn_hidden_units']\n",
    "    def forward(self, x,bi_x):\n",
    "        # 先区分出稀疏特征和稠密特征，这里是按照列来划分的，即所有的行都要进行筛选\n",
    "        #bi_x=bi_x.long()\n",
    "        #print(x.dtype)\n",
    "        # 求出线性部分\n",
    "        #x=F.relu(self.drop(self.BN_linear1(self.linear_model1(self.BN_num(x))))\n",
    "        #x=F.relu(self.drop(self.BN_linear1(self.linear_model1(x))))\n",
    "        x=self.BN_num(x)\n",
    "        #x=self.linear_model1(x)\n",
    "        \n",
    "        \n",
    "        #print(\"linear_output:\",linear_output)\n",
    "        #linear_output=linear_output.view(-1,self.__config['linear_hidden1'])\n",
    "        #linear_output=self.drop(linear_output)\n",
    "        #linear_output=self.BN_linear(linear_output)\n",
    "        # 求出稀疏特征的embedding向量\n",
    "        \n",
    "        bi_x=self.BN_bi(bi_x)\n",
    "        #print('bi_out.shape:',bi_out.shape)\n",
    "        #print(x.dtype)\n",
    "        #print(bi_out.dtype)\n",
    "        \n",
    "        input=x,bi_x#不能是list，必须是tensor\n",
    "        x1,x2=input\n",
    "        y1=self.relu1(self.drop1(self.bn1(self.linear1(x1))))\n",
    "        y2=self.relu2(self.drop2(self.bn2(self.linear2(y1))))\n",
    "        \n",
    "        #x3=torch.cat((y2,x2),dim=1)\n",
    "        x3=torch.cat((y2,x2),dim=1)\n",
    "        y3=self.relu3(self.bn3(self.linear3(x3)))\n",
    "        y=F.softmax(y3,dim=1)\n",
    "        \n",
    "        y_pred=y\n",
    "        #y_pred=self.dnn_softmax(dnn_output)#增加\n",
    "        #y_pred=F.softmax(dnn_output,dim=0)\n",
    "        # Final\n",
    "        #output = linear_output + y_pred#修改\n",
    "        #y_pred = self.dnn_softmax(output,dim=0)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b27272ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[340, 771, 302,  ..., 472, 355, 509],\n",
      "        [387, 728, 324,  ..., 437, 364, 477],\n",
      "        [396, 771, 321,  ..., 459, 378, 478],\n",
      "        ...,\n",
      "        [516, 598, 215,  ..., 343, 267, 394],\n",
      "        [525, 587, 239,  ..., 340, 259, 411],\n",
      "        [488, 604, 201,  ..., 363, 260, 421]])\n",
      "tensor([[ 0.0000e+00, -6.4823e+03, -6.8643e+03,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  2.7439e+05],\n",
      "        [ 0.0000e+00,  0.0000e+00,  5.5325e+04,  ..., -9.1842e+02,\n",
      "          0.0000e+00,  6.1174e+05],\n",
      "        [ 1.4414e+06,  1.1677e+04,  1.1519e+04,  ...,  2.1507e+03,\n",
      "         -7.6165e+03,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 6.5735e+05,  7.4699e+03,  3.7651e+04,  ...,  0.0000e+00,\n",
      "          1.9974e+05,  0.0000e+00],\n",
      "        [ 0.0000e+00,  3.5724e+03,  1.0303e+05,  ...,  0.0000e+00,\n",
      "          9.3441e+04,  0.0000e+00],\n",
      "        [ 0.0000e+00,  7.9623e+04,  8.6716e+04,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  7.1674e+04]], grad_fn=<ViewBackward0>)\n",
      "tensor([[0.8222, 0.0222, 0.0222,  ..., 0.0222, 0.0222, 0.0222],\n",
      "        [0.8222, 0.0222, 0.0222,  ..., 0.0222, 0.0222, 0.0222],\n",
      "        [0.8222, 0.0222, 0.0222,  ..., 0.0222, 0.0222, 0.0222],\n",
      "        ...,\n",
      "        [0.0222, 0.0222, 0.0222,  ..., 0.0222, 0.0222, 0.8222],\n",
      "        [0.0222, 0.0222, 0.0222,  ..., 0.0222, 0.0222, 0.8222],\n",
      "        [0.0222, 0.0222, 0.0222,  ..., 0.0222, 0.0222, 0.8222]])\n",
      "tensor([[290, 729, 278,  ..., 393, 310, 474],\n",
      "        [310, 713, 246,  ..., 380, 302, 446],\n",
      "        [297, 754, 238,  ..., 414, 315, 457],\n",
      "        ...,\n",
      "        [434, 826, 209,  ..., 431, 321, 478],\n",
      "        [215, 788, 232,  ..., 386, 320, 478],\n",
      "        [233, 840, 220,  ..., 477, 325, 469]])\n",
      "tensor([[-5.4452e+03,  2.0974e+05, -6.8709e+03,  ...,  6.8739e+03,\n",
      "         -9.0628e+02,  0.0000e+00],\n",
      "        [-7.7696e+03,  2.3737e+05, -5.5425e+03,  ...,  0.0000e+00,\n",
      "          1.0137e+04,  2.2812e+05],\n",
      "        [-0.0000e+00,  1.0414e+05, -6.0689e+03,  ...,  4.7896e+04,\n",
      "         -8.3240e+03,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 2.9255e+04,  0.0000e+00, -6.8604e+03,  ...,  0.0000e+00,\n",
      "          2.9450e+02,  4.7687e+05],\n",
      "        [ 1.0559e+04,  1.8479e+05, -4.6497e+03,  ...,  1.8377e+04,\n",
      "         -0.0000e+00,  0.0000e+00],\n",
      "        [-7.7635e+03,  1.1747e+05, -6.9955e+03,  ...,  0.0000e+00,\n",
      "         -8.5020e+03,  1.9958e+05]], grad_fn=<ViewBackward0>)\n",
      "tensor([[0.8222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.8222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.8222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.8222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.8222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.8222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222]])\n",
      "NFM: NFM(\n",
      "  (BN_num): BatchNorm1d(10477, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear1): Linear(in_features=10477, out_features=1000, bias=True)\n",
      "  (bn1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop1): Dropout(p=0.5, inplace=False)\n",
      "  (relu1): ReLU()\n",
      "  (linear2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop2): Dropout(p=0.5, inplace=False)\n",
      "  (relu2): ReLU()\n",
      "  (linear3): Linear(in_features=200, out_features=9, bias=True)\n",
      "  (bn3): BatchNorm1d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU()\n",
      "  (BN_bi): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "tensor([[427., 834., 337.,  ..., 471., 358., 489.],\n",
      "        [548., 525., 227.,  ..., 346., 269., 417.],\n",
      "        [385., 716., 301.,  ..., 417., 354., 513.],\n",
      "        ...,\n",
      "        [457., 591., 232.,  ..., 351., 260., 388.],\n",
      "        [512., 608., 231.,  ..., 350., 259., 405.],\n",
      "        [500., 605., 235.,  ..., 366., 262., 402.]], device='cuda:0')\n",
      "tensor([[762469,   -930,  44248,  ...,  -4170,      0,      0],\n",
      "        [877067,  -4386,  62672,  ..., 142638, 105007,  42006],\n",
      "        [747358,      0,      0,  ...,  -4973,  -7496, 258881],\n",
      "        ...,\n",
      "        [853877,      0,      0,  ...,  21895,  59079,  84238],\n",
      "        [616058,  -6980,  -2284,  ...,      0,      0,  70366],\n",
      "        [     0,  -6267,   7968,  ...,  83995,  94002,      0]],\n",
      "       device='cuda:0')\n",
      "tensor([[0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222],\n",
      "        [0.8222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.8222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222],\n",
      "        [0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.8222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222],\n",
      "        [0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.0222, 0.8222, 0.0222, 0.0222]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"batch_norm_stats_cuda\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-f746b5073306>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NFM:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnfm_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'dataset/qiuguan/model/NFM_bi_encode_1000/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-270e5332f4cd>\u001b[0m in \u001b[0;36mtrain_data\u001b[0;34m(model, train_loader, test_loader, batch_size, model_path)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbi_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;31m#print(\"y_predict:\",y_predict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m#loss = loss_func(y_predict.view(-1), labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-3e57110e14a5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, bi_x)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# 求出稀疏特征的embedding向量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mbi_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBN_bi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbi_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;31m#print('bi_out.shape:',bi_out.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m#print(x.dtype)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mbn_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mexponential_average_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         )\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2282\u001b[0m     return torch.batch_norm(\n\u001b[0;32m-> 2283\u001b[0;31m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2284\u001b[0m     )\n\u001b[1;32m   2285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"batch_norm_stats_cuda\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "train_dataset,train_loader=prepare_dataset(nfm_config['train_data'],nfm_config['train_label'],nfm_config['batch_size'],nfm_config['n_class'])\n",
    "test_dataset,test_loader=prepare_dataset(nfm_config['test_data'],nfm_config['test_label'],nfm_config['batch_size'],nfm_config['n_class'])\n",
    "#from MLP import MLP\n",
    "#from nfm_network_adjust import NFM\n",
    "#model=MLP(4224,1000,100,9)\n",
    "model=NFM(nfm_config)\n",
    "model.cuda()\n",
    "print(\"NFM:\",model)\n",
    "train_data(model,train_loader,test_loader,nfm_config['batch_size'],'dataset/qiuguan/model/NFM_bi_encode_1000/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77cc84e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
