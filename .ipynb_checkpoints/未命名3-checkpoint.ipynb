{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6730ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "\n",
    "import config\n",
    "\n",
    "config.train_libfm=\"data/frappe/train_data.csv\"\n",
    "config.valid_libfm=\"data/frappe/validate_data.csv\"\n",
    "config.test_libfm=\"data/frappe/test_data.csv\"\n",
    "\n",
    "\n",
    "\n",
    "config.train_label=\"data/frappe/train_label.csv\"\n",
    "config.valid_label=\"data/frappe/validate_label.csv\"\n",
    "config.test_label=\"data/frappe/test_label.csv\"\n",
    "\n",
    "csv_data=\"data/frappe/c_df_v_fff2000.csv\"\n",
    "label_file=\"data/frappe/label.csv\"\n",
    "#config.train_libfm=\"data/frappe/frappe.train.libfm\"\n",
    "#config.valid_libfm=\"data/frappe/frappe.validation.libfm\"\n",
    "#config.test_libfm=\"data/frappe/frappe.test.libfm\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f463ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "import config\n",
    "\n",
    "\n",
    "def read_features(file, features):\n",
    "    \"\"\" Read features from the given file. \"\"\"\n",
    "    file=\"data/frappe/c_df_v_fff2000.csv\"\n",
    "    num = len(features)\n",
    "    fd=pd.read_csv(file,sep=',')\n",
    "    nrow=fd.shape[0]\n",
    "    ncol=fd.shape[1]\n",
    "    n_fd=np.array(fd)\n",
    "    #print(n_fd)\n",
    "    n_fd=n_fd[:,2:]\n",
    "    print('n_fd.shape:')\n",
    "    print(n_fd.shape)\n",
    "    #print(n_fd)\n",
    "    #features={}\n",
    "\n",
    "    for i, row_list in enumerate(n_fd):\n",
    "        for j, col_value in enumerate(row_list):\n",
    "            #print(col_value)\n",
    "            if col_value not in features:\n",
    "                features[col_value]=num\n",
    "                num=num+1\n",
    "\n",
    "    #print(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d3d438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_features():\n",
    "    \"\"\" Get the number of existing features in all the three files. \"\"\"\n",
    "    features = {}\n",
    "    #features = read_features(csv_data, features)\n",
    "    \n",
    "    features = read_features(config.train_libfm, features)\n",
    "    features = read_features(config.valid_libfm, features)\n",
    "    features = read_features(config.test_libfm, features)\n",
    "    \n",
    "    \n",
    "    print(\"number of features: {}\".format(len(features)))\n",
    "    return features, len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce8269bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, classes, label_smoothing=0.2):\n",
    "    n = len(labels)\n",
    "    eoff = label_smoothing / classes\n",
    "    output = np.ones((n, classes), dtype=np.float32) * eoff\n",
    "    for row, label in enumerate(labels):\n",
    "        output[row, label] = 1 - label_smoothing + eoff\n",
    "        print(\"row:\",row,\"label:\",label)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc61f424",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMData(data.Dataset):\n",
    "    \"\"\" Construct the FM pytorch dataset. \"\"\"\n",
    "    def __init__(self, file,label_file, feature_map,n_class=16):\n",
    "        super(FMData, self).__init__()\n",
    "        self.label = []\n",
    "        self.features = []\n",
    "        self.feature_values = []\n",
    "        \n",
    "        features=[]\n",
    "        #feature_map.keys()\n",
    "        #self.features=np.array(feature_map)\n",
    "        #feature_map\n",
    "        #file=\"data/frappe/c_df_v_fff2000.csv\"\n",
    "        #num = len(features)\n",
    "        fd=pd.read_csv(file,sep=',')\n",
    "        #nrow=fd.shape[0]\n",
    "        #ncol=fd.shape[1]\n",
    "        n_fd=np.array(fd)\n",
    "        #print(n_fd)\n",
    "        n_fd=n_fd[:,2:]\n",
    "        for i, item in enumerate(n_fd):\n",
    "            u=[feature_map[x] for x in item]\n",
    "            features.append(u)\n",
    "        \n",
    "        \n",
    "        self.features=np.array(features)\n",
    "        #self.features=features.tolist()\n",
    "        \n",
    "        \n",
    "        nrow,ncol=n_fd.shape\n",
    "        #ncol=10150\n",
    "        feature_v=[]\n",
    "        \"\"\"\n",
    "            feature_v=[1 for i in range(ncol)]\n",
    "            #print(feature_v)\n",
    "            #feature_values=[feature_v for j in range(nrow)]\n",
    "\n",
    "            for item in range(nrow):\n",
    "            feature_values.append(feature_v)\n",
    "        \"\"\"\n",
    "\n",
    "        feature_v=[[1 for i in range(ncol)] for i in range(nrow)]\n",
    "        self.feature_values=np.array(feature_v)\n",
    "        #print(feature_value)\n",
    "        #self.feature_values=feature_value.tolist()\n",
    "        #feature_map,lenth=map_features()\n",
    "        #raw = [item for item in  enumerate(n_fd)]\n",
    "        #print(raw)\n",
    "        #raw=raw.tolist()\n",
    "        \n",
    "        #label_file=[]\n",
    "        label_fd=pd.read_csv(label_file,sep=',')\n",
    "        #print(features)\n",
    "        #print(label_fd)\n",
    "        label=np.array(label_fd)\n",
    "        label=label[:,1:]\n",
    "        label=one_hot(label,n_class)\n",
    "        self.label=label\n",
    "        print(\"label:\",label)\n",
    "        #print(label)\n",
    "        # convert labels\n",
    "        \"\"\"if config.loss_type == 'square_loss':\n",
    "            self.label.append(np.float32(items[0]))\n",
    "        else: # log_loss\n",
    "            label = 1 if float(items[0]) > 0 else 0\n",
    "            self.label.append(label)\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        assert all(len(item) == len(self.features[0]\n",
    "            ) for item in self.features), 'features are of different length'\n",
    "        \"\"\"\n",
    "        print(len(self.features))\n",
    "        print(len(self.feature_values))\n",
    "        print(len(self.label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.label[idx]\n",
    "        features = self.features[idx]\n",
    "        feature_values = self.feature_values[idx]\n",
    "        return features, feature_values, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20aae3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_fd.shape:\n",
      "(944, 10150)\n",
      "n_fd.shape:\n",
      "(944, 10150)\n",
      "n_fd.shape:\n",
      "(944, 10150)\n",
      "number of features: 100\n",
      "row: 0 label: [0]\n",
      "row: 1 label: [0]\n",
      "row: 2 label: [0]\n",
      "row: 3 label: [0]\n",
      "row: 4 label: [0]\n",
      "row: 5 label: [0]\n",
      "row: 6 label: [0]\n",
      "row: 7 label: [0]\n",
      "row: 8 label: [0]\n",
      "row: 9 label: [0]\n",
      "row: 10 label: [0]\n",
      "row: 11 label: [0]\n",
      "row: 12 label: [0]\n",
      "row: 13 label: [0]\n",
      "row: 14 label: [0]\n",
      "row: 15 label: [0]\n",
      "row: 16 label: [0]\n",
      "row: 17 label: [0]\n",
      "row: 18 label: [0]\n",
      "row: 19 label: [0]\n",
      "row: 20 label: [0]\n",
      "row: 21 label: [0]\n",
      "row: 22 label: [0]\n",
      "row: 23 label: [0]\n",
      "row: 24 label: [0]\n",
      "row: 25 label: [0]\n",
      "row: 26 label: [0]\n",
      "row: 27 label: [0]\n",
      "row: 28 label: [0]\n",
      "row: 29 label: [0]\n",
      "row: 30 label: [0]\n",
      "row: 31 label: [0]\n",
      "row: 32 label: [0]\n",
      "row: 33 label: [0]\n",
      "row: 34 label: [0]\n",
      "row: 35 label: [0]\n",
      "row: 36 label: [0]\n",
      "row: 37 label: [0]\n",
      "row: 38 label: [0]\n",
      "row: 39 label: [0]\n",
      "row: 40 label: [0]\n",
      "row: 41 label: [0]\n",
      "row: 42 label: [0]\n",
      "row: 43 label: [0]\n",
      "row: 44 label: [0]\n",
      "row: 45 label: [0]\n",
      "row: 46 label: [0]\n",
      "row: 47 label: [0]\n",
      "row: 48 label: [0]\n",
      "row: 49 label: [0]\n",
      "row: 50 label: [0]\n",
      "row: 51 label: [0]\n",
      "row: 52 label: [0]\n",
      "row: 53 label: [0]\n",
      "row: 54 label: [0]\n",
      "row: 55 label: [0]\n",
      "row: 56 label: [0]\n",
      "row: 57 label: [1]\n",
      "row: 58 label: [1]\n",
      "row: 59 label: [1]\n",
      "row: 60 label: [1]\n",
      "row: 61 label: [2]\n",
      "row: 62 label: [2]\n",
      "row: 63 label: [2]\n",
      "row: 64 label: [2]\n",
      "row: 65 label: [2]\n",
      "row: 66 label: [2]\n",
      "row: 67 label: [2]\n",
      "row: 68 label: [2]\n",
      "row: 69 label: [2]\n",
      "row: 70 label: [2]\n",
      "row: 71 label: [2]\n",
      "row: 72 label: [2]\n",
      "row: 73 label: [2]\n",
      "row: 74 label: [2]\n",
      "row: 75 label: [2]\n",
      "row: 76 label: [2]\n",
      "row: 77 label: [2]\n",
      "row: 78 label: [2]\n",
      "row: 79 label: [2]\n",
      "row: 80 label: [2]\n",
      "row: 81 label: [2]\n",
      "row: 82 label: [2]\n",
      "row: 83 label: [2]\n",
      "row: 84 label: [2]\n",
      "row: 85 label: [2]\n",
      "row: 86 label: [2]\n",
      "row: 87 label: [2]\n",
      "row: 88 label: [2]\n",
      "row: 89 label: [2]\n",
      "row: 90 label: [2]\n",
      "row: 91 label: [2]\n",
      "row: 92 label: [2]\n",
      "row: 93 label: [2]\n",
      "row: 94 label: [2]\n",
      "row: 95 label: [2]\n",
      "row: 96 label: [3]\n",
      "row: 97 label: [3]\n",
      "row: 98 label: [3]\n",
      "row: 99 label: [3]\n",
      "row: 100 label: [3]\n",
      "row: 101 label: [3]\n",
      "row: 102 label: [3]\n",
      "row: 103 label: [3]\n",
      "row: 104 label: [3]\n",
      "row: 105 label: [3]\n",
      "row: 106 label: [3]\n",
      "row: 107 label: [3]\n",
      "row: 108 label: [3]\n",
      "row: 109 label: [3]\n",
      "row: 110 label: [3]\n",
      "row: 111 label: [3]\n",
      "row: 112 label: [3]\n",
      "row: 113 label: [3]\n",
      "row: 114 label: [3]\n",
      "row: 115 label: [3]\n",
      "row: 116 label: [3]\n",
      "row: 117 label: [3]\n",
      "row: 118 label: [3]\n",
      "row: 119 label: [3]\n",
      "row: 120 label: [3]\n",
      "row: 121 label: [3]\n",
      "row: 122 label: [3]\n",
      "row: 123 label: [3]\n",
      "row: 124 label: [3]\n",
      "row: 125 label: [3]\n",
      "row: 126 label: [3]\n",
      "row: 127 label: [3]\n",
      "row: 128 label: [3]\n",
      "row: 129 label: [3]\n",
      "row: 130 label: [3]\n",
      "row: 131 label: [3]\n",
      "row: 132 label: [3]\n",
      "row: 133 label: [3]\n",
      "row: 134 label: [3]\n",
      "row: 135 label: [3]\n",
      "row: 136 label: [3]\n",
      "row: 137 label: [3]\n",
      "row: 138 label: [3]\n",
      "row: 139 label: [3]\n",
      "row: 140 label: [3]\n",
      "row: 141 label: [3]\n",
      "row: 142 label: [3]\n",
      "row: 143 label: [3]\n",
      "row: 144 label: [3]\n",
      "row: 145 label: [3]\n",
      "row: 146 label: [3]\n",
      "row: 147 label: [3]\n",
      "row: 148 label: [3]\n",
      "row: 149 label: [3]\n",
      "row: 150 label: [3]\n",
      "row: 151 label: [3]\n",
      "row: 152 label: [3]\n",
      "row: 153 label: [3]\n",
      "row: 154 label: [3]\n",
      "row: 155 label: [3]\n",
      "row: 156 label: [3]\n",
      "row: 157 label: [3]\n",
      "row: 158 label: [3]\n",
      "row: 159 label: [3]\n",
      "row: 160 label: [3]\n",
      "row: 161 label: [3]\n",
      "row: 162 label: [3]\n",
      "row: 163 label: [3]\n",
      "row: 164 label: [3]\n",
      "row: 165 label: [3]\n",
      "row: 166 label: [3]\n",
      "row: 167 label: [3]\n",
      "row: 168 label: [3]\n",
      "row: 169 label: [3]\n",
      "row: 170 label: [3]\n",
      "row: 171 label: [3]\n",
      "row: 172 label: [3]\n",
      "row: 173 label: [3]\n",
      "row: 174 label: [3]\n",
      "row: 175 label: [3]\n",
      "row: 176 label: [3]\n",
      "row: 177 label: [3]\n",
      "row: 178 label: [3]\n",
      "row: 179 label: [3]\n",
      "row: 180 label: [3]\n",
      "row: 181 label: [3]\n",
      "row: 182 label: [3]\n",
      "row: 183 label: [3]\n",
      "row: 184 label: [3]\n",
      "row: 185 label: [3]\n",
      "row: 186 label: [3]\n",
      "row: 187 label: [3]\n",
      "row: 188 label: [3]\n",
      "row: 189 label: [3]\n",
      "row: 190 label: [3]\n",
      "row: 191 label: [3]\n",
      "row: 192 label: [3]\n",
      "row: 193 label: [3]\n",
      "row: 194 label: [3]\n",
      "row: 195 label: [3]\n",
      "row: 196 label: [3]\n",
      "row: 197 label: [3]\n",
      "row: 198 label: [3]\n",
      "row: 199 label: [3]\n",
      "row: 200 label: [3]\n",
      "row: 201 label: [3]\n",
      "row: 202 label: [3]\n",
      "row: 203 label: [3]\n",
      "row: 204 label: [3]\n",
      "row: 205 label: [3]\n",
      "row: 206 label: [3]\n",
      "row: 207 label: [3]\n",
      "row: 208 label: [3]\n",
      "row: 209 label: [3]\n",
      "row: 210 label: [3]\n",
      "row: 211 label: [3]\n",
      "row: 212 label: [3]\n",
      "row: 213 label: [3]\n",
      "row: 214 label: [3]\n",
      "row: 215 label: [3]\n",
      "row: 216 label: [3]\n",
      "row: 217 label: [3]\n",
      "row: 218 label: [3]\n",
      "row: 219 label: [3]\n",
      "row: 220 label: [3]\n",
      "row: 221 label: [3]\n",
      "row: 222 label: [3]\n",
      "row: 223 label: [3]\n",
      "row: 224 label: [3]\n",
      "row: 225 label: [3]\n",
      "row: 226 label: [3]\n",
      "row: 227 label: [3]\n",
      "row: 228 label: [3]\n",
      "row: 229 label: [3]\n",
      "row: 230 label: [3]\n",
      "row: 231 label: [3]\n",
      "row: 232 label: [3]\n",
      "row: 233 label: [3]\n",
      "row: 234 label: [3]\n",
      "row: 235 label: [3]\n",
      "row: 236 label: [3]\n",
      "row: 237 label: [3]\n",
      "row: 238 label: [3]\n",
      "row: 239 label: [3]\n",
      "row: 240 label: [3]\n",
      "row: 241 label: [3]\n",
      "row: 242 label: [3]\n",
      "row: 243 label: [3]\n",
      "row: 244 label: [3]\n",
      "row: 245 label: [3]\n",
      "row: 246 label: [3]\n",
      "row: 247 label: [3]\n",
      "row: 248 label: [3]\n",
      "row: 249 label: [3]\n",
      "row: 250 label: [3]\n",
      "row: 251 label: [3]\n",
      "row: 252 label: [3]\n",
      "row: 253 label: [3]\n",
      "row: 254 label: [3]\n",
      "row: 255 label: [3]\n",
      "row: 256 label: [3]\n",
      "row: 257 label: [3]\n",
      "row: 258 label: [3]\n",
      "row: 259 label: [3]\n",
      "row: 260 label: [3]\n",
      "row: 261 label: [3]\n",
      "row: 262 label: [3]\n",
      "row: 263 label: [3]\n",
      "row: 264 label: [3]\n",
      "row: 265 label: [3]\n",
      "row: 266 label: [3]\n",
      "row: 267 label: [3]\n",
      "row: 268 label: [3]\n",
      "row: 269 label: [3]\n",
      "row: 270 label: [3]\n",
      "row: 271 label: [3]\n",
      "row: 272 label: [3]\n",
      "row: 273 label: [3]\n",
      "row: 274 label: [4]\n",
      "row: 275 label: [4]\n",
      "row: 276 label: [4]\n",
      "row: 277 label: [4]\n",
      "row: 278 label: [4]\n",
      "row: 279 label: [4]\n",
      "row: 280 label: [4]\n",
      "row: 281 label: [4]\n",
      "row: 282 label: [5]\n",
      "row: 283 label: [5]\n",
      "row: 284 label: [5]\n",
      "row: 285 label: [5]\n",
      "row: 286 label: [5]\n",
      "row: 287 label: [5]\n",
      "row: 288 label: [5]\n",
      "row: 289 label: [5]\n",
      "row: 290 label: [5]\n",
      "row: 291 label: [5]\n",
      "row: 292 label: [5]\n",
      "row: 293 label: [5]\n",
      "row: 294 label: [5]\n",
      "row: 295 label: [5]\n",
      "row: 296 label: [5]\n",
      "row: 297 label: [5]\n",
      "row: 298 label: [5]\n",
      "row: 299 label: [5]\n",
      "row: 300 label: [5]\n",
      "row: 301 label: [5]\n",
      "row: 302 label: [5]\n",
      "row: 303 label: [5]\n",
      "row: 304 label: [5]\n",
      "row: 305 label: [5]\n",
      "row: 306 label: [5]\n",
      "row: 307 label: [5]\n",
      "row: 308 label: [5]\n",
      "row: 309 label: [5]\n",
      "row: 310 label: [5]\n",
      "row: 311 label: [5]\n",
      "row: 312 label: [5]\n",
      "row: 313 label: [5]\n",
      "row: 314 label: [5]\n",
      "row: 315 label: [5]\n",
      "row: 316 label: [5]\n",
      "row: 317 label: [5]\n",
      "row: 318 label: [5]\n",
      "row: 319 label: [5]\n",
      "row: 320 label: [5]\n",
      "row: 321 label: [5]\n",
      "row: 322 label: [5]\n",
      "row: 323 label: [6]\n",
      "row: 324 label: [6]\n",
      "row: 325 label: [6]\n",
      "row: 326 label: [6]\n",
      "row: 327 label: [6]\n",
      "row: 328 label: [6]\n",
      "row: 329 label: [6]\n",
      "row: 330 label: [6]\n",
      "row: 331 label: [6]\n",
      "row: 332 label: [6]\n",
      "row: 333 label: [6]\n",
      "row: 334 label: [6]\n",
      "row: 335 label: [6]\n",
      "row: 336 label: [6]\n",
      "row: 337 label: [6]\n",
      "row: 338 label: [6]\n",
      "row: 339 label: [6]\n",
      "row: 340 label: [6]\n",
      "row: 341 label: [6]\n",
      "row: 342 label: [6]\n",
      "row: 343 label: [6]\n",
      "row: 344 label: [6]\n",
      "row: 345 label: [6]\n",
      "row: 346 label: [6]\n",
      "row: 347 label: [6]\n",
      "row: 348 label: [6]\n",
      "row: 349 label: [6]\n",
      "row: 350 label: [6]\n",
      "row: 351 label: [6]\n",
      "row: 352 label: [6]\n",
      "row: 353 label: [6]\n",
      "row: 354 label: [6]\n",
      "row: 355 label: [6]\n",
      "row: 356 label: [6]\n",
      "row: 357 label: [6]\n",
      "row: 358 label: [6]\n",
      "row: 359 label: [6]\n",
      "row: 360 label: [6]\n",
      "row: 361 label: [6]\n",
      "row: 362 label: [6]\n",
      "row: 363 label: [6]\n",
      "row: 364 label: [6]\n",
      "row: 365 label: [6]\n",
      "row: 366 label: [6]\n",
      "row: 367 label: [6]\n",
      "row: 368 label: [6]\n",
      "row: 369 label: [6]\n",
      "row: 370 label: [6]\n",
      "row: 371 label: [6]\n",
      "row: 372 label: [6]\n",
      "row: 373 label: [6]\n",
      "row: 374 label: [6]\n",
      "row: 375 label: [6]\n",
      "row: 376 label: [6]\n",
      "row: 377 label: [6]\n",
      "row: 378 label: [6]\n",
      "row: 379 label: [6]\n",
      "row: 380 label: [6]\n",
      "row: 381 label: [6]\n",
      "row: 382 label: [6]\n",
      "row: 383 label: [6]\n",
      "row: 384 label: [6]\n",
      "row: 385 label: [6]\n",
      "row: 386 label: [6]\n",
      "row: 387 label: [6]\n",
      "row: 388 label: [6]\n",
      "row: 389 label: [6]\n",
      "row: 390 label: [6]\n",
      "row: 391 label: [6]\n",
      "row: 392 label: [6]\n",
      "row: 393 label: [6]\n",
      "row: 394 label: [6]\n",
      "row: 395 label: [6]\n",
      "row: 396 label: [6]\n",
      "row: 397 label: [6]\n",
      "row: 398 label: [6]\n",
      "row: 399 label: [7]\n",
      "row: 400 label: [7]\n",
      "row: 401 label: [7]\n",
      "row: 402 label: [7]\n",
      "row: 403 label: [7]\n",
      "row: 404 label: [7]\n",
      "row: 405 label: [7]\n",
      "row: 406 label: [7]\n",
      "row: 407 label: [7]\n",
      "row: 408 label: [7]\n",
      "row: 409 label: [7]\n",
      "row: 410 label: [7]\n",
      "row: 411 label: [7]\n",
      "row: 412 label: [7]\n",
      "row: 413 label: [7]\n",
      "row: 414 label: [7]\n",
      "row: 415 label: [7]\n",
      "row: 416 label: [7]\n",
      "row: 417 label: [7]\n",
      "row: 418 label: [7]\n",
      "row: 419 label: [7]\n",
      "row: 420 label: [8]\n",
      "row: 421 label: [8]\n",
      "row: 422 label: [8]\n",
      "row: 423 label: [8]\n",
      "row: 424 label: [8]\n",
      "row: 425 label: [8]\n",
      "row: 426 label: [8]\n",
      "row: 427 label: [8]\n",
      "row: 428 label: [8]\n",
      "row: 429 label: [8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row: 430 label: [8]\n",
      "row: 431 label: [8]\n",
      "row: 432 label: [8]\n",
      "row: 433 label: [8]\n",
      "row: 434 label: [8]\n",
      "row: 435 label: [8]\n",
      "row: 436 label: [8]\n",
      "row: 437 label: [8]\n",
      "row: 438 label: [8]\n",
      "row: 439 label: [8]\n",
      "row: 440 label: [8]\n",
      "row: 441 label: [8]\n",
      "row: 442 label: [8]\n",
      "row: 443 label: [8]\n",
      "row: 444 label: [8]\n",
      "row: 445 label: [8]\n",
      "row: 446 label: [8]\n",
      "row: 447 label: [8]\n",
      "row: 448 label: [8]\n",
      "row: 449 label: [8]\n",
      "row: 450 label: [8]\n",
      "row: 451 label: [8]\n",
      "row: 452 label: [8]\n",
      "row: 453 label: [8]\n",
      "row: 454 label: [8]\n",
      "row: 455 label: [8]\n",
      "row: 456 label: [8]\n",
      "row: 457 label: [8]\n",
      "row: 458 label: [8]\n",
      "row: 459 label: [8]\n",
      "row: 460 label: [8]\n",
      "row: 461 label: [8]\n",
      "row: 462 label: [8]\n",
      "row: 463 label: [8]\n",
      "row: 464 label: [8]\n",
      "row: 465 label: [8]\n",
      "row: 466 label: [8]\n",
      "row: 467 label: [8]\n",
      "row: 468 label: [8]\n",
      "row: 469 label: [8]\n",
      "row: 470 label: [8]\n",
      "row: 471 label: [8]\n",
      "row: 472 label: [8]\n",
      "row: 473 label: [8]\n",
      "row: 474 label: [8]\n",
      "row: 475 label: [8]\n",
      "row: 476 label: [8]\n",
      "row: 477 label: [8]\n",
      "row: 478 label: [8]\n",
      "row: 479 label: [8]\n",
      "row: 480 label: [8]\n",
      "row: 481 label: [8]\n",
      "row: 482 label: [8]\n",
      "row: 483 label: [8]\n",
      "row: 484 label: [8]\n",
      "row: 485 label: [8]\n",
      "row: 486 label: [8]\n",
      "row: 487 label: [8]\n",
      "row: 488 label: [8]\n",
      "row: 489 label: [8]\n",
      "row: 490 label: [8]\n",
      "row: 491 label: [8]\n",
      "row: 492 label: [8]\n",
      "row: 493 label: [8]\n",
      "row: 494 label: [8]\n",
      "row: 495 label: [8]\n",
      "row: 496 label: [8]\n",
      "row: 497 label: [8]\n",
      "row: 498 label: [8]\n",
      "row: 499 label: [8]\n",
      "row: 500 label: [8]\n",
      "row: 501 label: [8]\n",
      "row: 502 label: [8]\n",
      "row: 503 label: [8]\n",
      "row: 504 label: [8]\n",
      "row: 505 label: [8]\n",
      "row: 506 label: [8]\n",
      "row: 507 label: [8]\n",
      "row: 508 label: [8]\n",
      "row: 509 label: [8]\n",
      "row: 510 label: [8]\n",
      "row: 511 label: [8]\n",
      "row: 512 label: [8]\n",
      "row: 513 label: [8]\n",
      "row: 514 label: [8]\n",
      "row: 515 label: [8]\n",
      "row: 516 label: [8]\n",
      "row: 517 label: [8]\n",
      "row: 518 label: [8]\n",
      "row: 519 label: [8]\n",
      "row: 520 label: [8]\n",
      "row: 521 label: [8]\n",
      "row: 522 label: [8]\n",
      "row: 523 label: [8]\n",
      "row: 524 label: [8]\n",
      "row: 525 label: [8]\n",
      "row: 526 label: [8]\n",
      "row: 527 label: [8]\n",
      "row: 528 label: [8]\n",
      "row: 529 label: [8]\n",
      "row: 530 label: [8]\n",
      "row: 531 label: [8]\n",
      "row: 532 label: [8]\n",
      "row: 533 label: [8]\n",
      "row: 534 label: [8]\n",
      "row: 535 label: [8]\n",
      "row: 536 label: [9]\n",
      "row: 537 label: [9]\n",
      "row: 538 label: [9]\n",
      "row: 539 label: [9]\n",
      "row: 540 label: [9]\n",
      "row: 541 label: [9]\n",
      "row: 542 label: [9]\n",
      "row: 543 label: [9]\n",
      "row: 544 label: [9]\n",
      "row: 545 label: [9]\n",
      "row: 546 label: [9]\n",
      "row: 547 label: [9]\n",
      "row: 548 label: [9]\n",
      "row: 549 label: [9]\n",
      "row: 550 label: [9]\n",
      "row: 551 label: [9]\n",
      "row: 552 label: [9]\n",
      "row: 553 label: [9]\n",
      "row: 554 label: [9]\n",
      "row: 555 label: [9]\n",
      "row: 556 label: [9]\n",
      "row: 557 label: [9]\n",
      "row: 558 label: [9]\n",
      "row: 559 label: [9]\n",
      "row: 560 label: [9]\n",
      "row: 561 label: [9]\n",
      "row: 562 label: [9]\n",
      "row: 563 label: [9]\n",
      "row: 564 label: [9]\n",
      "row: 565 label: [9]\n",
      "row: 566 label: [9]\n",
      "row: 567 label: [9]\n",
      "row: 568 label: [9]\n",
      "row: 569 label: [9]\n",
      "row: 570 label: [9]\n",
      "row: 571 label: [9]\n",
      "row: 572 label: [9]\n",
      "row: 573 label: [9]\n",
      "row: 574 label: [9]\n",
      "row: 575 label: [9]\n",
      "row: 576 label: [9]\n",
      "row: 577 label: [9]\n",
      "row: 578 label: [9]\n",
      "row: 579 label: [9]\n",
      "row: 580 label: [9]\n",
      "row: 581 label: [9]\n",
      "row: 582 label: [9]\n",
      "row: 583 label: [9]\n",
      "row: 584 label: [9]\n",
      "row: 585 label: [9]\n",
      "row: 586 label: [9]\n",
      "row: 587 label: [9]\n",
      "row: 588 label: [9]\n",
      "row: 589 label: [9]\n",
      "row: 590 label: [9]\n",
      "row: 591 label: [9]\n",
      "row: 592 label: [9]\n",
      "row: 593 label: [10]\n",
      "row: 594 label: [10]\n",
      "row: 595 label: [10]\n",
      "row: 596 label: [10]\n",
      "row: 597 label: [10]\n",
      "row: 598 label: [10]\n",
      "row: 599 label: [10]\n",
      "row: 600 label: [10]\n",
      "row: 601 label: [10]\n",
      "row: 602 label: [10]\n",
      "row: 603 label: [10]\n",
      "row: 604 label: [10]\n",
      "row: 605 label: [10]\n",
      "row: 606 label: [10]\n",
      "row: 607 label: [10]\n",
      "row: 608 label: [10]\n",
      "row: 609 label: [10]\n",
      "row: 610 label: [10]\n",
      "row: 611 label: [10]\n",
      "row: 612 label: [10]\n",
      "row: 613 label: [10]\n",
      "row: 614 label: [10]\n",
      "row: 615 label: [10]\n",
      "row: 616 label: [10]\n",
      "row: 617 label: [10]\n",
      "row: 618 label: [10]\n",
      "row: 619 label: [10]\n",
      "row: 620 label: [10]\n",
      "row: 621 label: [10]\n",
      "row: 622 label: [10]\n",
      "row: 623 label: [10]\n",
      "row: 624 label: [10]\n",
      "row: 625 label: [10]\n",
      "row: 626 label: [10]\n",
      "row: 627 label: [10]\n",
      "row: 628 label: [10]\n",
      "row: 629 label: [10]\n",
      "row: 630 label: [10]\n",
      "row: 631 label: [10]\n",
      "row: 632 label: [10]\n",
      "row: 633 label: [10]\n",
      "row: 634 label: [10]\n",
      "row: 635 label: [10]\n",
      "row: 636 label: [10]\n",
      "row: 637 label: [10]\n",
      "row: 638 label: [10]\n",
      "row: 639 label: [10]\n",
      "row: 640 label: [10]\n",
      "row: 641 label: [10]\n",
      "row: 642 label: [10]\n",
      "row: 643 label: [10]\n",
      "row: 644 label: [10]\n",
      "row: 645 label: [10]\n",
      "row: 646 label: [10]\n",
      "row: 647 label: [10]\n",
      "row: 648 label: [10]\n",
      "row: 649 label: [10]\n",
      "row: 650 label: [10]\n",
      "row: 651 label: [10]\n",
      "row: 652 label: [10]\n",
      "row: 653 label: [10]\n",
      "row: 654 label: [10]\n",
      "row: 655 label: [10]\n",
      "row: 656 label: [10]\n",
      "row: 657 label: [10]\n",
      "row: 658 label: [10]\n",
      "row: 659 label: [10]\n",
      "row: 660 label: [10]\n",
      "row: 661 label: [10]\n",
      "row: 662 label: [10]\n",
      "row: 663 label: [10]\n",
      "row: 664 label: [10]\n",
      "row: 665 label: [10]\n",
      "row: 666 label: [10]\n",
      "row: 667 label: [10]\n",
      "row: 668 label: [10]\n",
      "row: 669 label: [10]\n",
      "row: 670 label: [10]\n",
      "row: 671 label: [10]\n",
      "row: 672 label: [10]\n",
      "row: 673 label: [10]\n",
      "row: 674 label: [10]\n",
      "row: 675 label: [10]\n",
      "row: 676 label: [10]\n",
      "row: 677 label: [10]\n",
      "row: 678 label: [10]\n",
      "row: 679 label: [10]\n",
      "row: 680 label: [10]\n",
      "row: 681 label: [10]\n",
      "row: 682 label: [10]\n",
      "row: 683 label: [10]\n",
      "row: 684 label: [10]\n",
      "row: 685 label: [10]\n",
      "row: 686 label: [10]\n",
      "row: 687 label: [10]\n",
      "row: 688 label: [10]\n",
      "row: 689 label: [10]\n",
      "row: 690 label: [10]\n",
      "row: 691 label: [10]\n",
      "row: 692 label: [10]\n",
      "row: 693 label: [11]\n",
      "row: 694 label: [11]\n",
      "row: 695 label: [11]\n",
      "row: 696 label: [11]\n",
      "row: 697 label: [11]\n",
      "row: 698 label: [11]\n",
      "row: 699 label: [11]\n",
      "row: 700 label: [11]\n",
      "row: 701 label: [11]\n",
      "row: 702 label: [11]\n",
      "row: 703 label: [11]\n",
      "row: 704 label: [11]\n",
      "row: 705 label: [11]\n",
      "row: 706 label: [11]\n",
      "row: 707 label: [11]\n",
      "row: 708 label: [11]\n",
      "row: 709 label: [11]\n",
      "row: 710 label: [11]\n",
      "row: 711 label: [11]\n",
      "row: 712 label: [11]\n",
      "row: 713 label: [11]\n",
      "row: 714 label: [11]\n",
      "row: 715 label: [11]\n",
      "row: 716 label: [11]\n",
      "row: 717 label: [11]\n",
      "row: 718 label: [11]\n",
      "row: 719 label: [11]\n",
      "row: 720 label: [11]\n",
      "row: 721 label: [11]\n",
      "row: 722 label: [11]\n",
      "row: 723 label: [11]\n",
      "row: 724 label: [11]\n",
      "row: 725 label: [11]\n",
      "row: 726 label: [11]\n",
      "row: 727 label: [11]\n",
      "row: 728 label: [11]\n",
      "row: 729 label: [11]\n",
      "row: 730 label: [11]\n",
      "row: 731 label: [11]\n",
      "row: 732 label: [11]\n",
      "row: 733 label: [11]\n",
      "row: 734 label: [11]\n",
      "row: 735 label: [11]\n",
      "row: 736 label: [11]\n",
      "row: 737 label: [11]\n",
      "row: 738 label: [11]\n",
      "row: 739 label: [11]\n",
      "row: 740 label: [11]\n",
      "row: 741 label: [11]\n",
      "row: 742 label: [11]\n",
      "row: 743 label: [11]\n",
      "label: [[0.8125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " [0.8125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " [0.8125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " ...\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]]\n",
      "744\n",
      "744\n",
      "744\n",
      "row: 0 label: [11]\n",
      "row: 1 label: [11]\n",
      "row: 2 label: [11]\n",
      "row: 3 label: [11]\n",
      "row: 4 label: [11]\n",
      "row: 5 label: [11]\n",
      "row: 6 label: [11]\n",
      "row: 7 label: [11]\n",
      "row: 8 label: [11]\n",
      "row: 9 label: [11]\n",
      "row: 10 label: [11]\n",
      "row: 11 label: [11]\n",
      "row: 12 label: [11]\n",
      "row: 13 label: [11]\n",
      "row: 14 label: [11]\n",
      "row: 15 label: [11]\n",
      "row: 16 label: [11]\n",
      "row: 17 label: [11]\n",
      "row: 18 label: [11]\n",
      "row: 19 label: [11]\n",
      "row: 20 label: [11]\n",
      "row: 21 label: [11]\n",
      "row: 22 label: [11]\n",
      "row: 23 label: [11]\n",
      "row: 24 label: [11]\n",
      "row: 25 label: [11]\n",
      "row: 26 label: [11]\n",
      "row: 27 label: [11]\n",
      "row: 28 label: [11]\n",
      "row: 29 label: [11]\n",
      "row: 30 label: [11]\n",
      "row: 31 label: [11]\n",
      "row: 32 label: [11]\n",
      "row: 33 label: [11]\n",
      "row: 34 label: [11]\n",
      "row: 35 label: [11]\n",
      "row: 36 label: [11]\n",
      "row: 37 label: [11]\n",
      "row: 38 label: [11]\n",
      "row: 39 label: [11]\n",
      "row: 40 label: [11]\n",
      "row: 41 label: [11]\n",
      "row: 42 label: [11]\n",
      "row: 43 label: [11]\n",
      "row: 44 label: [11]\n",
      "row: 45 label: [11]\n",
      "row: 46 label: [11]\n",
      "row: 47 label: [11]\n",
      "row: 48 label: [11]\n",
      "row: 49 label: [11]\n",
      "row: 50 label: [11]\n",
      "row: 51 label: [11]\n",
      "row: 52 label: [11]\n",
      "row: 53 label: [11]\n",
      "row: 54 label: [11]\n",
      "row: 55 label: [11]\n",
      "row: 56 label: [11]\n",
      "row: 57 label: [11]\n",
      "row: 58 label: [11]\n",
      "row: 59 label: [11]\n",
      "row: 60 label: [11]\n",
      "row: 61 label: [11]\n",
      "row: 62 label: [11]\n",
      "row: 63 label: [11]\n",
      "row: 64 label: [11]\n",
      "row: 65 label: [11]\n",
      "row: 66 label: [11]\n",
      "row: 67 label: [11]\n",
      "row: 68 label: [11]\n",
      "row: 69 label: [11]\n",
      "row: 70 label: [11]\n",
      "row: 71 label: [12]\n",
      "row: 72 label: [12]\n",
      "row: 73 label: [12]\n",
      "row: 74 label: [12]\n",
      "row: 75 label: [12]\n",
      "row: 76 label: [12]\n",
      "row: 77 label: [12]\n",
      "row: 78 label: [12]\n",
      "row: 79 label: [12]\n",
      "row: 80 label: [12]\n",
      "row: 81 label: [12]\n",
      "row: 82 label: [12]\n",
      "row: 83 label: [12]\n",
      "row: 84 label: [12]\n",
      "row: 85 label: [12]\n",
      "row: 86 label: [12]\n",
      "row: 87 label: [12]\n",
      "row: 88 label: [12]\n",
      "row: 89 label: [12]\n",
      "row: 90 label: [12]\n",
      "row: 91 label: [12]\n",
      "row: 92 label: [12]\n",
      "row: 93 label: [12]\n",
      "row: 94 label: [12]\n",
      "row: 95 label: [12]\n",
      "row: 96 label: [12]\n",
      "row: 97 label: [12]\n",
      "row: 98 label: [12]\n",
      "row: 99 label: [12]\n",
      "label: [[0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " ...\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]]\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row: 0 label: [12]\n",
      "row: 1 label: [12]\n",
      "row: 2 label: [12]\n",
      "row: 3 label: [12]\n",
      "row: 4 label: [12]\n",
      "row: 5 label: [12]\n",
      "row: 6 label: [12]\n",
      "row: 7 label: [12]\n",
      "row: 8 label: [12]\n",
      "row: 9 label: [12]\n",
      "row: 10 label: [12]\n",
      "row: 11 label: [12]\n",
      "row: 12 label: [12]\n",
      "row: 13 label: [12]\n",
      "row: 14 label: [12]\n",
      "row: 15 label: [12]\n",
      "row: 16 label: [12]\n",
      "row: 17 label: [12]\n",
      "row: 18 label: [12]\n",
      "row: 19 label: [12]\n",
      "row: 20 label: [12]\n",
      "row: 21 label: [12]\n",
      "row: 22 label: [12]\n",
      "row: 23 label: [12]\n",
      "row: 24 label: [12]\n",
      "row: 25 label: [12]\n",
      "row: 26 label: [12]\n",
      "row: 27 label: [12]\n",
      "row: 28 label: [13]\n",
      "row: 29 label: [13]\n",
      "row: 30 label: [13]\n",
      "row: 31 label: [13]\n",
      "row: 32 label: [13]\n",
      "row: 33 label: [13]\n",
      "row: 34 label: [13]\n",
      "row: 35 label: [13]\n",
      "row: 36 label: [13]\n",
      "row: 37 label: [13]\n",
      "row: 38 label: [13]\n",
      "row: 39 label: [13]\n",
      "row: 40 label: [13]\n",
      "row: 41 label: [13]\n",
      "row: 42 label: [13]\n",
      "row: 43 label: [13]\n",
      "row: 44 label: [13]\n",
      "row: 45 label: [13]\n",
      "row: 46 label: [13]\n",
      "row: 47 label: [13]\n",
      "row: 48 label: [13]\n",
      "row: 49 label: [13]\n",
      "row: 50 label: [13]\n",
      "row: 51 label: [13]\n",
      "row: 52 label: [13]\n",
      "row: 53 label: [13]\n",
      "row: 54 label: [13]\n",
      "row: 55 label: [13]\n",
      "row: 56 label: [13]\n",
      "row: 57 label: [13]\n",
      "row: 58 label: [13]\n",
      "row: 59 label: [13]\n",
      "row: 60 label: [14]\n",
      "row: 61 label: [14]\n",
      "row: 62 label: [14]\n",
      "row: 63 label: [14]\n",
      "row: 64 label: [14]\n",
      "row: 65 label: [14]\n",
      "row: 66 label: [14]\n",
      "row: 67 label: [14]\n",
      "row: 68 label: [14]\n",
      "row: 69 label: [14]\n",
      "row: 70 label: [14]\n",
      "row: 71 label: [14]\n",
      "row: 72 label: [14]\n",
      "row: 73 label: [14]\n",
      "row: 74 label: [14]\n",
      "row: 75 label: [15]\n",
      "row: 76 label: [15]\n",
      "row: 77 label: [15]\n",
      "row: 78 label: [15]\n",
      "row: 79 label: [15]\n",
      "row: 80 label: [15]\n",
      "row: 81 label: [15]\n",
      "row: 82 label: [15]\n",
      "row: 83 label: [15]\n",
      "row: 84 label: [15]\n",
      "row: 85 label: [15]\n",
      "row: 86 label: [15]\n",
      "row: 87 label: [15]\n",
      "row: 88 label: [15]\n",
      "row: 89 label: [15]\n",
      "row: 90 label: [15]\n",
      "row: 91 label: [15]\n",
      "row: 92 label: [15]\n",
      "row: 93 label: [15]\n",
      "row: 94 label: [15]\n",
      "row: 95 label: [15]\n",
      "row: 96 label: [15]\n",
      "row: 97 label: [15]\n",
      "row: 98 label: [15]\n",
      "row: 99 label: [15]\n",
      "label: [[0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.0125]\n",
      " ...\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.8125]\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.8125]\n",
      " [0.0125 0.0125 0.0125 ... 0.0125 0.0125 0.8125]]\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "features_map,num_features=map_features()\n",
    "n_class=16\n",
    "train_dataset = FMData(config.train_libfm,config.train_label,features_map)\n",
    "train_loader = data.DataLoader(train_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "validate_dataset = FMData(config.valid_libfm,config.valid_label,features_map)\n",
    "validate_loader = data.DataLoader(train_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "test_dataset = FMData(config.test_libfm,config.test_label,features_map)\n",
    "test_loader = data.DataLoader(test_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87f6e0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import model\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--lr\", \n",
    "    type=float, \n",
    "    default=0.05, \n",
    "    help=\"learning rate\")\n",
    "\n",
    "\n",
    "parser.add_argument(\"--dropout\", \n",
    "    default='[0.7, 0.6]',  \n",
    "    help=\"dropout rate for FM and MLP\")\n",
    "parser.add_argument(\"--batch_size\", \n",
    "    type=int, \n",
    "    default=128, \n",
    "    help=\"batch size for training\")\n",
    "\n",
    "parser.add_argument(\"--epochs\", \n",
    "    type=int,\n",
    "    default=100, \n",
    "    help=\"training epochs\")\n",
    "\n",
    "parser.add_argument(\"--hidden_factor\", \n",
    "    type=int,\n",
    "    default=64, #被修改，原始值为64,代表embedding\n",
    "    help=\"predictive factors numbers in the model\")\n",
    "\n",
    "parser.add_argument(\"--n_class\",\n",
    "    type=int,\n",
    "    default=16,\n",
    "    help='the number of classes')\n",
    "\n",
    "parser.add_argument(\"--layers\", \n",
    "    default='[5000,1000]', \n",
    "    help=\"size of layers in MLP model, '[]' is NFM-0\")\n",
    "\n",
    "parser.add_argument(\"--lamda\", \n",
    "    type=float, \n",
    "    default=0.0, \n",
    "    help=\"regularizer for bilinear layers\")\n",
    "\n",
    "parser.add_argument(\"--batch_norm\", \n",
    "    default=True, \n",
    "    help=\"use batch_norm or not\")\n",
    "\n",
    "parser.add_argument(\"--pre_train\", \n",
    "    action='store_true', \n",
    "    default=False, \n",
    "    help=\"whether use the pre-train or not\")\n",
    "\n",
    "parser.add_argument(\"--out\", \n",
    "    default=True, \n",
    "    help=\"save model or not\")\n",
    "\n",
    "\n",
    "parser.add_argument(\"--act_function\", \n",
    "    default='relu', \n",
    "    help=\"activation function\")\n",
    "parser.add_argument(\"--gpu\", \n",
    "    type=str,\n",
    "    default=\"0\",  \n",
    "    help=\"gpu card ID\")\n",
    "\n",
    "args = parser.parse_args(args=[])#[]\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e82ad229",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f752abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NFM(nn.Module):\n",
    "    def __init__(self, num_features, num_factors, \n",
    "        act_function, layers, batch_norm, drop_prob,n_class, pretrain_FM):\n",
    "        super(NFM, self).__init__()\n",
    "        \"\"\"\n",
    "        num_features: number of features,\n",
    "        num_factors: number of hidden factors,\n",
    "        act_function: activation function for MLP layer,\n",
    "        layers: list of dimension of deep layers,\n",
    "        batch_norm: bool type, whether to use batch norm or not,\n",
    "        drop_prob: list of the dropout rate for FM and MLP,\n",
    "        pretrain_FM: the pre-trained FM weights.\n",
    "        \"\"\"\n",
    "        self.num_features = num_features\n",
    "        self.num_factors = num_factors\n",
    "        self.act_function = act_function\n",
    "        self.layers = layers\n",
    "        self.batch_norm = batch_norm\n",
    "        self.drop_prob = drop_prob\n",
    "        self.pretrain_FM = pretrain_FM\n",
    "        #self.last_layer = pt.nn.Linear(128,10)\n",
    "        \n",
    "        \n",
    "        self.embeddings = nn.Embedding(num_features, num_factors)\n",
    "        self.biases = nn.Embedding(num_features, 1)\n",
    "        self.bias_ = nn.Parameter(torch.tensor([0.0]))\n",
    "\n",
    "        FM_modules = []\n",
    "        if self.batch_norm:\n",
    "            FM_modules.append(nn.BatchNorm1d(num_factors))\n",
    "        FM_modules.append(nn.Dropout(drop_prob[0]))\n",
    "        self.FM_layers = nn.Sequential(*FM_modules)\n",
    "\n",
    "        MLP_module = []\n",
    "        in_dim = num_factors\n",
    "        for dim in self.layers:\n",
    "            out_dim = dim\n",
    "            MLP_module.append(nn.Linear(in_dim, out_dim))\n",
    "            in_dim = out_dim\n",
    "\n",
    "            if self.batch_norm:\n",
    "                MLP_module.append(nn.BatchNorm1d(out_dim))\n",
    "            if self.act_function == 'relu':\n",
    "                MLP_module.append(nn.ReLU())\n",
    "            elif self.act_function == 'sigmoid':\n",
    "                MLP_module.append(nn.Sigmoid())\n",
    "            elif self.act_function == 'tanh':\n",
    "                MLP_module.append(nn.Tanh())\n",
    "            #elif self.act_function == 'softmax':\n",
    "                #MLP_module.append(F.softmax(out_dim))#修改\n",
    "            #MLP_moule.append(F.softmax(nn.Linear(out_dim,n_class)))#增加\n",
    "            #MLP_moule.append(F.softmax(n_class))\n",
    "            MLP_module.append(nn.Dropout(drop_prob[-1]))\n",
    "        self.deep_layers = nn.Sequential(*MLP_module)\n",
    "\n",
    "        predict_size = layers[-1] if layers else num_factors\n",
    "        last_layer=nn.Linear(predict_size, n_class, bias=False)\n",
    "        self.prediction = last_layer\n",
    "\n",
    "        self._init_weight_()\n",
    "\n",
    "    def _init_weight_(self):\n",
    "        \"\"\" Try to mimic the original weight initialization. \"\"\"\n",
    "        if self.pretrain_FM:\n",
    "            self.embeddings.weight.data.copy_(\n",
    "                            self.pretrain_FM.embeddings.weight)\n",
    "            self.biases.weight.data.copy_(\n",
    "                            self.pretrain_FM.biases.weight)\n",
    "            self.bias_.data.copy_(self.pretrain_FM.bias_)\n",
    "        else:\n",
    "            nn.init.normal_(self.embeddings.weight, std=0.01)\n",
    "            nn.init.constant_(self.biases.weight, 0.0)\n",
    "\n",
    "        # for deep layers\n",
    "        if len(self.layers) > 0:\n",
    "            for m in self.deep_layers:\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_normal_(m.weight)\n",
    "            nn.init.xavier_normal_(self.prediction.weight)\n",
    "        else:\n",
    "            nn.init.constant_(self.prediction.weight, 1.0)\n",
    "\n",
    "    def forward(self, features, feature_values):\n",
    "        nonzero_embed = self.embeddings(features)\n",
    "        feature_values = feature_values.unsqueeze(dim=-1)\n",
    "        nonzero_embed = nonzero_embed * feature_values\n",
    "\n",
    "        # Bi-Interaction layer\n",
    "        sum_square_embed = nonzero_embed.sum(dim=1).pow(2)\n",
    "        square_sum_embed = (nonzero_embed.pow(2)).sum(dim=1)\n",
    "\n",
    "        # FM model\n",
    "        FM = 0.5 * (sum_square_embed - square_sum_embed)\n",
    "        FM = self.FM_layers(FM)#\n",
    "        if self.layers: # have deep layers\n",
    "            FM = self.deep_layers(FM)#\n",
    "        FM = self.prediction(FM)\n",
    "\n",
    "        # bias addition\n",
    "        feature_bias = self.biases(features)\n",
    "        feature_bias = (feature_bias * feature_values).sum(dim=1)\n",
    "        FM = FM + feature_bias + self.bias_\n",
    "        return FM.view(-1)\n",
    "\n",
    "\n",
    "class FM(nn.Module):\n",
    "    def __init__(self, num_features, num_factors, batch_norm, drop_prob):\n",
    "        super(FM, self).__init__()\n",
    "        \"\"\"\n",
    "        num_features: number of features,\n",
    "        num_factors: number of hidden factors,\n",
    "        batch_norm: bool type, whether to use batch norm or not,\n",
    "        drop_prob: list of the dropout rate for FM and MLP,\n",
    "        \"\"\"\n",
    "        self.num_features = num_features\n",
    "        self.num_factors = num_factors\n",
    "        self.batch_norm = batch_norm\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.embeddings = nn.Embedding(num_features, num_factors)\n",
    "        self.biases = nn.Embedding(num_features, 1)\n",
    "        self.bias_ = nn.Parameter(torch.tensor([0.0]))\n",
    "\n",
    "        FM_modules = []\n",
    "        if self.batch_norm:\n",
    "            FM_modules.append(nn.BatchNorm1d(num_factors))\n",
    "        FM_modules.append(nn.Dropout(drop_prob[0]))\n",
    "        self.FM_layers = nn.Sequential(*FM_modules)\n",
    "\n",
    "        nn.init.normal_(self.embeddings.weight, std=0.01)\n",
    "        nn.init.constant_(self.biases.weight, 0.0)\n",
    "\n",
    "\n",
    "    def forward(self, features, feature_values):\n",
    "        nonzero_embed = self.embeddings(features)\n",
    "        feature_values = feature_values.unsqueeze(dim=-1)\n",
    "        nonzero_embed = nonzero_embed * feature_values\n",
    "\n",
    "        # Bi-Interaction layer\n",
    "        sum_square_embed = nonzero_embed.sum(dim=1).pow(2)\n",
    "        square_sum_embed = (nonzero_embed.pow(2)).sum(dim=1)\n",
    "\n",
    "        # FM model\n",
    "        FM = 0.5 * (sum_square_embed - square_sum_embed)\n",
    "        FM = self.FM_layers(FM).sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # bias addition\n",
    "        feature_bias = self.biases(features)\n",
    "        feature_bias = (feature_bias * feature_values).sum(dim=1)\n",
    "        FM = FM + feature_bias + self.bias_\n",
    "        return FM.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8513697",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Linear' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-27342df33eb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         args.batch_norm, eval(args.dropout),n_class, FM_model)\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NFM-pyorch-master/NFM-pyorch-master/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_features, num_factors, act_function, layers, batch_norm, drop_prob, n_class, pretrain_FM)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mpredict_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnum_factors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;31m#self.prediction = F.softmax(nn.Linear(predict_size, n_class, bias=False))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_stacklevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1677\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1678\u001b[0;31m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1679\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1680\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1178\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Linear' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "#import model\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "\n",
    "#model = model.FM(10150, 100, eval(args.dropout=0.5))\n",
    "config.loss_type=='cross_entropy_loss'\n",
    "count, best_rmse = 0, 100\n",
    "criterion=nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "config.optimizer == 'Momentum'\n",
    "#optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.95)\n",
    "\n",
    "if args.pre_train:\n",
    "    assert os.path.exists(config.FM_model_path), 'lack of FM model'\n",
    "    assert config.model == 'NFM', 'only support NFM for now'\n",
    "    FM_model = torch.load(config.FM_model_path)\n",
    "else:\n",
    "    FM_model = None\n",
    "    \n",
    "model = model.NFM(\n",
    "        num_features, args.hidden_factor, \n",
    "        config.activation_function, eval(args.layers), \n",
    "        args.batch_norm, eval(args.dropout),n_class, FM_model)\n",
    "model.cuda()\n",
    "\n",
    "\n",
    "if args.pre_train:\n",
    "    assert os.path.exists(config.FM_model_path), 'lack of FM model'\n",
    "    assert config.model == 'NFM', 'only support NFM for now'\n",
    "    FM_model = torch.load(config.FM_model_path)\n",
    "else:\n",
    "    FM_model = None\n",
    "    \n",
    "    \n",
    "    \n",
    "for epoch in range(args.epochs):\n",
    "    model.train() # Enable dropout and batch_norm\n",
    "    start_time = time.time()\n",
    "\n",
    "    for features, feature_values, label in train_loader:\n",
    "        features = features.cuda()\n",
    "        feature_values = feature_values.cuda()\n",
    "        label = label.cuda()\n",
    "\n",
    "        model.zero_grad()\n",
    "        prediction = model(features, feature_values,act_function='relu')\n",
    "        loss = criterion(prediction, label) \n",
    "        loss += args.lamda * model.embeddings.weight.norm()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # writer.add_scalar('data/loss', loss.item(), count)\n",
    "        count += 1\n",
    "        \n",
    "    model.eval()\n",
    "    train_result = evaluate.metrics(model, train_loader)\n",
    "    valid_result = evaluate.metrics(model, valid_loader)\n",
    "    test_result = evaluate.metrics(model, test_loader)\n",
    "\n",
    "    print(\"Runing Epoch {:03d} \".format(epoch) + \"costs \" + time.strftime(\n",
    "                        \"%H: %M: %S\", time.gmtime(time.time()-start_time)))\n",
    "    print(\"Train_RMSE: {:.3f}, Valid_RMSE: {:.3f}, Test_RMSE: {:.3f}\".format(\n",
    "                        train_result, valid_result, test_result))\n",
    "\n",
    "    if test_result < best_rmse:\n",
    "        best_rmse, best_epoch = test_result, epoch\n",
    "        if args.out:\n",
    "            if not os.path.exists(config.model_path):\n",
    "                os.mkdir(config.model_path)\n",
    "            torch.save(model, \n",
    "                '{}{}.pth'.format(config.model_path, config.model))\n",
    "\n",
    "print(\"End. Best epoch {:03d}: Test_RMSE is {:.3f}\".format(best_epoch, best_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2d67c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
