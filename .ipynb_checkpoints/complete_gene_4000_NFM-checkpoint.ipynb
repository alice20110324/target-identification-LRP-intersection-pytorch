{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3f9408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import Trainer\n",
    "from network import NFM\n",
    "import torch.utils.data as Data\n",
    "from Utils.criteo_loader import getTestData, getTrainData\n",
    "\n",
    "nfm_config = \\\n",
    "{\n",
    "    'n_class':9,\n",
    "    'linear_hidden1':1000,\n",
    "    'linear_hidden':100,#线性模型输出层（隐层个数）\n",
    "    'embed_input_dim':1001,#embed输入维度\n",
    "    'embed_dim': 20, # 用于控制稀疏特征经过Embedding层后的稠密特征大小，embed输出维度\n",
    "    #'dnn_hidden_units': [100,11],#MLP隐层和输出层\n",
    "    \n",
    "    'dnn_hidden_units':[100,9],#MLP隐层\n",
    "    'num_sparse_features_cols':4224,#the number of the gene columns\n",
    "    'num_dense_features': 0,#dense features number\n",
    "    'bi_dropout': 0.3,#Bi-Interaction 的dropout\n",
    "    'num_epoch': 500,#训练epoch次数\n",
    "    'batch_size': 24,#batch_size\n",
    "    'lr': 1e-3,\n",
    "    'l2_regularization': 1e-4,\n",
    "    'device_id': 0,\n",
    "    'use_cuda': False,\n",
    "    'epoch':1000,\n",
    "    \n",
    "    #'train_file': '../Data/criteo/processed_data/train_set.csv',\n",
    "    #'fea_file': '../Data/criteo/processed_data/fea_col.npy',\n",
    "    #'validate_file': '../Data/criteo/processed_data/val_set.csv',\n",
    "    #'test_file': '../Data/criteo/processed_data/test_set.csv',\n",
    "    #'model_name': '../TrainedModels/NFM.model'\n",
    "    #'train_file':'data/xiaoqiu_gene_5000/train/final_5000_encode_100x.csv',\n",
    "    'train_data':'dataset/xiaoguan/RF/RF_for_train/train_class_9/train/train_encode_datax.csv',\n",
    "    'train_label':'dataset/xiaoguan/RF/RF_for_train/train_class_9/train/train_label.csv',\n",
    "    'test_data':'dataset/xiaoguan/RF/RF_for_train/train_class_9/test/test_encode_datax.csv',\n",
    "    'test_label':'dataset/xiaoguan/RF/RF_for_train/train_class_9/test/test_label.csv',\n",
    "    #'title':'dataset/xiaoguan/RF/RF_for_train/train_class_9/test/test_data.csv',\n",
    "    \n",
    "    #'all':''\n",
    "    #'title':'data/xiaoqiu_gene_5000/train/gene_5000_gene_name.csv',\n",
    "    #'all':'data/xiaoqiu_gene_5000/train/gene_5000_label_name.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7ce3f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#准备训练集\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "#from utils import \n",
    "#准备训练集\n",
    "#from new_dataset_processed import FMData\n",
    "from dataset_process import FMData\n",
    "def prepare_dataset(m_data,m_label,batch_size,n_class):\n",
    "    m_dataset=FMData(m_data,m_label,n_class)\n",
    "    m_dataloader=data.DataLoader(m_dataset, drop_last=True,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "    \n",
    "    return m_dataset,m_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbf480b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "import sys \n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "import torchmetrics\n",
    "def   train_data(model,train_loader,test_loader,batch_size,model_path):\n",
    "    #train_accuracy=torchmetrics.Accuracy()\n",
    "    #test_accuracy=torchmetrics.Accuracy()\n",
    "    BATCH_SIZE=batch_size\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    total = 0\n",
    "    \n",
    "    #loss_func = torch.nn.BCELoss()\n",
    "    loss_func=torch.nn.CrossEntropyLoss()\n",
    "    #loss_func=nn.MultiLabelSoftMarginLoss()\n",
    "    #loss_func=torch.nn.LogSoftmax()\n",
    "    num=1\n",
    "    #model=nn.Softmax(nn.Linear(10149,16)).to(device)\n",
    "    # 从DataLoader中获取小批量的id以及数据\n",
    "    for epoch_id in range(1000):\n",
    "        correct=0\n",
    "        total=0\n",
    "        total_test_acc=0\n",
    "        \n",
    "        for batch_idx, (x, labels) in enumerate(train_loader):\n",
    "            x = Variable(x)\n",
    "            labels = Variable(labels)\n",
    "            \n",
    "            \n",
    "            #x = torch.tensor(x, dtype=torch.float)\n",
    "            #x=x.clone().detach().requires_grad_(True)\n",
    "            x=torch.tensor(x,dtype=torch.float)\n",
    "            labels=torch.tensor(labels,dtype=torch.float)\n",
    "            x, labels = x.cuda(), labels.cuda()\n",
    "            labels_int=labels=torch.max(labels,1)[1]\n",
    "            #labels_int.cuda()\n",
    "            #print(\"labels:\",labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_predict = model(x)\n",
    "            #print(\"y_predict:\",y_predict)\n",
    "            #loss = loss_func(y_predict.view(-1), labels)\n",
    "            loss = loss_func(y_predict, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss = loss.item()\n",
    "            #loss, predicted = self._train_single_batch(x, labels)\n",
    "\n",
    "            total += loss\n",
    "            \n",
    "            \n",
    "            #predicted = torch.max(y_predict.data,1)\n",
    "            #print(\"predicted:\",predicted)\n",
    "            #predicted = torch.max(y_predict.data,1)[1]\n",
    "            #predicted=predicted.detach().cpu().numpy()\n",
    "            \n",
    "            #labels=torch.max(labels,1)\n",
    "            #print(\"labels:\",labels)\n",
    "            #labels=labels[1]\n",
    "            #y_predict=y_predict.cuda()\n",
    "            #labels=torch.max(labels,1)[1].cuda()\n",
    "            #labels=labels.detach().cpu().numpy()\n",
    "            #correct += (predicted == labels).sum()\n",
    "            #print(\"correct:\",correct)\n",
    "            #correct=correct[0]\n",
    "            #print(\"new_correct:\",float(correct))\n",
    "            #correct=float(correct)   \n",
    "            #if batch_idx % 10 == 0:\n",
    "            #print(\"batch_idx:\",batch_idx)\n",
    "            #print(correct/(BATCH_SIZE*(batch_idx+1)))\n",
    "            batch_train_acc=torchmetrics.functional.accuracy(y_predict,labels_int)\n",
    "            #print('batch_train_acc:',batch_train_acc)\n",
    "        #total_train_accuracy=torchmetrics.functional.compute_details()\n",
    "        #print('total_train_accuracy:',total_train_accuracy)\n",
    "        for i , (inputs , targets) in enumerate(test_loader):   \n",
    "            # evaluate the model on the test set   \n",
    "            #print(\\ inputs:\\  inputs)   \n",
    "            #print(\\ targets:\\  targets)   \n",
    "            inputs = Variable(inputs)   \n",
    "            targets = Variable(targets)     \n",
    "            #x = torch.tensor(x  dtype=torch.float)   \n",
    "            #x=x.clone().detach().requires_grad_(True)   \n",
    "            inputs=torch.tensor(inputs ,dtype=torch.float)   \n",
    "            targets=torch.tensor(targets ,dtype=torch.float)   \n",
    "            inputs , targets = inputs.cuda(),  targets.cuda()   \n",
    "            yhat = model(inputs)  \n",
    "            \n",
    "            #yhat = torch.max(yhat.data,1)[1]\n",
    "            #yhat=yhat.detach().cpu().numpy()\n",
    "            #print(\"predicted:\",predicted)\n",
    "            #predicted = torch.max(y_predict.data,1)[1]\n",
    "             #predicted = torch.max(y_predict.data,1)[1]\n",
    "            \n",
    "            \n",
    "            targets=torch.max(targets,1)[1]\n",
    "            #print(\"labels:\",labels)\n",
    "            #labels=labels[1]\n",
    "            #targets=targets.detach().cpu().numpy()\n",
    "            \n",
    "            \n",
    "            \n",
    "            batch_test_acc=torchmetrics.functional.accuracy(yhat,targets)\n",
    "            #print(\"batch_test_acc:\",batch_test_acc)\n",
    "            total_test_acc+=batch_test_acc\n",
    "            #total_test_accuracy=torchmetrics.functional.compute_details()\n",
    "        print('total_test_accuracy:',total_test_acc/(i+1))\n",
    "        \n",
    "                    \n",
    "                    \n",
    "            \n",
    "            #model.evaluate()\n",
    "            #model.eval()\n",
    "            #train_result = evaluate.metrics(model, train_loader)\n",
    "            #valid_result = evaluate.metrics(model, valid_loader)\n",
    "            #est_result = evaluate.metrics(model, test_loader)\n",
    "            #acturals,predictions,acc_test=evaluate_model(test_loader,model)\n",
    "            #print(\"acc_test:  %d  \" %(acc_test))\n",
    "            #print(\"Train_RMSE: {:.3f}, Valid_RMSE: {:.3f}, Test_RMSE: {:.3f}\".format(\n",
    "            #train_result, valid_result, test_result))\n",
    "            # print('[Training Epoch: {}] Batch: {}, Loss: {}'.format(epoch_id, batch_id, loss))\n",
    "        print(\"Training Epoch: %d, total loss: %f\" % (epoch_id, total))\n",
    "        #print(\"auc:\",roc_auc_score)\n",
    "    #功能：保存训练完的网络的各层参数（即weights和bias)\n",
    "    #path='dataset/xiaoguan/RF/RF_for_train/train_class_9/model/gene_4000_NFM.pkl'\n",
    "        if epoch_id %100==0:\n",
    "            num=num+1\n",
    "            path=os.path.join(model_path,'NMF'+str(num)+'.pkl')\n",
    "            torch.save(model.state_dict(),path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "645a8f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMData\n",
      "FMData\n",
      "NFM: NFM(\n",
      "  (drop): Dropout(p=0.7, inplace=False)\n",
      "  (linear_model1): Linear(in_features=4224, out_features=1000, bias=True)\n",
      "  (BN_linear1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear_model2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (BN_linear2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (embedding_layers): Embedding(1001, 100)\n",
      "  (bi_pooling): BiInteractionPooling()\n",
      "  (dropout): Dropout(p=0.7, inplace=False)\n",
      "  (BN_bi): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dnn_layers): ModuleList(\n",
      "    (0): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (1): Linear(in_features=100, out_features=9, bias=True)\n",
      "  )\n",
      "  (dnn_softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_test_accuracy: tensor(0.4028, device='cuda:0')\n",
      "Training Epoch: 0, total loss: 53.843615\n",
      "total_test_accuracy: tensor(0.3889, device='cuda:0')\n",
      "Training Epoch: 1, total loss: 52.288580\n",
      "total_test_accuracy: tensor(0.4306, device='cuda:0')\n",
      "Training Epoch: 2, total loss: 51.708305\n",
      "total_test_accuracy: tensor(0.4306, device='cuda:0')\n",
      "Training Epoch: 3, total loss: 51.038258\n",
      "total_test_accuracy: tensor(0.4028, device='cuda:0')\n",
      "Training Epoch: 4, total loss: 49.236827\n",
      "total_test_accuracy: tensor(0.4306, device='cuda:0')\n",
      "Training Epoch: 5, total loss: 48.744482\n",
      "total_test_accuracy: tensor(0.4583, device='cuda:0')\n",
      "Training Epoch: 6, total loss: 48.538286\n",
      "total_test_accuracy: tensor(0.3750, device='cuda:0')\n",
      "Training Epoch: 7, total loss: 48.276756\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 8, total loss: 48.135520\n",
      "total_test_accuracy: tensor(0.4028, device='cuda:0')\n",
      "Training Epoch: 9, total loss: 47.722233\n",
      "total_test_accuracy: tensor(0.4444, device='cuda:0')\n",
      "Training Epoch: 10, total loss: 47.921084\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 11, total loss: 47.468182\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 12, total loss: 48.019197\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 13, total loss: 47.685776\n",
      "total_test_accuracy: tensor(0.4306, device='cuda:0')\n",
      "Training Epoch: 14, total loss: 47.770855\n",
      "total_test_accuracy: tensor(0.4722, device='cuda:0')\n",
      "Training Epoch: 15, total loss: 48.195415\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 16, total loss: 47.374320\n",
      "total_test_accuracy: tensor(0.4722, device='cuda:0')\n",
      "Training Epoch: 17, total loss: 47.332942\n",
      "total_test_accuracy: tensor(0.4167, device='cuda:0')\n",
      "Training Epoch: 18, total loss: 46.528971\n",
      "total_test_accuracy: tensor(0.4583, device='cuda:0')\n",
      "Training Epoch: 19, total loss: 47.046713\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 20, total loss: 47.371502\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 21, total loss: 46.695568\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 22, total loss: 46.787185\n",
      "total_test_accuracy: tensor(0.4583, device='cuda:0')\n",
      "Training Epoch: 23, total loss: 47.042326\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 24, total loss: 47.550151\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 25, total loss: 46.937615\n",
      "total_test_accuracy: tensor(0.4722, device='cuda:0')\n",
      "Training Epoch: 26, total loss: 47.116264\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 27, total loss: 47.042400\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 28, total loss: 46.911496\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 29, total loss: 46.842618\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 30, total loss: 47.171733\n",
      "total_test_accuracy: tensor(0.4167, device='cuda:0')\n",
      "Training Epoch: 31, total loss: 46.787640\n",
      "total_test_accuracy: tensor(0.4444, device='cuda:0')\n",
      "Training Epoch: 32, total loss: 46.940118\n",
      "total_test_accuracy: tensor(0.4583, device='cuda:0')\n",
      "Training Epoch: 33, total loss: 47.631998\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 34, total loss: 47.367173\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 35, total loss: 46.693433\n",
      "total_test_accuracy: tensor(0.4167, device='cuda:0')\n",
      "Training Epoch: 36, total loss: 46.593383\n",
      "total_test_accuracy: tensor(0.3611, device='cuda:0')\n",
      "Training Epoch: 37, total loss: 47.339302\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 38, total loss: 46.585579\n",
      "total_test_accuracy: tensor(0.4722, device='cuda:0')\n",
      "Training Epoch: 39, total loss: 46.698185\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 40, total loss: 47.145619\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 41, total loss: 46.683666\n",
      "total_test_accuracy: tensor(0.4722, device='cuda:0')\n",
      "Training Epoch: 42, total loss: 46.623753\n",
      "total_test_accuracy: tensor(0.4444, device='cuda:0')\n",
      "Training Epoch: 43, total loss: 46.794884\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 44, total loss: 46.915160\n",
      "total_test_accuracy: tensor(0.4722, device='cuda:0')\n",
      "Training Epoch: 45, total loss: 46.607681\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 46, total loss: 46.156929\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 47, total loss: 46.271667\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 48, total loss: 46.290350\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 49, total loss: 46.615680\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 50, total loss: 45.763103\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 51, total loss: 46.402305\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 52, total loss: 46.157175\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 53, total loss: 46.594658\n",
      "total_test_accuracy: tensor(0.4583, device='cuda:0')\n",
      "Training Epoch: 54, total loss: 46.799789\n",
      "total_test_accuracy: tensor(0.4167, device='cuda:0')\n",
      "Training Epoch: 55, total loss: 46.203152\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 56, total loss: 46.125795\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 57, total loss: 46.516699\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 58, total loss: 46.180364\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 59, total loss: 46.575106\n",
      "total_test_accuracy: tensor(0.4722, device='cuda:0')\n",
      "Training Epoch: 60, total loss: 46.465449\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 61, total loss: 46.001289\n",
      "total_test_accuracy: tensor(0.4444, device='cuda:0')\n",
      "Training Epoch: 62, total loss: 46.071345\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 63, total loss: 46.173963\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 64, total loss: 46.032386\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 65, total loss: 45.865474\n",
      "total_test_accuracy: tensor(0.4722, device='cuda:0')\n",
      "Training Epoch: 66, total loss: 46.566269\n",
      "total_test_accuracy: tensor(0.4583, device='cuda:0')\n",
      "Training Epoch: 67, total loss: 46.049713\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 68, total loss: 46.353940\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 69, total loss: 45.952263\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 70, total loss: 46.337213\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 71, total loss: 46.395761\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 72, total loss: 46.258031\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 73, total loss: 46.091262\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 74, total loss: 46.083113\n",
      "total_test_accuracy: tensor(0.4722, device='cuda:0')\n",
      "Training Epoch: 75, total loss: 45.813878\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 76, total loss: 46.482608\n",
      "total_test_accuracy: tensor(0.4306, device='cuda:0')\n",
      "Training Epoch: 77, total loss: 46.100421\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 78, total loss: 45.926658\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 79, total loss: 45.480500\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 80, total loss: 45.667003\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 81, total loss: 45.343937\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 82, total loss: 46.155299\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 83, total loss: 45.448886\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 84, total loss: 45.187066\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 85, total loss: 45.426817\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 86, total loss: 46.239428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 87, total loss: 45.662748\n",
      "total_test_accuracy: tensor(0.4444, device='cuda:0')\n",
      "Training Epoch: 88, total loss: 45.814714\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 89, total loss: 46.405917\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 90, total loss: 46.244954\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 91, total loss: 46.035038\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 92, total loss: 45.293866\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 93, total loss: 45.459221\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 94, total loss: 45.533626\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 95, total loss: 45.336321\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 96, total loss: 45.283517\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 97, total loss: 45.808309\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 98, total loss: 46.084026\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 99, total loss: 45.897823\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 100, total loss: 45.897517\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 101, total loss: 46.106162\n",
      "total_test_accuracy: tensor(0.4722, device='cuda:0')\n",
      "Training Epoch: 102, total loss: 45.314577\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 103, total loss: 45.736123\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 104, total loss: 45.700939\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 105, total loss: 45.916123\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 106, total loss: 45.264225\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 107, total loss: 46.069554\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 108, total loss: 45.446503\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 109, total loss: 45.294985\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 110, total loss: 45.400594\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 111, total loss: 45.807644\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 112, total loss: 45.206842\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 113, total loss: 46.011756\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 114, total loss: 45.828215\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 115, total loss: 45.504363\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 116, total loss: 45.672379\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 117, total loss: 45.211549\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 118, total loss: 44.503694\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 119, total loss: 45.318316\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 120, total loss: 45.340976\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 121, total loss: 45.007083\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 122, total loss: 45.571974\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 123, total loss: 44.667224\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 124, total loss: 45.905243\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 125, total loss: 45.843438\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 126, total loss: 45.653700\n",
      "total_test_accuracy: tensor(0.4167, device='cuda:0')\n",
      "Training Epoch: 127, total loss: 45.689542\n",
      "total_test_accuracy: tensor(0.4444, device='cuda:0')\n",
      "Training Epoch: 128, total loss: 45.194392\n",
      "total_test_accuracy: tensor(0.4583, device='cuda:0')\n",
      "Training Epoch: 129, total loss: 45.205465\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 130, total loss: 45.843009\n",
      "total_test_accuracy: tensor(0.4722, device='cuda:0')\n",
      "Training Epoch: 131, total loss: 45.234732\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 132, total loss: 45.219508\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 133, total loss: 45.347134\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 134, total loss: 45.315875\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 135, total loss: 45.183099\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 136, total loss: 45.567857\n",
      "total_test_accuracy: tensor(0.4722, device='cuda:0')\n",
      "Training Epoch: 137, total loss: 46.025147\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 138, total loss: 45.594153\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 139, total loss: 45.158056\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 140, total loss: 45.367907\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 141, total loss: 45.065148\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 142, total loss: 45.531714\n",
      "total_test_accuracy: tensor(0.4444, device='cuda:0')\n",
      "Training Epoch: 143, total loss: 45.184519\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 144, total loss: 45.025008\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 145, total loss: 45.440361\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 146, total loss: 45.540825\n",
      "total_test_accuracy: tensor(0.4722, device='cuda:0')\n",
      "Training Epoch: 147, total loss: 45.063483\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 148, total loss: 45.951737\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 149, total loss: 45.004616\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 150, total loss: 45.401840\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 151, total loss: 45.187623\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 152, total loss: 45.222038\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 153, total loss: 45.118400\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 154, total loss: 45.486181\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 155, total loss: 45.382527\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 156, total loss: 45.135982\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 157, total loss: 45.111511\n",
      "total_test_accuracy: tensor(0.6528, device='cuda:0')\n",
      "Training Epoch: 158, total loss: 45.303748\n",
      "total_test_accuracy: tensor(0.6389, device='cuda:0')\n",
      "Training Epoch: 159, total loss: 45.603323\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 160, total loss: 45.685596\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 161, total loss: 45.419303\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 162, total loss: 45.342693\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 163, total loss: 45.366498\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 164, total loss: 44.771350\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 165, total loss: 44.819084\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 166, total loss: 45.186424\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 167, total loss: 44.843727\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 168, total loss: 44.904036\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 169, total loss: 45.505778\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 170, total loss: 44.830665\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 171, total loss: 45.006478\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 172, total loss: 46.225199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 173, total loss: 45.912659\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 174, total loss: 45.672644\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 175, total loss: 46.034121\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 176, total loss: 45.374912\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 177, total loss: 45.041681\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 178, total loss: 45.448885\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 179, total loss: 45.422914\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 180, total loss: 44.911497\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 181, total loss: 45.391086\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 182, total loss: 45.404096\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 183, total loss: 45.174862\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 184, total loss: 44.922421\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 185, total loss: 45.084484\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 186, total loss: 45.465418\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 187, total loss: 44.959767\n",
      "total_test_accuracy: tensor(0.4722, device='cuda:0')\n",
      "Training Epoch: 188, total loss: 45.175902\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 189, total loss: 45.444553\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 190, total loss: 45.130184\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 191, total loss: 45.596123\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 192, total loss: 45.186207\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 193, total loss: 45.919514\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 194, total loss: 45.399518\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 195, total loss: 45.304414\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 196, total loss: 44.964579\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 197, total loss: 45.519723\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 198, total loss: 45.134365\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 199, total loss: 45.445453\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 200, total loss: 45.191770\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 201, total loss: 44.670321\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 202, total loss: 44.654940\n",
      "total_test_accuracy: tensor(0.4583, device='cuda:0')\n",
      "Training Epoch: 203, total loss: 45.159044\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 204, total loss: 45.133941\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 205, total loss: 45.477319\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 206, total loss: 44.622183\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 207, total loss: 44.666529\n",
      "total_test_accuracy: tensor(0.6944, device='cuda:0')\n",
      "Training Epoch: 208, total loss: 44.460745\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 209, total loss: 44.829015\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 210, total loss: 44.803589\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 211, total loss: 45.141441\n",
      "total_test_accuracy: tensor(0.4722, device='cuda:0')\n",
      "Training Epoch: 212, total loss: 44.635657\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 213, total loss: 44.977591\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 214, total loss: 45.175903\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 215, total loss: 45.044007\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 216, total loss: 45.208660\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 217, total loss: 45.322452\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 218, total loss: 44.261340\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 219, total loss: 44.854387\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 220, total loss: 44.994231\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 221, total loss: 45.125540\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 222, total loss: 44.838710\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 223, total loss: 45.040775\n",
      "total_test_accuracy: tensor(0.6389, device='cuda:0')\n",
      "Training Epoch: 224, total loss: 44.694902\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 225, total loss: 44.786329\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 226, total loss: 45.220298\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 227, total loss: 44.945376\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 228, total loss: 45.020417\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 229, total loss: 45.497710\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 230, total loss: 44.653017\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 231, total loss: 44.877208\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 232, total loss: 44.826479\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 233, total loss: 46.092939\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 234, total loss: 45.534526\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 235, total loss: 45.521107\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 236, total loss: 45.408948\n",
      "total_test_accuracy: tensor(0.3889, device='cuda:0')\n",
      "Training Epoch: 237, total loss: 45.187337\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 238, total loss: 45.010154\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 239, total loss: 44.610195\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 240, total loss: 45.006506\n",
      "total_test_accuracy: tensor(0.4583, device='cuda:0')\n",
      "Training Epoch: 241, total loss: 45.048118\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 242, total loss: 45.005176\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 243, total loss: 44.344380\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 244, total loss: 44.713267\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 245, total loss: 44.672827\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 246, total loss: 44.250022\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 247, total loss: 44.892168\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 248, total loss: 45.080596\n",
      "total_test_accuracy: tensor(0.4583, device='cuda:0')\n",
      "Training Epoch: 249, total loss: 44.872856\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 250, total loss: 44.848962\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 251, total loss: 44.736845\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 252, total loss: 45.092168\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 253, total loss: 44.966490\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 254, total loss: 44.554589\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 255, total loss: 44.732532\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 256, total loss: 45.162987\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 257, total loss: 44.931628\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 258, total loss: 44.223554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 259, total loss: 44.345855\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 260, total loss: 44.975292\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 261, total loss: 45.325206\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 262, total loss: 45.446461\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 263, total loss: 44.834228\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 264, total loss: 45.006072\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 265, total loss: 44.417595\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 266, total loss: 45.086321\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 267, total loss: 44.447248\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 268, total loss: 44.729546\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 269, total loss: 45.294925\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 270, total loss: 46.108159\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 271, total loss: 44.705678\n",
      "total_test_accuracy: tensor(0.4444, device='cuda:0')\n",
      "Training Epoch: 272, total loss: 45.473782\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 273, total loss: 44.860741\n",
      "total_test_accuracy: tensor(0.4583, device='cuda:0')\n",
      "Training Epoch: 274, total loss: 44.926671\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 275, total loss: 44.875115\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 276, total loss: 45.380337\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 277, total loss: 44.995120\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 278, total loss: 44.744505\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 279, total loss: 44.885999\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 280, total loss: 45.376503\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 281, total loss: 44.500541\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 282, total loss: 44.763702\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 283, total loss: 44.929396\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 284, total loss: 45.275182\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 285, total loss: 44.994832\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 286, total loss: 45.394145\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 287, total loss: 44.894709\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 288, total loss: 45.459081\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 289, total loss: 44.749587\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 290, total loss: 44.671534\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 291, total loss: 44.355162\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 292, total loss: 44.686089\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 293, total loss: 44.433982\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 294, total loss: 44.973007\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 295, total loss: 44.352630\n",
      "total_test_accuracy: tensor(0.4167, device='cuda:0')\n",
      "Training Epoch: 296, total loss: 45.006765\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 297, total loss: 45.068569\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 298, total loss: 44.996828\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 299, total loss: 44.812892\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 300, total loss: 45.517142\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 301, total loss: 44.352719\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 302, total loss: 44.208056\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 303, total loss: 44.831197\n",
      "total_test_accuracy: tensor(0.4583, device='cuda:0')\n",
      "Training Epoch: 304, total loss: 45.007637\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 305, total loss: 45.036272\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 306, total loss: 44.992278\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 307, total loss: 45.016964\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 308, total loss: 44.821463\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 309, total loss: 45.199997\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 310, total loss: 44.720705\n",
      "total_test_accuracy: tensor(0.4306, device='cuda:0')\n",
      "Training Epoch: 311, total loss: 45.025925\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 312, total loss: 45.180262\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 313, total loss: 44.810852\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 314, total loss: 45.059104\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 315, total loss: 44.482807\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 316, total loss: 45.271640\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 317, total loss: 44.691005\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 318, total loss: 44.632850\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 319, total loss: 45.002763\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 320, total loss: 44.978124\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 321, total loss: 45.174205\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 322, total loss: 44.883332\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 323, total loss: 44.553562\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 324, total loss: 45.395838\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 325, total loss: 44.584227\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 326, total loss: 44.767672\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 327, total loss: 44.774784\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 328, total loss: 44.270967\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 329, total loss: 44.385173\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 330, total loss: 44.409663\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 331, total loss: 44.310489\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 332, total loss: 44.342419\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 333, total loss: 44.604037\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 334, total loss: 44.780824\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 335, total loss: 44.364175\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 336, total loss: 44.719341\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 337, total loss: 44.662783\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 338, total loss: 44.810969\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 339, total loss: 44.617182\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 340, total loss: 44.845775\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 341, total loss: 44.503950\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 342, total loss: 44.618948\n",
      "total_test_accuracy: tensor(0.4583, device='cuda:0')\n",
      "Training Epoch: 343, total loss: 44.248448\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 344, total loss: 44.922213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 345, total loss: 44.801348\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 346, total loss: 45.191727\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 347, total loss: 44.574747\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 348, total loss: 44.147812\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 349, total loss: 44.347067\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 350, total loss: 45.151695\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 351, total loss: 44.680225\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 352, total loss: 45.068105\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 353, total loss: 44.065200\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 354, total loss: 44.655764\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 355, total loss: 45.205207\n",
      "total_test_accuracy: tensor(0.4722, device='cuda:0')\n",
      "Training Epoch: 356, total loss: 44.847401\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 357, total loss: 44.671811\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 358, total loss: 44.343615\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 359, total loss: 44.073419\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 360, total loss: 44.859567\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 361, total loss: 44.790558\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 362, total loss: 44.152220\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 363, total loss: 44.100075\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 364, total loss: 45.029701\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 365, total loss: 44.543006\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 366, total loss: 44.891214\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 367, total loss: 45.067544\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 368, total loss: 44.786499\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 369, total loss: 44.857204\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 370, total loss: 44.815189\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 371, total loss: 44.957390\n",
      "total_test_accuracy: tensor(0.4722, device='cuda:0')\n",
      "Training Epoch: 372, total loss: 44.739516\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 373, total loss: 44.586303\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 374, total loss: 44.622648\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 375, total loss: 45.283153\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 376, total loss: 44.909543\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 377, total loss: 45.208885\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 378, total loss: 44.703614\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 379, total loss: 43.938457\n",
      "total_test_accuracy: tensor(0.6389, device='cuda:0')\n",
      "Training Epoch: 380, total loss: 44.229072\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 381, total loss: 44.543181\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 382, total loss: 44.849710\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 383, total loss: 44.653034\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 384, total loss: 44.425055\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 385, total loss: 44.385339\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 386, total loss: 44.873716\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 387, total loss: 44.838832\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 388, total loss: 44.634476\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 389, total loss: 44.531575\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 390, total loss: 44.960175\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 391, total loss: 44.076791\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 392, total loss: 44.351209\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 393, total loss: 44.416795\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 394, total loss: 44.125148\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 395, total loss: 44.492233\n",
      "total_test_accuracy: tensor(0.6389, device='cuda:0')\n",
      "Training Epoch: 396, total loss: 44.678692\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 397, total loss: 44.739348\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 398, total loss: 44.016441\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 399, total loss: 44.367283\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 400, total loss: 44.071883\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 401, total loss: 44.630741\n",
      "total_test_accuracy: tensor(0.6528, device='cuda:0')\n",
      "Training Epoch: 402, total loss: 44.713318\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 403, total loss: 44.593755\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 404, total loss: 44.703339\n",
      "total_test_accuracy: tensor(0.6389, device='cuda:0')\n",
      "Training Epoch: 405, total loss: 44.538892\n",
      "total_test_accuracy: tensor(0.6528, device='cuda:0')\n",
      "Training Epoch: 406, total loss: 44.713600\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 407, total loss: 44.281715\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 408, total loss: 44.274029\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 409, total loss: 44.376026\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 410, total loss: 44.386105\n",
      "total_test_accuracy: tensor(0.6944, device='cuda:0')\n",
      "Training Epoch: 411, total loss: 44.123602\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 412, total loss: 44.555410\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 413, total loss: 44.509136\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 414, total loss: 44.854602\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 415, total loss: 44.733414\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 416, total loss: 44.785752\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 417, total loss: 44.761594\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 418, total loss: 44.499523\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 419, total loss: 44.255928\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 420, total loss: 44.821821\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 421, total loss: 43.996114\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 422, total loss: 44.442429\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 423, total loss: 43.980309\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 424, total loss: 44.493199\n",
      "total_test_accuracy: tensor(0.4722, device='cuda:0')\n",
      "Training Epoch: 425, total loss: 44.454115\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 426, total loss: 44.642561\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 427, total loss: 43.736349\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 428, total loss: 44.124415\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 429, total loss: 44.518869\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 430, total loss: 43.951379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 431, total loss: 44.613971\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 432, total loss: 44.031946\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 433, total loss: 44.527723\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 434, total loss: 44.212567\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 435, total loss: 44.335607\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 436, total loss: 44.065406\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 437, total loss: 43.764662\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 438, total loss: 43.798785\n",
      "total_test_accuracy: tensor(0.6528, device='cuda:0')\n",
      "Training Epoch: 439, total loss: 44.770326\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 440, total loss: 44.789486\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 441, total loss: 44.557816\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 442, total loss: 44.119525\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 443, total loss: 44.464403\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 444, total loss: 44.004802\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 445, total loss: 44.358992\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 446, total loss: 43.581591\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 447, total loss: 43.903659\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 448, total loss: 44.316148\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 449, total loss: 43.930820\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 450, total loss: 44.754757\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 451, total loss: 44.211666\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 452, total loss: 44.413228\n",
      "total_test_accuracy: tensor(0.6389, device='cuda:0')\n",
      "Training Epoch: 453, total loss: 44.346622\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 454, total loss: 44.476553\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 455, total loss: 44.664057\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 456, total loss: 44.489624\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 457, total loss: 44.974522\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 458, total loss: 44.501419\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 459, total loss: 44.786936\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 460, total loss: 44.389836\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 461, total loss: 44.195571\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 462, total loss: 44.678191\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 463, total loss: 44.308035\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 464, total loss: 44.112720\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 465, total loss: 44.350709\n",
      "total_test_accuracy: tensor(0.4444, device='cuda:0')\n",
      "Training Epoch: 466, total loss: 44.036430\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 467, total loss: 44.268506\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 468, total loss: 44.464398\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 469, total loss: 44.462522\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 470, total loss: 44.502607\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 471, total loss: 43.901004\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 472, total loss: 44.248591\n",
      "total_test_accuracy: tensor(0.6528, device='cuda:0')\n",
      "Training Epoch: 473, total loss: 44.247831\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 474, total loss: 44.258489\n",
      "total_test_accuracy: tensor(0.6389, device='cuda:0')\n",
      "Training Epoch: 475, total loss: 44.355626\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 476, total loss: 44.369308\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 477, total loss: 44.259581\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 478, total loss: 44.486818\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 479, total loss: 44.397569\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 480, total loss: 44.565821\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 481, total loss: 43.863096\n",
      "total_test_accuracy: tensor(0.6528, device='cuda:0')\n",
      "Training Epoch: 482, total loss: 43.932217\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 483, total loss: 44.759095\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 484, total loss: 44.481794\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 485, total loss: 43.970569\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 486, total loss: 43.824255\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 487, total loss: 43.290583\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 488, total loss: 43.833321\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 489, total loss: 44.183867\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 490, total loss: 43.649381\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 491, total loss: 44.142391\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 492, total loss: 44.013157\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 493, total loss: 44.478740\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 494, total loss: 44.010168\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 495, total loss: 43.914905\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 496, total loss: 44.058153\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 497, total loss: 44.249329\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 498, total loss: 44.437834\n",
      "total_test_accuracy: tensor(0.6528, device='cuda:0')\n",
      "Training Epoch: 499, total loss: 44.721539\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 500, total loss: 44.260987\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 501, total loss: 44.676480\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 502, total loss: 44.154939\n",
      "total_test_accuracy: tensor(0.6389, device='cuda:0')\n",
      "Training Epoch: 503, total loss: 44.215333\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 504, total loss: 44.276545\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 505, total loss: 43.515899\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 506, total loss: 44.346453\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 507, total loss: 43.836027\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 508, total loss: 44.259392\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 509, total loss: 43.702691\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 510, total loss: 43.781503\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 511, total loss: 44.485469\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 512, total loss: 44.211255\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 513, total loss: 44.240106\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 514, total loss: 43.716615\n",
      "total_test_accuracy: tensor(0.6389, device='cuda:0')\n",
      "Training Epoch: 515, total loss: 43.761324\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 516, total loss: 44.508613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 517, total loss: 44.213824\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 518, total loss: 44.177813\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 519, total loss: 44.072635\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 520, total loss: 43.667587\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 521, total loss: 43.766877\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 522, total loss: 44.860714\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 523, total loss: 44.262980\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 524, total loss: 44.491594\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 525, total loss: 44.032846\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 526, total loss: 44.430565\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 527, total loss: 43.886675\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 528, total loss: 44.221069\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 529, total loss: 44.497285\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 530, total loss: 44.600738\n",
      "total_test_accuracy: tensor(0.7083, device='cuda:0')\n",
      "Training Epoch: 531, total loss: 43.924874\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 532, total loss: 43.435118\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 533, total loss: 44.579836\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 534, total loss: 44.385090\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 535, total loss: 44.966397\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 536, total loss: 44.146180\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 537, total loss: 44.173229\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 538, total loss: 44.209321\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 539, total loss: 44.073453\n",
      "total_test_accuracy: tensor(0.6528, device='cuda:0')\n",
      "Training Epoch: 540, total loss: 44.197263\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 541, total loss: 44.785511\n",
      "total_test_accuracy: tensor(0.6667, device='cuda:0')\n",
      "Training Epoch: 542, total loss: 44.088408\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 543, total loss: 44.218042\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 544, total loss: 44.960788\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 545, total loss: 44.855893\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 546, total loss: 44.256862\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 547, total loss: 44.472475\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 548, total loss: 44.230274\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 549, total loss: 44.353501\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 550, total loss: 44.928259\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 551, total loss: 44.338115\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 552, total loss: 44.441638\n",
      "total_test_accuracy: tensor(0.6667, device='cuda:0')\n",
      "Training Epoch: 553, total loss: 44.227768\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 554, total loss: 44.586053\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 555, total loss: 44.617835\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 556, total loss: 44.431430\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 557, total loss: 44.742703\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 558, total loss: 44.424297\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 559, total loss: 44.054961\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 560, total loss: 44.363098\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 561, total loss: 44.102621\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 562, total loss: 44.233621\n",
      "total_test_accuracy: tensor(0.6806, device='cuda:0')\n",
      "Training Epoch: 563, total loss: 44.338491\n",
      "total_test_accuracy: tensor(0.6806, device='cuda:0')\n",
      "Training Epoch: 564, total loss: 43.943024\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 565, total loss: 43.553844\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 566, total loss: 43.453021\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 567, total loss: 43.691674\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 568, total loss: 44.278022\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 569, total loss: 43.665379\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 570, total loss: 43.497277\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 571, total loss: 43.676781\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 572, total loss: 44.343242\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 573, total loss: 44.488835\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 574, total loss: 44.417989\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 575, total loss: 44.347805\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 576, total loss: 43.858299\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 577, total loss: 43.889259\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 578, total loss: 43.849989\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 579, total loss: 43.693576\n",
      "total_test_accuracy: tensor(0.6528, device='cuda:0')\n",
      "Training Epoch: 580, total loss: 44.500808\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 581, total loss: 44.069327\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 582, total loss: 43.795407\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 583, total loss: 44.116997\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 584, total loss: 44.200849\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 585, total loss: 44.007253\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 586, total loss: 43.810462\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 587, total loss: 44.326220\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 588, total loss: 44.305691\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 589, total loss: 44.438595\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 590, total loss: 44.264339\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 591, total loss: 44.407531\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 592, total loss: 43.634128\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 593, total loss: 43.562036\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 594, total loss: 44.090627\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 595, total loss: 43.951563\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 596, total loss: 44.030550\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 597, total loss: 43.944718\n",
      "total_test_accuracy: tensor(0.6667, device='cuda:0')\n",
      "Training Epoch: 598, total loss: 44.280342\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 599, total loss: 44.177766\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 600, total loss: 44.302246\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 601, total loss: 43.990632\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 602, total loss: 44.284582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 603, total loss: 44.567420\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 604, total loss: 44.452733\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 605, total loss: 43.995176\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 606, total loss: 45.060288\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 607, total loss: 44.245773\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 608, total loss: 44.409291\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 609, total loss: 44.432697\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 610, total loss: 44.056222\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 611, total loss: 44.006548\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 612, total loss: 44.333057\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 613, total loss: 44.623438\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 614, total loss: 44.215284\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 615, total loss: 44.283114\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 616, total loss: 44.156835\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 617, total loss: 43.729912\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 618, total loss: 44.568117\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 619, total loss: 43.967551\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 620, total loss: 44.189466\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 621, total loss: 44.424247\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 622, total loss: 43.953168\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 623, total loss: 44.072518\n",
      "total_test_accuracy: tensor(0.4861, device='cuda:0')\n",
      "Training Epoch: 624, total loss: 44.094869\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 625, total loss: 43.987012\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 626, total loss: 44.669760\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 627, total loss: 43.864981\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 628, total loss: 44.327482\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 629, total loss: 43.877516\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 630, total loss: 43.690635\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 631, total loss: 44.056507\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 632, total loss: 43.594139\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 633, total loss: 44.108325\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 634, total loss: 43.703273\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 635, total loss: 43.849324\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 636, total loss: 43.979869\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 637, total loss: 43.753554\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 638, total loss: 44.150870\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 639, total loss: 44.170000\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 640, total loss: 44.137591\n",
      "total_test_accuracy: tensor(0.6667, device='cuda:0')\n",
      "Training Epoch: 641, total loss: 44.284788\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 642, total loss: 44.393478\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 643, total loss: 44.455238\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 644, total loss: 44.105677\n",
      "total_test_accuracy: tensor(0.6389, device='cuda:0')\n",
      "Training Epoch: 645, total loss: 43.659700\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 646, total loss: 44.065698\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 647, total loss: 43.973975\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 648, total loss: 44.136537\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 649, total loss: 43.098318\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 650, total loss: 43.888554\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 651, total loss: 43.957098\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 652, total loss: 43.761730\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 653, total loss: 43.576921\n",
      "total_test_accuracy: tensor(0.6528, device='cuda:0')\n",
      "Training Epoch: 654, total loss: 43.727669\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 655, total loss: 43.823859\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 656, total loss: 43.438262\n",
      "total_test_accuracy: tensor(0.6528, device='cuda:0')\n",
      "Training Epoch: 657, total loss: 43.802217\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 658, total loss: 44.328064\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 659, total loss: 44.013198\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 660, total loss: 43.740615\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 661, total loss: 43.915270\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 662, total loss: 43.871758\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 663, total loss: 44.343423\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 664, total loss: 43.743465\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 665, total loss: 44.601720\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 666, total loss: 43.948835\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 667, total loss: 44.171035\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 668, total loss: 43.625184\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 669, total loss: 43.892611\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 670, total loss: 44.141498\n",
      "total_test_accuracy: tensor(0.6389, device='cuda:0')\n",
      "Training Epoch: 671, total loss: 44.243416\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 672, total loss: 44.010030\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 673, total loss: 44.170645\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 674, total loss: 44.626902\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 675, total loss: 44.277889\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 676, total loss: 44.495632\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 677, total loss: 43.926943\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 678, total loss: 44.292764\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 679, total loss: 43.531934\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 680, total loss: 44.087999\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 681, total loss: 43.908697\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 682, total loss: 43.714131\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 683, total loss: 43.777223\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 684, total loss: 44.046988\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 685, total loss: 44.672500\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 686, total loss: 43.624321\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 687, total loss: 44.123413\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 688, total loss: 44.265321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_test_accuracy: tensor(0.6389, device='cuda:0')\n",
      "Training Epoch: 689, total loss: 43.519685\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 690, total loss: 44.048000\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 691, total loss: 44.054782\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 692, total loss: 44.636665\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 693, total loss: 44.494282\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 694, total loss: 44.262701\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 695, total loss: 43.504912\n",
      "total_test_accuracy: tensor(0.6389, device='cuda:0')\n",
      "Training Epoch: 696, total loss: 43.462380\n",
      "total_test_accuracy: tensor(0.5139, device='cuda:0')\n",
      "Training Epoch: 697, total loss: 43.763817\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 698, total loss: 44.908552\n",
      "total_test_accuracy: tensor(0.4583, device='cuda:0')\n",
      "Training Epoch: 699, total loss: 43.816012\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 700, total loss: 44.245631\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 701, total loss: 44.184751\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 702, total loss: 44.034502\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 703, total loss: 44.089277\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 704, total loss: 43.894692\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 705, total loss: 44.061333\n",
      "total_test_accuracy: tensor(0.4306, device='cuda:0')\n",
      "Training Epoch: 706, total loss: 43.692227\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 707, total loss: 43.804394\n",
      "total_test_accuracy: tensor(0.4722, device='cuda:0')\n",
      "Training Epoch: 708, total loss: 44.377536\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 709, total loss: 44.180355\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 710, total loss: 44.821614\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 711, total loss: 44.007128\n",
      "total_test_accuracy: tensor(0.6389, device='cuda:0')\n",
      "Training Epoch: 712, total loss: 43.785332\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 713, total loss: 44.152077\n",
      "total_test_accuracy: tensor(0.4722, device='cuda:0')\n",
      "Training Epoch: 714, total loss: 44.222184\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 715, total loss: 43.752816\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 716, total loss: 44.540651\n",
      "total_test_accuracy: tensor(0.6528, device='cuda:0')\n",
      "Training Epoch: 717, total loss: 43.850108\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 718, total loss: 44.125082\n",
      "total_test_accuracy: tensor(0.5694, device='cuda:0')\n",
      "Training Epoch: 719, total loss: 44.196972\n",
      "total_test_accuracy: tensor(0.5972, device='cuda:0')\n",
      "Training Epoch: 720, total loss: 44.969919\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 721, total loss: 44.167510\n",
      "total_test_accuracy: tensor(0.6389, device='cuda:0')\n",
      "Training Epoch: 722, total loss: 44.186830\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 723, total loss: 44.442182\n",
      "total_test_accuracy: tensor(0.6389, device='cuda:0')\n",
      "Training Epoch: 724, total loss: 44.130437\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 725, total loss: 44.117295\n",
      "total_test_accuracy: tensor(0.6389, device='cuda:0')\n",
      "Training Epoch: 726, total loss: 44.344488\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 727, total loss: 44.190426\n",
      "total_test_accuracy: tensor(0.4722, device='cuda:0')\n",
      "Training Epoch: 728, total loss: 44.160825\n",
      "total_test_accuracy: tensor(0.6250, device='cuda:0')\n",
      "Training Epoch: 729, total loss: 44.134162\n",
      "total_test_accuracy: tensor(0.5417, device='cuda:0')\n",
      "Training Epoch: 730, total loss: 44.387185\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 731, total loss: 43.970295\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 732, total loss: 44.093163\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 733, total loss: 44.005703\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 734, total loss: 44.121092\n",
      "total_test_accuracy: tensor(0.5278, device='cuda:0')\n",
      "Training Epoch: 735, total loss: 43.737176\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 736, total loss: 44.328643\n",
      "total_test_accuracy: tensor(0.6111, device='cuda:0')\n",
      "Training Epoch: 737, total loss: 43.621366\n",
      "total_test_accuracy: tensor(0.5556, device='cuda:0')\n",
      "Training Epoch: 738, total loss: 44.311777\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 739, total loss: 43.940408\n",
      "total_test_accuracy: tensor(0.5000, device='cuda:0')\n",
      "Training Epoch: 740, total loss: 44.111561\n",
      "total_test_accuracy: tensor(0.5833, device='cuda:0')\n",
      "Training Epoch: 741, total loss: 44.257281\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e924e4eeb05e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NFM:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnfm_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'dataset/xiaoguan/RF/RF_for_train/train_class_9/model/NFM/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-9ff3c11273ab>\u001b[0m in \u001b[0;36mtrain_data\u001b[0;34m(model, train_loader, test_loader, batch_size, model_path)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;31m#loss = loss_func(y_predict.view(-1), labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset,train_loader=prepare_dataset(nfm_config['train_data'],nfm_config['train_label'],nfm_config['batch_size'],nfm_config['n_class'])\n",
    "test_dataset,test_loader=prepare_dataset(nfm_config['test_data'],nfm_config['test_label'],nfm_config['batch_size'],nfm_config['n_class'])\n",
    "#from MLP import MLP\n",
    "from new_nfm_network import NFM\n",
    "#model=MLP(4224,1000,100,9)\n",
    "model=NFM(nfm_config)\n",
    "model.cuda()\n",
    "print(\"NFM:\",model)\n",
    "train_data(model,train_loader,test_loader,nfm_config['batch_size'],'dataset/xiaoguan/RF/RF_for_train/train_class_9/model/NFM/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d4b4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NFM(\n",
      "  (drop): Dropout(p=0.7, inplace=False)\n",
      "  (linear_model1): Linear(in_features=4224, out_features=1000, bias=True)\n",
      "  (BN_linear1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear_model2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (BN_linear2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (embedding_layers): Embedding(1001, 100)\n",
      "  (bi_pooling): BiInteractionPooling()\n",
      "  (dropout): Dropout(p=0.8, inplace=False)\n",
      "  (BN_bi): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dnn_layers): ModuleList(\n",
      "    (0): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (1): Linear(in_features=100, out_features=9, bias=True)\n",
      "  )\n",
      "  (dnn_softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NFM(\n",
       "  (drop): Dropout(p=0.7, inplace=False)\n",
       "  (linear_model1): Linear(in_features=4224, out_features=1000, bias=True)\n",
       "  (BN_linear1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear_model2): Linear(in_features=1000, out_features=100, bias=True)\n",
       "  (BN_linear2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (embedding_layers): Embedding(1001, 100)\n",
       "  (bi_pooling): BiInteractionPooling()\n",
       "  (dropout): Dropout(p=0.8, inplace=False)\n",
       "  (BN_bi): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dnn_layers): ModuleList(\n",
       "    (0): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (1): Linear(in_features=100, out_features=9, bias=True)\n",
       "  )\n",
       "  (dnn_softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from new_nfm_network import NFM\n",
    "#加载模型\n",
    "#from MLP import MLP\n",
    "#model=MLP(4224,1000,100,9)\n",
    "#model.cuda()\n",
    "path='dataset/xiaoguan/RF/RF_for_train/train_class_9/model/NMF1.pkl'\n",
    "nfm = NFM(nfm_config).cuda()\n",
    "#net=model.cuda()\n",
    "model=nfm\n",
    "print(model)\n",
    "#net = nn.DataParallel(net)\n",
    "#net = net.to(device)\n",
    "model.load_state_dict(torch.load(path),strict=False)\n",
    "model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f440bae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FMData\n",
      "FMData\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:103: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_test_accuracy: tensor(0.2222, device='cuda:0')\n",
      "Training Epoch: 0, total loss: 52.376367\n",
      "total_test_accuracy: tensor(0.3889, device='cuda:0')\n",
      "Training Epoch: 1, total loss: 50.846844\n",
      "total_test_accuracy: tensor(0.3889, device='cuda:0')\n",
      "Training Epoch: 2, total loss: 49.872056\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-51087302f96d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnfm_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnfm_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnfm_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnfm_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnfm_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'dataset/xiaoguan/RF/RF_for_train/train_class_9/model/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-9ff3c11273ab>\u001b[0m in \u001b[0;36mtrain_data\u001b[0;34m(model, train_loader, test_loader, batch_size, model_path)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;31m#print(\"y_predict:\",y_predict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;31m#loss = loss_func(y_predict.view(-1), labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NFM-pyorch-master/NFM-pyorch-master/new_nfm_network.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m#linear_output=self.BN_linear(linear_output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# 求出稀疏特征的embedding向量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0msparse_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0msparse_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NFM-pyorch-master/NFM-pyorch-master/new_nfm_network.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m#linear_output=self.BN_linear(linear_output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# 求出稀疏特征的embedding向量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0msparse_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0msparse_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_embeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/new_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1162\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_full_backward_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset,train_loader=prepare_dataset(nfm_config['train_data'],nfm_config['train_label'],nfm_config['batch_size'],nfm_config['n_class'])\n",
    "test_dataset,test_loader=prepare_dataset(nfm_config['test_data'],nfm_config['test_label'],nfm_config['batch_size'],nfm_config['n_class'])\n",
    "\n",
    "train_data(model,train_loader,test_loader,nfm_config['batch_size'],'dataset/xiaoguan/RF/RF_for_train/train_class_9/model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aa083a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f16744d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from new_nfm_network import NFM\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "\n",
    "\n",
    "def   train_data(model,data_loader,batch_size,model_path):\n",
    "    \n",
    "    BATCH_SIZE=batch_size\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "    total = 0\n",
    "    #loss_func = torch.nn.BCELoss()\n",
    "    loss_func=torch.nn.CrossEntropyLoss()\n",
    "    #loss_func=nn.MultiLabelSoftMarginLoss()\n",
    "    #loss_func=torch.nn.LogSoftmax()\n",
    "    \n",
    "    #model=nn.Softmax(nn.Linear(10149,16)).to(device)\n",
    "    # 从DataLoader中获取小批量的id以及数据\n",
    "    for epoch_id in range(1000):\n",
    "        correct=0\n",
    "        total=0\n",
    "        for batch_idx, (x, labels) in enumerate(data_loader):\n",
    "            x = Variable(x)\n",
    "            labels = Variable(labels)\n",
    "            \n",
    "            \n",
    "            #x = torch.tensor(x, dtype=torch.float)\n",
    "            #x=x.clone().detach().requires_grad_(True)\n",
    "            x=torch.tensor(x,dtype=torch.float)\n",
    "            labels=torch.tensor(labels,dtype=torch.float)\n",
    "            x, labels = x.cuda(), labels.cuda()\n",
    "            \n",
    "            #print(\"labels:\",labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_predict = model(x)\n",
    "            #print(\"y_predict:\",y_predict)\n",
    "            #loss = loss_func(y_predict.view(-1), labels)\n",
    "            loss = loss_func(y_predict, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss = loss.item()\n",
    "            #loss, predicted = self._train_single_batch(x, labels)\n",
    "\n",
    "            total += loss\n",
    "            \n",
    "            \n",
    "            \n",
    "            predicted = torch.max(y_predict.data,1)\n",
    "            #print(\"predicted:\",predicted)\n",
    "            predicted = torch.max(y_predict.data,1)[1]\n",
    "            \n",
    "            \n",
    "            labels=torch.max(labels,1)\n",
    "            #print(\"labels:\",labels)\n",
    "            labels=labels[1]\n",
    "            \n",
    "            correct += (predicted == labels).sum()\n",
    "            #print(\"correct:\",correct)\n",
    "            #correct=correct[0]\n",
    "            #print(\"new_correct:\",float(correct))\n",
    "            correct=float(correct)   \n",
    "            #if batch_idx % 10 == 0:\n",
    "            print(\"batch_idx:\",batch_idx)\n",
    "            print(correct/(BATCH_SIZE*(batch_idx+1)))\n",
    "            \n",
    "            \"\"\"\n",
    "            #y_predict.detach().numpy()\n",
    "            pred = y_predict\n",
    "            print(\"pred:\",pred.shape)\n",
    "            y=labels.clone().detach().requires_grad_(True)\n",
    "            print(\"y:\",y.shape)\n",
    "            #y=labels.data.cpu().numpy()\n",
    "            #y = labels.detach().numpy()\n",
    "            roc_auc_score(y, pred)\n",
    "            \"\"\"\n",
    "           \n",
    "            # print('[Training Epoch: {}] Batch: {}, Loss: {}'.format(epoch_id, batch_id, loss))\n",
    "        print(\"Training Epoch: %d, total loss: %f\" % (epoch_id, total))\n",
    "        #print(\"auc:\",roc_auc_score)\n",
    "    #功能：保存训练完的网络的各层参数（即weights和bias)\n",
    "    #path='dataset/xiaoguan/RF/RF_for_train/train_class_9/model/gene_4000_NFM.pkl'\n",
    "    \n",
    "    torch.save(model.state_dict(),model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1188ba86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row: 0 label: [0]\n",
      "row: 1 label: [0]\n",
      "row: 2 label: [0]\n",
      "row: 3 label: [0]\n",
      "row: 4 label: [0]\n",
      "row: 5 label: [0]\n",
      "row: 6 label: [0]\n",
      "row: 7 label: [0]\n",
      "row: 8 label: [0]\n",
      "row: 9 label: [0]\n",
      "row: 10 label: [0]\n",
      "row: 11 label: [0]\n",
      "row: 12 label: [0]\n",
      "row: 13 label: [0]\n",
      "row: 14 label: [0]\n",
      "row: 15 label: [0]\n",
      "row: 16 label: [0]\n",
      "row: 17 label: [0]\n",
      "row: 18 label: [0]\n",
      "row: 19 label: [0]\n",
      "row: 20 label: [0]\n",
      "row: 21 label: [0]\n",
      "row: 22 label: [0]\n",
      "row: 23 label: [0]\n",
      "row: 24 label: [0]\n",
      "row: 25 label: [0]\n",
      "row: 26 label: [0]\n",
      "row: 27 label: [0]\n",
      "row: 28 label: [0]\n",
      "row: 29 label: [0]\n",
      "row: 30 label: [0]\n",
      "row: 31 label: [0]\n",
      "row: 32 label: [0]\n",
      "row: 33 label: [0]\n",
      "row: 34 label: [0]\n",
      "row: 35 label: [0]\n",
      "row: 36 label: [0]\n",
      "row: 37 label: [0]\n",
      "row: 38 label: [0]\n",
      "row: 39 label: [0]\n",
      "row: 40 label: [0]\n",
      "row: 41 label: [0]\n",
      "row: 42 label: [0]\n",
      "row: 43 label: [1]\n",
      "row: 44 label: [1]\n",
      "row: 45 label: [1]\n",
      "row: 46 label: [1]\n",
      "row: 47 label: [1]\n",
      "row: 48 label: [1]\n",
      "row: 49 label: [1]\n",
      "row: 50 label: [1]\n",
      "row: 51 label: [1]\n",
      "row: 52 label: [1]\n",
      "row: 53 label: [1]\n",
      "row: 54 label: [1]\n",
      "row: 55 label: [1]\n",
      "row: 56 label: [1]\n",
      "row: 57 label: [1]\n",
      "row: 58 label: [1]\n",
      "row: 59 label: [1]\n",
      "row: 60 label: [1]\n",
      "row: 61 label: [1]\n",
      "row: 62 label: [1]\n",
      "row: 63 label: [1]\n",
      "row: 64 label: [1]\n",
      "row: 65 label: [1]\n",
      "row: 66 label: [1]\n",
      "row: 67 label: [1]\n",
      "row: 68 label: [1]\n",
      "row: 69 label: [1]\n",
      "row: 70 label: [1]\n",
      "row: 71 label: [1]\n",
      "row: 72 label: [1]\n",
      "row: 73 label: [1]\n",
      "row: 74 label: [1]\n",
      "row: 75 label: [1]\n",
      "row: 76 label: [1]\n",
      "row: 77 label: [1]\n",
      "row: 78 label: [1]\n",
      "row: 79 label: [1]\n",
      "row: 80 label: [1]\n",
      "row: 81 label: [1]\n",
      "row: 82 label: [1]\n",
      "row: 83 label: [1]\n",
      "row: 84 label: [1]\n",
      "row: 85 label: [1]\n",
      "row: 86 label: [1]\n",
      "row: 87 label: [1]\n",
      "row: 88 label: [1]\n",
      "row: 89 label: [1]\n",
      "row: 90 label: [1]\n",
      "row: 91 label: [1]\n",
      "row: 92 label: [1]\n",
      "row: 93 label: [1]\n",
      "row: 94 label: [1]\n",
      "row: 95 label: [1]\n",
      "row: 96 label: [1]\n",
      "row: 97 label: [1]\n",
      "row: 98 label: [1]\n",
      "row: 99 label: [1]\n",
      "row: 100 label: [1]\n",
      "row: 101 label: [1]\n",
      "row: 102 label: [1]\n",
      "row: 103 label: [1]\n",
      "row: 104 label: [1]\n",
      "row: 105 label: [1]\n",
      "row: 106 label: [1]\n",
      "row: 107 label: [1]\n",
      "row: 108 label: [1]\n",
      "row: 109 label: [1]\n",
      "row: 110 label: [1]\n",
      "row: 111 label: [1]\n",
      "row: 112 label: [1]\n",
      "row: 113 label: [1]\n",
      "row: 114 label: [1]\n",
      "row: 115 label: [1]\n",
      "row: 116 label: [1]\n",
      "row: 117 label: [1]\n",
      "row: 118 label: [1]\n",
      "row: 119 label: [1]\n",
      "row: 120 label: [1]\n",
      "row: 121 label: [1]\n",
      "row: 122 label: [1]\n",
      "row: 123 label: [1]\n",
      "row: 124 label: [1]\n",
      "row: 125 label: [1]\n",
      "row: 126 label: [1]\n",
      "row: 127 label: [1]\n",
      "row: 128 label: [1]\n",
      "row: 129 label: [1]\n",
      "row: 130 label: [1]\n",
      "row: 131 label: [1]\n",
      "row: 132 label: [1]\n",
      "row: 133 label: [1]\n",
      "row: 134 label: [1]\n",
      "row: 135 label: [1]\n",
      "row: 136 label: [1]\n",
      "row: 137 label: [1]\n",
      "row: 138 label: [1]\n",
      "row: 139 label: [1]\n",
      "row: 140 label: [1]\n",
      "row: 141 label: [1]\n",
      "row: 142 label: [1]\n",
      "row: 143 label: [1]\n",
      "row: 144 label: [1]\n",
      "row: 145 label: [1]\n",
      "row: 146 label: [2]\n",
      "row: 147 label: [2]\n",
      "row: 148 label: [2]\n",
      "row: 149 label: [2]\n",
      "row: 150 label: [2]\n",
      "row: 151 label: [2]\n",
      "row: 152 label: [2]\n",
      "row: 153 label: [2]\n",
      "row: 154 label: [2]\n",
      "row: 155 label: [2]\n",
      "row: 156 label: [2]\n",
      "row: 157 label: [2]\n",
      "row: 158 label: [2]\n",
      "row: 159 label: [2]\n",
      "row: 160 label: [2]\n",
      "row: 161 label: [2]\n",
      "row: 162 label: [2]\n",
      "row: 163 label: [2]\n",
      "row: 164 label: [2]\n",
      "row: 165 label: [2]\n",
      "row: 166 label: [2]\n",
      "row: 167 label: [2]\n",
      "row: 168 label: [2]\n",
      "row: 169 label: [2]\n",
      "row: 170 label: [2]\n",
      "row: 171 label: [2]\n",
      "row: 172 label: [2]\n",
      "row: 173 label: [2]\n",
      "row: 174 label: [2]\n",
      "row: 175 label: [2]\n",
      "row: 176 label: [2]\n",
      "row: 177 label: [2]\n",
      "row: 178 label: [2]\n",
      "row: 179 label: [2]\n",
      "row: 180 label: [2]\n",
      "row: 181 label: [2]\n",
      "row: 182 label: [2]\n",
      "row: 183 label: [2]\n",
      "row: 184 label: [2]\n",
      "row: 185 label: [2]\n",
      "row: 186 label: [2]\n",
      "row: 187 label: [2]\n",
      "row: 188 label: [2]\n",
      "row: 189 label: [2]\n",
      "row: 190 label: [2]\n",
      "row: 191 label: [2]\n",
      "row: 192 label: [2]\n",
      "row: 193 label: [2]\n",
      "row: 194 label: [2]\n",
      "row: 195 label: [2]\n",
      "row: 196 label: [2]\n",
      "row: 197 label: [2]\n",
      "row: 198 label: [2]\n",
      "row: 199 label: [2]\n",
      "row: 200 label: [3]\n",
      "row: 201 label: [3]\n",
      "row: 202 label: [3]\n",
      "row: 203 label: [3]\n",
      "row: 204 label: [3]\n",
      "row: 205 label: [3]\n",
      "row: 206 label: [3]\n",
      "row: 207 label: [3]\n",
      "row: 208 label: [3]\n",
      "row: 209 label: [3]\n",
      "row: 210 label: [3]\n",
      "row: 211 label: [3]\n",
      "row: 212 label: [3]\n",
      "row: 213 label: [3]\n",
      "row: 214 label: [3]\n",
      "row: 215 label: [3]\n",
      "row: 216 label: [3]\n",
      "row: 217 label: [3]\n",
      "row: 218 label: [3]\n",
      "row: 219 label: [3]\n",
      "row: 220 label: [3]\n",
      "row: 221 label: [3]\n",
      "row: 222 label: [3]\n",
      "row: 223 label: [3]\n",
      "row: 224 label: [3]\n",
      "row: 225 label: [3]\n",
      "row: 226 label: [3]\n",
      "row: 227 label: [3]\n",
      "row: 228 label: [3]\n",
      "row: 229 label: [3]\n",
      "row: 230 label: [3]\n",
      "row: 231 label: [3]\n",
      "row: 232 label: [3]\n",
      "row: 233 label: [3]\n",
      "row: 234 label: [3]\n",
      "row: 235 label: [3]\n",
      "row: 236 label: [3]\n",
      "row: 237 label: [3]\n",
      "row: 238 label: [3]\n",
      "row: 239 label: [3]\n",
      "row: 240 label: [3]\n",
      "row: 241 label: [3]\n",
      "row: 242 label: [3]\n",
      "row: 243 label: [3]\n",
      "row: 244 label: [3]\n",
      "row: 245 label: [3]\n",
      "row: 246 label: [3]\n",
      "row: 247 label: [3]\n",
      "row: 248 label: [3]\n",
      "row: 249 label: [3]\n",
      "row: 250 label: [3]\n",
      "row: 251 label: [3]\n",
      "row: 252 label: [3]\n",
      "row: 253 label: [3]\n",
      "row: 254 label: [3]\n",
      "row: 255 label: [3]\n",
      "row: 256 label: [3]\n",
      "row: 257 label: [3]\n",
      "row: 258 label: [3]\n",
      "row: 259 label: [3]\n",
      "row: 260 label: [3]\n",
      "row: 261 label: [3]\n",
      "row: 262 label: [3]\n",
      "row: 263 label: [3]\n",
      "row: 264 label: [3]\n",
      "row: 265 label: [3]\n",
      "row: 266 label: [3]\n",
      "row: 267 label: [3]\n",
      "row: 268 label: [3]\n",
      "row: 269 label: [3]\n",
      "row: 270 label: [3]\n",
      "row: 271 label: [3]\n",
      "row: 272 label: [3]\n",
      "row: 273 label: [4]\n",
      "row: 274 label: [4]\n",
      "row: 275 label: [4]\n",
      "row: 276 label: [4]\n",
      "row: 277 label: [4]\n",
      "row: 278 label: [4]\n",
      "row: 279 label: [4]\n",
      "row: 280 label: [4]\n",
      "row: 281 label: [4]\n",
      "row: 282 label: [4]\n",
      "row: 283 label: [4]\n",
      "row: 284 label: [4]\n",
      "row: 285 label: [4]\n",
      "row: 286 label: [4]\n",
      "row: 287 label: [4]\n",
      "row: 288 label: [4]\n",
      "row: 289 label: [4]\n",
      "row: 290 label: [4]\n",
      "row: 291 label: [4]\n",
      "row: 292 label: [4]\n",
      "row: 293 label: [4]\n",
      "row: 294 label: [4]\n",
      "row: 295 label: [4]\n",
      "row: 296 label: [5]\n",
      "row: 297 label: [5]\n",
      "row: 298 label: [5]\n",
      "row: 299 label: [5]\n",
      "row: 300 label: [5]\n",
      "row: 301 label: [5]\n",
      "row: 302 label: [5]\n",
      "row: 303 label: [5]\n",
      "row: 304 label: [5]\n",
      "row: 305 label: [5]\n",
      "row: 306 label: [5]\n",
      "row: 307 label: [5]\n",
      "row: 308 label: [5]\n",
      "row: 309 label: [5]\n",
      "row: 310 label: [5]\n",
      "row: 311 label: [5]\n",
      "row: 312 label: [5]\n",
      "row: 313 label: [5]\n",
      "row: 314 label: [5]\n",
      "row: 315 label: [5]\n",
      "row: 316 label: [5]\n",
      "row: 317 label: [5]\n",
      "row: 318 label: [5]\n",
      "row: 319 label: [5]\n",
      "row: 320 label: [5]\n",
      "row: 321 label: [5]\n",
      "row: 322 label: [5]\n",
      "row: 323 label: [5]\n",
      "row: 324 label: [5]\n",
      "row: 325 label: [5]\n",
      "row: 326 label: [6]\n",
      "row: 327 label: [6]\n",
      "row: 328 label: [6]\n",
      "row: 329 label: [6]\n",
      "row: 330 label: [6]\n",
      "row: 331 label: [6]\n",
      "row: 332 label: [6]\n",
      "row: 333 label: [6]\n",
      "row: 334 label: [6]\n",
      "row: 335 label: [6]\n",
      "row: 336 label: [6]\n",
      "row: 337 label: [6]\n",
      "row: 338 label: [6]\n",
      "row: 339 label: [6]\n",
      "row: 340 label: [6]\n",
      "row: 341 label: [6]\n",
      "row: 342 label: [6]\n",
      "row: 343 label: [6]\n",
      "row: 344 label: [6]\n",
      "row: 345 label: [6]\n",
      "row: 346 label: [6]\n",
      "row: 347 label: [6]\n",
      "row: 348 label: [6]\n",
      "row: 349 label: [6]\n",
      "row: 350 label: [6]\n",
      "row: 351 label: [6]\n",
      "row: 352 label: [6]\n",
      "row: 353 label: [6]\n",
      "row: 354 label: [6]\n",
      "row: 355 label: [6]\n",
      "row: 356 label: [6]\n",
      "row: 357 label: [6]\n",
      "row: 358 label: [6]\n",
      "row: 359 label: [6]\n",
      "row: 360 label: [6]\n",
      "row: 361 label: [6]\n",
      "row: 362 label: [6]\n",
      "row: 363 label: [6]\n",
      "row: 364 label: [6]\n",
      "row: 365 label: [6]\n",
      "row: 366 label: [6]\n",
      "row: 367 label: [6]\n",
      "row: 368 label: [6]\n",
      "row: 369 label: [6]\n",
      "row: 370 label: [6]\n",
      "row: 371 label: [6]\n",
      "row: 372 label: [6]\n",
      "row: 373 label: [6]\n",
      "row: 374 label: [6]\n",
      "row: 375 label: [6]\n",
      "row: 376 label: [6]\n",
      "row: 377 label: [6]\n",
      "row: 378 label: [6]\n",
      "row: 379 label: [6]\n",
      "row: 380 label: [6]\n",
      "row: 381 label: [6]\n",
      "row: 382 label: [6]\n",
      "row: 383 label: [6]\n",
      "row: 384 label: [6]\n",
      "row: 385 label: [6]\n",
      "row: 386 label: [6]\n",
      "row: 387 label: [6]\n",
      "row: 388 label: [6]\n",
      "row: 389 label: [6]\n",
      "row: 390 label: [6]\n",
      "row: 391 label: [6]\n",
      "row: 392 label: [6]\n",
      "row: 393 label: [6]\n",
      "row: 394 label: [6]\n",
      "row: 395 label: [6]\n",
      "row: 396 label: [6]\n",
      "row: 397 label: [6]\n",
      "row: 398 label: [6]\n",
      "row: 399 label: [6]\n",
      "row: 400 label: [6]\n",
      "row: 401 label: [6]\n",
      "row: 402 label: [6]\n",
      "row: 403 label: [6]\n",
      "row: 404 label: [6]\n",
      "row: 405 label: [6]\n",
      "row: 406 label: [6]\n",
      "row: 407 label: [6]\n",
      "row: 408 label: [6]\n",
      "row: 409 label: [6]\n",
      "row: 410 label: [6]\n",
      "row: 411 label: [6]\n",
      "row: 412 label: [6]\n",
      "row: 413 label: [6]\n",
      "row: 414 label: [6]\n",
      "row: 415 label: [6]\n",
      "row: 416 label: [6]\n",
      "row: 417 label: [6]\n",
      "row: 418 label: [6]\n",
      "row: 419 label: [6]\n",
      "row: 420 label: [6]\n",
      "row: 421 label: [6]\n",
      "row: 422 label: [6]\n",
      "row: 423 label: [6]\n",
      "row: 424 label: [6]\n",
      "row: 425 label: [6]\n",
      "row: 426 label: [6]\n",
      "row: 427 label: [6]\n",
      "row: 428 label: [6]\n",
      "row: 429 label: [6]\n",
      "row: 430 label: [6]\n",
      "row: 431 label: [6]\n",
      "row: 432 label: [6]\n",
      "row: 433 label: [6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row: 434 label: [6]\n",
      "row: 435 label: [6]\n",
      "row: 436 label: [6]\n",
      "row: 437 label: [6]\n",
      "row: 438 label: [6]\n",
      "row: 439 label: [6]\n",
      "row: 440 label: [6]\n",
      "row: 441 label: [6]\n",
      "row: 442 label: [6]\n",
      "row: 443 label: [6]\n",
      "row: 444 label: [6]\n",
      "row: 445 label: [6]\n",
      "row: 446 label: [6]\n",
      "row: 447 label: [6]\n",
      "row: 448 label: [6]\n",
      "row: 449 label: [6]\n",
      "row: 450 label: [6]\n",
      "row: 451 label: [6]\n",
      "row: 452 label: [6]\n",
      "row: 453 label: [6]\n",
      "row: 454 label: [6]\n",
      "row: 455 label: [6]\n",
      "row: 456 label: [6]\n",
      "row: 457 label: [6]\n",
      "row: 458 label: [6]\n",
      "row: 459 label: [6]\n",
      "row: 460 label: [6]\n",
      "row: 461 label: [6]\n",
      "row: 462 label: [6]\n",
      "row: 463 label: [6]\n",
      "row: 464 label: [6]\n",
      "row: 465 label: [6]\n",
      "row: 466 label: [6]\n",
      "row: 467 label: [6]\n",
      "row: 468 label: [6]\n",
      "row: 469 label: [6]\n",
      "row: 470 label: [6]\n",
      "row: 471 label: [6]\n",
      "row: 472 label: [6]\n",
      "row: 473 label: [6]\n",
      "row: 474 label: [6]\n",
      "row: 475 label: [6]\n",
      "row: 476 label: [6]\n",
      "row: 477 label: [6]\n",
      "row: 478 label: [6]\n",
      "row: 479 label: [7]\n",
      "row: 480 label: [7]\n",
      "row: 481 label: [7]\n",
      "row: 482 label: [7]\n",
      "row: 483 label: [7]\n",
      "row: 484 label: [7]\n",
      "row: 485 label: [7]\n",
      "row: 486 label: [7]\n",
      "row: 487 label: [7]\n",
      "row: 488 label: [7]\n",
      "row: 489 label: [7]\n",
      "row: 490 label: [7]\n",
      "row: 491 label: [7]\n",
      "row: 492 label: [7]\n",
      "row: 493 label: [7]\n",
      "row: 494 label: [7]\n",
      "row: 495 label: [7]\n",
      "row: 496 label: [7]\n",
      "row: 497 label: [7]\n",
      "row: 498 label: [7]\n",
      "row: 499 label: [7]\n",
      "row: 500 label: [7]\n",
      "row: 501 label: [7]\n",
      "row: 502 label: [7]\n",
      "row: 503 label: [7]\n",
      "row: 504 label: [7]\n",
      "row: 505 label: [7]\n",
      "row: 506 label: [7]\n",
      "row: 507 label: [7]\n",
      "row: 508 label: [7]\n",
      "row: 509 label: [7]\n",
      "row: 510 label: [7]\n",
      "row: 511 label: [7]\n",
      "row: 512 label: [7]\n",
      "row: 513 label: [7]\n",
      "row: 514 label: [7]\n",
      "row: 515 label: [7]\n",
      "row: 516 label: [8]\n",
      "row: 517 label: [8]\n",
      "row: 518 label: [8]\n",
      "row: 519 label: [8]\n",
      "row: 520 label: [8]\n",
      "row: 521 label: [8]\n",
      "row: 522 label: [8]\n",
      "row: 523 label: [8]\n",
      "row: 524 label: [8]\n",
      "row: 525 label: [8]\n",
      "row: 526 label: [8]\n",
      "row: 527 label: [8]\n",
      "row: 528 label: [8]\n",
      "row: 529 label: [8]\n",
      "row: 530 label: [8]\n",
      "row: 531 label: [8]\n",
      "row: 532 label: [8]\n",
      "row: 533 label: [8]\n",
      "row: 534 label: [8]\n",
      "row: 535 label: [8]\n",
      "row: 536 label: [8]\n",
      "row: 537 label: [8]\n",
      "row: 538 label: [8]\n",
      "row: 539 label: [8]\n",
      "row: 540 label: [8]\n",
      "row: 541 label: [8]\n",
      "row: 542 label: [8]\n",
      "row: 543 label: [8]\n",
      "row: 544 label: [8]\n",
      "row: 545 label: [8]\n",
      "row: 546 label: [8]\n",
      "row: 547 label: [8]\n",
      "row: 548 label: [8]\n",
      "row: 549 label: [8]\n",
      "row: 550 label: [8]\n",
      "row: 551 label: [8]\n",
      "row: 552 label: [8]\n",
      "row: 553 label: [8]\n",
      "row: 554 label: [8]\n",
      "row: 555 label: [8]\n",
      "row: 556 label: [8]\n",
      "row: 557 label: [8]\n",
      "row: 558 label: [8]\n",
      "row: 559 label: [8]\n",
      "row: 560 label: [8]\n",
      "row: 561 label: [8]\n",
      "row: 562 label: [8]\n",
      "row: 563 label: [8]\n",
      "row: 564 label: [8]\n",
      "row: 565 label: [8]\n",
      "row: 566 label: [8]\n",
      "row: 567 label: [8]\n",
      "row: 568 label: [8]\n",
      "row: 569 label: [8]\n",
      "row: 570 label: [8]\n",
      "row: 571 label: [8]\n",
      "row: 572 label: [8]\n",
      "row: 573 label: [8]\n",
      "row: 574 label: [8]\n",
      "row: 575 label: [8]\n",
      "row: 576 label: [8]\n",
      "row: 577 label: [8]\n",
      "row: 578 label: [8]\n",
      "row: 579 label: [8]\n",
      "row: 580 label: [8]\n",
      "row: 581 label: [8]\n",
      "row: 582 label: [8]\n",
      "row: 583 label: [8]\n",
      "row: 584 label: [8]\n",
      "row: 585 label: [8]\n",
      "row: 586 label: [8]\n",
      "row: 587 label: [8]\n",
      "row: 588 label: [8]\n",
      "row: 589 label: [8]\n",
      "row: 590 label: [8]\n",
      "row: 591 label: [8]\n",
      "row: 592 label: [8]\n",
      "row: 593 label: [8]\n",
      "row: 594 label: [8]\n",
      "row: 595 label: [8]\n",
      "row: 596 label: [8]\n",
      "row: 597 label: [8]\n",
      "row: 598 label: [8]\n",
      "row: 599 label: [8]\n",
      "row: 600 label: [8]\n",
      "row: 601 label: [8]\n",
      "row: 602 label: [8]\n",
      "label: [[0.82222223 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.02222222]\n",
      " ...\n",
      " [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 ... 0.02222222 0.02222222 0.82222223]]\n",
      "features: [[35 46 26 ... 30 27 28]\n",
      " [35 41 28 ... 32 26 29]\n",
      " [35 45 27 ... 31 27 30]\n",
      " ...\n",
      " [36 38 24 ... 14 35 32]\n",
      " [32 42 14 ... 10 29 36]\n",
      " [37 46 11 ... 13 27 33]]\n",
      "nfm: NFM(\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (linear_model1): Linear(in_features=4224, out_features=1000, bias=True)\n",
      "  (BN_linear1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear_model2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (BN_linear2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (embedding_layers): Embedding(1001, 100)\n",
      "  (bi_pooling): BiInteractionPooling()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (BN_bi): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dnn_layers): ModuleList(\n",
      "    (0): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (1): Linear(in_features=100, out_features=9, bias=True)\n",
      "  )\n",
      "  (dnn_softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.08333333333333333\n",
      "batch_idx: 1\n",
      "0.0625\n",
      "batch_idx: 2\n",
      "0.1111111111111111\n",
      "batch_idx: 3\n",
      "0.125\n",
      "batch_idx: 4\n",
      "0.13333333333333333\n",
      "batch_idx: 5\n",
      "0.125\n",
      "batch_idx: 6\n",
      "0.13690476190476192\n",
      "batch_idx: 7\n",
      "0.14583333333333334\n",
      "batch_idx: 8\n",
      "0.14351851851851852\n",
      "batch_idx: 9\n",
      "0.15833333333333333\n",
      "batch_idx: 10\n",
      "0.17424242424242425\n",
      "batch_idx: 11\n",
      "0.1909722222222222\n",
      "batch_idx: 12\n",
      "0.20512820512820512\n",
      "batch_idx: 13\n",
      "0.21428571428571427\n",
      "batch_idx: 14\n",
      "0.20555555555555555\n",
      "batch_idx: 15\n",
      "0.21354166666666666\n",
      "batch_idx: 16\n",
      "0.2107843137254902\n",
      "batch_idx: 17\n",
      "0.2199074074074074\n",
      "batch_idx: 18\n",
      "0.22587719298245615\n",
      "batch_idx: 19\n",
      "0.225\n",
      "batch_idx: 20\n",
      "0.2261904761904762\n",
      "batch_idx: 21\n",
      "0.23295454545454544\n",
      "batch_idx: 22\n",
      "0.2318840579710145\n",
      "batch_idx: 23\n",
      "0.22916666666666666\n",
      "batch_idx: 24\n",
      "0.23166666666666666\n",
      "Training Epoch: 0, total loss: 54.173424\n",
      "batch_idx: 0\n",
      "0.4166666666666667\n",
      "batch_idx: 1\n",
      "0.3541666666666667\n",
      "batch_idx: 2\n",
      "0.3472222222222222\n",
      "batch_idx: 3\n",
      "0.3333333333333333\n",
      "batch_idx: 4\n",
      "0.35\n",
      "batch_idx: 5\n",
      "0.3125\n",
      "batch_idx: 6\n",
      "0.30952380952380953\n",
      "batch_idx: 7\n",
      "0.3125\n",
      "batch_idx: 8\n",
      "0.30092592592592593\n",
      "batch_idx: 9\n",
      "0.3125\n",
      "batch_idx: 10\n",
      "0.3068181818181818\n",
      "batch_idx: 11\n",
      "0.3159722222222222\n",
      "batch_idx: 12\n",
      "0.3141025641025641\n",
      "batch_idx: 13\n",
      "0.30952380952380953\n",
      "batch_idx: 14\n",
      "0.3111111111111111\n",
      "batch_idx: 15\n",
      "0.3125\n",
      "batch_idx: 16\n",
      "0.3112745098039216\n",
      "batch_idx: 17\n",
      "0.3148148148148148\n",
      "batch_idx: 18\n",
      "0.31140350877192985\n",
      "batch_idx: 19\n",
      "0.31666666666666665\n",
      "batch_idx: 20\n",
      "0.31746031746031744\n",
      "batch_idx: 21\n",
      "0.32007575757575757\n",
      "batch_idx: 22\n",
      "0.322463768115942\n",
      "batch_idx: 23\n",
      "0.3142361111111111\n",
      "batch_idx: 24\n",
      "0.315\n",
      "Training Epoch: 1, total loss: 51.962046\n",
      "batch_idx: 0\n",
      "0.25\n",
      "batch_idx: 1\n",
      "0.375\n",
      "batch_idx: 2\n",
      "0.3194444444444444\n",
      "batch_idx: 3\n",
      "0.3229166666666667\n",
      "batch_idx: 4\n",
      "0.325\n",
      "batch_idx: 5\n",
      "0.3263888888888889\n",
      "batch_idx: 6\n",
      "0.3273809523809524\n",
      "batch_idx: 7\n",
      "0.3229166666666667\n",
      "batch_idx: 8\n",
      "0.3333333333333333\n",
      "batch_idx: 9\n",
      "0.3416666666666667\n",
      "batch_idx: 10\n",
      "0.3333333333333333\n",
      "batch_idx: 11\n",
      "0.3472222222222222\n",
      "batch_idx: 12\n",
      "0.34935897435897434\n",
      "batch_idx: 13\n",
      "0.35119047619047616\n",
      "batch_idx: 14\n",
      "0.3472222222222222\n",
      "batch_idx: 15\n",
      "0.3489583333333333\n",
      "batch_idx: 16\n",
      "0.35784313725490197\n",
      "batch_idx: 17\n",
      "0.3587962962962963\n",
      "batch_idx: 18\n",
      "0.3574561403508772\n",
      "batch_idx: 19\n",
      "0.36041666666666666\n",
      "batch_idx: 20\n",
      "0.3611111111111111\n",
      "batch_idx: 21\n",
      "0.36742424242424243\n",
      "batch_idx: 22\n",
      "0.3713768115942029\n",
      "batch_idx: 23\n",
      "0.3697916666666667\n",
      "batch_idx: 24\n",
      "0.37166666666666665\n",
      "Training Epoch: 2, total loss: 51.279136\n",
      "batch_idx: 0\n",
      "0.4166666666666667\n",
      "batch_idx: 1\n",
      "0.375\n",
      "batch_idx: 2\n",
      "0.2916666666666667\n",
      "batch_idx: 3\n",
      "0.3125\n",
      "batch_idx: 4\n",
      "0.3333333333333333\n",
      "batch_idx: 5\n",
      "0.3541666666666667\n",
      "batch_idx: 6\n",
      "0.36904761904761907\n",
      "batch_idx: 7\n",
      "0.3697916666666667\n",
      "batch_idx: 8\n",
      "0.36574074074074076\n",
      "batch_idx: 9\n",
      "0.3625\n",
      "batch_idx: 10\n",
      "0.375\n",
      "batch_idx: 11\n",
      "0.3680555555555556\n",
      "batch_idx: 12\n",
      "0.38461538461538464\n",
      "batch_idx: 13\n",
      "0.3958333333333333\n",
      "batch_idx: 14\n",
      "0.3888888888888889\n",
      "batch_idx: 15\n",
      "0.3932291666666667\n",
      "batch_idx: 16\n",
      "0.4019607843137255\n",
      "batch_idx: 17\n",
      "0.4074074074074074\n",
      "batch_idx: 18\n",
      "0.40131578947368424\n",
      "batch_idx: 19\n",
      "0.4083333333333333\n",
      "batch_idx: 20\n",
      "0.4107142857142857\n",
      "batch_idx: 21\n",
      "0.42045454545454547\n",
      "batch_idx: 22\n",
      "0.4221014492753623\n",
      "batch_idx: 23\n",
      "0.4149305555555556\n",
      "batch_idx: 24\n",
      "0.42333333333333334\n",
      "Training Epoch: 3, total loss: 50.565354\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.5625\n",
      "batch_idx: 2\n",
      "0.5138888888888888\n",
      "batch_idx: 3\n",
      "0.5729166666666666\n",
      "batch_idx: 4\n",
      "0.55\n",
      "batch_idx: 5\n",
      "0.5347222222222222\n",
      "batch_idx: 6\n",
      "0.5178571428571429\n",
      "batch_idx: 7\n",
      "0.53125\n",
      "batch_idx: 8\n",
      "0.5092592592592593\n",
      "batch_idx: 9\n",
      "0.4875\n",
      "batch_idx: 10\n",
      "0.48863636363636365\n",
      "batch_idx: 11\n",
      "0.4861111111111111\n",
      "batch_idx: 12\n",
      "0.47435897435897434\n",
      "batch_idx: 13\n",
      "0.4880952380952381\n",
      "batch_idx: 14\n",
      "0.4777777777777778\n",
      "batch_idx: 15\n",
      "0.4791666666666667\n",
      "batch_idx: 16\n",
      "0.46568627450980393\n",
      "batch_idx: 17\n",
      "0.4675925925925926\n",
      "batch_idx: 18\n",
      "0.46271929824561403\n",
      "batch_idx: 19\n",
      "0.4666666666666667\n",
      "batch_idx: 20\n",
      "0.4662698412698413\n",
      "batch_idx: 21\n",
      "0.4621212121212121\n",
      "batch_idx: 22\n",
      "0.463768115942029\n",
      "batch_idx: 23\n",
      "0.4670138888888889\n",
      "batch_idx: 24\n",
      "0.4633333333333333\n",
      "Training Epoch: 4, total loss: 49.721010\n",
      "batch_idx: 0\n",
      "0.4166666666666667\n",
      "batch_idx: 1\n",
      "0.4583333333333333\n",
      "batch_idx: 2\n",
      "0.4305555555555556\n",
      "batch_idx: 3\n",
      "0.40625\n",
      "batch_idx: 4\n",
      "0.4083333333333333\n",
      "batch_idx: 5\n",
      "0.3888888888888889\n",
      "batch_idx: 6\n",
      "0.4107142857142857\n",
      "batch_idx: 7\n",
      "0.4322916666666667\n",
      "batch_idx: 8\n",
      "0.4398148148148148\n",
      "batch_idx: 9\n",
      "0.4375\n",
      "batch_idx: 10\n",
      "0.4431818181818182\n",
      "batch_idx: 11\n",
      "0.4479166666666667\n",
      "batch_idx: 12\n",
      "0.44551282051282054\n",
      "batch_idx: 13\n",
      "0.4523809523809524\n",
      "batch_idx: 14\n",
      "0.45\n",
      "batch_idx: 15\n",
      "0.4505208333333333\n",
      "batch_idx: 16\n",
      "0.45588235294117646\n",
      "batch_idx: 17\n",
      "0.4583333333333333\n",
      "batch_idx: 18\n",
      "0.4517543859649123\n",
      "batch_idx: 19\n",
      "0.45208333333333334\n",
      "batch_idx: 20\n",
      "0.4583333333333333\n",
      "batch_idx: 21\n",
      "0.4564393939393939\n",
      "batch_idx: 22\n",
      "0.45108695652173914\n",
      "batch_idx: 23\n",
      "0.4548611111111111\n",
      "batch_idx: 24\n",
      "0.4533333333333333\n",
      "Training Epoch: 5, total loss: 49.821304\n",
      "batch_idx: 0\n",
      "0.2916666666666667\n",
      "batch_idx: 1\n",
      "0.375\n",
      "batch_idx: 2\n",
      "0.3888888888888889\n",
      "batch_idx: 3\n",
      "0.4479166666666667\n",
      "batch_idx: 4\n",
      "0.4083333333333333\n",
      "batch_idx: 5\n",
      "0.4305555555555556\n",
      "batch_idx: 6\n",
      "0.44642857142857145\n",
      "batch_idx: 7\n",
      "0.4635416666666667\n",
      "batch_idx: 8\n",
      "0.4537037037037037\n",
      "batch_idx: 9\n",
      "0.4625\n",
      "batch_idx: 10\n",
      "0.4734848484848485\n",
      "batch_idx: 11\n",
      "0.4791666666666667\n",
      "batch_idx: 12\n",
      "0.46794871794871795\n",
      "batch_idx: 13\n",
      "0.4642857142857143\n",
      "batch_idx: 14\n",
      "0.475\n",
      "batch_idx: 15\n",
      "0.4635416666666667\n",
      "batch_idx: 16\n",
      "0.45098039215686275\n",
      "batch_idx: 17\n",
      "0.4513888888888889\n",
      "batch_idx: 18\n",
      "0.44298245614035087\n",
      "batch_idx: 19\n",
      "0.44166666666666665\n",
      "batch_idx: 20\n",
      "0.44246031746031744\n",
      "batch_idx: 21\n",
      "0.4375\n",
      "batch_idx: 22\n",
      "0.4438405797101449\n",
      "batch_idx: 23\n",
      "0.4444444444444444\n",
      "batch_idx: 24\n",
      "0.455\n",
      "Training Epoch: 6, total loss: 49.491583\n",
      "batch_idx: 0\n",
      "0.4583333333333333\n",
      "batch_idx: 1\n",
      "0.4791666666666667\n",
      "batch_idx: 2\n",
      "0.4861111111111111\n",
      "batch_idx: 3\n",
      "0.46875\n",
      "batch_idx: 4\n",
      "0.49166666666666664\n",
      "batch_idx: 5\n",
      "0.4930555555555556\n",
      "batch_idx: 6\n",
      "0.4880952380952381\n",
      "batch_idx: 7\n",
      "0.4791666666666667\n",
      "batch_idx: 8\n",
      "0.4722222222222222\n",
      "batch_idx: 9\n",
      "0.4625\n",
      "batch_idx: 10\n",
      "0.4696969696969697\n",
      "batch_idx: 11\n",
      "0.4618055555555556\n",
      "batch_idx: 12\n",
      "0.46794871794871795\n",
      "batch_idx: 13\n",
      "0.46130952380952384\n",
      "batch_idx: 14\n",
      "0.4583333333333333\n",
      "batch_idx: 15\n",
      "0.4583333333333333\n",
      "batch_idx: 16\n",
      "0.4632352941176471\n",
      "batch_idx: 17\n",
      "0.46296296296296297\n",
      "batch_idx: 18\n",
      "0.46271929824561403\n",
      "batch_idx: 19\n",
      "0.4583333333333333\n",
      "batch_idx: 20\n",
      "0.4623015873015873\n",
      "batch_idx: 21\n",
      "0.4602272727272727\n",
      "batch_idx: 22\n",
      "0.46195652173913043\n",
      "batch_idx: 23\n",
      "0.4722222222222222\n",
      "batch_idx: 24\n",
      "0.4766666666666667\n",
      "Training Epoch: 7, total loss: 49.320258\n",
      "batch_idx: 0\n",
      "0.4583333333333333\n",
      "batch_idx: 1\n",
      "0.4583333333333333\n",
      "batch_idx: 2\n",
      "0.4722222222222222\n",
      "batch_idx: 3\n",
      "0.4895833333333333\n",
      "batch_idx: 4\n",
      "0.48333333333333334\n",
      "batch_idx: 5\n",
      "0.4513888888888889\n",
      "batch_idx: 6\n",
      "0.4583333333333333\n",
      "batch_idx: 7\n",
      "0.453125\n",
      "batch_idx: 8\n",
      "0.48148148148148145\n",
      "batch_idx: 9\n",
      "0.4708333333333333\n",
      "batch_idx: 10\n",
      "0.4734848484848485\n",
      "batch_idx: 11\n",
      "0.4756944444444444\n",
      "batch_idx: 12\n",
      "0.48717948717948717\n",
      "batch_idx: 13\n",
      "0.4851190476190476\n",
      "batch_idx: 14\n",
      "0.48055555555555557\n",
      "batch_idx: 15\n",
      "0.4817708333333333\n",
      "batch_idx: 16\n",
      "0.47794117647058826\n",
      "batch_idx: 17\n",
      "0.4722222222222222\n",
      "batch_idx: 18\n",
      "0.47368421052631576\n",
      "batch_idx: 19\n",
      "0.48125\n",
      "batch_idx: 20\n",
      "0.4742063492063492\n",
      "batch_idx: 21\n",
      "0.4734848484848485\n",
      "batch_idx: 22\n",
      "0.4746376811594203\n",
      "batch_idx: 23\n",
      "0.4704861111111111\n",
      "batch_idx: 24\n",
      "0.47333333333333333\n",
      "Training Epoch: 8, total loss: 49.227407\n",
      "batch_idx: 0\n",
      "0.375\n",
      "batch_idx: 1\n",
      "0.4166666666666667\n",
      "batch_idx: 2\n",
      "0.4861111111111111\n",
      "batch_idx: 3\n",
      "0.46875\n",
      "batch_idx: 4\n",
      "0.45\n",
      "batch_idx: 5\n",
      "0.4444444444444444\n",
      "batch_idx: 6\n",
      "0.42857142857142855\n",
      "batch_idx: 7\n",
      "0.4427083333333333\n",
      "batch_idx: 8\n",
      "0.4675925925925926\n",
      "batch_idx: 9\n",
      "0.4625\n",
      "batch_idx: 10\n",
      "0.4621212121212121\n",
      "batch_idx: 11\n",
      "0.4548611111111111\n",
      "batch_idx: 12\n",
      "0.4551282051282051\n",
      "batch_idx: 13\n",
      "0.4583333333333333\n",
      "batch_idx: 14\n",
      "0.4583333333333333\n",
      "batch_idx: 15\n",
      "0.4609375\n",
      "batch_idx: 16\n",
      "0.4681372549019608\n",
      "batch_idx: 17\n",
      "0.4652777777777778\n",
      "batch_idx: 18\n",
      "0.46710526315789475\n",
      "batch_idx: 19\n",
      "0.47291666666666665\n",
      "batch_idx: 20\n",
      "0.4722222222222222\n",
      "batch_idx: 21\n",
      "0.4621212121212121\n",
      "batch_idx: 22\n",
      "0.4601449275362319\n",
      "batch_idx: 23\n",
      "0.4704861111111111\n",
      "batch_idx: 24\n",
      "0.47\n",
      "Training Epoch: 9, total loss: 49.373401\n",
      "batch_idx: 0\n",
      "0.4583333333333333\n",
      "batch_idx: 1\n",
      "0.4375\n",
      "batch_idx: 2\n",
      "0.4722222222222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 3\n",
      "0.4375\n",
      "batch_idx: 4\n",
      "0.43333333333333335\n",
      "batch_idx: 5\n",
      "0.4513888888888889\n",
      "batch_idx: 6\n",
      "0.4523809523809524\n",
      "batch_idx: 7\n",
      "0.4635416666666667\n",
      "batch_idx: 8\n",
      "0.48148148148148145\n",
      "batch_idx: 9\n",
      "0.4583333333333333\n",
      "batch_idx: 10\n",
      "0.4734848484848485\n",
      "batch_idx: 11\n",
      "0.4722222222222222\n",
      "batch_idx: 12\n",
      "0.47435897435897434\n",
      "batch_idx: 13\n",
      "0.4732142857142857\n",
      "batch_idx: 14\n",
      "0.4722222222222222\n",
      "batch_idx: 15\n",
      "0.4739583333333333\n",
      "batch_idx: 16\n",
      "0.4583333333333333\n",
      "batch_idx: 17\n",
      "0.44907407407407407\n",
      "batch_idx: 18\n",
      "0.4517543859649123\n",
      "batch_idx: 19\n",
      "0.44166666666666665\n",
      "batch_idx: 20\n",
      "0.44642857142857145\n",
      "batch_idx: 21\n",
      "0.44886363636363635\n",
      "batch_idx: 22\n",
      "0.45108695652173914\n",
      "batch_idx: 23\n",
      "0.4513888888888889\n",
      "batch_idx: 24\n",
      "0.45166666666666666\n",
      "Training Epoch: 10, total loss: 49.661125\n",
      "batch_idx: 0\n",
      "0.4166666666666667\n",
      "batch_idx: 1\n",
      "0.4791666666666667\n",
      "batch_idx: 2\n",
      "0.5138888888888888\n",
      "batch_idx: 3\n",
      "0.4895833333333333\n",
      "batch_idx: 4\n",
      "0.475\n",
      "batch_idx: 5\n",
      "0.4930555555555556\n",
      "batch_idx: 6\n",
      "0.5\n",
      "batch_idx: 7\n",
      "0.4791666666666667\n",
      "batch_idx: 8\n",
      "0.48148148148148145\n",
      "batch_idx: 9\n",
      "0.49166666666666664\n",
      "batch_idx: 10\n",
      "0.48484848484848486\n",
      "batch_idx: 11\n",
      "0.46875\n",
      "batch_idx: 12\n",
      "0.4807692307692308\n",
      "batch_idx: 13\n",
      "0.47619047619047616\n",
      "batch_idx: 14\n",
      "0.4722222222222222\n",
      "batch_idx: 15\n",
      "0.4869791666666667\n",
      "batch_idx: 16\n",
      "0.49264705882352944\n",
      "batch_idx: 17\n",
      "0.4930555555555556\n",
      "batch_idx: 18\n",
      "0.48026315789473684\n",
      "batch_idx: 19\n",
      "0.4791666666666667\n",
      "batch_idx: 20\n",
      "0.4781746031746032\n",
      "batch_idx: 21\n",
      "0.4734848484848485\n",
      "batch_idx: 22\n",
      "0.47282608695652173\n",
      "batch_idx: 23\n",
      "0.4670138888888889\n",
      "batch_idx: 24\n",
      "0.4666666666666667\n",
      "Training Epoch: 11, total loss: 49.225998\n",
      "batch_idx: 0\n",
      "0.4583333333333333\n",
      "batch_idx: 1\n",
      "0.4583333333333333\n",
      "batch_idx: 2\n",
      "0.4166666666666667\n",
      "batch_idx: 3\n",
      "0.4479166666666667\n",
      "batch_idx: 4\n",
      "0.4083333333333333\n",
      "batch_idx: 5\n",
      "0.3958333333333333\n",
      "batch_idx: 6\n",
      "0.4226190476190476\n",
      "batch_idx: 7\n",
      "0.421875\n",
      "batch_idx: 8\n",
      "0.42592592592592593\n",
      "batch_idx: 9\n",
      "0.44166666666666665\n",
      "batch_idx: 10\n",
      "0.4393939393939394\n",
      "batch_idx: 11\n",
      "0.4270833333333333\n",
      "batch_idx: 12\n",
      "0.4391025641025641\n",
      "batch_idx: 13\n",
      "0.44047619047619047\n",
      "batch_idx: 14\n",
      "0.44166666666666665\n",
      "batch_idx: 15\n",
      "0.4505208333333333\n",
      "batch_idx: 16\n",
      "0.4411764705882353\n",
      "batch_idx: 17\n",
      "0.4513888888888889\n",
      "batch_idx: 18\n",
      "0.45614035087719296\n",
      "batch_idx: 19\n",
      "0.4625\n",
      "batch_idx: 20\n",
      "0.4523809523809524\n",
      "batch_idx: 21\n",
      "0.45454545454545453\n",
      "batch_idx: 22\n",
      "0.447463768115942\n",
      "batch_idx: 23\n",
      "0.4444444444444444\n",
      "batch_idx: 24\n",
      "0.455\n",
      "Training Epoch: 12, total loss: 49.469893\n",
      "batch_idx: 0\n",
      "0.4583333333333333\n",
      "batch_idx: 1\n",
      "0.5\n",
      "batch_idx: 2\n",
      "0.4444444444444444\n",
      "batch_idx: 3\n",
      "0.5104166666666666\n",
      "batch_idx: 4\n",
      "0.475\n",
      "batch_idx: 5\n",
      "0.5\n",
      "batch_idx: 6\n",
      "0.5238095238095238\n",
      "batch_idx: 7\n",
      "0.5104166666666666\n",
      "batch_idx: 8\n",
      "0.5046296296296297\n",
      "batch_idx: 9\n",
      "0.5208333333333334\n",
      "batch_idx: 10\n",
      "0.5227272727272727\n",
      "batch_idx: 11\n",
      "0.5104166666666666\n",
      "batch_idx: 12\n",
      "0.4935897435897436\n",
      "batch_idx: 13\n",
      "0.5\n",
      "batch_idx: 14\n",
      "0.5027777777777778\n",
      "batch_idx: 15\n",
      "0.4947916666666667\n",
      "batch_idx: 16\n",
      "0.49019607843137253\n",
      "batch_idx: 17\n",
      "0.4861111111111111\n",
      "batch_idx: 18\n",
      "0.48026315789473684\n",
      "batch_idx: 19\n",
      "0.48125\n",
      "batch_idx: 20\n",
      "0.4781746031746032\n",
      "batch_idx: 21\n",
      "0.4791666666666667\n",
      "batch_idx: 22\n",
      "0.48007246376811596\n",
      "batch_idx: 23\n",
      "0.4791666666666667\n",
      "batch_idx: 24\n",
      "0.4816666666666667\n",
      "Training Epoch: 13, total loss: 49.046456\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.4166666666666667\n",
      "batch_idx: 2\n",
      "0.375\n",
      "batch_idx: 3\n",
      "0.4270833333333333\n",
      "batch_idx: 4\n",
      "0.44166666666666665\n",
      "batch_idx: 5\n",
      "0.4444444444444444\n",
      "batch_idx: 6\n",
      "0.4583333333333333\n",
      "batch_idx: 7\n",
      "0.4791666666666667\n",
      "batch_idx: 8\n",
      "0.48148148148148145\n",
      "batch_idx: 9\n",
      "0.4708333333333333\n",
      "batch_idx: 10\n",
      "0.4734848484848485\n",
      "batch_idx: 11\n",
      "0.4861111111111111\n",
      "batch_idx: 12\n",
      "0.48717948717948717\n",
      "batch_idx: 13\n",
      "0.4851190476190476\n",
      "batch_idx: 14\n",
      "0.48333333333333334\n",
      "batch_idx: 15\n",
      "0.4791666666666667\n",
      "batch_idx: 16\n",
      "0.48284313725490197\n",
      "batch_idx: 17\n",
      "0.4861111111111111\n",
      "batch_idx: 18\n",
      "0.48464912280701755\n",
      "batch_idx: 19\n",
      "0.49166666666666664\n",
      "batch_idx: 20\n",
      "0.49603174603174605\n",
      "batch_idx: 21\n",
      "0.48484848484848486\n",
      "batch_idx: 22\n",
      "0.48188405797101447\n",
      "batch_idx: 23\n",
      "0.4774305555555556\n",
      "batch_idx: 24\n",
      "0.4716666666666667\n",
      "Training Epoch: 14, total loss: 49.242771\n",
      "batch_idx: 0\n",
      "0.4166666666666667\n",
      "batch_idx: 1\n",
      "0.5416666666666666\n",
      "batch_idx: 2\n",
      "0.4861111111111111\n",
      "batch_idx: 3\n",
      "0.5208333333333334\n",
      "batch_idx: 4\n",
      "0.49166666666666664\n",
      "batch_idx: 5\n",
      "0.4930555555555556\n",
      "batch_idx: 6\n",
      "0.5\n",
      "batch_idx: 7\n",
      "0.5104166666666666\n",
      "batch_idx: 8\n",
      "0.5\n",
      "batch_idx: 9\n",
      "0.49583333333333335\n",
      "batch_idx: 10\n",
      "0.5113636363636364\n",
      "batch_idx: 11\n",
      "0.5\n",
      "batch_idx: 12\n",
      "0.5064102564102564\n",
      "batch_idx: 13\n",
      "0.5029761904761905\n",
      "batch_idx: 14\n",
      "0.49444444444444446\n",
      "batch_idx: 15\n",
      "0.4947916666666667\n",
      "batch_idx: 16\n",
      "0.49264705882352944\n",
      "batch_idx: 17\n",
      "0.48842592592592593\n",
      "batch_idx: 18\n",
      "0.4868421052631579\n",
      "batch_idx: 19\n",
      "0.49375\n",
      "batch_idx: 20\n",
      "0.49007936507936506\n",
      "batch_idx: 21\n",
      "0.49053030303030304\n",
      "batch_idx: 22\n",
      "0.4945652173913043\n",
      "batch_idx: 23\n",
      "0.4947916666666667\n",
      "batch_idx: 24\n",
      "0.49\n",
      "Training Epoch: 15, total loss: 48.985646\n",
      "batch_idx: 0\n",
      "0.4166666666666667\n",
      "batch_idx: 1\n",
      "0.4583333333333333\n",
      "batch_idx: 2\n",
      "0.4583333333333333\n",
      "batch_idx: 3\n",
      "0.4791666666666667\n",
      "batch_idx: 4\n",
      "0.4666666666666667\n",
      "batch_idx: 5\n",
      "0.4652777777777778\n",
      "batch_idx: 6\n",
      "0.4523809523809524\n",
      "batch_idx: 7\n",
      "0.4583333333333333\n",
      "batch_idx: 8\n",
      "0.4675925925925926\n",
      "batch_idx: 9\n",
      "0.4708333333333333\n",
      "batch_idx: 10\n",
      "0.4772727272727273\n",
      "batch_idx: 11\n",
      "0.4756944444444444\n",
      "batch_idx: 12\n",
      "0.4775641025641026\n",
      "batch_idx: 13\n",
      "0.4791666666666667\n",
      "batch_idx: 14\n",
      "0.48055555555555557\n",
      "batch_idx: 15\n",
      "0.4921875\n",
      "batch_idx: 16\n",
      "0.49264705882352944\n",
      "batch_idx: 17\n",
      "0.4976851851851852\n",
      "batch_idx: 18\n",
      "0.48903508771929827\n",
      "batch_idx: 19\n",
      "0.4875\n",
      "batch_idx: 20\n",
      "0.4801587301587302\n",
      "batch_idx: 21\n",
      "0.4753787878787879\n",
      "batch_idx: 22\n",
      "0.47282608695652173\n",
      "batch_idx: 23\n",
      "0.4722222222222222\n",
      "batch_idx: 24\n",
      "0.4683333333333333\n",
      "Training Epoch: 16, total loss: 49.217897\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.4791666666666667\n",
      "batch_idx: 2\n",
      "0.5277777777777778\n",
      "batch_idx: 3\n",
      "0.46875\n",
      "batch_idx: 4\n",
      "0.48333333333333334\n",
      "batch_idx: 5\n",
      "0.4791666666666667\n",
      "batch_idx: 6\n",
      "0.5\n",
      "batch_idx: 7\n",
      "0.5052083333333334\n",
      "batch_idx: 8\n",
      "0.49537037037037035\n",
      "batch_idx: 9\n",
      "0.5041666666666667\n",
      "batch_idx: 10\n",
      "0.5151515151515151\n",
      "batch_idx: 11\n",
      "0.5208333333333334\n",
      "batch_idx: 12\n",
      "0.5064102564102564\n",
      "batch_idx: 13\n",
      "0.5089285714285714\n",
      "batch_idx: 14\n",
      "0.5055555555555555\n",
      "batch_idx: 15\n",
      "0.4895833333333333\n",
      "batch_idx: 16\n",
      "0.48284313725490197\n",
      "batch_idx: 17\n",
      "0.4837962962962963\n",
      "batch_idx: 18\n",
      "0.4956140350877193\n",
      "batch_idx: 19\n",
      "0.4895833333333333\n",
      "batch_idx: 20\n",
      "0.4880952380952381\n",
      "batch_idx: 21\n",
      "0.48484848484848486\n",
      "batch_idx: 22\n",
      "0.48188405797101447\n",
      "batch_idx: 23\n",
      "0.4861111111111111\n",
      "batch_idx: 24\n",
      "0.49\n",
      "Training Epoch: 17, total loss: 48.802389\n",
      "batch_idx: 0\n",
      "0.4583333333333333\n",
      "batch_idx: 1\n",
      "0.3958333333333333\n",
      "batch_idx: 2\n",
      "0.3888888888888889\n",
      "batch_idx: 3\n",
      "0.46875\n",
      "batch_idx: 4\n",
      "0.475\n",
      "batch_idx: 5\n",
      "0.4652777777777778\n",
      "batch_idx: 6\n",
      "0.49404761904761907\n",
      "batch_idx: 7\n",
      "0.4895833333333333\n",
      "batch_idx: 8\n",
      "0.48148148148148145\n",
      "batch_idx: 9\n",
      "0.4708333333333333\n",
      "batch_idx: 10\n",
      "0.48484848484848486\n",
      "batch_idx: 11\n",
      "0.4791666666666667\n",
      "batch_idx: 12\n",
      "0.4807692307692308\n",
      "batch_idx: 13\n",
      "0.4851190476190476\n",
      "batch_idx: 14\n",
      "0.49444444444444446\n",
      "batch_idx: 15\n",
      "0.4973958333333333\n",
      "batch_idx: 16\n",
      "0.5\n",
      "batch_idx: 17\n",
      "0.4976851851851852\n",
      "batch_idx: 18\n",
      "0.4956140350877193\n",
      "batch_idx: 19\n",
      "0.49375\n",
      "batch_idx: 20\n",
      "0.5\n",
      "batch_idx: 21\n",
      "0.4943181818181818\n",
      "batch_idx: 22\n",
      "0.483695652173913\n",
      "batch_idx: 23\n",
      "0.4774305555555556\n",
      "batch_idx: 24\n",
      "0.47333333333333333\n",
      "Training Epoch: 18, total loss: 49.094435\n",
      "batch_idx: 0\n",
      "0.4166666666666667\n",
      "batch_idx: 1\n",
      "0.5\n",
      "batch_idx: 2\n",
      "0.4722222222222222\n",
      "batch_idx: 3\n",
      "0.4791666666666667\n",
      "batch_idx: 4\n",
      "0.49166666666666664\n",
      "batch_idx: 5\n",
      "0.4722222222222222\n",
      "batch_idx: 6\n",
      "0.47023809523809523\n",
      "batch_idx: 7\n",
      "0.4791666666666667\n",
      "batch_idx: 8\n",
      "0.4861111111111111\n",
      "batch_idx: 9\n",
      "0.49166666666666664\n",
      "batch_idx: 10\n",
      "0.5\n",
      "batch_idx: 11\n",
      "0.5034722222222222\n",
      "batch_idx: 12\n",
      "0.5064102564102564\n",
      "batch_idx: 13\n",
      "0.5119047619047619\n",
      "batch_idx: 14\n",
      "0.5111111111111111\n",
      "batch_idx: 15\n",
      "0.4973958333333333\n",
      "batch_idx: 16\n",
      "0.5073529411764706\n",
      "batch_idx: 17\n",
      "0.5023148148148148\n",
      "batch_idx: 18\n",
      "0.5043859649122807\n",
      "batch_idx: 19\n",
      "0.5125\n",
      "batch_idx: 20\n",
      "0.5099206349206349\n",
      "batch_idx: 21\n",
      "0.5113636363636364\n",
      "batch_idx: 22\n",
      "0.5072463768115942\n",
      "batch_idx: 23\n",
      "0.5086805555555556\n",
      "batch_idx: 24\n",
      "0.5133333333333333\n",
      "Training Epoch: 19, total loss: 48.673031\n",
      "batch_idx: 0\n",
      "0.3333333333333333\n",
      "batch_idx: 1\n",
      "0.4166666666666667\n",
      "batch_idx: 2\n",
      "0.4444444444444444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 3\n",
      "0.4583333333333333\n",
      "batch_idx: 4\n",
      "0.45\n",
      "batch_idx: 5\n",
      "0.4583333333333333\n",
      "batch_idx: 6\n",
      "0.4583333333333333\n",
      "batch_idx: 7\n",
      "0.453125\n",
      "batch_idx: 8\n",
      "0.44907407407407407\n",
      "batch_idx: 9\n",
      "0.44583333333333336\n",
      "batch_idx: 10\n",
      "0.45454545454545453\n",
      "batch_idx: 11\n",
      "0.4652777777777778\n",
      "batch_idx: 12\n",
      "0.46153846153846156\n",
      "batch_idx: 13\n",
      "0.45535714285714285\n",
      "batch_idx: 14\n",
      "0.4666666666666667\n",
      "batch_idx: 15\n",
      "0.4557291666666667\n",
      "batch_idx: 16\n",
      "0.46078431372549017\n",
      "batch_idx: 17\n",
      "0.4722222222222222\n",
      "batch_idx: 18\n",
      "0.4758771929824561\n",
      "batch_idx: 19\n",
      "0.48125\n",
      "batch_idx: 20\n",
      "0.4861111111111111\n",
      "batch_idx: 21\n",
      "0.4810606060606061\n",
      "batch_idx: 22\n",
      "0.48188405797101447\n",
      "batch_idx: 23\n",
      "0.4791666666666667\n",
      "batch_idx: 24\n",
      "0.48\n",
      "Training Epoch: 20, total loss: 48.937139\n",
      "batch_idx: 0\n",
      "0.2916666666666667\n",
      "batch_idx: 1\n",
      "0.3125\n",
      "batch_idx: 2\n",
      "0.3472222222222222\n",
      "batch_idx: 3\n",
      "0.40625\n",
      "batch_idx: 4\n",
      "0.4583333333333333\n",
      "batch_idx: 5\n",
      "0.4444444444444444\n",
      "batch_idx: 6\n",
      "0.4523809523809524\n",
      "batch_idx: 7\n",
      "0.4375\n",
      "batch_idx: 8\n",
      "0.4583333333333333\n",
      "batch_idx: 9\n",
      "0.4583333333333333\n",
      "batch_idx: 10\n",
      "0.4734848484848485\n",
      "batch_idx: 11\n",
      "0.4583333333333333\n",
      "batch_idx: 12\n",
      "0.47115384615384615\n",
      "batch_idx: 13\n",
      "0.47619047619047616\n",
      "batch_idx: 14\n",
      "0.4777777777777778\n",
      "batch_idx: 15\n",
      "0.4609375\n",
      "batch_idx: 16\n",
      "0.4534313725490196\n",
      "batch_idx: 17\n",
      "0.4652777777777778\n",
      "batch_idx: 18\n",
      "0.44956140350877194\n",
      "batch_idx: 19\n",
      "0.45\n",
      "batch_idx: 20\n",
      "0.45634920634920634\n",
      "batch_idx: 21\n",
      "0.45075757575757575\n",
      "batch_idx: 22\n",
      "0.4528985507246377\n",
      "batch_idx: 23\n",
      "0.4618055555555556\n",
      "batch_idx: 24\n",
      "0.46166666666666667\n",
      "Training Epoch: 21, total loss: 49.347708\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.5833333333333334\n",
      "batch_idx: 3\n",
      "0.5729166666666666\n",
      "batch_idx: 4\n",
      "0.55\n",
      "batch_idx: 5\n",
      "0.5277777777777778\n",
      "batch_idx: 6\n",
      "0.5059523809523809\n",
      "batch_idx: 7\n",
      "0.515625\n",
      "batch_idx: 8\n",
      "0.5092592592592593\n",
      "batch_idx: 9\n",
      "0.5041666666666667\n",
      "batch_idx: 10\n",
      "0.5\n",
      "batch_idx: 11\n",
      "0.4861111111111111\n",
      "batch_idx: 12\n",
      "0.483974358974359\n",
      "batch_idx: 13\n",
      "0.48214285714285715\n",
      "batch_idx: 14\n",
      "0.46944444444444444\n",
      "batch_idx: 15\n",
      "0.46875\n",
      "batch_idx: 16\n",
      "0.4852941176470588\n",
      "batch_idx: 17\n",
      "0.4791666666666667\n",
      "batch_idx: 18\n",
      "0.4824561403508772\n",
      "batch_idx: 19\n",
      "0.47708333333333336\n",
      "batch_idx: 20\n",
      "0.4742063492063492\n",
      "batch_idx: 21\n",
      "0.4734848484848485\n",
      "batch_idx: 22\n",
      "0.47282608695652173\n",
      "batch_idx: 23\n",
      "0.4704861111111111\n",
      "batch_idx: 24\n",
      "0.4716666666666667\n",
      "Training Epoch: 22, total loss: 49.258011\n",
      "batch_idx: 0\n",
      "0.4166666666666667\n",
      "batch_idx: 1\n",
      "0.4166666666666667\n",
      "batch_idx: 2\n",
      "0.4583333333333333\n",
      "batch_idx: 3\n",
      "0.4583333333333333\n",
      "batch_idx: 4\n",
      "0.48333333333333334\n",
      "batch_idx: 5\n",
      "0.4722222222222222\n",
      "batch_idx: 6\n",
      "0.5\n",
      "batch_idx: 7\n",
      "0.4947916666666667\n",
      "batch_idx: 8\n",
      "0.49537037037037035\n",
      "batch_idx: 9\n",
      "0.48333333333333334\n",
      "batch_idx: 10\n",
      "0.4772727272727273\n",
      "batch_idx: 11\n",
      "0.4826388888888889\n",
      "batch_idx: 12\n",
      "0.483974358974359\n",
      "batch_idx: 13\n",
      "0.49107142857142855\n",
      "batch_idx: 14\n",
      "0.4888888888888889\n",
      "batch_idx: 15\n",
      "0.4791666666666667\n",
      "batch_idx: 16\n",
      "0.47549019607843135\n",
      "batch_idx: 17\n",
      "0.47685185185185186\n",
      "batch_idx: 18\n",
      "0.48464912280701755\n",
      "batch_idx: 19\n",
      "0.4895833333333333\n",
      "batch_idx: 20\n",
      "0.4880952380952381\n",
      "batch_idx: 21\n",
      "0.4810606060606061\n",
      "batch_idx: 22\n",
      "0.48731884057971014\n",
      "batch_idx: 23\n",
      "0.4895833333333333\n",
      "batch_idx: 24\n",
      "0.49\n",
      "Training Epoch: 23, total loss: 48.761846\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.5555555555555556\n",
      "batch_idx: 3\n",
      "0.5729166666666666\n",
      "batch_idx: 4\n",
      "0.5666666666666667\n",
      "batch_idx: 5\n",
      "0.5694444444444444\n",
      "batch_idx: 6\n",
      "0.5416666666666666\n",
      "batch_idx: 7\n",
      "0.5572916666666666\n",
      "batch_idx: 8\n",
      "0.5462962962962963\n",
      "batch_idx: 9\n",
      "0.5375\n",
      "batch_idx: 10\n",
      "0.5454545454545454\n",
      "batch_idx: 11\n",
      "0.53125\n",
      "batch_idx: 12\n",
      "0.5288461538461539\n",
      "batch_idx: 13\n",
      "0.5267857142857143\n",
      "batch_idx: 14\n",
      "0.5194444444444445\n",
      "batch_idx: 15\n",
      "0.53125\n",
      "batch_idx: 16\n",
      "0.5220588235294118\n",
      "batch_idx: 17\n",
      "0.5231481481481481\n",
      "batch_idx: 18\n",
      "0.5285087719298246\n",
      "batch_idx: 19\n",
      "0.5229166666666667\n",
      "batch_idx: 20\n",
      "0.5297619047619048\n",
      "batch_idx: 21\n",
      "0.5303030303030303\n",
      "batch_idx: 22\n",
      "0.5235507246376812\n",
      "batch_idx: 23\n",
      "0.5190972222222222\n",
      "batch_idx: 24\n",
      "0.5183333333333333\n",
      "Training Epoch: 24, total loss: 48.468272\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.5208333333333334\n",
      "batch_idx: 2\n",
      "0.4861111111111111\n",
      "batch_idx: 3\n",
      "0.5104166666666666\n",
      "batch_idx: 4\n",
      "0.5333333333333333\n",
      "batch_idx: 5\n",
      "0.5694444444444444\n",
      "batch_idx: 6\n",
      "0.5654761904761905\n",
      "batch_idx: 7\n",
      "0.5520833333333334\n",
      "batch_idx: 8\n",
      "0.5462962962962963\n",
      "batch_idx: 9\n",
      "0.55\n",
      "batch_idx: 10\n",
      "0.5454545454545454\n",
      "batch_idx: 11\n",
      "0.5451388888888888\n",
      "batch_idx: 12\n",
      "0.5448717948717948\n",
      "batch_idx: 13\n",
      "0.5476190476190477\n",
      "batch_idx: 14\n",
      "0.5388888888888889\n",
      "batch_idx: 15\n",
      "0.5390625\n",
      "batch_idx: 16\n",
      "0.5392156862745098\n",
      "batch_idx: 17\n",
      "0.5347222222222222\n",
      "batch_idx: 18\n",
      "0.5263157894736842\n",
      "batch_idx: 19\n",
      "0.5208333333333334\n",
      "batch_idx: 20\n",
      "0.5138888888888888\n",
      "batch_idx: 21\n",
      "0.5151515151515151\n",
      "batch_idx: 22\n",
      "0.5163043478260869\n",
      "batch_idx: 23\n",
      "0.515625\n",
      "batch_idx: 24\n",
      "0.5066666666666667\n",
      "Training Epoch: 25, total loss: 48.719519\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.4583333333333333\n",
      "batch_idx: 2\n",
      "0.4305555555555556\n",
      "batch_idx: 3\n",
      "0.4791666666666667\n",
      "batch_idx: 4\n",
      "0.5083333333333333\n",
      "batch_idx: 5\n",
      "0.4930555555555556\n",
      "batch_idx: 6\n",
      "0.5119047619047619\n",
      "batch_idx: 7\n",
      "0.5052083333333334\n",
      "batch_idx: 8\n",
      "0.5324074074074074\n",
      "batch_idx: 9\n",
      "0.5291666666666667\n",
      "batch_idx: 10\n",
      "0.5340909090909091\n",
      "batch_idx: 11\n",
      "0.5208333333333334\n",
      "batch_idx: 12\n",
      "0.5224358974358975\n",
      "batch_idx: 13\n",
      "0.5297619047619048\n",
      "batch_idx: 14\n",
      "0.525\n",
      "batch_idx: 15\n",
      "0.5208333333333334\n",
      "batch_idx: 16\n",
      "0.5147058823529411\n",
      "batch_idx: 17\n",
      "0.5\n",
      "batch_idx: 18\n",
      "0.5\n",
      "batch_idx: 19\n",
      "0.4979166666666667\n",
      "batch_idx: 20\n",
      "0.503968253968254\n",
      "batch_idx: 21\n",
      "0.5\n",
      "batch_idx: 22\n",
      "0.4963768115942029\n",
      "batch_idx: 23\n",
      "0.4930555555555556\n",
      "batch_idx: 24\n",
      "0.49666666666666665\n",
      "Training Epoch: 26, total loss: 48.847865\n",
      "batch_idx: 0\n",
      "0.3333333333333333\n",
      "batch_idx: 1\n",
      "0.2916666666666667\n",
      "batch_idx: 2\n",
      "0.3472222222222222\n",
      "batch_idx: 3\n",
      "0.3645833333333333\n",
      "batch_idx: 4\n",
      "0.38333333333333336\n",
      "batch_idx: 5\n",
      "0.4305555555555556\n",
      "batch_idx: 6\n",
      "0.4583333333333333\n",
      "batch_idx: 7\n",
      "0.453125\n",
      "batch_idx: 8\n",
      "0.4537037037037037\n",
      "batch_idx: 9\n",
      "0.4583333333333333\n",
      "batch_idx: 10\n",
      "0.45454545454545453\n",
      "batch_idx: 11\n",
      "0.4479166666666667\n",
      "batch_idx: 12\n",
      "0.4551282051282051\n",
      "batch_idx: 13\n",
      "0.46726190476190477\n",
      "batch_idx: 14\n",
      "0.4722222222222222\n",
      "batch_idx: 15\n",
      "0.46875\n",
      "batch_idx: 16\n",
      "0.4730392156862745\n",
      "batch_idx: 17\n",
      "0.4652777777777778\n",
      "batch_idx: 18\n",
      "0.4649122807017544\n",
      "batch_idx: 19\n",
      "0.4666666666666667\n",
      "batch_idx: 20\n",
      "0.4623015873015873\n",
      "batch_idx: 21\n",
      "0.4659090909090909\n",
      "batch_idx: 22\n",
      "0.463768115942029\n",
      "batch_idx: 23\n",
      "0.4652777777777778\n",
      "batch_idx: 24\n",
      "0.45666666666666667\n",
      "Training Epoch: 27, total loss: 49.450173\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.5208333333333334\n",
      "batch_idx: 2\n",
      "0.5\n",
      "batch_idx: 3\n",
      "0.4479166666666667\n",
      "batch_idx: 4\n",
      "0.43333333333333335\n",
      "batch_idx: 5\n",
      "0.4236111111111111\n",
      "batch_idx: 6\n",
      "0.43452380952380953\n",
      "batch_idx: 7\n",
      "0.453125\n",
      "batch_idx: 8\n",
      "0.4537037037037037\n",
      "batch_idx: 9\n",
      "0.45\n",
      "batch_idx: 10\n",
      "0.44696969696969696\n",
      "batch_idx: 11\n",
      "0.4548611111111111\n",
      "batch_idx: 12\n",
      "0.47115384615384615\n",
      "batch_idx: 13\n",
      "0.4642857142857143\n",
      "batch_idx: 14\n",
      "0.4722222222222222\n",
      "batch_idx: 15\n",
      "0.4609375\n",
      "batch_idx: 16\n",
      "0.46078431372549017\n",
      "batch_idx: 17\n",
      "0.45601851851851855\n",
      "batch_idx: 18\n",
      "0.46271929824561403\n",
      "batch_idx: 19\n",
      "0.4666666666666667\n",
      "batch_idx: 20\n",
      "0.4623015873015873\n",
      "batch_idx: 21\n",
      "0.4583333333333333\n",
      "batch_idx: 22\n",
      "0.4692028985507246\n",
      "batch_idx: 23\n",
      "0.4704861111111111\n",
      "batch_idx: 24\n",
      "0.4683333333333333\n",
      "Training Epoch: 28, total loss: 49.227222\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.5208333333333334\n",
      "batch_idx: 2\n",
      "0.5694444444444444\n",
      "batch_idx: 3\n",
      "0.5416666666666666\n",
      "batch_idx: 4\n",
      "0.5166666666666667\n",
      "batch_idx: 5\n",
      "0.5416666666666666\n",
      "batch_idx: 6\n",
      "0.5654761904761905\n",
      "batch_idx: 7\n",
      "0.5729166666666666\n",
      "batch_idx: 8\n",
      "0.5555555555555556\n",
      "batch_idx: 9\n",
      "0.5583333333333333\n",
      "batch_idx: 10\n",
      "0.5492424242424242\n",
      "batch_idx: 11\n",
      "0.5625\n",
      "batch_idx: 12\n",
      "0.5641025641025641\n",
      "batch_idx: 13\n",
      "0.5565476190476191\n",
      "batch_idx: 14\n",
      "0.5583333333333333\n",
      "batch_idx: 15\n",
      "0.5520833333333334\n",
      "batch_idx: 16\n",
      "0.5490196078431373\n",
      "batch_idx: 17\n",
      "0.5393518518518519\n",
      "batch_idx: 18\n",
      "0.5307017543859649\n",
      "batch_idx: 19\n",
      "0.5333333333333333\n",
      "batch_idx: 20\n",
      "0.5257936507936508\n",
      "batch_idx: 21\n",
      "0.5246212121212122\n",
      "batch_idx: 22\n",
      "0.5344202898550725\n",
      "batch_idx: 23\n",
      "0.5260416666666666\n",
      "batch_idx: 24\n",
      "0.5216666666666666\n",
      "Training Epoch: 29, total loss: 48.350680\n",
      "batch_idx: 0\n",
      "0.375\n",
      "batch_idx: 1\n",
      "0.4375\n",
      "batch_idx: 2\n",
      "0.4027777777777778\n",
      "batch_idx: 3\n",
      "0.4479166666666667\n",
      "batch_idx: 4\n",
      "0.44166666666666665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 5\n",
      "0.4861111111111111\n",
      "batch_idx: 6\n",
      "0.5059523809523809\n",
      "batch_idx: 7\n",
      "0.5260416666666666\n",
      "batch_idx: 8\n",
      "0.5092592592592593\n",
      "batch_idx: 9\n",
      "0.5208333333333334\n",
      "batch_idx: 10\n",
      "0.5303030303030303\n",
      "batch_idx: 11\n",
      "0.5347222222222222\n",
      "batch_idx: 12\n",
      "0.5416666666666666\n",
      "batch_idx: 13\n",
      "0.5416666666666666\n",
      "batch_idx: 14\n",
      "0.5444444444444444\n",
      "batch_idx: 15\n",
      "0.5416666666666666\n",
      "batch_idx: 16\n",
      "0.5367647058823529\n",
      "batch_idx: 17\n",
      "0.5416666666666666\n",
      "batch_idx: 18\n",
      "0.5350877192982456\n",
      "batch_idx: 19\n",
      "0.525\n",
      "batch_idx: 20\n",
      "0.5238095238095238\n",
      "batch_idx: 21\n",
      "0.5284090909090909\n",
      "batch_idx: 22\n",
      "0.5271739130434783\n",
      "batch_idx: 23\n",
      "0.5277777777777778\n",
      "batch_idx: 24\n",
      "0.52\n",
      "Training Epoch: 30, total loss: 48.450769\n",
      "batch_idx: 0\n",
      "0.4583333333333333\n",
      "batch_idx: 1\n",
      "0.5625\n",
      "batch_idx: 2\n",
      "0.5555555555555556\n",
      "batch_idx: 3\n",
      "0.53125\n",
      "batch_idx: 4\n",
      "0.5166666666666667\n",
      "batch_idx: 5\n",
      "0.5208333333333334\n",
      "batch_idx: 6\n",
      "0.5357142857142857\n",
      "batch_idx: 7\n",
      "0.5364583333333334\n",
      "batch_idx: 8\n",
      "0.5324074074074074\n",
      "batch_idx: 9\n",
      "0.5333333333333333\n",
      "batch_idx: 10\n",
      "0.5378787878787878\n",
      "batch_idx: 11\n",
      "0.5416666666666666\n",
      "batch_idx: 12\n",
      "0.5448717948717948\n",
      "batch_idx: 13\n",
      "0.5446428571428571\n",
      "batch_idx: 14\n",
      "0.55\n",
      "batch_idx: 15\n",
      "0.5416666666666666\n",
      "batch_idx: 16\n",
      "0.5416666666666666\n",
      "batch_idx: 17\n",
      "0.5439814814814815\n",
      "batch_idx: 18\n",
      "0.5460526315789473\n",
      "batch_idx: 19\n",
      "0.5395833333333333\n",
      "batch_idx: 20\n",
      "0.5376984126984127\n",
      "batch_idx: 21\n",
      "0.5378787878787878\n",
      "batch_idx: 22\n",
      "0.5380434782608695\n",
      "batch_idx: 23\n",
      "0.5381944444444444\n",
      "batch_idx: 24\n",
      "0.54\n",
      "Training Epoch: 31, total loss: 48.061019\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.4791666666666667\n",
      "batch_idx: 2\n",
      "0.5138888888888888\n",
      "batch_idx: 3\n",
      "0.5104166666666666\n",
      "batch_idx: 4\n",
      "0.5333333333333333\n",
      "batch_idx: 5\n",
      "0.5625\n",
      "batch_idx: 6\n",
      "0.5535714285714286\n",
      "batch_idx: 7\n",
      "0.5625\n",
      "batch_idx: 8\n",
      "0.5555555555555556\n",
      "batch_idx: 9\n",
      "0.5416666666666666\n",
      "batch_idx: 10\n",
      "0.5378787878787878\n",
      "batch_idx: 11\n",
      "0.5243055555555556\n",
      "batch_idx: 12\n",
      "0.5128205128205128\n",
      "batch_idx: 13\n",
      "0.5059523809523809\n",
      "batch_idx: 14\n",
      "0.5027777777777778\n",
      "batch_idx: 15\n",
      "0.5026041666666666\n",
      "batch_idx: 16\n",
      "0.5122549019607843\n",
      "batch_idx: 17\n",
      "0.5185185185185185\n",
      "batch_idx: 18\n",
      "0.5197368421052632\n",
      "batch_idx: 19\n",
      "0.5229166666666667\n",
      "batch_idx: 20\n",
      "0.5238095238095238\n",
      "batch_idx: 21\n",
      "0.5208333333333334\n",
      "batch_idx: 22\n",
      "0.519927536231884\n",
      "batch_idx: 23\n",
      "0.5138888888888888\n",
      "batch_idx: 24\n",
      "0.52\n",
      "Training Epoch: 32, total loss: 48.396585\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.5208333333333334\n",
      "batch_idx: 2\n",
      "0.5277777777777778\n",
      "batch_idx: 3\n",
      "0.5416666666666666\n",
      "batch_idx: 4\n",
      "0.5333333333333333\n",
      "batch_idx: 5\n",
      "0.5555555555555556\n",
      "batch_idx: 6\n",
      "0.5714285714285714\n",
      "batch_idx: 7\n",
      "0.5833333333333334\n",
      "batch_idx: 8\n",
      "0.5694444444444444\n",
      "batch_idx: 9\n",
      "0.5708333333333333\n",
      "batch_idx: 10\n",
      "0.5606060606060606\n",
      "batch_idx: 11\n",
      "0.5451388888888888\n",
      "batch_idx: 12\n",
      "0.5352564102564102\n",
      "batch_idx: 13\n",
      "0.5267857142857143\n",
      "batch_idx: 14\n",
      "0.525\n",
      "batch_idx: 15\n",
      "0.5286458333333334\n",
      "batch_idx: 16\n",
      "0.5294117647058824\n",
      "batch_idx: 17\n",
      "0.5254629629629629\n",
      "batch_idx: 18\n",
      "0.5241228070175439\n",
      "batch_idx: 19\n",
      "0.5291666666666667\n",
      "batch_idx: 20\n",
      "0.5238095238095238\n",
      "batch_idx: 21\n",
      "0.5246212121212122\n",
      "batch_idx: 22\n",
      "0.5217391304347826\n",
      "batch_idx: 23\n",
      "0.515625\n",
      "batch_idx: 24\n",
      "0.52\n",
      "Training Epoch: 33, total loss: 48.305573\n",
      "batch_idx: 0\n",
      "0.4583333333333333\n",
      "batch_idx: 1\n",
      "0.4583333333333333\n",
      "batch_idx: 2\n",
      "0.4861111111111111\n",
      "batch_idx: 3\n",
      "0.4479166666666667\n",
      "batch_idx: 4\n",
      "0.4583333333333333\n",
      "batch_idx: 5\n",
      "0.4444444444444444\n",
      "batch_idx: 6\n",
      "0.44642857142857145\n",
      "batch_idx: 7\n",
      "0.4479166666666667\n",
      "batch_idx: 8\n",
      "0.4583333333333333\n",
      "batch_idx: 9\n",
      "0.4666666666666667\n",
      "batch_idx: 10\n",
      "0.4734848484848485\n",
      "batch_idx: 11\n",
      "0.4652777777777778\n",
      "batch_idx: 12\n",
      "0.4551282051282051\n",
      "batch_idx: 13\n",
      "0.44642857142857145\n",
      "batch_idx: 14\n",
      "0.45555555555555555\n",
      "batch_idx: 15\n",
      "0.4791666666666667\n",
      "batch_idx: 16\n",
      "0.4877450980392157\n",
      "batch_idx: 17\n",
      "0.47685185185185186\n",
      "batch_idx: 18\n",
      "0.4780701754385965\n",
      "batch_idx: 19\n",
      "0.49375\n",
      "batch_idx: 20\n",
      "0.498015873015873\n",
      "batch_idx: 21\n",
      "0.5037878787878788\n",
      "batch_idx: 22\n",
      "0.49818840579710144\n",
      "batch_idx: 23\n",
      "0.4947916666666667\n",
      "batch_idx: 24\n",
      "0.49666666666666665\n",
      "Training Epoch: 34, total loss: 48.888785\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.5625\n",
      "batch_idx: 2\n",
      "0.5555555555555556\n",
      "batch_idx: 3\n",
      "0.6145833333333334\n",
      "batch_idx: 4\n",
      "0.5916666666666667\n",
      "batch_idx: 5\n",
      "0.6180555555555556\n",
      "batch_idx: 6\n",
      "0.6130952380952381\n",
      "batch_idx: 7\n",
      "0.6145833333333334\n",
      "batch_idx: 8\n",
      "0.5879629629629629\n",
      "batch_idx: 9\n",
      "0.5916666666666667\n",
      "batch_idx: 10\n",
      "0.5757575757575758\n",
      "batch_idx: 11\n",
      "0.5729166666666666\n",
      "batch_idx: 12\n",
      "0.5865384615384616\n",
      "batch_idx: 13\n",
      "0.5714285714285714\n",
      "batch_idx: 14\n",
      "0.5638888888888889\n",
      "batch_idx: 15\n",
      "0.5572916666666666\n",
      "batch_idx: 16\n",
      "0.5514705882352942\n",
      "batch_idx: 17\n",
      "0.5439814814814815\n",
      "batch_idx: 18\n",
      "0.5328947368421053\n",
      "batch_idx: 19\n",
      "0.5291666666666667\n",
      "batch_idx: 20\n",
      "0.5297619047619048\n",
      "batch_idx: 21\n",
      "0.5227272727272727\n",
      "batch_idx: 22\n",
      "0.5253623188405797\n",
      "batch_idx: 23\n",
      "0.5208333333333334\n",
      "batch_idx: 24\n",
      "0.52\n",
      "Training Epoch: 35, total loss: 48.391508\n",
      "batch_idx: 0\n",
      "0.4166666666666667\n",
      "batch_idx: 1\n",
      "0.5416666666666666\n",
      "batch_idx: 2\n",
      "0.4861111111111111\n",
      "batch_idx: 3\n",
      "0.5\n",
      "batch_idx: 4\n",
      "0.5083333333333333\n",
      "batch_idx: 5\n",
      "0.4722222222222222\n",
      "batch_idx: 6\n",
      "0.47619047619047616\n",
      "batch_idx: 7\n",
      "0.5\n",
      "batch_idx: 8\n",
      "0.49537037037037035\n",
      "batch_idx: 9\n",
      "0.49583333333333335\n",
      "batch_idx: 10\n",
      "0.5037878787878788\n",
      "batch_idx: 11\n",
      "0.4965277777777778\n",
      "batch_idx: 12\n",
      "0.5064102564102564\n",
      "batch_idx: 13\n",
      "0.5148809523809523\n",
      "batch_idx: 14\n",
      "0.5222222222222223\n",
      "batch_idx: 15\n",
      "0.5182291666666666\n",
      "batch_idx: 16\n",
      "0.5294117647058824\n",
      "batch_idx: 17\n",
      "0.5231481481481481\n",
      "batch_idx: 18\n",
      "0.5263157894736842\n",
      "batch_idx: 19\n",
      "0.5270833333333333\n",
      "batch_idx: 20\n",
      "0.5297619047619048\n",
      "batch_idx: 21\n",
      "0.5321969696969697\n",
      "batch_idx: 22\n",
      "0.5307971014492754\n",
      "batch_idx: 23\n",
      "0.5277777777777778\n",
      "batch_idx: 24\n",
      "0.525\n",
      "Training Epoch: 36, total loss: 48.370228\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.5\n",
      "batch_idx: 2\n",
      "0.5416666666666666\n",
      "batch_idx: 3\n",
      "0.5625\n",
      "batch_idx: 4\n",
      "0.6\n",
      "batch_idx: 5\n",
      "0.5694444444444444\n",
      "batch_idx: 6\n",
      "0.5654761904761905\n",
      "batch_idx: 7\n",
      "0.5364583333333334\n",
      "batch_idx: 8\n",
      "0.5277777777777778\n",
      "batch_idx: 9\n",
      "0.525\n",
      "batch_idx: 10\n",
      "0.5151515151515151\n",
      "batch_idx: 11\n",
      "0.5104166666666666\n",
      "batch_idx: 12\n",
      "0.5032051282051282\n",
      "batch_idx: 13\n",
      "0.49702380952380953\n",
      "batch_idx: 14\n",
      "0.49722222222222223\n",
      "batch_idx: 15\n",
      "0.4869791666666667\n",
      "batch_idx: 16\n",
      "0.49264705882352944\n",
      "batch_idx: 17\n",
      "0.48842592592592593\n",
      "batch_idx: 18\n",
      "0.48026315789473684\n",
      "batch_idx: 19\n",
      "0.4708333333333333\n",
      "batch_idx: 20\n",
      "0.4662698412698413\n",
      "batch_idx: 21\n",
      "0.4678030303030303\n",
      "batch_idx: 22\n",
      "0.47101449275362317\n",
      "batch_idx: 23\n",
      "0.46875\n",
      "batch_idx: 24\n",
      "0.4716666666666667\n",
      "Training Epoch: 37, total loss: 49.016306\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.5555555555555556\n",
      "batch_idx: 3\n",
      "0.53125\n",
      "batch_idx: 4\n",
      "0.5166666666666667\n",
      "batch_idx: 5\n",
      "0.5\n",
      "batch_idx: 6\n",
      "0.5238095238095238\n",
      "batch_idx: 7\n",
      "0.515625\n",
      "batch_idx: 8\n",
      "0.5185185185185185\n",
      "batch_idx: 9\n",
      "0.5208333333333334\n",
      "batch_idx: 10\n",
      "0.5151515151515151\n",
      "batch_idx: 11\n",
      "0.5069444444444444\n",
      "batch_idx: 12\n",
      "0.5192307692307693\n",
      "batch_idx: 13\n",
      "0.5297619047619048\n",
      "batch_idx: 14\n",
      "0.5222222222222223\n",
      "batch_idx: 15\n",
      "0.5208333333333334\n",
      "batch_idx: 16\n",
      "0.5049019607843137\n",
      "batch_idx: 17\n",
      "0.5115740740740741\n",
      "batch_idx: 18\n",
      "0.5219298245614035\n",
      "batch_idx: 19\n",
      "0.5229166666666667\n",
      "batch_idx: 20\n",
      "0.5238095238095238\n",
      "batch_idx: 21\n",
      "0.5246212121212122\n",
      "batch_idx: 22\n",
      "0.5181159420289855\n",
      "batch_idx: 23\n",
      "0.5225694444444444\n",
      "batch_idx: 24\n",
      "0.525\n",
      "Training Epoch: 38, total loss: 48.115042\n",
      "batch_idx: 0\n",
      "0.4166666666666667\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.5833333333333334\n",
      "batch_idx: 3\n",
      "0.5625\n",
      "batch_idx: 4\n",
      "0.5416666666666666\n",
      "batch_idx: 5\n",
      "0.5416666666666666\n",
      "batch_idx: 6\n",
      "0.5476190476190477\n",
      "batch_idx: 7\n",
      "0.5260416666666666\n",
      "batch_idx: 8\n",
      "0.5416666666666666\n",
      "batch_idx: 9\n",
      "0.5416666666666666\n",
      "batch_idx: 10\n",
      "0.5416666666666666\n",
      "batch_idx: 11\n",
      "0.5381944444444444\n",
      "batch_idx: 12\n",
      "0.532051282051282\n",
      "batch_idx: 13\n",
      "0.5357142857142857\n",
      "batch_idx: 14\n",
      "0.5333333333333333\n",
      "batch_idx: 15\n",
      "0.5286458333333334\n",
      "batch_idx: 16\n",
      "0.5269607843137255\n",
      "batch_idx: 17\n",
      "0.5254629629629629\n",
      "batch_idx: 18\n",
      "0.5307017543859649\n",
      "batch_idx: 19\n",
      "0.525\n",
      "batch_idx: 20\n",
      "0.5277777777777778\n",
      "batch_idx: 21\n",
      "0.5208333333333334\n",
      "batch_idx: 22\n",
      "0.5235507246376812\n",
      "batch_idx: 23\n",
      "0.5277777777777778\n",
      "batch_idx: 24\n",
      "0.5216666666666666\n",
      "Training Epoch: 39, total loss: 48.258526\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.5625\n",
      "batch_idx: 2\n",
      "0.5555555555555556\n",
      "batch_idx: 3\n",
      "0.5416666666666666\n",
      "batch_idx: 4\n",
      "0.525\n",
      "batch_idx: 5\n",
      "0.5\n",
      "batch_idx: 6\n",
      "0.5357142857142857\n",
      "batch_idx: 7\n",
      "0.53125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 8\n",
      "0.5231481481481481\n",
      "batch_idx: 9\n",
      "0.5458333333333333\n",
      "batch_idx: 10\n",
      "0.553030303030303\n",
      "batch_idx: 11\n",
      "0.5520833333333334\n",
      "batch_idx: 12\n",
      "0.5512820512820513\n",
      "batch_idx: 13\n",
      "0.5357142857142857\n",
      "batch_idx: 14\n",
      "0.5388888888888889\n",
      "batch_idx: 15\n",
      "0.53125\n",
      "batch_idx: 16\n",
      "0.5318627450980392\n",
      "batch_idx: 17\n",
      "0.5254629629629629\n",
      "batch_idx: 18\n",
      "0.5219298245614035\n",
      "batch_idx: 19\n",
      "0.53125\n",
      "batch_idx: 20\n",
      "0.5277777777777778\n",
      "batch_idx: 21\n",
      "0.5208333333333334\n",
      "batch_idx: 22\n",
      "0.519927536231884\n",
      "batch_idx: 23\n",
      "0.5225694444444444\n",
      "batch_idx: 24\n",
      "0.525\n",
      "Training Epoch: 40, total loss: 48.324298\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.5\n",
      "batch_idx: 3\n",
      "0.5\n",
      "batch_idx: 4\n",
      "0.49166666666666664\n",
      "batch_idx: 5\n",
      "0.5069444444444444\n",
      "batch_idx: 6\n",
      "0.4880952380952381\n",
      "batch_idx: 7\n",
      "0.4791666666666667\n",
      "batch_idx: 8\n",
      "0.48148148148148145\n",
      "batch_idx: 9\n",
      "0.4875\n",
      "batch_idx: 10\n",
      "0.4962121212121212\n",
      "batch_idx: 11\n",
      "0.5104166666666666\n",
      "batch_idx: 12\n",
      "0.5128205128205128\n",
      "batch_idx: 13\n",
      "0.5178571428571429\n",
      "batch_idx: 14\n",
      "0.5138888888888888\n",
      "batch_idx: 15\n",
      "0.5052083333333334\n",
      "batch_idx: 16\n",
      "0.49754901960784315\n",
      "batch_idx: 17\n",
      "0.5\n",
      "batch_idx: 18\n",
      "0.5087719298245614\n",
      "batch_idx: 19\n",
      "0.5125\n",
      "batch_idx: 20\n",
      "0.5138888888888888\n",
      "batch_idx: 21\n",
      "0.5170454545454546\n",
      "batch_idx: 22\n",
      "0.5181159420289855\n",
      "batch_idx: 23\n",
      "0.5243055555555556\n",
      "batch_idx: 24\n",
      "0.5266666666666666\n",
      "Training Epoch: 41, total loss: 47.987548\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.5694444444444444\n",
      "batch_idx: 3\n",
      "0.5729166666666666\n",
      "batch_idx: 4\n",
      "0.55\n",
      "batch_idx: 5\n",
      "0.5277777777777778\n",
      "batch_idx: 6\n",
      "0.5535714285714286\n",
      "batch_idx: 7\n",
      "0.5520833333333334\n",
      "batch_idx: 8\n",
      "0.5324074074074074\n",
      "batch_idx: 9\n",
      "0.5166666666666667\n",
      "batch_idx: 10\n",
      "0.5151515151515151\n",
      "batch_idx: 11\n",
      "0.5208333333333334\n",
      "batch_idx: 12\n",
      "0.5192307692307693\n",
      "batch_idx: 13\n",
      "0.5208333333333334\n",
      "batch_idx: 14\n",
      "0.5083333333333333\n",
      "batch_idx: 15\n",
      "0.515625\n",
      "batch_idx: 16\n",
      "0.5196078431372549\n",
      "batch_idx: 17\n",
      "0.5138888888888888\n",
      "batch_idx: 18\n",
      "0.5131578947368421\n",
      "batch_idx: 19\n",
      "0.51875\n",
      "batch_idx: 20\n",
      "0.5138888888888888\n",
      "batch_idx: 21\n",
      "0.5075757575757576\n",
      "batch_idx: 22\n",
      "0.5054347826086957\n",
      "batch_idx: 23\n",
      "0.5121527777777778\n",
      "batch_idx: 24\n",
      "0.5133333333333333\n",
      "Training Epoch: 42, total loss: 48.520020\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.5208333333333334\n",
      "batch_idx: 2\n",
      "0.5138888888888888\n",
      "batch_idx: 3\n",
      "0.5104166666666666\n",
      "batch_idx: 4\n",
      "0.5333333333333333\n",
      "batch_idx: 5\n",
      "0.5486111111111112\n",
      "batch_idx: 6\n",
      "0.5535714285714286\n",
      "batch_idx: 7\n",
      "0.5729166666666666\n",
      "batch_idx: 8\n",
      "0.5601851851851852\n",
      "batch_idx: 9\n",
      "0.5583333333333333\n",
      "batch_idx: 10\n",
      "0.5568181818181818\n",
      "batch_idx: 11\n",
      "0.5486111111111112\n",
      "batch_idx: 12\n",
      "0.5384615384615384\n",
      "batch_idx: 13\n",
      "0.5446428571428571\n",
      "batch_idx: 14\n",
      "0.55\n",
      "batch_idx: 15\n",
      "0.5442708333333334\n",
      "batch_idx: 16\n",
      "0.5490196078431373\n",
      "batch_idx: 17\n",
      "0.5486111111111112\n",
      "batch_idx: 18\n",
      "0.5350877192982456\n",
      "batch_idx: 19\n",
      "0.5333333333333333\n",
      "batch_idx: 20\n",
      "0.5416666666666666\n",
      "batch_idx: 21\n",
      "0.5454545454545454\n",
      "batch_idx: 22\n",
      "0.5416666666666666\n",
      "batch_idx: 23\n",
      "0.5451388888888888\n",
      "batch_idx: 24\n",
      "0.5416666666666666\n",
      "Training Epoch: 43, total loss: 48.099198\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.5694444444444444\n",
      "batch_idx: 3\n",
      "0.5625\n",
      "batch_idx: 4\n",
      "0.5666666666666667\n",
      "batch_idx: 5\n",
      "0.5347222222222222\n",
      "batch_idx: 6\n",
      "0.5476190476190477\n",
      "batch_idx: 7\n",
      "0.5416666666666666\n",
      "batch_idx: 8\n",
      "0.5277777777777778\n",
      "batch_idx: 9\n",
      "0.5333333333333333\n",
      "batch_idx: 10\n",
      "0.5151515151515151\n",
      "batch_idx: 11\n",
      "0.5034722222222222\n",
      "batch_idx: 12\n",
      "0.5032051282051282\n",
      "batch_idx: 13\n",
      "0.5059523809523809\n",
      "batch_idx: 14\n",
      "0.5138888888888888\n",
      "batch_idx: 15\n",
      "0.5130208333333334\n",
      "batch_idx: 16\n",
      "0.5147058823529411\n",
      "batch_idx: 17\n",
      "0.5092592592592593\n",
      "batch_idx: 18\n",
      "0.5153508771929824\n",
      "batch_idx: 19\n",
      "0.5145833333333333\n",
      "batch_idx: 20\n",
      "0.5198412698412699\n",
      "batch_idx: 21\n",
      "0.5170454545454546\n",
      "batch_idx: 22\n",
      "0.5253623188405797\n",
      "batch_idx: 23\n",
      "0.5208333333333334\n",
      "batch_idx: 24\n",
      "0.5166666666666667\n",
      "Training Epoch: 44, total loss: 48.378922\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.6145833333333334\n",
      "batch_idx: 4\n",
      "0.625\n",
      "batch_idx: 5\n",
      "0.5833333333333334\n",
      "batch_idx: 6\n",
      "0.5714285714285714\n",
      "batch_idx: 7\n",
      "0.5520833333333334\n",
      "batch_idx: 8\n",
      "0.5555555555555556\n",
      "batch_idx: 9\n",
      "0.5583333333333333\n",
      "batch_idx: 10\n",
      "0.553030303030303\n",
      "batch_idx: 11\n",
      "0.5625\n",
      "batch_idx: 12\n",
      "0.5673076923076923\n",
      "batch_idx: 13\n",
      "0.5505952380952381\n",
      "batch_idx: 14\n",
      "0.5388888888888889\n",
      "batch_idx: 15\n",
      "0.53125\n",
      "batch_idx: 16\n",
      "0.5294117647058824\n",
      "batch_idx: 17\n",
      "0.5277777777777778\n",
      "batch_idx: 18\n",
      "0.5350877192982456\n",
      "batch_idx: 19\n",
      "0.5270833333333333\n",
      "batch_idx: 20\n",
      "0.5218253968253969\n",
      "batch_idx: 21\n",
      "0.5189393939393939\n",
      "batch_idx: 22\n",
      "0.5253623188405797\n",
      "batch_idx: 23\n",
      "0.5208333333333334\n",
      "batch_idx: 24\n",
      "0.5266666666666666\n",
      "Training Epoch: 45, total loss: 48.236261\n",
      "batch_idx: 0\n",
      "0.375\n",
      "batch_idx: 1\n",
      "0.5416666666666666\n",
      "batch_idx: 2\n",
      "0.5972222222222222\n",
      "batch_idx: 3\n",
      "0.5625\n",
      "batch_idx: 4\n",
      "0.5833333333333334\n",
      "batch_idx: 5\n",
      "0.5833333333333334\n",
      "batch_idx: 6\n",
      "0.5654761904761905\n",
      "batch_idx: 7\n",
      "0.578125\n",
      "batch_idx: 8\n",
      "0.5555555555555556\n",
      "batch_idx: 9\n",
      "0.5541666666666667\n",
      "batch_idx: 10\n",
      "0.5606060606060606\n",
      "batch_idx: 11\n",
      "0.5590277777777778\n",
      "batch_idx: 12\n",
      "0.5673076923076923\n",
      "batch_idx: 13\n",
      "0.5595238095238095\n",
      "batch_idx: 14\n",
      "0.5611111111111111\n",
      "batch_idx: 15\n",
      "0.5546875\n",
      "batch_idx: 16\n",
      "0.5514705882352942\n",
      "batch_idx: 17\n",
      "0.5555555555555556\n",
      "batch_idx: 18\n",
      "0.5548245614035088\n",
      "batch_idx: 19\n",
      "0.5583333333333333\n",
      "batch_idx: 20\n",
      "0.5476190476190477\n",
      "batch_idx: 21\n",
      "0.5397727272727273\n",
      "batch_idx: 22\n",
      "0.532608695652174\n",
      "batch_idx: 23\n",
      "0.5347222222222222\n",
      "batch_idx: 24\n",
      "0.5316666666666666\n",
      "Training Epoch: 46, total loss: 48.206734\n",
      "batch_idx: 0\n",
      "0.3333333333333333\n",
      "batch_idx: 1\n",
      "0.4583333333333333\n",
      "batch_idx: 2\n",
      "0.4861111111111111\n",
      "batch_idx: 3\n",
      "0.46875\n",
      "batch_idx: 4\n",
      "0.4666666666666667\n",
      "batch_idx: 5\n",
      "0.4861111111111111\n",
      "batch_idx: 6\n",
      "0.47619047619047616\n",
      "batch_idx: 7\n",
      "0.46875\n",
      "batch_idx: 8\n",
      "0.46296296296296297\n",
      "batch_idx: 9\n",
      "0.45\n",
      "batch_idx: 10\n",
      "0.45454545454545453\n",
      "batch_idx: 11\n",
      "0.46875\n",
      "batch_idx: 12\n",
      "0.46794871794871795\n",
      "batch_idx: 13\n",
      "0.46726190476190477\n",
      "batch_idx: 14\n",
      "0.4888888888888889\n",
      "batch_idx: 15\n",
      "0.4869791666666667\n",
      "batch_idx: 16\n",
      "0.49264705882352944\n",
      "batch_idx: 17\n",
      "0.4930555555555556\n",
      "batch_idx: 18\n",
      "0.49122807017543857\n",
      "batch_idx: 19\n",
      "0.49583333333333335\n",
      "batch_idx: 20\n",
      "0.501984126984127\n",
      "batch_idx: 21\n",
      "0.5037878787878788\n",
      "batch_idx: 22\n",
      "0.5036231884057971\n",
      "batch_idx: 23\n",
      "0.5\n",
      "batch_idx: 24\n",
      "0.49666666666666665\n",
      "Training Epoch: 47, total loss: 48.789583\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.5\n",
      "batch_idx: 2\n",
      "0.4722222222222222\n",
      "batch_idx: 3\n",
      "0.4479166666666667\n",
      "batch_idx: 4\n",
      "0.43333333333333335\n",
      "batch_idx: 5\n",
      "0.4444444444444444\n",
      "batch_idx: 6\n",
      "0.47023809523809523\n",
      "batch_idx: 7\n",
      "0.4791666666666667\n",
      "batch_idx: 8\n",
      "0.5\n",
      "batch_idx: 9\n",
      "0.49583333333333335\n",
      "batch_idx: 10\n",
      "0.4962121212121212\n",
      "batch_idx: 11\n",
      "0.4895833333333333\n",
      "batch_idx: 12\n",
      "0.49038461538461536\n",
      "batch_idx: 13\n",
      "0.4851190476190476\n",
      "batch_idx: 14\n",
      "0.4777777777777778\n",
      "batch_idx: 15\n",
      "0.4817708333333333\n",
      "batch_idx: 16\n",
      "0.4877450980392157\n",
      "batch_idx: 17\n",
      "0.4837962962962963\n",
      "batch_idx: 18\n",
      "0.4956140350877193\n",
      "batch_idx: 19\n",
      "0.5020833333333333\n",
      "batch_idx: 20\n",
      "0.49603174603174605\n",
      "batch_idx: 21\n",
      "0.5037878787878788\n",
      "batch_idx: 22\n",
      "0.5018115942028986\n",
      "batch_idx: 23\n",
      "0.5104166666666666\n",
      "batch_idx: 24\n",
      "0.5083333333333333\n",
      "Training Epoch: 48, total loss: 48.731398\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.5694444444444444\n",
      "batch_idx: 3\n",
      "0.5729166666666666\n",
      "batch_idx: 4\n",
      "0.5416666666666666\n",
      "batch_idx: 5\n",
      "0.5416666666666666\n",
      "batch_idx: 6\n",
      "0.5654761904761905\n",
      "batch_idx: 7\n",
      "0.5520833333333334\n",
      "batch_idx: 8\n",
      "0.5370370370370371\n",
      "batch_idx: 9\n",
      "0.5416666666666666\n",
      "batch_idx: 10\n",
      "0.5416666666666666\n",
      "batch_idx: 11\n",
      "0.5347222222222222\n",
      "batch_idx: 12\n",
      "0.5416666666666666\n",
      "batch_idx: 13\n",
      "0.5446428571428571\n",
      "batch_idx: 14\n",
      "0.5388888888888889\n",
      "batch_idx: 15\n",
      "0.5364583333333334\n",
      "batch_idx: 16\n",
      "0.5367647058823529\n",
      "batch_idx: 17\n",
      "0.5393518518518519\n",
      "batch_idx: 18\n",
      "0.5416666666666666\n",
      "batch_idx: 19\n",
      "0.5458333333333333\n",
      "batch_idx: 20\n",
      "0.5476190476190477\n",
      "batch_idx: 21\n",
      "0.5492424242424242\n",
      "batch_idx: 22\n",
      "0.5507246376811594\n",
      "batch_idx: 23\n",
      "0.5416666666666666\n",
      "batch_idx: 24\n",
      "0.5416666666666666\n",
      "Training Epoch: 49, total loss: 48.082362\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.6041666666666666\n",
      "batch_idx: 4\n",
      "0.5916666666666667\n",
      "batch_idx: 5\n",
      "0.5972222222222222\n",
      "batch_idx: 6\n",
      "0.6071428571428571\n",
      "batch_idx: 7\n",
      "0.5729166666666666\n",
      "batch_idx: 8\n",
      "0.5787037037037037\n",
      "batch_idx: 9\n",
      "0.5875\n",
      "batch_idx: 10\n",
      "0.5795454545454546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 11\n",
      "0.5798611111111112\n",
      "batch_idx: 12\n",
      "0.5737179487179487\n",
      "batch_idx: 13\n",
      "0.5625\n",
      "batch_idx: 14\n",
      "0.5472222222222223\n",
      "batch_idx: 15\n",
      "0.5390625\n",
      "batch_idx: 16\n",
      "0.5367647058823529\n",
      "batch_idx: 17\n",
      "0.5324074074074074\n",
      "batch_idx: 18\n",
      "0.5285087719298246\n",
      "batch_idx: 19\n",
      "0.5229166666666667\n",
      "batch_idx: 20\n",
      "0.5158730158730159\n",
      "batch_idx: 21\n",
      "0.5227272727272727\n",
      "batch_idx: 22\n",
      "0.5271739130434783\n",
      "batch_idx: 23\n",
      "0.5190972222222222\n",
      "batch_idx: 24\n",
      "0.5216666666666666\n",
      "Training Epoch: 50, total loss: 48.194261\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.5208333333333334\n",
      "batch_idx: 2\n",
      "0.5694444444444444\n",
      "batch_idx: 3\n",
      "0.5520833333333334\n",
      "batch_idx: 4\n",
      "0.55\n",
      "batch_idx: 5\n",
      "0.5625\n",
      "batch_idx: 6\n",
      "0.5535714285714286\n",
      "batch_idx: 7\n",
      "0.5572916666666666\n",
      "batch_idx: 8\n",
      "0.5555555555555556\n",
      "batch_idx: 9\n",
      "0.5666666666666667\n",
      "batch_idx: 10\n",
      "0.5643939393939394\n",
      "batch_idx: 11\n",
      "0.5590277777777778\n",
      "batch_idx: 12\n",
      "0.5512820512820513\n",
      "batch_idx: 13\n",
      "0.5505952380952381\n",
      "batch_idx: 14\n",
      "0.5444444444444444\n",
      "batch_idx: 15\n",
      "0.546875\n",
      "batch_idx: 16\n",
      "0.5612745098039216\n",
      "batch_idx: 17\n",
      "0.5601851851851852\n",
      "batch_idx: 18\n",
      "0.5570175438596491\n",
      "batch_idx: 19\n",
      "0.55625\n",
      "batch_idx: 20\n",
      "0.5595238095238095\n",
      "batch_idx: 21\n",
      "0.5492424242424242\n",
      "batch_idx: 22\n",
      "0.552536231884058\n",
      "batch_idx: 23\n",
      "0.5486111111111112\n",
      "batch_idx: 24\n",
      "0.5433333333333333\n",
      "Training Epoch: 51, total loss: 47.946706\n",
      "batch_idx: 0\n",
      "0.4583333333333333\n",
      "batch_idx: 1\n",
      "0.4583333333333333\n",
      "batch_idx: 2\n",
      "0.4166666666666667\n",
      "batch_idx: 3\n",
      "0.4270833333333333\n",
      "batch_idx: 4\n",
      "0.44166666666666665\n",
      "batch_idx: 5\n",
      "0.4583333333333333\n",
      "batch_idx: 6\n",
      "0.47619047619047616\n",
      "batch_idx: 7\n",
      "0.4791666666666667\n",
      "batch_idx: 8\n",
      "0.49074074074074076\n",
      "batch_idx: 9\n",
      "0.5\n",
      "batch_idx: 10\n",
      "0.5\n",
      "batch_idx: 11\n",
      "0.5173611111111112\n",
      "batch_idx: 12\n",
      "0.5224358974358975\n",
      "batch_idx: 13\n",
      "0.5357142857142857\n",
      "batch_idx: 14\n",
      "0.5277777777777778\n",
      "batch_idx: 15\n",
      "0.5390625\n",
      "batch_idx: 16\n",
      "0.5294117647058824\n",
      "batch_idx: 17\n",
      "0.5300925925925926\n",
      "batch_idx: 18\n",
      "0.5328947368421053\n",
      "batch_idx: 19\n",
      "0.5416666666666666\n",
      "batch_idx: 20\n",
      "0.5416666666666666\n",
      "batch_idx: 21\n",
      "0.5454545454545454\n",
      "batch_idx: 22\n",
      "0.5380434782608695\n",
      "batch_idx: 23\n",
      "0.5399305555555556\n",
      "batch_idx: 24\n",
      "0.5433333333333333\n",
      "Training Epoch: 52, total loss: 47.925809\n",
      "batch_idx: 0\n",
      "0.3333333333333333\n",
      "batch_idx: 1\n",
      "0.5208333333333334\n",
      "batch_idx: 2\n",
      "0.5694444444444444\n",
      "batch_idx: 3\n",
      "0.5625\n",
      "batch_idx: 4\n",
      "0.5333333333333333\n",
      "batch_idx: 5\n",
      "0.5416666666666666\n",
      "batch_idx: 6\n",
      "0.5595238095238095\n",
      "batch_idx: 7\n",
      "0.5625\n",
      "batch_idx: 8\n",
      "0.5787037037037037\n",
      "batch_idx: 9\n",
      "0.5666666666666667\n",
      "batch_idx: 10\n",
      "0.5606060606060606\n",
      "batch_idx: 11\n",
      "0.5451388888888888\n",
      "batch_idx: 12\n",
      "0.5416666666666666\n",
      "batch_idx: 13\n",
      "0.5416666666666666\n",
      "batch_idx: 14\n",
      "0.5444444444444444\n",
      "batch_idx: 15\n",
      "0.546875\n",
      "batch_idx: 16\n",
      "0.5465686274509803\n",
      "batch_idx: 17\n",
      "0.5416666666666666\n",
      "batch_idx: 18\n",
      "0.5416666666666666\n",
      "batch_idx: 19\n",
      "0.5416666666666666\n",
      "batch_idx: 20\n",
      "0.5357142857142857\n",
      "batch_idx: 21\n",
      "0.5378787878787878\n",
      "batch_idx: 22\n",
      "0.5344202898550725\n",
      "batch_idx: 23\n",
      "0.53125\n",
      "batch_idx: 24\n",
      "0.5333333333333333\n",
      "Training Epoch: 53, total loss: 48.019287\n",
      "batch_idx: 0\n",
      "0.4583333333333333\n",
      "batch_idx: 1\n",
      "0.5416666666666666\n",
      "batch_idx: 2\n",
      "0.5972222222222222\n",
      "batch_idx: 3\n",
      "0.5729166666666666\n",
      "batch_idx: 4\n",
      "0.5666666666666667\n",
      "batch_idx: 5\n",
      "0.5555555555555556\n",
      "batch_idx: 6\n",
      "0.5178571428571429\n",
      "batch_idx: 7\n",
      "0.515625\n",
      "batch_idx: 8\n",
      "0.5185185185185185\n",
      "batch_idx: 9\n",
      "0.5333333333333333\n",
      "batch_idx: 10\n",
      "0.5227272727272727\n",
      "batch_idx: 11\n",
      "0.5243055555555556\n",
      "batch_idx: 12\n",
      "0.5288461538461539\n",
      "batch_idx: 13\n",
      "0.5238095238095238\n",
      "batch_idx: 14\n",
      "0.5277777777777778\n",
      "batch_idx: 15\n",
      "0.5338541666666666\n",
      "batch_idx: 16\n",
      "0.5294117647058824\n",
      "batch_idx: 17\n",
      "0.5324074074074074\n",
      "batch_idx: 18\n",
      "0.5372807017543859\n",
      "batch_idx: 19\n",
      "0.5395833333333333\n",
      "batch_idx: 20\n",
      "0.5456349206349206\n",
      "batch_idx: 21\n",
      "0.5378787878787878\n",
      "batch_idx: 22\n",
      "0.5380434782608695\n",
      "batch_idx: 23\n",
      "0.5347222222222222\n",
      "batch_idx: 24\n",
      "0.535\n",
      "Training Epoch: 54, total loss: 47.979764\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.5416666666666666\n",
      "batch_idx: 3\n",
      "0.5833333333333334\n",
      "batch_idx: 4\n",
      "0.5666666666666667\n",
      "batch_idx: 5\n",
      "0.5833333333333334\n",
      "batch_idx: 6\n",
      "0.5654761904761905\n",
      "batch_idx: 7\n",
      "0.5677083333333334\n",
      "batch_idx: 8\n",
      "0.5555555555555556\n",
      "batch_idx: 9\n",
      "0.5625\n",
      "batch_idx: 10\n",
      "0.5568181818181818\n",
      "batch_idx: 11\n",
      "0.5520833333333334\n",
      "batch_idx: 12\n",
      "0.5448717948717948\n",
      "batch_idx: 13\n",
      "0.5476190476190477\n",
      "batch_idx: 14\n",
      "0.5444444444444444\n",
      "batch_idx: 15\n",
      "0.5572916666666666\n",
      "batch_idx: 16\n",
      "0.5588235294117647\n",
      "batch_idx: 17\n",
      "0.5509259259259259\n",
      "batch_idx: 18\n",
      "0.5570175438596491\n",
      "batch_idx: 19\n",
      "0.5520833333333334\n",
      "batch_idx: 20\n",
      "0.5575396825396826\n",
      "batch_idx: 21\n",
      "0.5625\n",
      "batch_idx: 22\n",
      "0.5634057971014492\n",
      "batch_idx: 23\n",
      "0.5659722222222222\n",
      "batch_idx: 24\n",
      "0.5716666666666667\n",
      "Training Epoch: 55, total loss: 47.336933\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.5694444444444444\n",
      "batch_idx: 3\n",
      "0.5729166666666666\n",
      "batch_idx: 4\n",
      "0.5583333333333333\n",
      "batch_idx: 5\n",
      "0.5763888888888888\n",
      "batch_idx: 6\n",
      "0.5654761904761905\n",
      "batch_idx: 7\n",
      "0.5729166666666666\n",
      "batch_idx: 8\n",
      "0.5694444444444444\n",
      "batch_idx: 9\n",
      "0.5666666666666667\n",
      "batch_idx: 10\n",
      "0.571969696969697\n",
      "batch_idx: 11\n",
      "0.5451388888888888\n",
      "batch_idx: 12\n",
      "0.5512820512820513\n",
      "batch_idx: 13\n",
      "0.5625\n",
      "batch_idx: 14\n",
      "0.5444444444444444\n",
      "batch_idx: 15\n",
      "0.5442708333333334\n",
      "batch_idx: 16\n",
      "0.5514705882352942\n",
      "batch_idx: 17\n",
      "0.5532407407407407\n",
      "batch_idx: 18\n",
      "0.5482456140350878\n",
      "batch_idx: 19\n",
      "0.5395833333333333\n",
      "batch_idx: 20\n",
      "0.5436507936507936\n",
      "batch_idx: 21\n",
      "0.5473484848484849\n",
      "batch_idx: 22\n",
      "0.5561594202898551\n",
      "batch_idx: 23\n",
      "0.5572916666666666\n",
      "batch_idx: 24\n",
      "0.5616666666666666\n",
      "Training Epoch: 56, total loss: 47.486402\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.5694444444444444\n",
      "batch_idx: 3\n",
      "0.5625\n",
      "batch_idx: 4\n",
      "0.5416666666666666\n",
      "batch_idx: 5\n",
      "0.5555555555555556\n",
      "batch_idx: 6\n",
      "0.5654761904761905\n",
      "batch_idx: 7\n",
      "0.5625\n",
      "batch_idx: 8\n",
      "0.5462962962962963\n",
      "batch_idx: 9\n",
      "0.5333333333333333\n",
      "batch_idx: 10\n",
      "0.5378787878787878\n",
      "batch_idx: 11\n",
      "0.5451388888888888\n",
      "batch_idx: 12\n",
      "0.5544871794871795\n",
      "batch_idx: 13\n",
      "0.5446428571428571\n",
      "batch_idx: 14\n",
      "0.5333333333333333\n",
      "batch_idx: 15\n",
      "0.5338541666666666\n",
      "batch_idx: 16\n",
      "0.5343137254901961\n",
      "batch_idx: 17\n",
      "0.5231481481481481\n",
      "batch_idx: 18\n",
      "0.5131578947368421\n",
      "batch_idx: 19\n",
      "0.5104166666666666\n",
      "batch_idx: 20\n",
      "0.5198412698412699\n",
      "batch_idx: 21\n",
      "0.5189393939393939\n",
      "batch_idx: 22\n",
      "0.5235507246376812\n",
      "batch_idx: 23\n",
      "0.5243055555555556\n",
      "batch_idx: 24\n",
      "0.5233333333333333\n",
      "Training Epoch: 57, total loss: 48.129419\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.5972222222222222\n",
      "batch_idx: 3\n",
      "0.5520833333333334\n",
      "batch_idx: 4\n",
      "0.5583333333333333\n",
      "batch_idx: 5\n",
      "0.5486111111111112\n",
      "batch_idx: 6\n",
      "0.5416666666666666\n",
      "batch_idx: 7\n",
      "0.515625\n",
      "batch_idx: 8\n",
      "0.5277777777777778\n",
      "batch_idx: 9\n",
      "0.5166666666666667\n",
      "batch_idx: 10\n",
      "0.5340909090909091\n",
      "batch_idx: 11\n",
      "0.5277777777777778\n",
      "batch_idx: 12\n",
      "0.532051282051282\n",
      "batch_idx: 13\n",
      "0.5386904761904762\n",
      "batch_idx: 14\n",
      "0.5222222222222223\n",
      "batch_idx: 15\n",
      "0.5130208333333334\n",
      "batch_idx: 16\n",
      "0.5122549019607843\n",
      "batch_idx: 17\n",
      "0.5162037037037037\n",
      "batch_idx: 18\n",
      "0.5197368421052632\n",
      "batch_idx: 19\n",
      "0.5125\n",
      "batch_idx: 20\n",
      "0.5138888888888888\n",
      "batch_idx: 21\n",
      "0.509469696969697\n",
      "batch_idx: 22\n",
      "0.5072463768115942\n",
      "batch_idx: 23\n",
      "0.5069444444444444\n",
      "batch_idx: 24\n",
      "0.5083333333333333\n",
      "Training Epoch: 58, total loss: 48.612984\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.6041666666666666\n",
      "batch_idx: 4\n",
      "0.6083333333333333\n",
      "batch_idx: 5\n",
      "0.6180555555555556\n",
      "batch_idx: 6\n",
      "0.5952380952380952\n",
      "batch_idx: 7\n",
      "0.5677083333333334\n",
      "batch_idx: 8\n",
      "0.5509259259259259\n",
      "batch_idx: 9\n",
      "0.5458333333333333\n",
      "batch_idx: 10\n",
      "0.5416666666666666\n",
      "batch_idx: 11\n",
      "0.5555555555555556\n",
      "batch_idx: 12\n",
      "0.5512820512820513\n",
      "batch_idx: 13\n",
      "0.5476190476190477\n",
      "batch_idx: 14\n",
      "0.5416666666666666\n",
      "batch_idx: 15\n",
      "0.5390625\n",
      "batch_idx: 16\n",
      "0.5318627450980392\n",
      "batch_idx: 17\n",
      "0.5324074074074074\n",
      "batch_idx: 18\n",
      "0.5285087719298246\n",
      "batch_idx: 19\n",
      "0.525\n",
      "batch_idx: 20\n",
      "0.5178571428571429\n",
      "batch_idx: 21\n",
      "0.5189393939393939\n",
      "batch_idx: 22\n",
      "0.5108695652173914\n",
      "batch_idx: 23\n",
      "0.5138888888888888\n",
      "batch_idx: 24\n",
      "0.5133333333333333\n",
      "Training Epoch: 59, total loss: 48.252836\n",
      "batch_idx: 0\n",
      "0.4166666666666667\n",
      "batch_idx: 1\n",
      "0.4375\n",
      "batch_idx: 2\n",
      "0.4722222222222222\n",
      "batch_idx: 3\n",
      "0.4895833333333333\n",
      "batch_idx: 4\n",
      "0.5083333333333333\n",
      "batch_idx: 5\n",
      "0.5277777777777778\n",
      "batch_idx: 6\n",
      "0.5416666666666666\n",
      "batch_idx: 7\n",
      "0.5208333333333334\n",
      "batch_idx: 8\n",
      "0.5324074074074074\n",
      "batch_idx: 9\n",
      "0.5416666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 10\n",
      "0.5568181818181818\n",
      "batch_idx: 11\n",
      "0.5659722222222222\n",
      "batch_idx: 12\n",
      "0.5512820512820513\n",
      "batch_idx: 13\n",
      "0.5505952380952381\n",
      "batch_idx: 14\n",
      "0.5388888888888889\n",
      "batch_idx: 15\n",
      "0.5390625\n",
      "batch_idx: 16\n",
      "0.5318627450980392\n",
      "batch_idx: 17\n",
      "0.5347222222222222\n",
      "batch_idx: 18\n",
      "0.5350877192982456\n",
      "batch_idx: 19\n",
      "0.54375\n",
      "batch_idx: 20\n",
      "0.5476190476190477\n",
      "batch_idx: 21\n",
      "0.5492424242424242\n",
      "batch_idx: 22\n",
      "0.5471014492753623\n",
      "batch_idx: 23\n",
      "0.5486111111111112\n",
      "batch_idx: 24\n",
      "0.5533333333333333\n",
      "Training Epoch: 60, total loss: 47.804003\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.5208333333333334\n",
      "batch_idx: 2\n",
      "0.4861111111111111\n",
      "batch_idx: 3\n",
      "0.5208333333333334\n",
      "batch_idx: 4\n",
      "0.5583333333333333\n",
      "batch_idx: 5\n",
      "0.5694444444444444\n",
      "batch_idx: 6\n",
      "0.5654761904761905\n",
      "batch_idx: 7\n",
      "0.5729166666666666\n",
      "batch_idx: 8\n",
      "0.5787037037037037\n",
      "batch_idx: 9\n",
      "0.5833333333333334\n",
      "batch_idx: 10\n",
      "0.5871212121212122\n",
      "batch_idx: 11\n",
      "0.6006944444444444\n",
      "batch_idx: 12\n",
      "0.592948717948718\n",
      "batch_idx: 13\n",
      "0.5922619047619048\n",
      "batch_idx: 14\n",
      "0.5833333333333334\n",
      "batch_idx: 15\n",
      "0.5755208333333334\n",
      "batch_idx: 16\n",
      "0.5759803921568627\n",
      "batch_idx: 17\n",
      "0.5763888888888888\n",
      "batch_idx: 18\n",
      "0.5745614035087719\n",
      "batch_idx: 19\n",
      "0.5770833333333333\n",
      "batch_idx: 20\n",
      "0.5714285714285714\n",
      "batch_idx: 21\n",
      "0.5700757575757576\n",
      "batch_idx: 22\n",
      "0.5760869565217391\n",
      "batch_idx: 23\n",
      "0.5815972222222222\n",
      "batch_idx: 24\n",
      "0.5833333333333334\n",
      "Training Epoch: 61, total loss: 47.131244\n",
      "batch_idx: 0\n",
      "0.4583333333333333\n",
      "batch_idx: 1\n",
      "0.4166666666666667\n",
      "batch_idx: 2\n",
      "0.4166666666666667\n",
      "batch_idx: 3\n",
      "0.4791666666666667\n",
      "batch_idx: 4\n",
      "0.5083333333333333\n",
      "batch_idx: 5\n",
      "0.4722222222222222\n",
      "batch_idx: 6\n",
      "0.5\n",
      "batch_idx: 7\n",
      "0.4895833333333333\n",
      "batch_idx: 8\n",
      "0.5092592592592593\n",
      "batch_idx: 9\n",
      "0.5291666666666667\n",
      "batch_idx: 10\n",
      "0.5265151515151515\n",
      "batch_idx: 11\n",
      "0.5277777777777778\n",
      "batch_idx: 12\n",
      "0.5256410256410257\n",
      "batch_idx: 13\n",
      "0.5327380952380952\n",
      "batch_idx: 14\n",
      "0.5222222222222223\n",
      "batch_idx: 15\n",
      "0.5260416666666666\n",
      "batch_idx: 16\n",
      "0.5392156862745098\n",
      "batch_idx: 17\n",
      "0.5439814814814815\n",
      "batch_idx: 18\n",
      "0.5526315789473685\n",
      "batch_idx: 19\n",
      "0.5541666666666667\n",
      "batch_idx: 20\n",
      "0.5515873015873016\n",
      "batch_idx: 21\n",
      "0.5587121212121212\n",
      "batch_idx: 22\n",
      "0.5597826086956522\n",
      "batch_idx: 23\n",
      "0.5625\n",
      "batch_idx: 24\n",
      "0.5666666666666667\n",
      "Training Epoch: 62, total loss: 47.421471\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.5833333333333334\n",
      "batch_idx: 3\n",
      "0.5625\n",
      "batch_idx: 4\n",
      "0.5583333333333333\n",
      "batch_idx: 5\n",
      "0.5694444444444444\n",
      "batch_idx: 6\n",
      "0.5773809523809523\n",
      "batch_idx: 7\n",
      "0.5520833333333334\n",
      "batch_idx: 8\n",
      "0.5416666666666666\n",
      "batch_idx: 9\n",
      "0.525\n",
      "batch_idx: 10\n",
      "0.5189393939393939\n",
      "batch_idx: 11\n",
      "0.5243055555555556\n",
      "batch_idx: 12\n",
      "0.5128205128205128\n",
      "batch_idx: 13\n",
      "0.5178571428571429\n",
      "batch_idx: 14\n",
      "0.5277777777777778\n",
      "batch_idx: 15\n",
      "0.515625\n",
      "batch_idx: 16\n",
      "0.5098039215686274\n",
      "batch_idx: 17\n",
      "0.5162037037037037\n",
      "batch_idx: 18\n",
      "0.5241228070175439\n",
      "batch_idx: 19\n",
      "0.525\n",
      "batch_idx: 20\n",
      "0.5277777777777778\n",
      "batch_idx: 21\n",
      "0.5208333333333334\n",
      "batch_idx: 22\n",
      "0.519927536231884\n",
      "batch_idx: 23\n",
      "0.5173611111111112\n",
      "batch_idx: 24\n",
      "0.515\n",
      "Training Epoch: 63, total loss: 48.208504\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.5972222222222222\n",
      "batch_idx: 3\n",
      "0.6041666666666666\n",
      "batch_idx: 4\n",
      "0.5916666666666667\n",
      "batch_idx: 5\n",
      "0.5694444444444444\n",
      "batch_idx: 6\n",
      "0.5535714285714286\n",
      "batch_idx: 7\n",
      "0.53125\n",
      "batch_idx: 8\n",
      "0.5277777777777778\n",
      "batch_idx: 9\n",
      "0.5208333333333334\n",
      "batch_idx: 10\n",
      "0.5189393939393939\n",
      "batch_idx: 11\n",
      "0.5277777777777778\n",
      "batch_idx: 12\n",
      "0.5384615384615384\n",
      "batch_idx: 13\n",
      "0.5505952380952381\n",
      "batch_idx: 14\n",
      "0.5527777777777778\n",
      "batch_idx: 15\n",
      "0.5625\n",
      "batch_idx: 16\n",
      "0.5735294117647058\n",
      "batch_idx: 17\n",
      "0.5740740740740741\n",
      "batch_idx: 18\n",
      "0.5657894736842105\n",
      "batch_idx: 19\n",
      "0.5604166666666667\n",
      "batch_idx: 20\n",
      "0.5595238095238095\n",
      "batch_idx: 21\n",
      "0.5643939393939394\n",
      "batch_idx: 22\n",
      "0.5561594202898551\n",
      "batch_idx: 23\n",
      "0.5555555555555556\n",
      "batch_idx: 24\n",
      "0.56\n",
      "Training Epoch: 64, total loss: 47.492741\n",
      "batch_idx: 0\n",
      "0.4166666666666667\n",
      "batch_idx: 1\n",
      "0.4583333333333333\n",
      "batch_idx: 2\n",
      "0.5277777777777778\n",
      "batch_idx: 3\n",
      "0.5208333333333334\n",
      "batch_idx: 4\n",
      "0.5083333333333333\n",
      "batch_idx: 5\n",
      "0.4930555555555556\n",
      "batch_idx: 6\n",
      "0.49404761904761907\n",
      "batch_idx: 7\n",
      "0.5052083333333334\n",
      "batch_idx: 8\n",
      "0.5092592592592593\n",
      "batch_idx: 9\n",
      "0.5\n",
      "batch_idx: 10\n",
      "0.5075757575757576\n",
      "batch_idx: 11\n",
      "0.5173611111111112\n",
      "batch_idx: 12\n",
      "0.5224358974358975\n",
      "batch_idx: 13\n",
      "0.5297619047619048\n",
      "batch_idx: 14\n",
      "0.5333333333333333\n",
      "batch_idx: 15\n",
      "0.5260416666666666\n",
      "batch_idx: 16\n",
      "0.5269607843137255\n",
      "batch_idx: 17\n",
      "0.5370370370370371\n",
      "batch_idx: 18\n",
      "0.5394736842105263\n",
      "batch_idx: 19\n",
      "0.5375\n",
      "batch_idx: 20\n",
      "0.5416666666666666\n",
      "batch_idx: 21\n",
      "0.5397727272727273\n",
      "batch_idx: 22\n",
      "0.5471014492753623\n",
      "batch_idx: 23\n",
      "0.5451388888888888\n",
      "batch_idx: 24\n",
      "0.5483333333333333\n",
      "Training Epoch: 65, total loss: 47.648594\n",
      "batch_idx: 0\n",
      "0.375\n",
      "batch_idx: 1\n",
      "0.3333333333333333\n",
      "batch_idx: 2\n",
      "0.3333333333333333\n",
      "batch_idx: 3\n",
      "0.3854166666666667\n",
      "batch_idx: 4\n",
      "0.425\n",
      "batch_idx: 5\n",
      "0.4513888888888889\n",
      "batch_idx: 6\n",
      "0.4583333333333333\n",
      "batch_idx: 7\n",
      "0.4583333333333333\n",
      "batch_idx: 8\n",
      "0.4861111111111111\n",
      "batch_idx: 9\n",
      "0.49166666666666664\n",
      "batch_idx: 10\n",
      "0.5037878787878788\n",
      "batch_idx: 11\n",
      "0.5138888888888888\n",
      "batch_idx: 12\n",
      "0.5096153846153846\n",
      "batch_idx: 13\n",
      "0.5208333333333334\n",
      "batch_idx: 14\n",
      "0.5361111111111111\n",
      "batch_idx: 15\n",
      "0.53125\n",
      "batch_idx: 16\n",
      "0.5269607843137255\n",
      "batch_idx: 17\n",
      "0.5231481481481481\n",
      "batch_idx: 18\n",
      "0.5263157894736842\n",
      "batch_idx: 19\n",
      "0.525\n",
      "batch_idx: 20\n",
      "0.5317460317460317\n",
      "batch_idx: 21\n",
      "0.5265151515151515\n",
      "batch_idx: 22\n",
      "0.5289855072463768\n",
      "batch_idx: 23\n",
      "0.5381944444444444\n",
      "batch_idx: 24\n",
      "0.5333333333333333\n",
      "Training Epoch: 66, total loss: 48.042891\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.5208333333333334\n",
      "batch_idx: 2\n",
      "0.5277777777777778\n",
      "batch_idx: 3\n",
      "0.5\n",
      "batch_idx: 4\n",
      "0.49166666666666664\n",
      "batch_idx: 5\n",
      "0.4861111111111111\n",
      "batch_idx: 6\n",
      "0.4880952380952381\n",
      "batch_idx: 7\n",
      "0.4895833333333333\n",
      "batch_idx: 8\n",
      "0.49537037037037035\n",
      "batch_idx: 9\n",
      "0.5125\n",
      "batch_idx: 10\n",
      "0.5227272727272727\n",
      "batch_idx: 11\n",
      "0.53125\n",
      "batch_idx: 12\n",
      "0.5352564102564102\n",
      "batch_idx: 13\n",
      "0.5327380952380952\n",
      "batch_idx: 14\n",
      "0.5333333333333333\n",
      "batch_idx: 15\n",
      "0.53125\n",
      "batch_idx: 16\n",
      "0.5220588235294118\n",
      "batch_idx: 17\n",
      "0.5393518518518519\n",
      "batch_idx: 18\n",
      "0.5394736842105263\n",
      "batch_idx: 19\n",
      "0.5291666666666667\n",
      "batch_idx: 20\n",
      "0.5257936507936508\n",
      "batch_idx: 21\n",
      "0.5265151515151515\n",
      "batch_idx: 22\n",
      "0.519927536231884\n",
      "batch_idx: 23\n",
      "0.5260416666666666\n",
      "batch_idx: 24\n",
      "0.5166666666666667\n",
      "Training Epoch: 67, total loss: 48.300091\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.4791666666666667\n",
      "batch_idx: 2\n",
      "0.4444444444444444\n",
      "batch_idx: 3\n",
      "0.4375\n",
      "batch_idx: 4\n",
      "0.43333333333333335\n",
      "batch_idx: 5\n",
      "0.4583333333333333\n",
      "batch_idx: 6\n",
      "0.43452380952380953\n",
      "batch_idx: 7\n",
      "0.4635416666666667\n",
      "batch_idx: 8\n",
      "0.4537037037037037\n",
      "batch_idx: 9\n",
      "0.45\n",
      "batch_idx: 10\n",
      "0.45075757575757575\n",
      "batch_idx: 11\n",
      "0.4513888888888889\n",
      "batch_idx: 12\n",
      "0.4775641025641026\n",
      "batch_idx: 13\n",
      "0.4851190476190476\n",
      "batch_idx: 14\n",
      "0.4861111111111111\n",
      "batch_idx: 15\n",
      "0.4947916666666667\n",
      "batch_idx: 16\n",
      "0.5073529411764706\n",
      "batch_idx: 17\n",
      "0.5092592592592593\n",
      "batch_idx: 18\n",
      "0.5131578947368421\n",
      "batch_idx: 19\n",
      "0.5166666666666667\n",
      "batch_idx: 20\n",
      "0.5198412698412699\n",
      "batch_idx: 21\n",
      "0.5208333333333334\n",
      "batch_idx: 22\n",
      "0.5108695652173914\n",
      "batch_idx: 23\n",
      "0.5190972222222222\n",
      "batch_idx: 24\n",
      "0.5266666666666666\n",
      "Training Epoch: 68, total loss: 48.049972\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.5694444444444444\n",
      "batch_idx: 3\n",
      "0.5729166666666666\n",
      "batch_idx: 4\n",
      "0.625\n",
      "batch_idx: 5\n",
      "0.6111111111111112\n",
      "batch_idx: 6\n",
      "0.5952380952380952\n",
      "batch_idx: 7\n",
      "0.5833333333333334\n",
      "batch_idx: 8\n",
      "0.5833333333333334\n",
      "batch_idx: 9\n",
      "0.5833333333333334\n",
      "batch_idx: 10\n",
      "0.5833333333333334\n",
      "batch_idx: 11\n",
      "0.5902777777777778\n",
      "batch_idx: 12\n",
      "0.5897435897435898\n",
      "batch_idx: 13\n",
      "0.5892857142857143\n",
      "batch_idx: 14\n",
      "0.5833333333333334\n",
      "batch_idx: 15\n",
      "0.5755208333333334\n",
      "batch_idx: 16\n",
      "0.5735294117647058\n",
      "batch_idx: 17\n",
      "0.5625\n",
      "batch_idx: 18\n",
      "0.5614035087719298\n",
      "batch_idx: 19\n",
      "0.55625\n",
      "batch_idx: 20\n",
      "0.5535714285714286\n",
      "batch_idx: 21\n",
      "0.5606060606060606\n",
      "batch_idx: 22\n",
      "0.5634057971014492\n",
      "batch_idx: 23\n",
      "0.5694444444444444\n",
      "batch_idx: 24\n",
      "0.565\n",
      "Training Epoch: 69, total loss: 47.451642\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.625\n",
      "batch_idx: 4\n",
      "0.5833333333333334\n",
      "batch_idx: 5\n",
      "0.5833333333333334\n",
      "batch_idx: 6\n",
      "0.5773809523809523\n",
      "batch_idx: 7\n",
      "0.5729166666666666\n",
      "batch_idx: 8\n",
      "0.5740740740740741\n",
      "batch_idx: 9\n",
      "0.5541666666666667\n",
      "batch_idx: 10\n",
      "0.553030303030303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 11\n",
      "0.5486111111111112\n",
      "batch_idx: 12\n",
      "0.5480769230769231\n",
      "batch_idx: 13\n",
      "0.5505952380952381\n",
      "batch_idx: 14\n",
      "0.5638888888888889\n",
      "batch_idx: 15\n",
      "0.5651041666666666\n",
      "batch_idx: 16\n",
      "0.571078431372549\n",
      "batch_idx: 17\n",
      "0.5671296296296297\n",
      "batch_idx: 18\n",
      "0.5548245614035088\n",
      "batch_idx: 19\n",
      "0.5541666666666667\n",
      "batch_idx: 20\n",
      "0.5575396825396826\n",
      "batch_idx: 21\n",
      "0.5511363636363636\n",
      "batch_idx: 22\n",
      "0.5507246376811594\n",
      "batch_idx: 23\n",
      "0.5520833333333334\n",
      "batch_idx: 24\n",
      "0.55\n",
      "Training Epoch: 70, total loss: 47.638683\n",
      "batch_idx: 0\n",
      "0.4166666666666667\n",
      "batch_idx: 1\n",
      "0.5208333333333334\n",
      "batch_idx: 2\n",
      "0.5972222222222222\n",
      "batch_idx: 3\n",
      "0.59375\n",
      "batch_idx: 4\n",
      "0.6\n",
      "batch_idx: 5\n",
      "0.6111111111111112\n",
      "batch_idx: 6\n",
      "0.625\n",
      "batch_idx: 7\n",
      "0.640625\n",
      "batch_idx: 8\n",
      "0.6296296296296297\n",
      "batch_idx: 9\n",
      "0.6333333333333333\n",
      "batch_idx: 10\n",
      "0.6060606060606061\n",
      "batch_idx: 11\n",
      "0.5972222222222222\n",
      "batch_idx: 12\n",
      "0.6089743589743589\n",
      "batch_idx: 13\n",
      "0.6130952380952381\n",
      "batch_idx: 14\n",
      "0.6055555555555555\n",
      "batch_idx: 15\n",
      "0.609375\n",
      "batch_idx: 16\n",
      "0.6029411764705882\n",
      "batch_idx: 17\n",
      "0.6018518518518519\n",
      "batch_idx: 18\n",
      "0.6030701754385965\n",
      "batch_idx: 19\n",
      "0.5958333333333333\n",
      "batch_idx: 20\n",
      "0.5912698412698413\n",
      "batch_idx: 21\n",
      "0.5909090909090909\n",
      "batch_idx: 22\n",
      "0.5905797101449275\n",
      "batch_idx: 23\n",
      "0.5902777777777778\n",
      "batch_idx: 24\n",
      "0.585\n",
      "Training Epoch: 71, total loss: 46.969118\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.5625\n",
      "batch_idx: 2\n",
      "0.5694444444444444\n",
      "batch_idx: 3\n",
      "0.625\n",
      "batch_idx: 4\n",
      "0.6416666666666667\n",
      "batch_idx: 5\n",
      "0.6388888888888888\n",
      "batch_idx: 6\n",
      "0.6309523809523809\n",
      "batch_idx: 7\n",
      "0.6354166666666666\n",
      "batch_idx: 8\n",
      "0.6111111111111112\n",
      "batch_idx: 9\n",
      "0.6125\n",
      "batch_idx: 10\n",
      "0.5833333333333334\n",
      "batch_idx: 11\n",
      "0.5833333333333334\n",
      "batch_idx: 12\n",
      "0.5769230769230769\n",
      "batch_idx: 13\n",
      "0.5892857142857143\n",
      "batch_idx: 14\n",
      "0.5861111111111111\n",
      "batch_idx: 15\n",
      "0.5885416666666666\n",
      "batch_idx: 16\n",
      "0.5931372549019608\n",
      "batch_idx: 17\n",
      "0.5995370370370371\n",
      "batch_idx: 18\n",
      "0.5921052631578947\n",
      "batch_idx: 19\n",
      "0.5854166666666667\n",
      "batch_idx: 20\n",
      "0.5793650793650794\n",
      "batch_idx: 21\n",
      "0.5776515151515151\n",
      "batch_idx: 22\n",
      "0.5760869565217391\n",
      "batch_idx: 23\n",
      "0.578125\n",
      "batch_idx: 24\n",
      "0.5816666666666667\n",
      "Training Epoch: 72, total loss: 47.024258\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.5208333333333334\n",
      "batch_idx: 2\n",
      "0.5\n",
      "batch_idx: 3\n",
      "0.5416666666666666\n",
      "batch_idx: 4\n",
      "0.55\n",
      "batch_idx: 5\n",
      "0.5694444444444444\n",
      "batch_idx: 6\n",
      "0.5833333333333334\n",
      "batch_idx: 7\n",
      "0.5677083333333334\n",
      "batch_idx: 8\n",
      "0.5555555555555556\n",
      "batch_idx: 9\n",
      "0.5625\n",
      "batch_idx: 10\n",
      "0.5568181818181818\n",
      "batch_idx: 11\n",
      "0.5590277777777778\n",
      "batch_idx: 12\n",
      "0.5705128205128205\n",
      "batch_idx: 13\n",
      "0.5684523809523809\n",
      "batch_idx: 14\n",
      "0.5611111111111111\n",
      "batch_idx: 15\n",
      "0.5546875\n",
      "batch_idx: 16\n",
      "0.5588235294117647\n",
      "batch_idx: 17\n",
      "0.5625\n",
      "batch_idx: 18\n",
      "0.5592105263157895\n",
      "batch_idx: 19\n",
      "0.56875\n",
      "batch_idx: 20\n",
      "0.5694444444444444\n",
      "batch_idx: 21\n",
      "0.5662878787878788\n",
      "batch_idx: 22\n",
      "0.5742753623188406\n",
      "batch_idx: 23\n",
      "0.5729166666666666\n",
      "batch_idx: 24\n",
      "0.57\n",
      "Training Epoch: 73, total loss: 47.162193\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.6388888888888888\n",
      "batch_idx: 3\n",
      "0.6145833333333334\n",
      "batch_idx: 4\n",
      "0.6333333333333333\n",
      "batch_idx: 5\n",
      "0.625\n",
      "batch_idx: 6\n",
      "0.6071428571428571\n",
      "batch_idx: 7\n",
      "0.59375\n",
      "batch_idx: 8\n",
      "0.5972222222222222\n",
      "batch_idx: 9\n",
      "0.5958333333333333\n",
      "batch_idx: 10\n",
      "0.5795454545454546\n",
      "batch_idx: 11\n",
      "0.5763888888888888\n",
      "batch_idx: 12\n",
      "0.5705128205128205\n",
      "batch_idx: 13\n",
      "0.5744047619047619\n",
      "batch_idx: 14\n",
      "0.5666666666666667\n",
      "batch_idx: 15\n",
      "0.578125\n",
      "batch_idx: 16\n",
      "0.571078431372549\n",
      "batch_idx: 17\n",
      "0.5763888888888888\n",
      "batch_idx: 18\n",
      "0.5723684210526315\n",
      "batch_idx: 19\n",
      "0.5770833333333333\n",
      "batch_idx: 20\n",
      "0.5734126984126984\n",
      "batch_idx: 21\n",
      "0.5700757575757576\n",
      "batch_idx: 22\n",
      "0.5670289855072463\n",
      "batch_idx: 23\n",
      "0.5659722222222222\n",
      "batch_idx: 24\n",
      "0.5633333333333334\n",
      "Training Epoch: 74, total loss: 47.609834\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.6354166666666666\n",
      "batch_idx: 4\n",
      "0.6\n",
      "batch_idx: 5\n",
      "0.5902777777777778\n",
      "batch_idx: 6\n",
      "0.5952380952380952\n",
      "batch_idx: 7\n",
      "0.5677083333333334\n",
      "batch_idx: 8\n",
      "0.5601851851851852\n",
      "batch_idx: 9\n",
      "0.5625\n",
      "batch_idx: 10\n",
      "0.571969696969697\n",
      "batch_idx: 11\n",
      "0.5694444444444444\n",
      "batch_idx: 12\n",
      "0.5576923076923077\n",
      "batch_idx: 13\n",
      "0.5684523809523809\n",
      "batch_idx: 14\n",
      "0.5638888888888889\n",
      "batch_idx: 15\n",
      "0.5572916666666666\n",
      "batch_idx: 16\n",
      "0.5490196078431373\n",
      "batch_idx: 17\n",
      "0.5439814814814815\n",
      "batch_idx: 18\n",
      "0.543859649122807\n",
      "batch_idx: 19\n",
      "0.54375\n",
      "batch_idx: 20\n",
      "0.5515873015873016\n",
      "batch_idx: 21\n",
      "0.5549242424242424\n",
      "batch_idx: 22\n",
      "0.5597826086956522\n",
      "batch_idx: 23\n",
      "0.5555555555555556\n",
      "batch_idx: 24\n",
      "0.5516666666666666\n",
      "Training Epoch: 75, total loss: 47.681811\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.5416666666666666\n",
      "batch_idx: 2\n",
      "0.5833333333333334\n",
      "batch_idx: 3\n",
      "0.6145833333333334\n",
      "batch_idx: 4\n",
      "0.575\n",
      "batch_idx: 5\n",
      "0.5763888888888888\n",
      "batch_idx: 6\n",
      "0.5773809523809523\n",
      "batch_idx: 7\n",
      "0.5729166666666666\n",
      "batch_idx: 8\n",
      "0.5787037037037037\n",
      "batch_idx: 9\n",
      "0.5708333333333333\n",
      "batch_idx: 10\n",
      "0.5757575757575758\n",
      "batch_idx: 11\n",
      "0.5763888888888888\n",
      "batch_idx: 12\n",
      "0.5865384615384616\n",
      "batch_idx: 13\n",
      "0.5773809523809523\n",
      "batch_idx: 14\n",
      "0.575\n",
      "batch_idx: 15\n",
      "0.5729166666666666\n",
      "batch_idx: 16\n",
      "0.5857843137254902\n",
      "batch_idx: 17\n",
      "0.5879629629629629\n",
      "batch_idx: 18\n",
      "0.581140350877193\n",
      "batch_idx: 19\n",
      "0.575\n",
      "batch_idx: 20\n",
      "0.5753968253968254\n",
      "batch_idx: 21\n",
      "0.571969696969697\n",
      "batch_idx: 22\n",
      "0.5778985507246377\n",
      "batch_idx: 23\n",
      "0.5833333333333334\n",
      "batch_idx: 24\n",
      "0.5866666666666667\n",
      "Training Epoch: 76, total loss: 47.012138\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.5\n",
      "batch_idx: 2\n",
      "0.5416666666666666\n",
      "batch_idx: 3\n",
      "0.5104166666666666\n",
      "batch_idx: 4\n",
      "0.5583333333333333\n",
      "batch_idx: 5\n",
      "0.5833333333333334\n",
      "batch_idx: 6\n",
      "0.5833333333333334\n",
      "batch_idx: 7\n",
      "0.6197916666666666\n",
      "batch_idx: 8\n",
      "0.6111111111111112\n",
      "batch_idx: 9\n",
      "0.6\n",
      "batch_idx: 10\n",
      "0.5833333333333334\n",
      "batch_idx: 11\n",
      "0.5868055555555556\n",
      "batch_idx: 12\n",
      "0.5833333333333334\n",
      "batch_idx: 13\n",
      "0.5773809523809523\n",
      "batch_idx: 14\n",
      "0.5777777777777777\n",
      "batch_idx: 15\n",
      "0.5885416666666666\n",
      "batch_idx: 16\n",
      "0.5808823529411765\n",
      "batch_idx: 17\n",
      "0.5763888888888888\n",
      "batch_idx: 18\n",
      "0.5723684210526315\n",
      "batch_idx: 19\n",
      "0.575\n",
      "batch_idx: 20\n",
      "0.5694444444444444\n",
      "batch_idx: 21\n",
      "0.5700757575757576\n",
      "batch_idx: 22\n",
      "0.5634057971014492\n",
      "batch_idx: 23\n",
      "0.5572916666666666\n",
      "batch_idx: 24\n",
      "0.5566666666666666\n",
      "Training Epoch: 77, total loss: 47.421825\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.5416666666666666\n",
      "batch_idx: 3\n",
      "0.53125\n",
      "batch_idx: 4\n",
      "0.525\n",
      "batch_idx: 5\n",
      "0.5277777777777778\n",
      "batch_idx: 6\n",
      "0.5357142857142857\n",
      "batch_idx: 7\n",
      "0.53125\n",
      "batch_idx: 8\n",
      "0.5416666666666666\n",
      "batch_idx: 9\n",
      "0.5416666666666666\n",
      "batch_idx: 10\n",
      "0.553030303030303\n",
      "batch_idx: 11\n",
      "0.5590277777777778\n",
      "batch_idx: 12\n",
      "0.5608974358974359\n",
      "batch_idx: 13\n",
      "0.5714285714285714\n",
      "batch_idx: 14\n",
      "0.5638888888888889\n",
      "batch_idx: 15\n",
      "0.5677083333333334\n",
      "batch_idx: 16\n",
      "0.5612745098039216\n",
      "batch_idx: 17\n",
      "0.5555555555555556\n",
      "batch_idx: 18\n",
      "0.5614035087719298\n",
      "batch_idx: 19\n",
      "0.5583333333333333\n",
      "batch_idx: 20\n",
      "0.5575396825396826\n",
      "batch_idx: 21\n",
      "0.5606060606060606\n",
      "batch_idx: 22\n",
      "0.5634057971014492\n",
      "batch_idx: 23\n",
      "0.5625\n",
      "batch_idx: 24\n",
      "0.555\n",
      "Training Epoch: 78, total loss: 47.449958\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.5555555555555556\n",
      "batch_idx: 3\n",
      "0.5520833333333334\n",
      "batch_idx: 4\n",
      "0.5916666666666667\n",
      "batch_idx: 5\n",
      "0.5833333333333334\n",
      "batch_idx: 6\n",
      "0.5773809523809523\n",
      "batch_idx: 7\n",
      "0.5729166666666666\n",
      "batch_idx: 8\n",
      "0.5879629629629629\n",
      "batch_idx: 9\n",
      "0.5875\n",
      "batch_idx: 10\n",
      "0.5795454545454546\n",
      "batch_idx: 11\n",
      "0.5763888888888888\n",
      "batch_idx: 12\n",
      "0.5961538461538461\n",
      "batch_idx: 13\n",
      "0.5982142857142857\n",
      "batch_idx: 14\n",
      "0.6111111111111112\n",
      "batch_idx: 15\n",
      "0.6119791666666666\n",
      "batch_idx: 16\n",
      "0.6102941176470589\n",
      "batch_idx: 17\n",
      "0.5995370370370371\n",
      "batch_idx: 18\n",
      "0.5942982456140351\n",
      "batch_idx: 19\n",
      "0.6\n",
      "batch_idx: 20\n",
      "0.6031746031746031\n",
      "batch_idx: 21\n",
      "0.6060606060606061\n",
      "batch_idx: 22\n",
      "0.6068840579710145\n",
      "batch_idx: 23\n",
      "0.6059027777777778\n",
      "batch_idx: 24\n",
      "0.605\n",
      "Training Epoch: 79, total loss: 46.624452\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.5972222222222222\n",
      "batch_idx: 3\n",
      "0.6041666666666666\n",
      "batch_idx: 4\n",
      "0.625\n",
      "batch_idx: 5\n",
      "0.625\n",
      "batch_idx: 6\n",
      "0.6130952380952381\n",
      "batch_idx: 7\n",
      "0.6197916666666666\n",
      "batch_idx: 8\n",
      "0.6157407407407407\n",
      "batch_idx: 9\n",
      "0.6291666666666667\n",
      "batch_idx: 10\n",
      "0.6325757575757576\n",
      "batch_idx: 11\n",
      "0.6354166666666666\n",
      "batch_idx: 12\n",
      "0.6346153846153846\n",
      "batch_idx: 13\n",
      "0.6428571428571429\n",
      "batch_idx: 14\n",
      "0.6444444444444445\n",
      "batch_idx: 15\n",
      "0.640625\n",
      "batch_idx: 16\n",
      "0.6348039215686274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 17\n",
      "0.6319444444444444\n",
      "batch_idx: 18\n",
      "0.6293859649122807\n",
      "batch_idx: 19\n",
      "0.6270833333333333\n",
      "batch_idx: 20\n",
      "0.6309523809523809\n",
      "batch_idx: 21\n",
      "0.625\n",
      "batch_idx: 22\n",
      "0.6213768115942029\n",
      "batch_idx: 23\n",
      "0.6128472222222222\n",
      "batch_idx: 24\n",
      "0.6016666666666667\n",
      "Training Epoch: 80, total loss: 46.553602\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.5208333333333334\n",
      "batch_idx: 2\n",
      "0.5833333333333334\n",
      "batch_idx: 3\n",
      "0.6041666666666666\n",
      "batch_idx: 4\n",
      "0.625\n",
      "batch_idx: 5\n",
      "0.6111111111111112\n",
      "batch_idx: 6\n",
      "0.6369047619047619\n",
      "batch_idx: 7\n",
      "0.6354166666666666\n",
      "batch_idx: 8\n",
      "0.6435185185185185\n",
      "batch_idx: 9\n",
      "0.6458333333333334\n",
      "batch_idx: 10\n",
      "0.6363636363636364\n",
      "batch_idx: 11\n",
      "0.6388888888888888\n",
      "batch_idx: 12\n",
      "0.6442307692307693\n",
      "batch_idx: 13\n",
      "0.6369047619047619\n",
      "batch_idx: 14\n",
      "0.625\n",
      "batch_idx: 15\n",
      "0.6171875\n",
      "batch_idx: 16\n",
      "0.6127450980392157\n",
      "batch_idx: 17\n",
      "0.6157407407407407\n",
      "batch_idx: 18\n",
      "0.6140350877192983\n",
      "batch_idx: 19\n",
      "0.6041666666666666\n",
      "batch_idx: 20\n",
      "0.6071428571428571\n",
      "batch_idx: 21\n",
      "0.6117424242424242\n",
      "batch_idx: 22\n",
      "0.6141304347826086\n",
      "batch_idx: 23\n",
      "0.609375\n",
      "batch_idx: 24\n",
      "0.6033333333333334\n",
      "Training Epoch: 81, total loss: 46.525134\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.4791666666666667\n",
      "batch_idx: 2\n",
      "0.5277777777777778\n",
      "batch_idx: 3\n",
      "0.5520833333333334\n",
      "batch_idx: 4\n",
      "0.5416666666666666\n",
      "batch_idx: 5\n",
      "0.5416666666666666\n",
      "batch_idx: 6\n",
      "0.5416666666666666\n",
      "batch_idx: 7\n",
      "0.5520833333333334\n",
      "batch_idx: 8\n",
      "0.5509259259259259\n",
      "batch_idx: 9\n",
      "0.5541666666666667\n",
      "batch_idx: 10\n",
      "0.5606060606060606\n",
      "batch_idx: 11\n",
      "0.5763888888888888\n",
      "batch_idx: 12\n",
      "0.5769230769230769\n",
      "batch_idx: 13\n",
      "0.5625\n",
      "batch_idx: 14\n",
      "0.5666666666666667\n",
      "batch_idx: 15\n",
      "0.5625\n",
      "batch_idx: 16\n",
      "0.5661764705882353\n",
      "batch_idx: 17\n",
      "0.5717592592592593\n",
      "batch_idx: 18\n",
      "0.5745614035087719\n",
      "batch_idx: 19\n",
      "0.575\n",
      "batch_idx: 20\n",
      "0.5813492063492064\n",
      "batch_idx: 21\n",
      "0.5890151515151515\n",
      "batch_idx: 22\n",
      "0.5869565217391305\n",
      "batch_idx: 23\n",
      "0.5902777777777778\n",
      "batch_idx: 24\n",
      "0.59\n",
      "Training Epoch: 82, total loss: 46.843962\n",
      "batch_idx: 0\n",
      "0.4583333333333333\n",
      "batch_idx: 1\n",
      "0.5625\n",
      "batch_idx: 2\n",
      "0.5694444444444444\n",
      "batch_idx: 3\n",
      "0.5520833333333334\n",
      "batch_idx: 4\n",
      "0.5666666666666667\n",
      "batch_idx: 5\n",
      "0.5763888888888888\n",
      "batch_idx: 6\n",
      "0.5773809523809523\n",
      "batch_idx: 7\n",
      "0.578125\n",
      "batch_idx: 8\n",
      "0.5648148148148148\n",
      "batch_idx: 9\n",
      "0.5875\n",
      "batch_idx: 10\n",
      "0.5681818181818182\n",
      "batch_idx: 11\n",
      "0.5694444444444444\n",
      "batch_idx: 12\n",
      "0.5641025641025641\n",
      "batch_idx: 13\n",
      "0.5654761904761905\n",
      "batch_idx: 14\n",
      "0.575\n",
      "batch_idx: 15\n",
      "0.5755208333333334\n",
      "batch_idx: 16\n",
      "0.571078431372549\n",
      "batch_idx: 17\n",
      "0.5740740740740741\n",
      "batch_idx: 18\n",
      "0.5701754385964912\n",
      "batch_idx: 19\n",
      "0.575\n",
      "batch_idx: 20\n",
      "0.5753968253968254\n",
      "batch_idx: 21\n",
      "0.5833333333333334\n",
      "batch_idx: 22\n",
      "0.5815217391304348\n",
      "batch_idx: 23\n",
      "0.5798611111111112\n",
      "batch_idx: 24\n",
      "0.5783333333333334\n",
      "Training Epoch: 83, total loss: 47.126118\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.6388888888888888\n",
      "batch_idx: 3\n",
      "0.6145833333333334\n",
      "batch_idx: 4\n",
      "0.65\n",
      "batch_idx: 5\n",
      "0.6458333333333334\n",
      "batch_idx: 6\n",
      "0.6190476190476191\n",
      "batch_idx: 7\n",
      "0.609375\n",
      "batch_idx: 8\n",
      "0.5787037037037037\n",
      "batch_idx: 9\n",
      "0.5916666666666667\n",
      "batch_idx: 10\n",
      "0.6022727272727273\n",
      "batch_idx: 11\n",
      "0.6041666666666666\n",
      "batch_idx: 12\n",
      "0.5961538461538461\n",
      "batch_idx: 13\n",
      "0.5952380952380952\n",
      "batch_idx: 14\n",
      "0.5833333333333334\n",
      "batch_idx: 15\n",
      "0.5755208333333334\n",
      "batch_idx: 16\n",
      "0.5735294117647058\n",
      "batch_idx: 17\n",
      "0.5671296296296297\n",
      "batch_idx: 18\n",
      "0.5657894736842105\n",
      "batch_idx: 19\n",
      "0.5729166666666666\n",
      "batch_idx: 20\n",
      "0.5734126984126984\n",
      "batch_idx: 21\n",
      "0.5757575757575758\n",
      "batch_idx: 22\n",
      "0.5688405797101449\n",
      "batch_idx: 23\n",
      "0.5677083333333334\n",
      "batch_idx: 24\n",
      "0.5766666666666667\n",
      "Training Epoch: 84, total loss: 47.214795\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.6111111111111112\n",
      "batch_idx: 3\n",
      "0.625\n",
      "batch_idx: 4\n",
      "0.5833333333333334\n",
      "batch_idx: 5\n",
      "0.5763888888888888\n",
      "batch_idx: 6\n",
      "0.5535714285714286\n",
      "batch_idx: 7\n",
      "0.5520833333333334\n",
      "batch_idx: 8\n",
      "0.5416666666666666\n",
      "batch_idx: 9\n",
      "0.5666666666666667\n",
      "batch_idx: 10\n",
      "0.5833333333333334\n",
      "batch_idx: 11\n",
      "0.5972222222222222\n",
      "batch_idx: 12\n",
      "0.5993589743589743\n",
      "batch_idx: 13\n",
      "0.5863095238095238\n",
      "batch_idx: 14\n",
      "0.5916666666666667\n",
      "batch_idx: 15\n",
      "0.5859375\n",
      "batch_idx: 16\n",
      "0.5906862745098039\n",
      "batch_idx: 17\n",
      "0.5902777777777778\n",
      "batch_idx: 18\n",
      "0.5789473684210527\n",
      "batch_idx: 19\n",
      "0.575\n",
      "batch_idx: 20\n",
      "0.5714285714285714\n",
      "batch_idx: 21\n",
      "0.5662878787878788\n",
      "batch_idx: 22\n",
      "0.5652173913043478\n",
      "batch_idx: 23\n",
      "0.5694444444444444\n",
      "batch_idx: 24\n",
      "0.5666666666666667\n",
      "Training Epoch: 85, total loss: 47.348356\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.625\n",
      "batch_idx: 4\n",
      "0.65\n",
      "batch_idx: 5\n",
      "0.6458333333333334\n",
      "batch_idx: 6\n",
      "0.6190476190476191\n",
      "batch_idx: 7\n",
      "0.5989583333333334\n",
      "batch_idx: 8\n",
      "0.5972222222222222\n",
      "batch_idx: 9\n",
      "0.575\n",
      "batch_idx: 10\n",
      "0.571969696969697\n",
      "batch_idx: 11\n",
      "0.5763888888888888\n",
      "batch_idx: 12\n",
      "0.5769230769230769\n",
      "batch_idx: 13\n",
      "0.5744047619047619\n",
      "batch_idx: 14\n",
      "0.5666666666666667\n",
      "batch_idx: 15\n",
      "0.5677083333333334\n",
      "batch_idx: 16\n",
      "0.5588235294117647\n",
      "batch_idx: 17\n",
      "0.5625\n",
      "batch_idx: 18\n",
      "0.5701754385964912\n",
      "batch_idx: 19\n",
      "0.56875\n",
      "batch_idx: 20\n",
      "0.5615079365079365\n",
      "batch_idx: 21\n",
      "0.5625\n",
      "batch_idx: 22\n",
      "0.5652173913043478\n",
      "batch_idx: 23\n",
      "0.5659722222222222\n",
      "batch_idx: 24\n",
      "0.57\n",
      "Training Epoch: 86, total loss: 47.268334\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.5277777777777778\n",
      "batch_idx: 3\n",
      "0.5416666666666666\n",
      "batch_idx: 4\n",
      "0.5333333333333333\n",
      "batch_idx: 5\n",
      "0.5625\n",
      "batch_idx: 6\n",
      "0.5476190476190477\n",
      "batch_idx: 7\n",
      "0.546875\n",
      "batch_idx: 8\n",
      "0.5416666666666666\n",
      "batch_idx: 9\n",
      "0.5625\n",
      "batch_idx: 10\n",
      "0.5643939393939394\n",
      "batch_idx: 11\n",
      "0.5763888888888888\n",
      "batch_idx: 12\n",
      "0.5769230769230769\n",
      "batch_idx: 13\n",
      "0.5714285714285714\n",
      "batch_idx: 14\n",
      "0.5833333333333334\n",
      "batch_idx: 15\n",
      "0.578125\n",
      "batch_idx: 16\n",
      "0.5857843137254902\n",
      "batch_idx: 17\n",
      "0.5717592592592593\n",
      "batch_idx: 18\n",
      "0.5789473684210527\n",
      "batch_idx: 19\n",
      "0.5833333333333334\n",
      "batch_idx: 20\n",
      "0.5853174603174603\n",
      "batch_idx: 21\n",
      "0.5833333333333334\n",
      "batch_idx: 22\n",
      "0.5851449275362319\n",
      "batch_idx: 23\n",
      "0.5868055555555556\n",
      "batch_idx: 24\n",
      "0.5883333333333334\n",
      "Training Epoch: 87, total loss: 47.002140\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.4791666666666667\n",
      "batch_idx: 2\n",
      "0.4305555555555556\n",
      "batch_idx: 3\n",
      "0.46875\n",
      "batch_idx: 4\n",
      "0.48333333333333334\n",
      "batch_idx: 5\n",
      "0.4861111111111111\n",
      "batch_idx: 6\n",
      "0.48214285714285715\n",
      "batch_idx: 7\n",
      "0.5\n",
      "batch_idx: 8\n",
      "0.5138888888888888\n",
      "batch_idx: 9\n",
      "0.5083333333333333\n",
      "batch_idx: 10\n",
      "0.5075757575757576\n",
      "batch_idx: 11\n",
      "0.5034722222222222\n",
      "batch_idx: 12\n",
      "0.5192307692307693\n",
      "batch_idx: 13\n",
      "0.5297619047619048\n",
      "batch_idx: 14\n",
      "0.5416666666666666\n",
      "batch_idx: 15\n",
      "0.5520833333333334\n",
      "batch_idx: 16\n",
      "0.5514705882352942\n",
      "batch_idx: 17\n",
      "0.5486111111111112\n",
      "batch_idx: 18\n",
      "0.5504385964912281\n",
      "batch_idx: 19\n",
      "0.5520833333333334\n",
      "batch_idx: 20\n",
      "0.5575396825396826\n",
      "batch_idx: 21\n",
      "0.5606060606060606\n",
      "batch_idx: 22\n",
      "0.5652173913043478\n",
      "batch_idx: 23\n",
      "0.5677083333333334\n",
      "batch_idx: 24\n",
      "0.575\n",
      "Training Epoch: 88, total loss: 47.058498\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.6388888888888888\n",
      "batch_idx: 3\n",
      "0.6145833333333334\n",
      "batch_idx: 4\n",
      "0.625\n",
      "batch_idx: 5\n",
      "0.5972222222222222\n",
      "batch_idx: 6\n",
      "0.5952380952380952\n",
      "batch_idx: 7\n",
      "0.6041666666666666\n",
      "batch_idx: 8\n",
      "0.6111111111111112\n",
      "batch_idx: 9\n",
      "0.6083333333333333\n",
      "batch_idx: 10\n",
      "0.6022727272727273\n",
      "batch_idx: 11\n",
      "0.5972222222222222\n",
      "batch_idx: 12\n",
      "0.6025641025641025\n",
      "batch_idx: 13\n",
      "0.6101190476190477\n",
      "batch_idx: 14\n",
      "0.6166666666666667\n",
      "batch_idx: 15\n",
      "0.6171875\n",
      "batch_idx: 16\n",
      "0.6225490196078431\n",
      "batch_idx: 17\n",
      "0.6180555555555556\n",
      "batch_idx: 18\n",
      "0.6206140350877193\n",
      "batch_idx: 19\n",
      "0.61875\n",
      "batch_idx: 20\n",
      "0.621031746031746\n",
      "batch_idx: 21\n",
      "0.6193181818181818\n",
      "batch_idx: 22\n",
      "0.6177536231884058\n",
      "batch_idx: 23\n",
      "0.6180555555555556\n",
      "batch_idx: 24\n",
      "0.6166666666666667\n",
      "Training Epoch: 89, total loss: 46.261385\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.6111111111111112\n",
      "batch_idx: 3\n",
      "0.6145833333333334\n",
      "batch_idx: 4\n",
      "0.6083333333333333\n",
      "batch_idx: 5\n",
      "0.625\n",
      "batch_idx: 6\n",
      "0.6011904761904762\n",
      "batch_idx: 7\n",
      "0.6041666666666666\n",
      "batch_idx: 8\n",
      "0.6111111111111112\n",
      "batch_idx: 9\n",
      "0.6083333333333333\n",
      "batch_idx: 10\n",
      "0.6136363636363636\n",
      "batch_idx: 11\n",
      "0.5833333333333334\n",
      "batch_idx: 12\n",
      "0.5897435897435898\n",
      "batch_idx: 13\n",
      "0.5952380952380952\n",
      "batch_idx: 14\n",
      "0.5944444444444444\n",
      "batch_idx: 15\n",
      "0.5963541666666666\n",
      "batch_idx: 16\n",
      "0.5980392156862745\n",
      "batch_idx: 17\n",
      "0.5925925925925926\n",
      "batch_idx: 18\n",
      "0.5942982456140351\n",
      "batch_idx: 19\n",
      "0.5958333333333333\n",
      "batch_idx: 20\n",
      "0.6011904761904762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 21\n",
      "0.5984848484848485\n",
      "batch_idx: 22\n",
      "0.5978260869565217\n",
      "batch_idx: 23\n",
      "0.5885416666666666\n",
      "batch_idx: 24\n",
      "0.59\n",
      "Training Epoch: 90, total loss: 46.847184\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.625\n",
      "batch_idx: 4\n",
      "0.6166666666666667\n",
      "batch_idx: 5\n",
      "0.5972222222222222\n",
      "batch_idx: 6\n",
      "0.5833333333333334\n",
      "batch_idx: 7\n",
      "0.5885416666666666\n",
      "batch_idx: 8\n",
      "0.6018518518518519\n",
      "batch_idx: 9\n",
      "0.6083333333333333\n",
      "batch_idx: 10\n",
      "0.6212121212121212\n",
      "batch_idx: 11\n",
      "0.625\n",
      "batch_idx: 12\n",
      "0.6378205128205128\n",
      "batch_idx: 13\n",
      "0.6428571428571429\n",
      "batch_idx: 14\n",
      "0.65\n",
      "batch_idx: 15\n",
      "0.6432291666666666\n",
      "batch_idx: 16\n",
      "0.6323529411764706\n",
      "batch_idx: 17\n",
      "0.6296296296296297\n",
      "batch_idx: 18\n",
      "0.6403508771929824\n",
      "batch_idx: 19\n",
      "0.6375\n",
      "batch_idx: 20\n",
      "0.623015873015873\n",
      "batch_idx: 21\n",
      "0.634469696969697\n",
      "batch_idx: 22\n",
      "0.6304347826086957\n",
      "batch_idx: 23\n",
      "0.6302083333333334\n",
      "batch_idx: 24\n",
      "0.6316666666666667\n",
      "Training Epoch: 91, total loss: 46.110190\n",
      "batch_idx: 0\n",
      "0.4583333333333333\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.5416666666666666\n",
      "batch_idx: 3\n",
      "0.5729166666666666\n",
      "batch_idx: 4\n",
      "0.5833333333333334\n",
      "batch_idx: 5\n",
      "0.5625\n",
      "batch_idx: 6\n",
      "0.5833333333333334\n",
      "batch_idx: 7\n",
      "0.578125\n",
      "batch_idx: 8\n",
      "0.5972222222222222\n",
      "batch_idx: 9\n",
      "0.5875\n",
      "batch_idx: 10\n",
      "0.6022727272727273\n",
      "batch_idx: 11\n",
      "0.6006944444444444\n",
      "batch_idx: 12\n",
      "0.5961538461538461\n",
      "batch_idx: 13\n",
      "0.5982142857142857\n",
      "batch_idx: 14\n",
      "0.6027777777777777\n",
      "batch_idx: 15\n",
      "0.5885416666666666\n",
      "batch_idx: 16\n",
      "0.5906862745098039\n",
      "batch_idx: 17\n",
      "0.5995370370370371\n",
      "batch_idx: 18\n",
      "0.5942982456140351\n",
      "batch_idx: 19\n",
      "0.5958333333333333\n",
      "batch_idx: 20\n",
      "0.5972222222222222\n",
      "batch_idx: 21\n",
      "0.6041666666666666\n",
      "batch_idx: 22\n",
      "0.5996376811594203\n",
      "batch_idx: 23\n",
      "0.6041666666666666\n",
      "batch_idx: 24\n",
      "0.61\n",
      "Training Epoch: 92, total loss: 46.460249\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6388888888888888\n",
      "batch_idx: 6\n",
      "0.6547619047619048\n",
      "batch_idx: 7\n",
      "0.6458333333333334\n",
      "batch_idx: 8\n",
      "0.6481481481481481\n",
      "batch_idx: 9\n",
      "0.6375\n",
      "batch_idx: 10\n",
      "0.6287878787878788\n",
      "batch_idx: 11\n",
      "0.6076388888888888\n",
      "batch_idx: 12\n",
      "0.6057692307692307\n",
      "batch_idx: 13\n",
      "0.6071428571428571\n",
      "batch_idx: 14\n",
      "0.6055555555555555\n",
      "batch_idx: 15\n",
      "0.6067708333333334\n",
      "batch_idx: 16\n",
      "0.6004901960784313\n",
      "batch_idx: 17\n",
      "0.5949074074074074\n",
      "batch_idx: 18\n",
      "0.5986842105263158\n",
      "batch_idx: 19\n",
      "0.59375\n",
      "batch_idx: 20\n",
      "0.5972222222222222\n",
      "batch_idx: 21\n",
      "0.5909090909090909\n",
      "batch_idx: 22\n",
      "0.5905797101449275\n",
      "batch_idx: 23\n",
      "0.5989583333333334\n",
      "batch_idx: 24\n",
      "0.595\n",
      "Training Epoch: 93, total loss: 46.803282\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.6458333333333334\n",
      "batch_idx: 4\n",
      "0.6166666666666667\n",
      "batch_idx: 5\n",
      "0.6041666666666666\n",
      "batch_idx: 6\n",
      "0.6011904761904762\n",
      "batch_idx: 7\n",
      "0.6145833333333334\n",
      "batch_idx: 8\n",
      "0.6018518518518519\n",
      "batch_idx: 9\n",
      "0.5958333333333333\n",
      "batch_idx: 10\n",
      "0.5909090909090909\n",
      "batch_idx: 11\n",
      "0.5902777777777778\n",
      "batch_idx: 12\n",
      "0.6025641025641025\n",
      "batch_idx: 13\n",
      "0.6071428571428571\n",
      "batch_idx: 14\n",
      "0.6027777777777777\n",
      "batch_idx: 15\n",
      "0.5963541666666666\n",
      "batch_idx: 16\n",
      "0.5980392156862745\n",
      "batch_idx: 17\n",
      "0.5856481481481481\n",
      "batch_idx: 18\n",
      "0.5899122807017544\n",
      "batch_idx: 19\n",
      "0.5875\n",
      "batch_idx: 20\n",
      "0.5892857142857143\n",
      "batch_idx: 21\n",
      "0.5909090909090909\n",
      "batch_idx: 22\n",
      "0.5869565217391305\n",
      "batch_idx: 23\n",
      "0.5868055555555556\n",
      "batch_idx: 24\n",
      "0.595\n",
      "Training Epoch: 94, total loss: 46.779306\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.6145833333333334\n",
      "batch_idx: 4\n",
      "0.625\n",
      "batch_idx: 5\n",
      "0.6180555555555556\n",
      "batch_idx: 6\n",
      "0.6369047619047619\n",
      "batch_idx: 7\n",
      "0.6302083333333334\n",
      "batch_idx: 8\n",
      "0.6157407407407407\n",
      "batch_idx: 9\n",
      "0.6041666666666666\n",
      "batch_idx: 10\n",
      "0.6022727272727273\n",
      "batch_idx: 11\n",
      "0.5798611111111112\n",
      "batch_idx: 12\n",
      "0.592948717948718\n",
      "batch_idx: 13\n",
      "0.5982142857142857\n",
      "batch_idx: 14\n",
      "0.6111111111111112\n",
      "batch_idx: 15\n",
      "0.6171875\n",
      "batch_idx: 16\n",
      "0.6078431372549019\n",
      "batch_idx: 17\n",
      "0.6111111111111112\n",
      "batch_idx: 18\n",
      "0.6096491228070176\n",
      "batch_idx: 19\n",
      "0.6125\n",
      "batch_idx: 20\n",
      "0.6111111111111112\n",
      "batch_idx: 21\n",
      "0.6060606060606061\n",
      "batch_idx: 22\n",
      "0.605072463768116\n",
      "batch_idx: 23\n",
      "0.6041666666666666\n",
      "batch_idx: 24\n",
      "0.6\n",
      "Training Epoch: 95, total loss: 46.687702\n",
      "batch_idx: 0\n",
      "0.375\n",
      "batch_idx: 1\n",
      "0.4791666666666667\n",
      "batch_idx: 2\n",
      "0.5833333333333334\n",
      "batch_idx: 3\n",
      "0.5625\n",
      "batch_idx: 4\n",
      "0.5833333333333334\n",
      "batch_idx: 5\n",
      "0.5833333333333334\n",
      "batch_idx: 6\n",
      "0.5892857142857143\n",
      "batch_idx: 7\n",
      "0.5833333333333334\n",
      "batch_idx: 8\n",
      "0.5833333333333334\n",
      "batch_idx: 9\n",
      "0.575\n",
      "batch_idx: 10\n",
      "0.5795454545454546\n",
      "batch_idx: 11\n",
      "0.5868055555555556\n",
      "batch_idx: 12\n",
      "0.592948717948718\n",
      "batch_idx: 13\n",
      "0.5952380952380952\n",
      "batch_idx: 14\n",
      "0.5944444444444444\n",
      "batch_idx: 15\n",
      "0.5963541666666666\n",
      "batch_idx: 16\n",
      "0.6004901960784313\n",
      "batch_idx: 17\n",
      "0.6087962962962963\n",
      "batch_idx: 18\n",
      "0.6052631578947368\n",
      "batch_idx: 19\n",
      "0.6104166666666667\n",
      "batch_idx: 20\n",
      "0.6051587301587301\n",
      "batch_idx: 21\n",
      "0.6022727272727273\n",
      "batch_idx: 22\n",
      "0.6014492753623188\n",
      "batch_idx: 23\n",
      "0.5989583333333334\n",
      "batch_idx: 24\n",
      "0.6\n",
      "Training Epoch: 96, total loss: 46.617062\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.5625\n",
      "batch_idx: 2\n",
      "0.5\n",
      "batch_idx: 3\n",
      "0.5104166666666666\n",
      "batch_idx: 4\n",
      "0.5083333333333333\n",
      "batch_idx: 5\n",
      "0.5208333333333334\n",
      "batch_idx: 6\n",
      "0.5535714285714286\n",
      "batch_idx: 7\n",
      "0.578125\n",
      "batch_idx: 8\n",
      "0.5740740740740741\n",
      "batch_idx: 9\n",
      "0.5625\n",
      "batch_idx: 10\n",
      "0.571969696969697\n",
      "batch_idx: 11\n",
      "0.5798611111111112\n",
      "batch_idx: 12\n",
      "0.5769230769230769\n",
      "batch_idx: 13\n",
      "0.5744047619047619\n",
      "batch_idx: 14\n",
      "0.5805555555555556\n",
      "batch_idx: 15\n",
      "0.5833333333333334\n",
      "batch_idx: 16\n",
      "0.5759803921568627\n",
      "batch_idx: 17\n",
      "0.5810185185185185\n",
      "batch_idx: 18\n",
      "0.5855263157894737\n",
      "batch_idx: 19\n",
      "0.5875\n",
      "batch_idx: 20\n",
      "0.5873015873015873\n",
      "batch_idx: 21\n",
      "0.5890151515151515\n",
      "batch_idx: 22\n",
      "0.5869565217391305\n",
      "batch_idx: 23\n",
      "0.5902777777777778\n",
      "batch_idx: 24\n",
      "0.5933333333333334\n",
      "Training Epoch: 97, total loss: 46.821671\n",
      "batch_idx: 0\n",
      "0.4583333333333333\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.6145833333333334\n",
      "batch_idx: 4\n",
      "0.625\n",
      "batch_idx: 5\n",
      "0.6111111111111112\n",
      "batch_idx: 6\n",
      "0.6071428571428571\n",
      "batch_idx: 7\n",
      "0.609375\n",
      "batch_idx: 8\n",
      "0.5879629629629629\n",
      "batch_idx: 9\n",
      "0.575\n",
      "batch_idx: 10\n",
      "0.5643939393939394\n",
      "batch_idx: 11\n",
      "0.5590277777777778\n",
      "batch_idx: 12\n",
      "0.5737179487179487\n",
      "batch_idx: 13\n",
      "0.5684523809523809\n",
      "batch_idx: 14\n",
      "0.5722222222222222\n",
      "batch_idx: 15\n",
      "0.5677083333333334\n",
      "batch_idx: 16\n",
      "0.5808823529411765\n",
      "batch_idx: 17\n",
      "0.5833333333333334\n",
      "batch_idx: 18\n",
      "0.5877192982456141\n",
      "batch_idx: 19\n",
      "0.5958333333333333\n",
      "batch_idx: 20\n",
      "0.5952380952380952\n",
      "batch_idx: 21\n",
      "0.5909090909090909\n",
      "batch_idx: 22\n",
      "0.5905797101449275\n",
      "batch_idx: 23\n",
      "0.5920138888888888\n",
      "batch_idx: 24\n",
      "0.5916666666666667\n",
      "Training Epoch: 98, total loss: 46.755556\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.5833333333333334\n",
      "batch_idx: 3\n",
      "0.59375\n",
      "batch_idx: 4\n",
      "0.5833333333333334\n",
      "batch_idx: 5\n",
      "0.5625\n",
      "batch_idx: 6\n",
      "0.5595238095238095\n",
      "batch_idx: 7\n",
      "0.5833333333333334\n",
      "batch_idx: 8\n",
      "0.5972222222222222\n",
      "batch_idx: 9\n",
      "0.5916666666666667\n",
      "batch_idx: 10\n",
      "0.5833333333333334\n",
      "batch_idx: 11\n",
      "0.59375\n",
      "batch_idx: 12\n",
      "0.5993589743589743\n",
      "batch_idx: 13\n",
      "0.5892857142857143\n",
      "batch_idx: 14\n",
      "0.5972222222222222\n",
      "batch_idx: 15\n",
      "0.6119791666666666\n",
      "batch_idx: 16\n",
      "0.6151960784313726\n",
      "batch_idx: 17\n",
      "0.625\n",
      "batch_idx: 18\n",
      "0.6293859649122807\n",
      "batch_idx: 19\n",
      "0.6145833333333334\n",
      "batch_idx: 20\n",
      "0.623015873015873\n",
      "batch_idx: 21\n",
      "0.6268939393939394\n",
      "batch_idx: 22\n",
      "0.625\n",
      "batch_idx: 23\n",
      "0.6197916666666666\n",
      "batch_idx: 24\n",
      "0.62\n",
      "Training Epoch: 99, total loss: 46.438532\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.65625\n",
      "batch_idx: 4\n",
      "0.6416666666666667\n",
      "batch_idx: 5\n",
      "0.6458333333333334\n",
      "batch_idx: 6\n",
      "0.6190476190476191\n",
      "batch_idx: 7\n",
      "0.6302083333333334\n",
      "batch_idx: 8\n",
      "0.6296296296296297\n",
      "batch_idx: 9\n",
      "0.625\n",
      "batch_idx: 10\n",
      "0.625\n",
      "batch_idx: 11\n",
      "0.6145833333333334\n",
      "batch_idx: 12\n",
      "0.6217948717948718\n",
      "batch_idx: 13\n",
      "0.625\n",
      "batch_idx: 14\n",
      "0.6194444444444445\n",
      "batch_idx: 15\n",
      "0.6145833333333334\n",
      "batch_idx: 16\n",
      "0.6127450980392157\n",
      "batch_idx: 17\n",
      "0.6087962962962963\n",
      "batch_idx: 18\n",
      "0.6008771929824561\n",
      "batch_idx: 19\n",
      "0.6\n",
      "batch_idx: 20\n",
      "0.5972222222222222\n",
      "batch_idx: 21\n",
      "0.5928030303030303\n",
      "batch_idx: 22\n",
      "0.592391304347826\n",
      "batch_idx: 23\n",
      "0.5954861111111112\n",
      "batch_idx: 24\n",
      "0.5933333333333334\n",
      "Training Epoch: 100, total loss: 46.758880\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.5625\n",
      "batch_idx: 2\n",
      "0.5\n",
      "batch_idx: 3\n",
      "0.5416666666666666\n",
      "batch_idx: 4\n",
      "0.55\n",
      "batch_idx: 5\n",
      "0.5972222222222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 6\n",
      "0.5833333333333334\n",
      "batch_idx: 7\n",
      "0.5885416666666666\n",
      "batch_idx: 8\n",
      "0.5972222222222222\n",
      "batch_idx: 9\n",
      "0.5833333333333334\n",
      "batch_idx: 10\n",
      "0.5833333333333334\n",
      "batch_idx: 11\n",
      "0.5972222222222222\n",
      "batch_idx: 12\n",
      "0.5833333333333334\n",
      "batch_idx: 13\n",
      "0.5773809523809523\n",
      "batch_idx: 14\n",
      "0.5972222222222222\n",
      "batch_idx: 15\n",
      "0.6067708333333334\n",
      "batch_idx: 16\n",
      "0.6029411764705882\n",
      "batch_idx: 17\n",
      "0.6018518518518519\n",
      "batch_idx: 18\n",
      "0.6008771929824561\n",
      "batch_idx: 19\n",
      "0.59375\n",
      "batch_idx: 20\n",
      "0.5952380952380952\n",
      "batch_idx: 21\n",
      "0.5965909090909091\n",
      "batch_idx: 22\n",
      "0.592391304347826\n",
      "batch_idx: 23\n",
      "0.5972222222222222\n",
      "batch_idx: 24\n",
      "0.5983333333333334\n",
      "Training Epoch: 101, total loss: 46.670252\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.5555555555555556\n",
      "batch_idx: 3\n",
      "0.5625\n",
      "batch_idx: 4\n",
      "0.575\n",
      "batch_idx: 5\n",
      "0.5902777777777778\n",
      "batch_idx: 6\n",
      "0.5833333333333334\n",
      "batch_idx: 7\n",
      "0.5677083333333334\n",
      "batch_idx: 8\n",
      "0.5972222222222222\n",
      "batch_idx: 9\n",
      "0.5875\n",
      "batch_idx: 10\n",
      "0.6174242424242424\n",
      "batch_idx: 11\n",
      "0.6145833333333334\n",
      "batch_idx: 12\n",
      "0.6217948717948718\n",
      "batch_idx: 13\n",
      "0.6190476190476191\n",
      "batch_idx: 14\n",
      "0.6222222222222222\n",
      "batch_idx: 15\n",
      "0.6145833333333334\n",
      "batch_idx: 16\n",
      "0.6102941176470589\n",
      "batch_idx: 17\n",
      "0.6018518518518519\n",
      "batch_idx: 18\n",
      "0.6030701754385965\n",
      "batch_idx: 19\n",
      "0.6041666666666666\n",
      "batch_idx: 20\n",
      "0.6051587301587301\n",
      "batch_idx: 21\n",
      "0.6060606060606061\n",
      "batch_idx: 22\n",
      "0.605072463768116\n",
      "batch_idx: 23\n",
      "0.6041666666666666\n",
      "batch_idx: 24\n",
      "0.5983333333333334\n",
      "Training Epoch: 102, total loss: 46.653450\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.625\n",
      "batch_idx: 4\n",
      "0.6416666666666667\n",
      "batch_idx: 5\n",
      "0.6597222222222222\n",
      "batch_idx: 6\n",
      "0.6130952380952381\n",
      "batch_idx: 7\n",
      "0.6197916666666666\n",
      "batch_idx: 8\n",
      "0.6435185185185185\n",
      "batch_idx: 9\n",
      "0.6375\n",
      "batch_idx: 10\n",
      "0.6212121212121212\n",
      "batch_idx: 11\n",
      "0.6180555555555556\n",
      "batch_idx: 12\n",
      "0.6121794871794872\n",
      "batch_idx: 13\n",
      "0.6041666666666666\n",
      "batch_idx: 14\n",
      "0.6027777777777777\n",
      "batch_idx: 15\n",
      "0.6041666666666666\n",
      "batch_idx: 16\n",
      "0.6004901960784313\n",
      "batch_idx: 17\n",
      "0.5972222222222222\n",
      "batch_idx: 18\n",
      "0.5942982456140351\n",
      "batch_idx: 19\n",
      "0.5916666666666667\n",
      "batch_idx: 20\n",
      "0.5892857142857143\n",
      "batch_idx: 21\n",
      "0.5890151515151515\n",
      "batch_idx: 22\n",
      "0.5869565217391305\n",
      "batch_idx: 23\n",
      "0.5920138888888888\n",
      "batch_idx: 24\n",
      "0.5883333333333334\n",
      "Training Epoch: 103, total loss: 46.895954\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.5555555555555556\n",
      "batch_idx: 3\n",
      "0.5833333333333334\n",
      "batch_idx: 4\n",
      "0.6\n",
      "batch_idx: 5\n",
      "0.5902777777777778\n",
      "batch_idx: 6\n",
      "0.5833333333333334\n",
      "batch_idx: 7\n",
      "0.59375\n",
      "batch_idx: 8\n",
      "0.5740740740740741\n",
      "batch_idx: 9\n",
      "0.5916666666666667\n",
      "batch_idx: 10\n",
      "0.5871212121212122\n",
      "batch_idx: 11\n",
      "0.5902777777777778\n",
      "batch_idx: 12\n",
      "0.5897435897435898\n",
      "batch_idx: 13\n",
      "0.5892857142857143\n",
      "batch_idx: 14\n",
      "0.5833333333333334\n",
      "batch_idx: 15\n",
      "0.5833333333333334\n",
      "batch_idx: 16\n",
      "0.5882352941176471\n",
      "batch_idx: 17\n",
      "0.5902777777777778\n",
      "batch_idx: 18\n",
      "0.5921052631578947\n",
      "batch_idx: 19\n",
      "0.5916666666666667\n",
      "batch_idx: 20\n",
      "0.5952380952380952\n",
      "batch_idx: 21\n",
      "0.5965909090909091\n",
      "batch_idx: 22\n",
      "0.5960144927536232\n",
      "batch_idx: 23\n",
      "0.6024305555555556\n",
      "batch_idx: 24\n",
      "0.5916666666666667\n",
      "Training Epoch: 104, total loss: 46.728303\n",
      "batch_idx: 0\n",
      "0.4583333333333333\n",
      "batch_idx: 1\n",
      "0.5416666666666666\n",
      "batch_idx: 2\n",
      "0.5416666666666666\n",
      "batch_idx: 3\n",
      "0.5416666666666666\n",
      "batch_idx: 4\n",
      "0.5583333333333333\n",
      "batch_idx: 5\n",
      "0.5694444444444444\n",
      "batch_idx: 6\n",
      "0.5833333333333334\n",
      "batch_idx: 7\n",
      "0.5989583333333334\n",
      "batch_idx: 8\n",
      "0.5925925925925926\n",
      "batch_idx: 9\n",
      "0.5916666666666667\n",
      "batch_idx: 10\n",
      "0.6136363636363636\n",
      "batch_idx: 11\n",
      "0.6145833333333334\n",
      "batch_idx: 12\n",
      "0.6185897435897436\n",
      "batch_idx: 13\n",
      "0.6279761904761905\n",
      "batch_idx: 14\n",
      "0.6194444444444445\n",
      "batch_idx: 15\n",
      "0.625\n",
      "batch_idx: 16\n",
      "0.6200980392156863\n",
      "batch_idx: 17\n",
      "0.6111111111111112\n",
      "batch_idx: 18\n",
      "0.6096491228070176\n",
      "batch_idx: 19\n",
      "0.6125\n",
      "batch_idx: 20\n",
      "0.6071428571428571\n",
      "batch_idx: 21\n",
      "0.6098484848484849\n",
      "batch_idx: 22\n",
      "0.6159420289855072\n",
      "batch_idx: 23\n",
      "0.6215277777777778\n",
      "batch_idx: 24\n",
      "0.62\n",
      "Training Epoch: 105, total loss: 46.330346\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6111111111111112\n",
      "batch_idx: 3\n",
      "0.6354166666666666\n",
      "batch_idx: 4\n",
      "0.6583333333333333\n",
      "batch_idx: 5\n",
      "0.6111111111111112\n",
      "batch_idx: 6\n",
      "0.6130952380952381\n",
      "batch_idx: 7\n",
      "0.609375\n",
      "batch_idx: 8\n",
      "0.6203703703703703\n",
      "batch_idx: 9\n",
      "0.6166666666666667\n",
      "batch_idx: 10\n",
      "0.625\n",
      "batch_idx: 11\n",
      "0.6284722222222222\n",
      "batch_idx: 12\n",
      "0.6346153846153846\n",
      "batch_idx: 13\n",
      "0.6398809523809523\n",
      "batch_idx: 14\n",
      "0.6444444444444445\n",
      "batch_idx: 15\n",
      "0.6354166666666666\n",
      "batch_idx: 16\n",
      "0.6446078431372549\n",
      "batch_idx: 17\n",
      "0.6412037037037037\n",
      "batch_idx: 18\n",
      "0.6359649122807017\n",
      "batch_idx: 19\n",
      "0.6333333333333333\n",
      "batch_idx: 20\n",
      "0.6329365079365079\n",
      "batch_idx: 21\n",
      "0.6325757575757576\n",
      "batch_idx: 22\n",
      "0.6304347826086957\n",
      "batch_idx: 23\n",
      "0.625\n",
      "batch_idx: 24\n",
      "0.6283333333333333\n",
      "Training Epoch: 106, total loss: 46.131209\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6388888888888888\n",
      "batch_idx: 3\n",
      "0.6666666666666666\n",
      "batch_idx: 4\n",
      "0.6666666666666666\n",
      "batch_idx: 5\n",
      "0.6388888888888888\n",
      "batch_idx: 6\n",
      "0.6428571428571429\n",
      "batch_idx: 7\n",
      "0.671875\n",
      "batch_idx: 8\n",
      "0.6435185185185185\n",
      "batch_idx: 9\n",
      "0.6375\n",
      "batch_idx: 10\n",
      "0.625\n",
      "batch_idx: 11\n",
      "0.6215277777777778\n",
      "batch_idx: 12\n",
      "0.6346153846153846\n",
      "batch_idx: 13\n",
      "0.6190476190476191\n",
      "batch_idx: 14\n",
      "0.625\n",
      "batch_idx: 15\n",
      "0.6380208333333334\n",
      "batch_idx: 16\n",
      "0.6470588235294118\n",
      "batch_idx: 17\n",
      "0.6458333333333334\n",
      "batch_idx: 18\n",
      "0.631578947368421\n",
      "batch_idx: 19\n",
      "0.6333333333333333\n",
      "batch_idx: 20\n",
      "0.6408730158730159\n",
      "batch_idx: 21\n",
      "0.6363636363636364\n",
      "batch_idx: 22\n",
      "0.6340579710144928\n",
      "batch_idx: 23\n",
      "0.6371527777777778\n",
      "batch_idx: 24\n",
      "0.6333333333333333\n",
      "Training Epoch: 107, total loss: 46.016517\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.5833333333333334\n",
      "batch_idx: 3\n",
      "0.6145833333333334\n",
      "batch_idx: 4\n",
      "0.6083333333333333\n",
      "batch_idx: 5\n",
      "0.6458333333333334\n",
      "batch_idx: 6\n",
      "0.625\n",
      "batch_idx: 7\n",
      "0.59375\n",
      "batch_idx: 8\n",
      "0.6064814814814815\n",
      "batch_idx: 9\n",
      "0.6125\n",
      "batch_idx: 10\n",
      "0.6060606060606061\n",
      "batch_idx: 11\n",
      "0.6006944444444444\n",
      "batch_idx: 12\n",
      "0.5993589743589743\n",
      "batch_idx: 13\n",
      "0.5892857142857143\n",
      "batch_idx: 14\n",
      "0.5944444444444444\n",
      "batch_idx: 15\n",
      "0.6015625\n",
      "batch_idx: 16\n",
      "0.6004901960784313\n",
      "batch_idx: 17\n",
      "0.5949074074074074\n",
      "batch_idx: 18\n",
      "0.5986842105263158\n",
      "batch_idx: 19\n",
      "0.5854166666666667\n",
      "batch_idx: 20\n",
      "0.5873015873015873\n",
      "batch_idx: 21\n",
      "0.5909090909090909\n",
      "batch_idx: 22\n",
      "0.5942028985507246\n",
      "batch_idx: 23\n",
      "0.5954861111111112\n",
      "batch_idx: 24\n",
      "0.595\n",
      "Training Epoch: 108, total loss: 46.724815\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.5625\n",
      "batch_idx: 2\n",
      "0.5277777777777778\n",
      "batch_idx: 3\n",
      "0.5729166666666666\n",
      "batch_idx: 4\n",
      "0.5666666666666667\n",
      "batch_idx: 5\n",
      "0.5763888888888888\n",
      "batch_idx: 6\n",
      "0.5654761904761905\n",
      "batch_idx: 7\n",
      "0.5885416666666666\n",
      "batch_idx: 8\n",
      "0.5879629629629629\n",
      "batch_idx: 9\n",
      "0.5958333333333333\n",
      "batch_idx: 10\n",
      "0.5795454545454546\n",
      "batch_idx: 11\n",
      "0.5902777777777778\n",
      "batch_idx: 12\n",
      "0.5897435897435898\n",
      "batch_idx: 13\n",
      "0.5952380952380952\n",
      "batch_idx: 14\n",
      "0.6055555555555555\n",
      "batch_idx: 15\n",
      "0.609375\n",
      "batch_idx: 16\n",
      "0.6200980392156863\n",
      "batch_idx: 17\n",
      "0.6203703703703703\n",
      "batch_idx: 18\n",
      "0.6096491228070176\n",
      "batch_idx: 19\n",
      "0.6104166666666667\n",
      "batch_idx: 20\n",
      "0.6091269841269841\n",
      "batch_idx: 21\n",
      "0.6060606060606061\n",
      "batch_idx: 22\n",
      "0.605072463768116\n",
      "batch_idx: 23\n",
      "0.6076388888888888\n",
      "batch_idx: 24\n",
      "0.605\n",
      "Training Epoch: 109, total loss: 46.607328\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.5416666666666666\n",
      "batch_idx: 2\n",
      "0.5555555555555556\n",
      "batch_idx: 3\n",
      "0.53125\n",
      "batch_idx: 4\n",
      "0.5833333333333334\n",
      "batch_idx: 5\n",
      "0.5833333333333334\n",
      "batch_idx: 6\n",
      "0.5892857142857143\n",
      "batch_idx: 7\n",
      "0.59375\n",
      "batch_idx: 8\n",
      "0.5833333333333334\n",
      "batch_idx: 9\n",
      "0.575\n",
      "batch_idx: 10\n",
      "0.571969696969697\n",
      "batch_idx: 11\n",
      "0.5694444444444444\n",
      "batch_idx: 12\n",
      "0.5705128205128205\n",
      "batch_idx: 13\n",
      "0.5535714285714286\n",
      "batch_idx: 14\n",
      "0.5611111111111111\n",
      "batch_idx: 15\n",
      "0.5598958333333334\n",
      "batch_idx: 16\n",
      "0.5563725490196079\n",
      "batch_idx: 17\n",
      "0.5462962962962963\n",
      "batch_idx: 18\n",
      "0.5460526315789473\n",
      "batch_idx: 19\n",
      "0.5520833333333334\n",
      "batch_idx: 20\n",
      "0.5575396825396826\n",
      "batch_idx: 21\n",
      "0.5606060606060606\n",
      "batch_idx: 22\n",
      "0.5706521739130435\n",
      "batch_idx: 23\n",
      "0.5746527777777778\n",
      "batch_idx: 24\n",
      "0.5766666666666667\n",
      "Training Epoch: 110, total loss: 46.909886\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.5208333333333334\n",
      "batch_idx: 2\n",
      "0.5416666666666666\n",
      "batch_idx: 3\n",
      "0.6041666666666666\n",
      "batch_idx: 4\n",
      "0.6\n",
      "batch_idx: 5\n",
      "0.5972222222222222\n",
      "batch_idx: 6\n",
      "0.6071428571428571\n",
      "batch_idx: 7\n",
      "0.609375\n",
      "batch_idx: 8\n",
      "0.6157407407407407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 9\n",
      "0.6166666666666667\n",
      "batch_idx: 10\n",
      "0.6098484848484849\n",
      "batch_idx: 11\n",
      "0.5972222222222222\n",
      "batch_idx: 12\n",
      "0.592948717948718\n",
      "batch_idx: 13\n",
      "0.5922619047619048\n",
      "batch_idx: 14\n",
      "0.5916666666666667\n",
      "batch_idx: 15\n",
      "0.5963541666666666\n",
      "batch_idx: 16\n",
      "0.5906862745098039\n",
      "batch_idx: 17\n",
      "0.5925925925925926\n",
      "batch_idx: 18\n",
      "0.5942982456140351\n",
      "batch_idx: 19\n",
      "0.5958333333333333\n",
      "batch_idx: 20\n",
      "0.5972222222222222\n",
      "batch_idx: 21\n",
      "0.6041666666666666\n",
      "batch_idx: 22\n",
      "0.6105072463768116\n",
      "batch_idx: 23\n",
      "0.6059027777777778\n",
      "batch_idx: 24\n",
      "0.6033333333333334\n",
      "Training Epoch: 111, total loss: 46.739715\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.6583333333333333\n",
      "batch_idx: 5\n",
      "0.6319444444444444\n",
      "batch_idx: 6\n",
      "0.6369047619047619\n",
      "batch_idx: 7\n",
      "0.609375\n",
      "batch_idx: 8\n",
      "0.5972222222222222\n",
      "batch_idx: 9\n",
      "0.5833333333333334\n",
      "batch_idx: 10\n",
      "0.5871212121212122\n",
      "batch_idx: 11\n",
      "0.5763888888888888\n",
      "batch_idx: 12\n",
      "0.5576923076923077\n",
      "batch_idx: 13\n",
      "0.5684523809523809\n",
      "batch_idx: 14\n",
      "0.5638888888888889\n",
      "batch_idx: 15\n",
      "0.5729166666666666\n",
      "batch_idx: 16\n",
      "0.5784313725490197\n",
      "batch_idx: 17\n",
      "0.5856481481481481\n",
      "batch_idx: 18\n",
      "0.5833333333333334\n",
      "batch_idx: 19\n",
      "0.5854166666666667\n",
      "batch_idx: 20\n",
      "0.5853174603174603\n",
      "batch_idx: 21\n",
      "0.5776515151515151\n",
      "batch_idx: 22\n",
      "0.5778985507246377\n",
      "batch_idx: 23\n",
      "0.5729166666666666\n",
      "batch_idx: 24\n",
      "0.575\n",
      "Training Epoch: 112, total loss: 47.091712\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.6736111111111112\n",
      "batch_idx: 6\n",
      "0.6547619047619048\n",
      "batch_idx: 7\n",
      "0.6666666666666666\n",
      "batch_idx: 8\n",
      "0.6481481481481481\n",
      "batch_idx: 9\n",
      "0.6291666666666667\n",
      "batch_idx: 10\n",
      "0.6325757575757576\n",
      "batch_idx: 11\n",
      "0.6354166666666666\n",
      "batch_idx: 12\n",
      "0.6314102564102564\n",
      "batch_idx: 13\n",
      "0.6279761904761905\n",
      "batch_idx: 14\n",
      "0.6166666666666667\n",
      "batch_idx: 15\n",
      "0.6276041666666666\n",
      "batch_idx: 16\n",
      "0.6225490196078431\n",
      "batch_idx: 17\n",
      "0.6226851851851852\n",
      "batch_idx: 18\n",
      "0.6271929824561403\n",
      "batch_idx: 19\n",
      "0.6229166666666667\n",
      "batch_idx: 20\n",
      "0.623015873015873\n",
      "batch_idx: 21\n",
      "0.6212121212121212\n",
      "batch_idx: 22\n",
      "0.6123188405797102\n",
      "batch_idx: 23\n",
      "0.6059027777777778\n",
      "batch_idx: 24\n",
      "0.6033333333333334\n",
      "Training Epoch: 113, total loss: 46.658722\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.5972222222222222\n",
      "batch_idx: 3\n",
      "0.59375\n",
      "batch_idx: 4\n",
      "0.6083333333333333\n",
      "batch_idx: 5\n",
      "0.625\n",
      "batch_idx: 6\n",
      "0.6130952380952381\n",
      "batch_idx: 7\n",
      "0.609375\n",
      "batch_idx: 8\n",
      "0.6018518518518519\n",
      "batch_idx: 9\n",
      "0.6041666666666666\n",
      "batch_idx: 10\n",
      "0.6060606060606061\n",
      "batch_idx: 11\n",
      "0.625\n",
      "batch_idx: 12\n",
      "0.6314102564102564\n",
      "batch_idx: 13\n",
      "0.6190476190476191\n",
      "batch_idx: 14\n",
      "0.6194444444444445\n",
      "batch_idx: 15\n",
      "0.625\n",
      "batch_idx: 16\n",
      "0.6225490196078431\n",
      "batch_idx: 17\n",
      "0.6203703703703703\n",
      "batch_idx: 18\n",
      "0.6293859649122807\n",
      "batch_idx: 19\n",
      "0.6270833333333333\n",
      "batch_idx: 20\n",
      "0.6130952380952381\n",
      "batch_idx: 21\n",
      "0.615530303030303\n",
      "batch_idx: 22\n",
      "0.6159420289855072\n",
      "batch_idx: 23\n",
      "0.6163194444444444\n",
      "batch_idx: 24\n",
      "0.6166666666666667\n",
      "Training Epoch: 114, total loss: 46.327430\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.5625\n",
      "batch_idx: 2\n",
      "0.5833333333333334\n",
      "batch_idx: 3\n",
      "0.59375\n",
      "batch_idx: 4\n",
      "0.625\n",
      "batch_idx: 5\n",
      "0.6388888888888888\n",
      "batch_idx: 6\n",
      "0.6011904761904762\n",
      "batch_idx: 7\n",
      "0.5885416666666666\n",
      "batch_idx: 8\n",
      "0.5879629629629629\n",
      "batch_idx: 9\n",
      "0.5791666666666667\n",
      "batch_idx: 10\n",
      "0.571969696969697\n",
      "batch_idx: 11\n",
      "0.5902777777777778\n",
      "batch_idx: 12\n",
      "0.6025641025641025\n",
      "batch_idx: 13\n",
      "0.6101190476190477\n",
      "batch_idx: 14\n",
      "0.6111111111111112\n",
      "batch_idx: 15\n",
      "0.6171875\n",
      "batch_idx: 16\n",
      "0.6151960784313726\n",
      "batch_idx: 17\n",
      "0.6226851851851852\n",
      "batch_idx: 18\n",
      "0.618421052631579\n",
      "batch_idx: 19\n",
      "0.6208333333333333\n",
      "batch_idx: 20\n",
      "0.6190476190476191\n",
      "batch_idx: 21\n",
      "0.6212121212121212\n",
      "batch_idx: 22\n",
      "0.6177536231884058\n",
      "batch_idx: 23\n",
      "0.6197916666666666\n",
      "batch_idx: 24\n",
      "0.615\n",
      "Training Epoch: 115, total loss: 46.472190\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.6111111111111112\n",
      "batch_idx: 3\n",
      "0.59375\n",
      "batch_idx: 4\n",
      "0.5833333333333334\n",
      "batch_idx: 5\n",
      "0.6041666666666666\n",
      "batch_idx: 6\n",
      "0.6071428571428571\n",
      "batch_idx: 7\n",
      "0.5885416666666666\n",
      "batch_idx: 8\n",
      "0.6064814814814815\n",
      "batch_idx: 9\n",
      "0.6083333333333333\n",
      "batch_idx: 10\n",
      "0.6174242424242424\n",
      "batch_idx: 11\n",
      "0.6145833333333334\n",
      "batch_idx: 12\n",
      "0.6217948717948718\n",
      "batch_idx: 13\n",
      "0.6279761904761905\n",
      "batch_idx: 14\n",
      "0.6305555555555555\n",
      "batch_idx: 15\n",
      "0.6276041666666666\n",
      "batch_idx: 16\n",
      "0.6348039215686274\n",
      "batch_idx: 17\n",
      "0.625\n",
      "batch_idx: 18\n",
      "0.631578947368421\n",
      "batch_idx: 19\n",
      "0.6354166666666666\n",
      "batch_idx: 20\n",
      "0.6309523809523809\n",
      "batch_idx: 21\n",
      "0.6363636363636364\n",
      "batch_idx: 22\n",
      "0.6376811594202898\n",
      "batch_idx: 23\n",
      "0.6336805555555556\n",
      "batch_idx: 24\n",
      "0.6366666666666667\n",
      "Training Epoch: 116, total loss: 46.153163\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.5972222222222222\n",
      "batch_idx: 3\n",
      "0.625\n",
      "batch_idx: 4\n",
      "0.6166666666666667\n",
      "batch_idx: 5\n",
      "0.625\n",
      "batch_idx: 6\n",
      "0.6011904761904762\n",
      "batch_idx: 7\n",
      "0.5833333333333334\n",
      "batch_idx: 8\n",
      "0.5972222222222222\n",
      "batch_idx: 9\n",
      "0.6083333333333333\n",
      "batch_idx: 10\n",
      "0.6136363636363636\n",
      "batch_idx: 11\n",
      "0.6006944444444444\n",
      "batch_idx: 12\n",
      "0.6121794871794872\n",
      "batch_idx: 13\n",
      "0.6101190476190477\n",
      "batch_idx: 14\n",
      "0.6222222222222222\n",
      "batch_idx: 15\n",
      "0.6171875\n",
      "batch_idx: 16\n",
      "0.6176470588235294\n",
      "batch_idx: 17\n",
      "0.6203703703703703\n",
      "batch_idx: 18\n",
      "0.6162280701754386\n",
      "batch_idx: 19\n",
      "0.61875\n",
      "batch_idx: 20\n",
      "0.6130952380952381\n",
      "batch_idx: 21\n",
      "0.6174242424242424\n",
      "batch_idx: 22\n",
      "0.6195652173913043\n",
      "batch_idx: 23\n",
      "0.6267361111111112\n",
      "batch_idx: 24\n",
      "0.6216666666666667\n",
      "Training Epoch: 117, total loss: 46.199046\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.6770833333333334\n",
      "batch_idx: 4\n",
      "0.6583333333333333\n",
      "batch_idx: 5\n",
      "0.6527777777777778\n",
      "batch_idx: 6\n",
      "0.6607142857142857\n",
      "batch_idx: 7\n",
      "0.6875\n",
      "batch_idx: 8\n",
      "0.6712962962962963\n",
      "batch_idx: 9\n",
      "0.6708333333333333\n",
      "batch_idx: 10\n",
      "0.6590909090909091\n",
      "batch_idx: 11\n",
      "0.6458333333333334\n",
      "batch_idx: 12\n",
      "0.6410256410256411\n",
      "batch_idx: 13\n",
      "0.6339285714285714\n",
      "batch_idx: 14\n",
      "0.6305555555555555\n",
      "batch_idx: 15\n",
      "0.6354166666666666\n",
      "batch_idx: 16\n",
      "0.6397058823529411\n",
      "batch_idx: 17\n",
      "0.6435185185185185\n",
      "batch_idx: 18\n",
      "0.6425438596491229\n",
      "batch_idx: 19\n",
      "0.6395833333333333\n",
      "batch_idx: 20\n",
      "0.6408730158730159\n",
      "batch_idx: 21\n",
      "0.6382575757575758\n",
      "batch_idx: 22\n",
      "0.6340579710144928\n",
      "batch_idx: 23\n",
      "0.6302083333333334\n",
      "batch_idx: 24\n",
      "0.6283333333333333\n",
      "Training Epoch: 118, total loss: 46.226899\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.5833333333333334\n",
      "batch_idx: 3\n",
      "0.5416666666666666\n",
      "batch_idx: 4\n",
      "0.5666666666666667\n",
      "batch_idx: 5\n",
      "0.5972222222222222\n",
      "batch_idx: 6\n",
      "0.6071428571428571\n",
      "batch_idx: 7\n",
      "0.609375\n",
      "batch_idx: 8\n",
      "0.6203703703703703\n",
      "batch_idx: 9\n",
      "0.6208333333333333\n",
      "batch_idx: 10\n",
      "0.6136363636363636\n",
      "batch_idx: 11\n",
      "0.6076388888888888\n",
      "batch_idx: 12\n",
      "0.6153846153846154\n",
      "batch_idx: 13\n",
      "0.6130952380952381\n",
      "batch_idx: 14\n",
      "0.6111111111111112\n",
      "batch_idx: 15\n",
      "0.6145833333333334\n",
      "batch_idx: 16\n",
      "0.6127450980392157\n",
      "batch_idx: 17\n",
      "0.6111111111111112\n",
      "batch_idx: 18\n",
      "0.6118421052631579\n",
      "batch_idx: 19\n",
      "0.6104166666666667\n",
      "batch_idx: 20\n",
      "0.6071428571428571\n",
      "batch_idx: 21\n",
      "0.6079545454545454\n",
      "batch_idx: 22\n",
      "0.6068840579710145\n",
      "batch_idx: 23\n",
      "0.6111111111111112\n",
      "batch_idx: 24\n",
      "0.6066666666666667\n",
      "Training Epoch: 119, total loss: 46.551778\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.5972222222222222\n",
      "batch_idx: 3\n",
      "0.625\n",
      "batch_idx: 4\n",
      "0.6083333333333333\n",
      "batch_idx: 5\n",
      "0.625\n",
      "batch_idx: 6\n",
      "0.6369047619047619\n",
      "batch_idx: 7\n",
      "0.609375\n",
      "batch_idx: 8\n",
      "0.6111111111111112\n",
      "batch_idx: 9\n",
      "0.6208333333333333\n",
      "batch_idx: 10\n",
      "0.625\n",
      "batch_idx: 11\n",
      "0.6284722222222222\n",
      "batch_idx: 12\n",
      "0.6217948717948718\n",
      "batch_idx: 13\n",
      "0.6279761904761905\n",
      "batch_idx: 14\n",
      "0.6333333333333333\n",
      "batch_idx: 15\n",
      "0.6276041666666666\n",
      "batch_idx: 16\n",
      "0.6323529411764706\n",
      "batch_idx: 17\n",
      "0.625\n",
      "batch_idx: 18\n",
      "0.6271929824561403\n",
      "batch_idx: 19\n",
      "0.6333333333333333\n",
      "batch_idx: 20\n",
      "0.6369047619047619\n",
      "batch_idx: 21\n",
      "0.6439393939393939\n",
      "batch_idx: 22\n",
      "0.6431159420289855\n",
      "batch_idx: 23\n",
      "0.6388888888888888\n",
      "batch_idx: 24\n",
      "0.6333333333333333\n",
      "Training Epoch: 120, total loss: 45.907802\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.65625\n",
      "batch_idx: 4\n",
      "0.625\n",
      "batch_idx: 5\n",
      "0.6180555555555556\n",
      "batch_idx: 6\n",
      "0.6428571428571429\n",
      "batch_idx: 7\n",
      "0.640625\n",
      "batch_idx: 8\n",
      "0.6342592592592593\n",
      "batch_idx: 9\n",
      "0.6208333333333333\n",
      "batch_idx: 10\n",
      "0.6174242424242424\n",
      "batch_idx: 11\n",
      "0.6145833333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 12\n",
      "0.6217948717948718\n",
      "batch_idx: 13\n",
      "0.6458333333333334\n",
      "batch_idx: 14\n",
      "0.6361111111111111\n",
      "batch_idx: 15\n",
      "0.6354166666666666\n",
      "batch_idx: 16\n",
      "0.6299019607843137\n",
      "batch_idx: 17\n",
      "0.6342592592592593\n",
      "batch_idx: 18\n",
      "0.631578947368421\n",
      "batch_idx: 19\n",
      "0.6354166666666666\n",
      "batch_idx: 20\n",
      "0.628968253968254\n",
      "batch_idx: 21\n",
      "0.625\n",
      "batch_idx: 22\n",
      "0.6304347826086957\n",
      "batch_idx: 23\n",
      "0.6336805555555556\n",
      "batch_idx: 24\n",
      "0.6266666666666667\n",
      "Training Epoch: 121, total loss: 46.304919\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.6666666666666666\n",
      "batch_idx: 4\n",
      "0.6583333333333333\n",
      "batch_idx: 5\n",
      "0.6388888888888888\n",
      "batch_idx: 6\n",
      "0.6190476190476191\n",
      "batch_idx: 7\n",
      "0.6197916666666666\n",
      "batch_idx: 8\n",
      "0.5833333333333334\n",
      "batch_idx: 9\n",
      "0.5791666666666667\n",
      "batch_idx: 10\n",
      "0.5833333333333334\n",
      "batch_idx: 11\n",
      "0.6006944444444444\n",
      "batch_idx: 12\n",
      "0.5993589743589743\n",
      "batch_idx: 13\n",
      "0.6130952380952381\n",
      "batch_idx: 14\n",
      "0.6138888888888889\n",
      "batch_idx: 15\n",
      "0.6223958333333334\n",
      "batch_idx: 16\n",
      "0.6225490196078431\n",
      "batch_idx: 17\n",
      "0.6180555555555556\n",
      "batch_idx: 18\n",
      "0.618421052631579\n",
      "batch_idx: 19\n",
      "0.61875\n",
      "batch_idx: 20\n",
      "0.6130952380952381\n",
      "batch_idx: 21\n",
      "0.6098484848484849\n",
      "batch_idx: 22\n",
      "0.6123188405797102\n",
      "batch_idx: 23\n",
      "0.6163194444444444\n",
      "batch_idx: 24\n",
      "0.6216666666666667\n",
      "Training Epoch: 122, total loss: 46.300943\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.6875\n",
      "batch_idx: 6\n",
      "0.6785714285714286\n",
      "batch_idx: 7\n",
      "0.671875\n",
      "batch_idx: 8\n",
      "0.6574074074074074\n",
      "batch_idx: 9\n",
      "0.6416666666666667\n",
      "batch_idx: 10\n",
      "0.625\n",
      "batch_idx: 11\n",
      "0.6284722222222222\n",
      "batch_idx: 12\n",
      "0.6282051282051282\n",
      "batch_idx: 13\n",
      "0.6398809523809523\n",
      "batch_idx: 14\n",
      "0.6361111111111111\n",
      "batch_idx: 15\n",
      "0.6380208333333334\n",
      "batch_idx: 16\n",
      "0.6323529411764706\n",
      "batch_idx: 17\n",
      "0.6412037037037037\n",
      "batch_idx: 18\n",
      "0.6337719298245614\n",
      "batch_idx: 19\n",
      "0.6395833333333333\n",
      "batch_idx: 20\n",
      "0.628968253968254\n",
      "batch_idx: 21\n",
      "0.6231060606060606\n",
      "batch_idx: 22\n",
      "0.625\n",
      "batch_idx: 23\n",
      "0.6232638888888888\n",
      "batch_idx: 24\n",
      "0.6133333333333333\n",
      "Training Epoch: 123, total loss: 46.292146\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.65625\n",
      "batch_idx: 4\n",
      "0.6166666666666667\n",
      "batch_idx: 5\n",
      "0.6180555555555556\n",
      "batch_idx: 6\n",
      "0.6011904761904762\n",
      "batch_idx: 7\n",
      "0.59375\n",
      "batch_idx: 8\n",
      "0.5879629629629629\n",
      "batch_idx: 9\n",
      "0.5875\n",
      "batch_idx: 10\n",
      "0.5795454545454546\n",
      "batch_idx: 11\n",
      "0.5868055555555556\n",
      "batch_idx: 12\n",
      "0.5993589743589743\n",
      "batch_idx: 13\n",
      "0.5952380952380952\n",
      "batch_idx: 14\n",
      "0.6\n",
      "batch_idx: 15\n",
      "0.5963541666666666\n",
      "batch_idx: 16\n",
      "0.5906862745098039\n",
      "batch_idx: 17\n",
      "0.5925925925925926\n",
      "batch_idx: 18\n",
      "0.5986842105263158\n",
      "batch_idx: 19\n",
      "0.5979166666666667\n",
      "batch_idx: 20\n",
      "0.5992063492063492\n",
      "batch_idx: 21\n",
      "0.6041666666666666\n",
      "batch_idx: 22\n",
      "0.605072463768116\n",
      "batch_idx: 23\n",
      "0.6128472222222222\n",
      "batch_idx: 24\n",
      "0.6183333333333333\n",
      "Training Epoch: 124, total loss: 46.271556\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.6388888888888888\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.65\n",
      "batch_idx: 5\n",
      "0.6319444444444444\n",
      "batch_idx: 6\n",
      "0.6309523809523809\n",
      "batch_idx: 7\n",
      "0.6302083333333334\n",
      "batch_idx: 8\n",
      "0.6203703703703703\n",
      "batch_idx: 9\n",
      "0.6166666666666667\n",
      "batch_idx: 10\n",
      "0.6136363636363636\n",
      "batch_idx: 11\n",
      "0.6388888888888888\n",
      "batch_idx: 12\n",
      "0.6378205128205128\n",
      "batch_idx: 13\n",
      "0.6369047619047619\n",
      "batch_idx: 14\n",
      "0.6166666666666667\n",
      "batch_idx: 15\n",
      "0.6119791666666666\n",
      "batch_idx: 16\n",
      "0.6029411764705882\n",
      "batch_idx: 17\n",
      "0.6064814814814815\n",
      "batch_idx: 18\n",
      "0.618421052631579\n",
      "batch_idx: 19\n",
      "0.6270833333333333\n",
      "batch_idx: 20\n",
      "0.625\n",
      "batch_idx: 21\n",
      "0.625\n",
      "batch_idx: 22\n",
      "0.6159420289855072\n",
      "batch_idx: 23\n",
      "0.6145833333333334\n",
      "batch_idx: 24\n",
      "0.615\n",
      "Training Epoch: 125, total loss: 46.359943\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.5694444444444444\n",
      "batch_idx: 3\n",
      "0.5833333333333334\n",
      "batch_idx: 4\n",
      "0.575\n",
      "batch_idx: 5\n",
      "0.5555555555555556\n",
      "batch_idx: 6\n",
      "0.5654761904761905\n",
      "batch_idx: 7\n",
      "0.578125\n",
      "batch_idx: 8\n",
      "0.5694444444444444\n",
      "batch_idx: 9\n",
      "0.5958333333333333\n",
      "batch_idx: 10\n",
      "0.6060606060606061\n",
      "batch_idx: 11\n",
      "0.6006944444444444\n",
      "batch_idx: 12\n",
      "0.6121794871794872\n",
      "batch_idx: 13\n",
      "0.6101190476190477\n",
      "batch_idx: 14\n",
      "0.6\n",
      "batch_idx: 15\n",
      "0.6041666666666666\n",
      "batch_idx: 16\n",
      "0.6029411764705882\n",
      "batch_idx: 17\n",
      "0.6064814814814815\n",
      "batch_idx: 18\n",
      "0.6140350877192983\n",
      "batch_idx: 19\n",
      "0.6208333333333333\n",
      "batch_idx: 20\n",
      "0.625\n",
      "batch_idx: 21\n",
      "0.6306818181818182\n",
      "batch_idx: 22\n",
      "0.625\n",
      "batch_idx: 23\n",
      "0.6215277777777778\n",
      "batch_idx: 24\n",
      "0.6266666666666667\n",
      "Training Epoch: 126, total loss: 46.220024\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.6805555555555556\n",
      "batch_idx: 6\n",
      "0.6607142857142857\n",
      "batch_idx: 7\n",
      "0.6458333333333334\n",
      "batch_idx: 8\n",
      "0.625\n",
      "batch_idx: 9\n",
      "0.6333333333333333\n",
      "batch_idx: 10\n",
      "0.6136363636363636\n",
      "batch_idx: 11\n",
      "0.6006944444444444\n",
      "batch_idx: 12\n",
      "0.6153846153846154\n",
      "batch_idx: 13\n",
      "0.6190476190476191\n",
      "batch_idx: 14\n",
      "0.6194444444444445\n",
      "batch_idx: 15\n",
      "0.625\n",
      "batch_idx: 16\n",
      "0.625\n",
      "batch_idx: 17\n",
      "0.6203703703703703\n",
      "batch_idx: 18\n",
      "0.6271929824561403\n",
      "batch_idx: 19\n",
      "0.6270833333333333\n",
      "batch_idx: 20\n",
      "0.621031746031746\n",
      "batch_idx: 21\n",
      "0.615530303030303\n",
      "batch_idx: 22\n",
      "0.6213768115942029\n",
      "batch_idx: 23\n",
      "0.6215277777777778\n",
      "batch_idx: 24\n",
      "0.615\n",
      "Training Epoch: 127, total loss: 46.436208\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.6354166666666666\n",
      "batch_idx: 4\n",
      "0.6333333333333333\n",
      "batch_idx: 5\n",
      "0.5972222222222222\n",
      "batch_idx: 6\n",
      "0.5714285714285714\n",
      "batch_idx: 7\n",
      "0.5729166666666666\n",
      "batch_idx: 8\n",
      "0.6018518518518519\n",
      "batch_idx: 9\n",
      "0.5875\n",
      "batch_idx: 10\n",
      "0.5909090909090909\n",
      "batch_idx: 11\n",
      "0.6076388888888888\n",
      "batch_idx: 12\n",
      "0.6185897435897436\n",
      "batch_idx: 13\n",
      "0.6190476190476191\n",
      "batch_idx: 14\n",
      "0.6111111111111112\n",
      "batch_idx: 15\n",
      "0.6041666666666666\n",
      "batch_idx: 16\n",
      "0.6004901960784313\n",
      "batch_idx: 17\n",
      "0.6041666666666666\n",
      "batch_idx: 18\n",
      "0.6030701754385965\n",
      "batch_idx: 19\n",
      "0.6020833333333333\n",
      "batch_idx: 20\n",
      "0.6051587301587301\n",
      "batch_idx: 21\n",
      "0.6079545454545454\n",
      "batch_idx: 22\n",
      "0.6105072463768116\n",
      "batch_idx: 23\n",
      "0.6059027777777778\n",
      "batch_idx: 24\n",
      "0.605\n",
      "Training Epoch: 128, total loss: 46.439392\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.5625\n",
      "batch_idx: 2\n",
      "0.5833333333333334\n",
      "batch_idx: 3\n",
      "0.5625\n",
      "batch_idx: 4\n",
      "0.5916666666666667\n",
      "batch_idx: 5\n",
      "0.5902777777777778\n",
      "batch_idx: 6\n",
      "0.6190476190476191\n",
      "batch_idx: 7\n",
      "0.625\n",
      "batch_idx: 8\n",
      "0.6157407407407407\n",
      "batch_idx: 9\n",
      "0.6333333333333333\n",
      "batch_idx: 10\n",
      "0.6325757575757576\n",
      "batch_idx: 11\n",
      "0.6388888888888888\n",
      "batch_idx: 12\n",
      "0.6346153846153846\n",
      "batch_idx: 13\n",
      "0.625\n",
      "batch_idx: 14\n",
      "0.6361111111111111\n",
      "batch_idx: 15\n",
      "0.640625\n",
      "batch_idx: 16\n",
      "0.6470588235294118\n",
      "batch_idx: 17\n",
      "0.6550925925925926\n",
      "batch_idx: 18\n",
      "0.6557017543859649\n",
      "batch_idx: 19\n",
      "0.6604166666666667\n",
      "batch_idx: 20\n",
      "0.6646825396825397\n",
      "batch_idx: 21\n",
      "0.6685606060606061\n",
      "batch_idx: 22\n",
      "0.6612318840579711\n",
      "batch_idx: 23\n",
      "0.6510416666666666\n",
      "batch_idx: 24\n",
      "0.6483333333333333\n",
      "Training Epoch: 129, total loss: 45.800620\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.5833333333333334\n",
      "batch_idx: 3\n",
      "0.6041666666666666\n",
      "batch_idx: 4\n",
      "0.65\n",
      "batch_idx: 5\n",
      "0.6527777777777778\n",
      "batch_idx: 6\n",
      "0.6488095238095238\n",
      "batch_idx: 7\n",
      "0.6510416666666666\n",
      "batch_idx: 8\n",
      "0.6342592592592593\n",
      "batch_idx: 9\n",
      "0.6125\n",
      "batch_idx: 10\n",
      "0.6212121212121212\n",
      "batch_idx: 11\n",
      "0.6006944444444444\n",
      "batch_idx: 12\n",
      "0.6153846153846154\n",
      "batch_idx: 13\n",
      "0.6071428571428571\n",
      "batch_idx: 14\n",
      "0.6111111111111112\n",
      "batch_idx: 15\n",
      "0.6223958333333334\n",
      "batch_idx: 16\n",
      "0.6176470588235294\n",
      "batch_idx: 17\n",
      "0.6180555555555556\n",
      "batch_idx: 18\n",
      "0.6162280701754386\n",
      "batch_idx: 19\n",
      "0.6208333333333333\n",
      "batch_idx: 20\n",
      "0.6329365079365079\n",
      "batch_idx: 21\n",
      "0.634469696969697\n",
      "batch_idx: 22\n",
      "0.6431159420289855\n",
      "batch_idx: 23\n",
      "0.6388888888888888\n",
      "batch_idx: 24\n",
      "0.6433333333333333\n",
      "Training Epoch: 130, total loss: 45.853941\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.625\n",
      "batch_idx: 4\n",
      "0.6333333333333333\n",
      "batch_idx: 5\n",
      "0.6597222222222222\n",
      "batch_idx: 6\n",
      "0.6666666666666666\n",
      "batch_idx: 7\n",
      "0.671875\n",
      "batch_idx: 8\n",
      "0.6666666666666666\n",
      "batch_idx: 9\n",
      "0.6666666666666666\n",
      "batch_idx: 10\n",
      "0.6590909090909091\n",
      "batch_idx: 11\n",
      "0.6631944444444444\n",
      "batch_idx: 12\n",
      "0.6730769230769231\n",
      "batch_idx: 13\n",
      "0.6785714285714286\n",
      "batch_idx: 14\n",
      "0.6777777777777778\n",
      "batch_idx: 15\n",
      "0.6640625\n",
      "batch_idx: 16\n",
      "0.6642156862745098\n",
      "batch_idx: 17\n",
      "0.6597222222222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 18\n",
      "0.6622807017543859\n",
      "batch_idx: 19\n",
      "0.6604166666666667\n",
      "batch_idx: 20\n",
      "0.6626984126984127\n",
      "batch_idx: 21\n",
      "0.6590909090909091\n",
      "batch_idx: 22\n",
      "0.6648550724637681\n",
      "batch_idx: 23\n",
      "0.6614583333333334\n",
      "batch_idx: 24\n",
      "0.6616666666666666\n",
      "Training Epoch: 131, total loss: 45.504514\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.7013888888888888\n",
      "batch_idx: 6\n",
      "0.6726190476190477\n",
      "batch_idx: 7\n",
      "0.6510416666666666\n",
      "batch_idx: 8\n",
      "0.6157407407407407\n",
      "batch_idx: 9\n",
      "0.6166666666666667\n",
      "batch_idx: 10\n",
      "0.6098484848484849\n",
      "batch_idx: 11\n",
      "0.6076388888888888\n",
      "batch_idx: 12\n",
      "0.6185897435897436\n",
      "batch_idx: 13\n",
      "0.6279761904761905\n",
      "batch_idx: 14\n",
      "0.6277777777777778\n",
      "batch_idx: 15\n",
      "0.6328125\n",
      "batch_idx: 16\n",
      "0.6446078431372549\n",
      "batch_idx: 17\n",
      "0.6435185185185185\n",
      "batch_idx: 18\n",
      "0.6403508771929824\n",
      "batch_idx: 19\n",
      "0.6395833333333333\n",
      "batch_idx: 20\n",
      "0.621031746031746\n",
      "batch_idx: 21\n",
      "0.6268939393939394\n",
      "batch_idx: 22\n",
      "0.6268115942028986\n",
      "batch_idx: 23\n",
      "0.6354166666666666\n",
      "batch_idx: 24\n",
      "0.6333333333333333\n",
      "Training Epoch: 132, total loss: 46.079200\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.6388888888888888\n",
      "batch_idx: 3\n",
      "0.6354166666666666\n",
      "batch_idx: 4\n",
      "0.6333333333333333\n",
      "batch_idx: 5\n",
      "0.6597222222222222\n",
      "batch_idx: 6\n",
      "0.6488095238095238\n",
      "batch_idx: 7\n",
      "0.6458333333333334\n",
      "batch_idx: 8\n",
      "0.6574074074074074\n",
      "batch_idx: 9\n",
      "0.6583333333333333\n",
      "batch_idx: 10\n",
      "0.6742424242424242\n",
      "batch_idx: 11\n",
      "0.6701388888888888\n",
      "batch_idx: 12\n",
      "0.6762820512820513\n",
      "batch_idx: 13\n",
      "0.6577380952380952\n",
      "batch_idx: 14\n",
      "0.6583333333333333\n",
      "batch_idx: 15\n",
      "0.6510416666666666\n",
      "batch_idx: 16\n",
      "0.6519607843137255\n",
      "batch_idx: 17\n",
      "0.6435185185185185\n",
      "batch_idx: 18\n",
      "0.6337719298245614\n",
      "batch_idx: 19\n",
      "0.63125\n",
      "batch_idx: 20\n",
      "0.6329365079365079\n",
      "batch_idx: 21\n",
      "0.6268939393939394\n",
      "batch_idx: 22\n",
      "0.625\n",
      "batch_idx: 23\n",
      "0.625\n",
      "batch_idx: 24\n",
      "0.635\n",
      "Training Epoch: 133, total loss: 45.991020\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.6666666666666666\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6388888888888888\n",
      "batch_idx: 6\n",
      "0.6488095238095238\n",
      "batch_idx: 7\n",
      "0.6666666666666666\n",
      "batch_idx: 8\n",
      "0.6666666666666666\n",
      "batch_idx: 9\n",
      "0.6541666666666667\n",
      "batch_idx: 10\n",
      "0.6553030303030303\n",
      "batch_idx: 11\n",
      "0.6597222222222222\n",
      "batch_idx: 12\n",
      "0.6634615384615384\n",
      "batch_idx: 13\n",
      "0.6666666666666666\n",
      "batch_idx: 14\n",
      "0.6777777777777778\n",
      "batch_idx: 15\n",
      "0.6692708333333334\n",
      "batch_idx: 16\n",
      "0.6691176470588235\n",
      "batch_idx: 17\n",
      "0.6666666666666666\n",
      "batch_idx: 18\n",
      "0.6666666666666666\n",
      "batch_idx: 19\n",
      "0.6625\n",
      "batch_idx: 20\n",
      "0.6607142857142857\n",
      "batch_idx: 21\n",
      "0.6553030303030303\n",
      "batch_idx: 22\n",
      "0.6539855072463768\n",
      "batch_idx: 23\n",
      "0.6545138888888888\n",
      "batch_idx: 24\n",
      "0.65\n",
      "Training Epoch: 134, total loss: 45.639540\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.6111111111111112\n",
      "batch_idx: 3\n",
      "0.59375\n",
      "batch_idx: 4\n",
      "0.625\n",
      "batch_idx: 5\n",
      "0.6388888888888888\n",
      "batch_idx: 6\n",
      "0.6428571428571429\n",
      "batch_idx: 7\n",
      "0.6197916666666666\n",
      "batch_idx: 8\n",
      "0.625\n",
      "batch_idx: 9\n",
      "0.6333333333333333\n",
      "batch_idx: 10\n",
      "0.6401515151515151\n",
      "batch_idx: 11\n",
      "0.6319444444444444\n",
      "batch_idx: 12\n",
      "0.6282051282051282\n",
      "batch_idx: 13\n",
      "0.6339285714285714\n",
      "batch_idx: 14\n",
      "0.6361111111111111\n",
      "batch_idx: 15\n",
      "0.640625\n",
      "batch_idx: 16\n",
      "0.6397058823529411\n",
      "batch_idx: 17\n",
      "0.6388888888888888\n",
      "batch_idx: 18\n",
      "0.6403508771929824\n",
      "batch_idx: 19\n",
      "0.6354166666666666\n",
      "batch_idx: 20\n",
      "0.6329365079365079\n",
      "batch_idx: 21\n",
      "0.6382575757575758\n",
      "batch_idx: 22\n",
      "0.6394927536231884\n",
      "batch_idx: 23\n",
      "0.6371527777777778\n",
      "batch_idx: 24\n",
      "0.6333333333333333\n",
      "Training Epoch: 135, total loss: 46.043482\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.5972222222222222\n",
      "batch_idx: 3\n",
      "0.5833333333333334\n",
      "batch_idx: 4\n",
      "0.5916666666666667\n",
      "batch_idx: 5\n",
      "0.5972222222222222\n",
      "batch_idx: 6\n",
      "0.5952380952380952\n",
      "batch_idx: 7\n",
      "0.578125\n",
      "batch_idx: 8\n",
      "0.5879629629629629\n",
      "batch_idx: 9\n",
      "0.6\n",
      "batch_idx: 10\n",
      "0.6098484848484849\n",
      "batch_idx: 11\n",
      "0.6076388888888888\n",
      "batch_idx: 12\n",
      "0.6089743589743589\n",
      "batch_idx: 13\n",
      "0.6130952380952381\n",
      "batch_idx: 14\n",
      "0.6194444444444445\n",
      "batch_idx: 15\n",
      "0.6223958333333334\n",
      "batch_idx: 16\n",
      "0.6323529411764706\n",
      "batch_idx: 17\n",
      "0.6296296296296297\n",
      "batch_idx: 18\n",
      "0.625\n",
      "batch_idx: 19\n",
      "0.625\n",
      "batch_idx: 20\n",
      "0.6309523809523809\n",
      "batch_idx: 21\n",
      "0.6268939393939394\n",
      "batch_idx: 22\n",
      "0.6286231884057971\n",
      "batch_idx: 23\n",
      "0.6215277777777778\n",
      "batch_idx: 24\n",
      "0.62\n",
      "Training Epoch: 136, total loss: 46.565249\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.5833333333333334\n",
      "batch_idx: 3\n",
      "0.6145833333333334\n",
      "batch_idx: 4\n",
      "0.625\n",
      "batch_idx: 5\n",
      "0.6388888888888888\n",
      "batch_idx: 6\n",
      "0.6488095238095238\n",
      "batch_idx: 7\n",
      "0.6302083333333334\n",
      "batch_idx: 8\n",
      "0.625\n",
      "batch_idx: 9\n",
      "0.6125\n",
      "batch_idx: 10\n",
      "0.6098484848484849\n",
      "batch_idx: 11\n",
      "0.625\n",
      "batch_idx: 12\n",
      "0.6282051282051282\n",
      "batch_idx: 13\n",
      "0.6279761904761905\n",
      "batch_idx: 14\n",
      "0.6361111111111111\n",
      "batch_idx: 15\n",
      "0.6276041666666666\n",
      "batch_idx: 16\n",
      "0.6323529411764706\n",
      "batch_idx: 17\n",
      "0.6365740740740741\n",
      "batch_idx: 18\n",
      "0.6337719298245614\n",
      "batch_idx: 19\n",
      "0.6270833333333333\n",
      "batch_idx: 20\n",
      "0.623015873015873\n",
      "batch_idx: 21\n",
      "0.6193181818181818\n",
      "batch_idx: 22\n",
      "0.6177536231884058\n",
      "batch_idx: 23\n",
      "0.6197916666666666\n",
      "batch_idx: 24\n",
      "0.6183333333333333\n",
      "Training Epoch: 137, total loss: 46.306323\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6111111111111112\n",
      "batch_idx: 3\n",
      "0.625\n",
      "batch_idx: 4\n",
      "0.6416666666666667\n",
      "batch_idx: 5\n",
      "0.6319444444444444\n",
      "batch_idx: 6\n",
      "0.6428571428571429\n",
      "batch_idx: 7\n",
      "0.6458333333333334\n",
      "batch_idx: 8\n",
      "0.6388888888888888\n",
      "batch_idx: 9\n",
      "0.6458333333333334\n",
      "batch_idx: 10\n",
      "0.6477272727272727\n",
      "batch_idx: 11\n",
      "0.6423611111111112\n",
      "batch_idx: 12\n",
      "0.6378205128205128\n",
      "batch_idx: 13\n",
      "0.6309523809523809\n",
      "batch_idx: 14\n",
      "0.625\n",
      "batch_idx: 15\n",
      "0.6223958333333334\n",
      "batch_idx: 16\n",
      "0.6274509803921569\n",
      "batch_idx: 17\n",
      "0.6203703703703703\n",
      "batch_idx: 18\n",
      "0.618421052631579\n",
      "batch_idx: 19\n",
      "0.6166666666666667\n",
      "batch_idx: 20\n",
      "0.6150793650793651\n",
      "batch_idx: 21\n",
      "0.6117424242424242\n",
      "batch_idx: 22\n",
      "0.6105072463768116\n",
      "batch_idx: 23\n",
      "0.6041666666666666\n",
      "batch_idx: 24\n",
      "0.6033333333333334\n",
      "Training Epoch: 138, total loss: 46.733444\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.5694444444444444\n",
      "batch_idx: 3\n",
      "0.5625\n",
      "batch_idx: 4\n",
      "0.575\n",
      "batch_idx: 5\n",
      "0.5486111111111112\n",
      "batch_idx: 6\n",
      "0.5595238095238095\n",
      "batch_idx: 7\n",
      "0.5572916666666666\n",
      "batch_idx: 8\n",
      "0.5787037037037037\n",
      "batch_idx: 9\n",
      "0.5791666666666667\n",
      "batch_idx: 10\n",
      "0.6098484848484849\n",
      "batch_idx: 11\n",
      "0.6041666666666666\n",
      "batch_idx: 12\n",
      "0.6089743589743589\n",
      "batch_idx: 13\n",
      "0.6041666666666666\n",
      "batch_idx: 14\n",
      "0.5888888888888889\n",
      "batch_idx: 15\n",
      "0.5859375\n",
      "batch_idx: 16\n",
      "0.5857843137254902\n",
      "batch_idx: 17\n",
      "0.5902777777777778\n",
      "batch_idx: 18\n",
      "0.5877192982456141\n",
      "batch_idx: 19\n",
      "0.59375\n",
      "batch_idx: 20\n",
      "0.5892857142857143\n",
      "batch_idx: 21\n",
      "0.5871212121212122\n",
      "batch_idx: 22\n",
      "0.5869565217391305\n",
      "batch_idx: 23\n",
      "0.5868055555555556\n",
      "batch_idx: 24\n",
      "0.5833333333333334\n",
      "Training Epoch: 139, total loss: 46.956907\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.5694444444444444\n",
      "batch_idx: 3\n",
      "0.5520833333333334\n",
      "batch_idx: 4\n",
      "0.5333333333333333\n",
      "batch_idx: 5\n",
      "0.5486111111111112\n",
      "batch_idx: 6\n",
      "0.5357142857142857\n",
      "batch_idx: 7\n",
      "0.5364583333333334\n",
      "batch_idx: 8\n",
      "0.5509259259259259\n",
      "batch_idx: 9\n",
      "0.5541666666666667\n",
      "batch_idx: 10\n",
      "0.5568181818181818\n",
      "batch_idx: 11\n",
      "0.5555555555555556\n",
      "batch_idx: 12\n",
      "0.5576923076923077\n",
      "batch_idx: 13\n",
      "0.5535714285714286\n",
      "batch_idx: 14\n",
      "0.5583333333333333\n",
      "batch_idx: 15\n",
      "0.5598958333333334\n",
      "batch_idx: 16\n",
      "0.571078431372549\n",
      "batch_idx: 17\n",
      "0.5810185185185185\n",
      "batch_idx: 18\n",
      "0.581140350877193\n",
      "batch_idx: 19\n",
      "0.5729166666666666\n",
      "batch_idx: 20\n",
      "0.5853174603174603\n",
      "batch_idx: 21\n",
      "0.5814393939393939\n",
      "batch_idx: 22\n",
      "0.5815217391304348\n",
      "batch_idx: 23\n",
      "0.578125\n",
      "batch_idx: 24\n",
      "0.5783333333333334\n",
      "Training Epoch: 140, total loss: 47.130180\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.7\n",
      "batch_idx: 5\n",
      "0.6944444444444444\n",
      "batch_idx: 6\n",
      "0.6726190476190477\n",
      "batch_idx: 7\n",
      "0.6822916666666666\n",
      "batch_idx: 8\n",
      "0.6759259259259259\n",
      "batch_idx: 9\n",
      "0.6875\n",
      "batch_idx: 10\n",
      "0.6893939393939394\n",
      "batch_idx: 11\n",
      "0.6875\n",
      "batch_idx: 12\n",
      "0.6730769230769231\n",
      "batch_idx: 13\n",
      "0.6666666666666666\n",
      "batch_idx: 14\n",
      "0.6666666666666666\n",
      "batch_idx: 15\n",
      "0.671875\n",
      "batch_idx: 16\n",
      "0.6691176470588235\n",
      "batch_idx: 17\n",
      "0.6689814814814815\n",
      "batch_idx: 18\n",
      "0.6666666666666666\n",
      "batch_idx: 19\n",
      "0.6583333333333333\n",
      "batch_idx: 20\n",
      "0.6626984126984127\n",
      "batch_idx: 21\n",
      "0.6609848484848485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 22\n",
      "0.6612318840579711\n",
      "batch_idx: 23\n",
      "0.6545138888888888\n",
      "batch_idx: 24\n",
      "0.6516666666666666\n",
      "Training Epoch: 141, total loss: 45.845388\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.7\n",
      "batch_idx: 5\n",
      "0.6875\n",
      "batch_idx: 6\n",
      "0.6726190476190477\n",
      "batch_idx: 7\n",
      "0.6822916666666666\n",
      "batch_idx: 8\n",
      "0.6759259259259259\n",
      "batch_idx: 9\n",
      "0.6625\n",
      "batch_idx: 10\n",
      "0.6477272727272727\n",
      "batch_idx: 11\n",
      "0.6527777777777778\n",
      "batch_idx: 12\n",
      "0.6506410256410257\n",
      "batch_idx: 13\n",
      "0.6428571428571429\n",
      "batch_idx: 14\n",
      "0.6388888888888888\n",
      "batch_idx: 15\n",
      "0.6302083333333334\n",
      "batch_idx: 16\n",
      "0.6323529411764706\n",
      "batch_idx: 17\n",
      "0.6319444444444444\n",
      "batch_idx: 18\n",
      "0.6293859649122807\n",
      "batch_idx: 19\n",
      "0.6354166666666666\n",
      "batch_idx: 20\n",
      "0.6309523809523809\n",
      "batch_idx: 21\n",
      "0.6287878787878788\n",
      "batch_idx: 22\n",
      "0.6286231884057971\n",
      "batch_idx: 23\n",
      "0.6302083333333334\n",
      "batch_idx: 24\n",
      "0.6266666666666667\n",
      "Training Epoch: 142, total loss: 45.971114\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.65625\n",
      "batch_idx: 4\n",
      "0.6583333333333333\n",
      "batch_idx: 5\n",
      "0.6666666666666666\n",
      "batch_idx: 6\n",
      "0.6666666666666666\n",
      "batch_idx: 7\n",
      "0.6666666666666666\n",
      "batch_idx: 8\n",
      "0.6712962962962963\n",
      "batch_idx: 9\n",
      "0.6708333333333333\n",
      "batch_idx: 10\n",
      "0.6666666666666666\n",
      "batch_idx: 11\n",
      "0.6597222222222222\n",
      "batch_idx: 12\n",
      "0.6602564102564102\n",
      "batch_idx: 13\n",
      "0.6488095238095238\n",
      "batch_idx: 14\n",
      "0.6472222222222223\n",
      "batch_idx: 15\n",
      "0.6588541666666666\n",
      "batch_idx: 16\n",
      "0.6568627450980392\n",
      "batch_idx: 17\n",
      "0.6574074074074074\n",
      "batch_idx: 18\n",
      "0.6622807017543859\n",
      "batch_idx: 19\n",
      "0.6583333333333333\n",
      "batch_idx: 20\n",
      "0.6547619047619048\n",
      "batch_idx: 21\n",
      "0.6496212121212122\n",
      "batch_idx: 22\n",
      "0.6503623188405797\n",
      "batch_idx: 23\n",
      "0.6423611111111112\n",
      "batch_idx: 24\n",
      "0.64\n",
      "Training Epoch: 143, total loss: 45.839261\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6111111111111112\n",
      "batch_idx: 3\n",
      "0.65625\n",
      "batch_idx: 4\n",
      "0.6666666666666666\n",
      "batch_idx: 5\n",
      "0.6597222222222222\n",
      "batch_idx: 6\n",
      "0.6666666666666666\n",
      "batch_idx: 7\n",
      "0.65625\n",
      "batch_idx: 8\n",
      "0.6527777777777778\n",
      "batch_idx: 9\n",
      "0.6541666666666667\n",
      "batch_idx: 10\n",
      "0.6515151515151515\n",
      "batch_idx: 11\n",
      "0.6493055555555556\n",
      "batch_idx: 12\n",
      "0.6410256410256411\n",
      "batch_idx: 13\n",
      "0.6428571428571429\n",
      "batch_idx: 14\n",
      "0.6444444444444445\n",
      "batch_idx: 15\n",
      "0.6302083333333334\n",
      "batch_idx: 16\n",
      "0.6274509803921569\n",
      "batch_idx: 17\n",
      "0.6134259259259259\n",
      "batch_idx: 18\n",
      "0.6228070175438597\n",
      "batch_idx: 19\n",
      "0.6270833333333333\n",
      "batch_idx: 20\n",
      "0.626984126984127\n",
      "batch_idx: 21\n",
      "0.625\n",
      "batch_idx: 22\n",
      "0.625\n",
      "batch_idx: 23\n",
      "0.6267361111111112\n",
      "batch_idx: 24\n",
      "0.625\n",
      "Training Epoch: 144, total loss: 46.281440\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.5416666666666666\n",
      "batch_idx: 3\n",
      "0.5625\n",
      "batch_idx: 4\n",
      "0.5666666666666667\n",
      "batch_idx: 5\n",
      "0.5416666666666666\n",
      "batch_idx: 6\n",
      "0.5595238095238095\n",
      "batch_idx: 7\n",
      "0.5572916666666666\n",
      "batch_idx: 8\n",
      "0.5925925925925926\n",
      "batch_idx: 9\n",
      "0.5958333333333333\n",
      "batch_idx: 10\n",
      "0.5909090909090909\n",
      "batch_idx: 11\n",
      "0.5833333333333334\n",
      "batch_idx: 12\n",
      "0.5833333333333334\n",
      "batch_idx: 13\n",
      "0.5803571428571429\n",
      "batch_idx: 14\n",
      "0.5722222222222222\n",
      "batch_idx: 15\n",
      "0.5651041666666666\n",
      "batch_idx: 16\n",
      "0.5661764705882353\n",
      "batch_idx: 17\n",
      "0.5648148148148148\n",
      "batch_idx: 18\n",
      "0.5679824561403509\n",
      "batch_idx: 19\n",
      "0.5708333333333333\n",
      "batch_idx: 20\n",
      "0.5753968253968254\n",
      "batch_idx: 21\n",
      "0.5795454545454546\n",
      "batch_idx: 22\n",
      "0.5869565217391305\n",
      "batch_idx: 23\n",
      "0.5885416666666666\n",
      "batch_idx: 24\n",
      "0.5933333333333334\n",
      "Training Epoch: 145, total loss: 46.692306\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.6041666666666666\n",
      "batch_idx: 4\n",
      "0.5916666666666667\n",
      "batch_idx: 5\n",
      "0.625\n",
      "batch_idx: 6\n",
      "0.6130952380952381\n",
      "batch_idx: 7\n",
      "0.625\n",
      "batch_idx: 8\n",
      "0.6064814814814815\n",
      "batch_idx: 9\n",
      "0.6083333333333333\n",
      "batch_idx: 10\n",
      "0.6098484848484849\n",
      "batch_idx: 11\n",
      "0.6180555555555556\n",
      "batch_idx: 12\n",
      "0.6185897435897436\n",
      "batch_idx: 13\n",
      "0.6160714285714286\n",
      "batch_idx: 14\n",
      "0.6138888888888889\n",
      "batch_idx: 15\n",
      "0.6171875\n",
      "batch_idx: 16\n",
      "0.6200980392156863\n",
      "batch_idx: 17\n",
      "0.6273148148148148\n",
      "batch_idx: 18\n",
      "0.6206140350877193\n",
      "batch_idx: 19\n",
      "0.6208333333333333\n",
      "batch_idx: 20\n",
      "0.625\n",
      "batch_idx: 21\n",
      "0.625\n",
      "batch_idx: 22\n",
      "0.6286231884057971\n",
      "batch_idx: 23\n",
      "0.625\n",
      "batch_idx: 24\n",
      "0.63\n",
      "Training Epoch: 146, total loss: 46.032106\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.5416666666666666\n",
      "batch_idx: 2\n",
      "0.5694444444444444\n",
      "batch_idx: 3\n",
      "0.5625\n",
      "batch_idx: 4\n",
      "0.6\n",
      "batch_idx: 5\n",
      "0.6111111111111112\n",
      "batch_idx: 6\n",
      "0.6130952380952381\n",
      "batch_idx: 7\n",
      "0.6302083333333334\n",
      "batch_idx: 8\n",
      "0.6388888888888888\n",
      "batch_idx: 9\n",
      "0.6458333333333334\n",
      "batch_idx: 10\n",
      "0.6401515151515151\n",
      "batch_idx: 11\n",
      "0.6319444444444444\n",
      "batch_idx: 12\n",
      "0.625\n",
      "batch_idx: 13\n",
      "0.6160714285714286\n",
      "batch_idx: 14\n",
      "0.6111111111111112\n",
      "batch_idx: 15\n",
      "0.5963541666666666\n",
      "batch_idx: 16\n",
      "0.5955882352941176\n",
      "batch_idx: 17\n",
      "0.5995370370370371\n",
      "batch_idx: 18\n",
      "0.6008771929824561\n",
      "batch_idx: 19\n",
      "0.60625\n",
      "batch_idx: 20\n",
      "0.6111111111111112\n",
      "batch_idx: 21\n",
      "0.6193181818181818\n",
      "batch_idx: 22\n",
      "0.6141304347826086\n",
      "batch_idx: 23\n",
      "0.6145833333333334\n",
      "batch_idx: 24\n",
      "0.615\n",
      "Training Epoch: 147, total loss: 46.266754\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6111111111111112\n",
      "batch_idx: 3\n",
      "0.6354166666666666\n",
      "batch_idx: 4\n",
      "0.6166666666666667\n",
      "batch_idx: 5\n",
      "0.6180555555555556\n",
      "batch_idx: 6\n",
      "0.6011904761904762\n",
      "batch_idx: 7\n",
      "0.6197916666666666\n",
      "batch_idx: 8\n",
      "0.5925925925925926\n",
      "batch_idx: 9\n",
      "0.6083333333333333\n",
      "batch_idx: 10\n",
      "0.6060606060606061\n",
      "batch_idx: 11\n",
      "0.6076388888888888\n",
      "batch_idx: 12\n",
      "0.6089743589743589\n",
      "batch_idx: 13\n",
      "0.6130952380952381\n",
      "batch_idx: 14\n",
      "0.6194444444444445\n",
      "batch_idx: 15\n",
      "0.6171875\n",
      "batch_idx: 16\n",
      "0.6151960784313726\n",
      "batch_idx: 17\n",
      "0.6134259259259259\n",
      "batch_idx: 18\n",
      "0.6118421052631579\n",
      "batch_idx: 19\n",
      "0.6208333333333333\n",
      "batch_idx: 20\n",
      "0.628968253968254\n",
      "batch_idx: 21\n",
      "0.625\n",
      "batch_idx: 22\n",
      "0.6286231884057971\n",
      "batch_idx: 23\n",
      "0.6197916666666666\n",
      "batch_idx: 24\n",
      "0.6166666666666667\n",
      "Training Epoch: 148, total loss: 46.484128\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.5138888888888888\n",
      "batch_idx: 3\n",
      "0.53125\n",
      "batch_idx: 4\n",
      "0.5666666666666667\n",
      "batch_idx: 5\n",
      "0.5763888888888888\n",
      "batch_idx: 6\n",
      "0.5952380952380952\n",
      "batch_idx: 7\n",
      "0.609375\n",
      "batch_idx: 8\n",
      "0.5972222222222222\n",
      "batch_idx: 9\n",
      "0.6125\n",
      "batch_idx: 10\n",
      "0.6174242424242424\n",
      "batch_idx: 11\n",
      "0.6111111111111112\n",
      "batch_idx: 12\n",
      "0.6153846153846154\n",
      "batch_idx: 13\n",
      "0.6101190476190477\n",
      "batch_idx: 14\n",
      "0.6194444444444445\n",
      "batch_idx: 15\n",
      "0.6328125\n",
      "batch_idx: 16\n",
      "0.6397058823529411\n",
      "batch_idx: 17\n",
      "0.6458333333333334\n",
      "batch_idx: 18\n",
      "0.6425438596491229\n",
      "batch_idx: 19\n",
      "0.6375\n",
      "batch_idx: 20\n",
      "0.6388888888888888\n",
      "batch_idx: 21\n",
      "0.6325757575757576\n",
      "batch_idx: 22\n",
      "0.6340579710144928\n",
      "batch_idx: 23\n",
      "0.6284722222222222\n",
      "batch_idx: 24\n",
      "0.625\n",
      "Training Epoch: 149, total loss: 46.124611\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.65625\n",
      "batch_idx: 4\n",
      "0.6333333333333333\n",
      "batch_idx: 5\n",
      "0.6388888888888888\n",
      "batch_idx: 6\n",
      "0.625\n",
      "batch_idx: 7\n",
      "0.6302083333333334\n",
      "batch_idx: 8\n",
      "0.6203703703703703\n",
      "batch_idx: 9\n",
      "0.6083333333333333\n",
      "batch_idx: 10\n",
      "0.6060606060606061\n",
      "batch_idx: 11\n",
      "0.6076388888888888\n",
      "batch_idx: 12\n",
      "0.6121794871794872\n",
      "batch_idx: 13\n",
      "0.6101190476190477\n",
      "batch_idx: 14\n",
      "0.6055555555555555\n",
      "batch_idx: 15\n",
      "0.6041666666666666\n",
      "batch_idx: 16\n",
      "0.5931372549019608\n",
      "batch_idx: 17\n",
      "0.5902777777777778\n",
      "batch_idx: 18\n",
      "0.5964912280701754\n",
      "batch_idx: 19\n",
      "0.59375\n",
      "batch_idx: 20\n",
      "0.5952380952380952\n",
      "batch_idx: 21\n",
      "0.5928030303030303\n",
      "batch_idx: 22\n",
      "0.592391304347826\n",
      "batch_idx: 23\n",
      "0.5902777777777778\n",
      "batch_idx: 24\n",
      "0.5916666666666667\n",
      "Training Epoch: 150, total loss: 46.908520\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.5416666666666666\n",
      "batch_idx: 2\n",
      "0.5694444444444444\n",
      "batch_idx: 3\n",
      "0.53125\n",
      "batch_idx: 4\n",
      "0.575\n",
      "batch_idx: 5\n",
      "0.5972222222222222\n",
      "batch_idx: 6\n",
      "0.625\n",
      "batch_idx: 7\n",
      "0.6302083333333334\n",
      "batch_idx: 8\n",
      "0.6435185185185185\n",
      "batch_idx: 9\n",
      "0.6416666666666667\n",
      "batch_idx: 10\n",
      "0.6325757575757576\n",
      "batch_idx: 11\n",
      "0.6388888888888888\n",
      "batch_idx: 12\n",
      "0.6474358974358975\n",
      "batch_idx: 13\n",
      "0.6398809523809523\n",
      "batch_idx: 14\n",
      "0.6277777777777778\n",
      "batch_idx: 15\n",
      "0.6328125\n",
      "batch_idx: 16\n",
      "0.625\n",
      "batch_idx: 17\n",
      "0.625\n",
      "batch_idx: 18\n",
      "0.6271929824561403\n",
      "batch_idx: 19\n",
      "0.6270833333333333\n",
      "batch_idx: 20\n",
      "0.623015873015873\n",
      "batch_idx: 21\n",
      "0.6306818181818182\n",
      "batch_idx: 22\n",
      "0.6268115942028986\n",
      "batch_idx: 23\n",
      "0.6232638888888888\n",
      "batch_idx: 24\n",
      "0.6233333333333333\n",
      "Training Epoch: 151, total loss: 46.366308\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 3\n",
      "0.6666666666666666\n",
      "batch_idx: 4\n",
      "0.6583333333333333\n",
      "batch_idx: 5\n",
      "0.6666666666666666\n",
      "batch_idx: 6\n",
      "0.6785714285714286\n",
      "batch_idx: 7\n",
      "0.6770833333333334\n",
      "batch_idx: 8\n",
      "0.6666666666666666\n",
      "batch_idx: 9\n",
      "0.6666666666666666\n",
      "batch_idx: 10\n",
      "0.6515151515151515\n",
      "batch_idx: 11\n",
      "0.6458333333333334\n",
      "batch_idx: 12\n",
      "0.6666666666666666\n",
      "batch_idx: 13\n",
      "0.6577380952380952\n",
      "batch_idx: 14\n",
      "0.6472222222222223\n",
      "batch_idx: 15\n",
      "0.6510416666666666\n",
      "batch_idx: 16\n",
      "0.6421568627450981\n",
      "batch_idx: 17\n",
      "0.6388888888888888\n",
      "batch_idx: 18\n",
      "0.6381578947368421\n",
      "batch_idx: 19\n",
      "0.6458333333333334\n",
      "batch_idx: 20\n",
      "0.6468253968253969\n",
      "batch_idx: 21\n",
      "0.6496212121212122\n",
      "batch_idx: 22\n",
      "0.6413043478260869\n",
      "batch_idx: 23\n",
      "0.6388888888888888\n",
      "batch_idx: 24\n",
      "0.6333333333333333\n",
      "Training Epoch: 152, total loss: 46.127376\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.6666666666666666\n",
      "batch_idx: 4\n",
      "0.6833333333333333\n",
      "batch_idx: 5\n",
      "0.6875\n",
      "batch_idx: 6\n",
      "0.6904761904761905\n",
      "batch_idx: 7\n",
      "0.6770833333333334\n",
      "batch_idx: 8\n",
      "0.6851851851851852\n",
      "batch_idx: 9\n",
      "0.7041666666666667\n",
      "batch_idx: 10\n",
      "0.6893939393939394\n",
      "batch_idx: 11\n",
      "0.6875\n",
      "batch_idx: 12\n",
      "0.6891025641025641\n",
      "batch_idx: 13\n",
      "0.6934523809523809\n",
      "batch_idx: 14\n",
      "0.6888888888888889\n",
      "batch_idx: 15\n",
      "0.6796875\n",
      "batch_idx: 16\n",
      "0.6887254901960784\n",
      "batch_idx: 17\n",
      "0.6875\n",
      "batch_idx: 18\n",
      "0.6929824561403509\n",
      "batch_idx: 19\n",
      "0.6895833333333333\n",
      "batch_idx: 20\n",
      "0.6884920634920635\n",
      "batch_idx: 21\n",
      "0.6875\n",
      "batch_idx: 22\n",
      "0.6829710144927537\n",
      "batch_idx: 23\n",
      "0.6805555555555556\n",
      "batch_idx: 24\n",
      "0.6766666666666666\n",
      "Training Epoch: 153, total loss: 45.327801\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.65625\n",
      "batch_idx: 4\n",
      "0.6833333333333333\n",
      "batch_idx: 5\n",
      "0.6805555555555556\n",
      "batch_idx: 6\n",
      "0.6607142857142857\n",
      "batch_idx: 7\n",
      "0.65625\n",
      "batch_idx: 8\n",
      "0.6574074074074074\n",
      "batch_idx: 9\n",
      "0.6541666666666667\n",
      "batch_idx: 10\n",
      "0.6590909090909091\n",
      "batch_idx: 11\n",
      "0.6458333333333334\n",
      "batch_idx: 12\n",
      "0.6314102564102564\n",
      "batch_idx: 13\n",
      "0.6220238095238095\n",
      "batch_idx: 14\n",
      "0.625\n",
      "batch_idx: 15\n",
      "0.6197916666666666\n",
      "batch_idx: 16\n",
      "0.6200980392156863\n",
      "batch_idx: 17\n",
      "0.6342592592592593\n",
      "batch_idx: 18\n",
      "0.631578947368421\n",
      "batch_idx: 19\n",
      "0.6291666666666667\n",
      "batch_idx: 20\n",
      "0.626984126984127\n",
      "batch_idx: 21\n",
      "0.634469696969697\n",
      "batch_idx: 22\n",
      "0.6322463768115942\n",
      "batch_idx: 23\n",
      "0.6319444444444444\n",
      "batch_idx: 24\n",
      "0.6316666666666667\n",
      "Training Epoch: 154, total loss: 46.079213\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.5833333333333334\n",
      "batch_idx: 3\n",
      "0.5208333333333334\n",
      "batch_idx: 4\n",
      "0.5583333333333333\n",
      "batch_idx: 5\n",
      "0.5347222222222222\n",
      "batch_idx: 6\n",
      "0.5535714285714286\n",
      "batch_idx: 7\n",
      "0.5572916666666666\n",
      "batch_idx: 8\n",
      "0.5648148148148148\n",
      "batch_idx: 9\n",
      "0.5708333333333333\n",
      "batch_idx: 10\n",
      "0.5757575757575758\n",
      "batch_idx: 11\n",
      "0.5798611111111112\n",
      "batch_idx: 12\n",
      "0.5801282051282052\n",
      "batch_idx: 13\n",
      "0.5744047619047619\n",
      "batch_idx: 14\n",
      "0.575\n",
      "batch_idx: 15\n",
      "0.578125\n",
      "batch_idx: 16\n",
      "0.5808823529411765\n",
      "batch_idx: 17\n",
      "0.5833333333333334\n",
      "batch_idx: 18\n",
      "0.5833333333333334\n",
      "batch_idx: 19\n",
      "0.5833333333333334\n",
      "batch_idx: 20\n",
      "0.5833333333333334\n",
      "batch_idx: 21\n",
      "0.5890151515151515\n",
      "batch_idx: 22\n",
      "0.5905797101449275\n",
      "batch_idx: 23\n",
      "0.6006944444444444\n",
      "batch_idx: 24\n",
      "0.5983333333333334\n",
      "Training Epoch: 155, total loss: 46.798817\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.5416666666666666\n",
      "batch_idx: 2\n",
      "0.5555555555555556\n",
      "batch_idx: 3\n",
      "0.5833333333333334\n",
      "batch_idx: 4\n",
      "0.625\n",
      "batch_idx: 5\n",
      "0.6458333333333334\n",
      "batch_idx: 6\n",
      "0.6428571428571429\n",
      "batch_idx: 7\n",
      "0.6458333333333334\n",
      "batch_idx: 8\n",
      "0.6527777777777778\n",
      "batch_idx: 9\n",
      "0.6458333333333334\n",
      "batch_idx: 10\n",
      "0.6477272727272727\n",
      "batch_idx: 11\n",
      "0.6597222222222222\n",
      "batch_idx: 12\n",
      "0.6602564102564102\n",
      "batch_idx: 13\n",
      "0.6577380952380952\n",
      "batch_idx: 14\n",
      "0.6527777777777778\n",
      "batch_idx: 15\n",
      "0.6432291666666666\n",
      "batch_idx: 16\n",
      "0.6446078431372549\n",
      "batch_idx: 17\n",
      "0.6365740740740741\n",
      "batch_idx: 18\n",
      "0.6447368421052632\n",
      "batch_idx: 19\n",
      "0.6416666666666667\n",
      "batch_idx: 20\n",
      "0.6448412698412699\n",
      "batch_idx: 21\n",
      "0.6420454545454546\n",
      "batch_idx: 22\n",
      "0.6358695652173914\n",
      "batch_idx: 23\n",
      "0.6388888888888888\n",
      "batch_idx: 24\n",
      "0.6416666666666667\n",
      "Training Epoch: 156, total loss: 45.941918\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.5694444444444444\n",
      "batch_idx: 3\n",
      "0.5729166666666666\n",
      "batch_idx: 4\n",
      "0.6\n",
      "batch_idx: 5\n",
      "0.6111111111111112\n",
      "batch_idx: 6\n",
      "0.6011904761904762\n",
      "batch_idx: 7\n",
      "0.5885416666666666\n",
      "batch_idx: 8\n",
      "0.5972222222222222\n",
      "batch_idx: 9\n",
      "0.5875\n",
      "batch_idx: 10\n",
      "0.5984848484848485\n",
      "batch_idx: 11\n",
      "0.59375\n",
      "batch_idx: 12\n",
      "0.592948717948718\n",
      "batch_idx: 13\n",
      "0.5833333333333334\n",
      "batch_idx: 14\n",
      "0.5861111111111111\n",
      "batch_idx: 15\n",
      "0.5859375\n",
      "batch_idx: 16\n",
      "0.5906862745098039\n",
      "batch_idx: 17\n",
      "0.5972222222222222\n",
      "batch_idx: 18\n",
      "0.5964912280701754\n",
      "batch_idx: 19\n",
      "0.5979166666666667\n",
      "batch_idx: 20\n",
      "0.6011904761904762\n",
      "batch_idx: 21\n",
      "0.6022727272727273\n",
      "batch_idx: 22\n",
      "0.6105072463768116\n",
      "batch_idx: 23\n",
      "0.6145833333333334\n",
      "batch_idx: 24\n",
      "0.6066666666666667\n",
      "Training Epoch: 157, total loss: 46.448885\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.6666666666666666\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6736111111111112\n",
      "batch_idx: 6\n",
      "0.6666666666666666\n",
      "batch_idx: 7\n",
      "0.6614583333333334\n",
      "batch_idx: 8\n",
      "0.6574074074074074\n",
      "batch_idx: 9\n",
      "0.6416666666666667\n",
      "batch_idx: 10\n",
      "0.6553030303030303\n",
      "batch_idx: 11\n",
      "0.6388888888888888\n",
      "batch_idx: 12\n",
      "0.6378205128205128\n",
      "batch_idx: 13\n",
      "0.6309523809523809\n",
      "batch_idx: 14\n",
      "0.6305555555555555\n",
      "batch_idx: 15\n",
      "0.6354166666666666\n",
      "batch_idx: 16\n",
      "0.6397058823529411\n",
      "batch_idx: 17\n",
      "0.6365740740740741\n",
      "batch_idx: 18\n",
      "0.6359649122807017\n",
      "batch_idx: 19\n",
      "0.6375\n",
      "batch_idx: 20\n",
      "0.6369047619047619\n",
      "batch_idx: 21\n",
      "0.634469696969697\n",
      "batch_idx: 22\n",
      "0.6376811594202898\n",
      "batch_idx: 23\n",
      "0.6302083333333334\n",
      "batch_idx: 24\n",
      "0.6266666666666667\n",
      "Training Epoch: 158, total loss: 46.253508\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.5972222222222222\n",
      "batch_idx: 3\n",
      "0.6145833333333334\n",
      "batch_idx: 4\n",
      "0.5833333333333334\n",
      "batch_idx: 5\n",
      "0.5972222222222222\n",
      "batch_idx: 6\n",
      "0.5952380952380952\n",
      "batch_idx: 7\n",
      "0.59375\n",
      "batch_idx: 8\n",
      "0.6064814814814815\n",
      "batch_idx: 9\n",
      "0.6166666666666667\n",
      "batch_idx: 10\n",
      "0.6060606060606061\n",
      "batch_idx: 11\n",
      "0.5972222222222222\n",
      "batch_idx: 12\n",
      "0.5993589743589743\n",
      "batch_idx: 13\n",
      "0.5982142857142857\n",
      "batch_idx: 14\n",
      "0.5944444444444444\n",
      "batch_idx: 15\n",
      "0.6015625\n",
      "batch_idx: 16\n",
      "0.6004901960784313\n",
      "batch_idx: 17\n",
      "0.5902777777777778\n",
      "batch_idx: 18\n",
      "0.5964912280701754\n",
      "batch_idx: 19\n",
      "0.5958333333333333\n",
      "batch_idx: 20\n",
      "0.5992063492063492\n",
      "batch_idx: 21\n",
      "0.6003787878787878\n",
      "batch_idx: 22\n",
      "0.5942028985507246\n",
      "batch_idx: 23\n",
      "0.6006944444444444\n",
      "batch_idx: 24\n",
      "0.6016666666666667\n",
      "Training Epoch: 159, total loss: 46.652204\n",
      "batch_idx: 0\n",
      "0.4166666666666667\n",
      "batch_idx: 1\n",
      "0.4583333333333333\n",
      "batch_idx: 2\n",
      "0.4444444444444444\n",
      "batch_idx: 3\n",
      "0.4895833333333333\n",
      "batch_idx: 4\n",
      "0.475\n",
      "batch_idx: 5\n",
      "0.4583333333333333\n",
      "batch_idx: 6\n",
      "0.5059523809523809\n",
      "batch_idx: 7\n",
      "0.53125\n",
      "batch_idx: 8\n",
      "0.5462962962962963\n",
      "batch_idx: 9\n",
      "0.575\n",
      "batch_idx: 10\n",
      "0.5643939393939394\n",
      "batch_idx: 11\n",
      "0.5798611111111112\n",
      "batch_idx: 12\n",
      "0.5897435897435898\n",
      "batch_idx: 13\n",
      "0.6041666666666666\n",
      "batch_idx: 14\n",
      "0.6194444444444445\n",
      "batch_idx: 15\n",
      "0.6328125\n",
      "batch_idx: 16\n",
      "0.6274509803921569\n",
      "batch_idx: 17\n",
      "0.6342592592592593\n",
      "batch_idx: 18\n",
      "0.6359649122807017\n",
      "batch_idx: 19\n",
      "0.6333333333333333\n",
      "batch_idx: 20\n",
      "0.626984126984127\n",
      "batch_idx: 21\n",
      "0.6231060606060606\n",
      "batch_idx: 22\n",
      "0.6231884057971014\n",
      "batch_idx: 23\n",
      "0.6180555555555556\n",
      "batch_idx: 24\n",
      "0.6183333333333333\n",
      "Training Epoch: 160, total loss: 46.397834\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.5416666666666666\n",
      "batch_idx: 2\n",
      "0.5277777777777778\n",
      "batch_idx: 3\n",
      "0.5416666666666666\n",
      "batch_idx: 4\n",
      "0.5666666666666667\n",
      "batch_idx: 5\n",
      "0.5486111111111112\n",
      "batch_idx: 6\n",
      "0.5714285714285714\n",
      "batch_idx: 7\n",
      "0.59375\n",
      "batch_idx: 8\n",
      "0.6064814814814815\n",
      "batch_idx: 9\n",
      "0.6083333333333333\n",
      "batch_idx: 10\n",
      "0.6060606060606061\n",
      "batch_idx: 11\n",
      "0.6006944444444444\n",
      "batch_idx: 12\n",
      "0.6089743589743589\n",
      "batch_idx: 13\n",
      "0.6130952380952381\n",
      "batch_idx: 14\n",
      "0.6055555555555555\n",
      "batch_idx: 15\n",
      "0.6015625\n",
      "batch_idx: 16\n",
      "0.5955882352941176\n",
      "batch_idx: 17\n",
      "0.6018518518518519\n",
      "batch_idx: 18\n",
      "0.6008771929824561\n",
      "batch_idx: 19\n",
      "0.6125\n",
      "batch_idx: 20\n",
      "0.6051587301587301\n",
      "batch_idx: 21\n",
      "0.6117424242424242\n",
      "batch_idx: 22\n",
      "0.6141304347826086\n",
      "batch_idx: 23\n",
      "0.6145833333333334\n",
      "batch_idx: 24\n",
      "0.6133333333333333\n",
      "Training Epoch: 161, total loss: 46.449836\n",
      "batch_idx: 0\n",
      "0.4583333333333333\n",
      "batch_idx: 1\n",
      "0.4791666666666667\n",
      "batch_idx: 2\n",
      "0.4861111111111111\n",
      "batch_idx: 3\n",
      "0.5104166666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 4\n",
      "0.575\n",
      "batch_idx: 5\n",
      "0.6111111111111112\n",
      "batch_idx: 6\n",
      "0.5892857142857143\n",
      "batch_idx: 7\n",
      "0.6145833333333334\n",
      "batch_idx: 8\n",
      "0.6203703703703703\n",
      "batch_idx: 9\n",
      "0.6125\n",
      "batch_idx: 10\n",
      "0.6325757575757576\n",
      "batch_idx: 11\n",
      "0.6145833333333334\n",
      "batch_idx: 12\n",
      "0.6282051282051282\n",
      "batch_idx: 13\n",
      "0.625\n",
      "batch_idx: 14\n",
      "0.6194444444444445\n",
      "batch_idx: 15\n",
      "0.6197916666666666\n",
      "batch_idx: 16\n",
      "0.6299019607843137\n",
      "batch_idx: 17\n",
      "0.6296296296296297\n",
      "batch_idx: 18\n",
      "0.6271929824561403\n",
      "batch_idx: 19\n",
      "0.625\n",
      "batch_idx: 20\n",
      "0.6309523809523809\n",
      "batch_idx: 21\n",
      "0.6325757575757576\n",
      "batch_idx: 22\n",
      "0.6376811594202898\n",
      "batch_idx: 23\n",
      "0.6354166666666666\n",
      "batch_idx: 24\n",
      "0.635\n",
      "Training Epoch: 162, total loss: 45.954871\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.6111111111111112\n",
      "batch_idx: 3\n",
      "0.625\n",
      "batch_idx: 4\n",
      "0.6166666666666667\n",
      "batch_idx: 5\n",
      "0.6527777777777778\n",
      "batch_idx: 6\n",
      "0.6428571428571429\n",
      "batch_idx: 7\n",
      "0.6354166666666666\n",
      "batch_idx: 8\n",
      "0.6203703703703703\n",
      "batch_idx: 9\n",
      "0.6041666666666666\n",
      "batch_idx: 10\n",
      "0.6174242424242424\n",
      "batch_idx: 11\n",
      "0.625\n",
      "batch_idx: 12\n",
      "0.6089743589743589\n",
      "batch_idx: 13\n",
      "0.6160714285714286\n",
      "batch_idx: 14\n",
      "0.6138888888888889\n",
      "batch_idx: 15\n",
      "0.6145833333333334\n",
      "batch_idx: 16\n",
      "0.6200980392156863\n",
      "batch_idx: 17\n",
      "0.6273148148148148\n",
      "batch_idx: 18\n",
      "0.625\n",
      "batch_idx: 19\n",
      "0.6229166666666667\n",
      "batch_idx: 20\n",
      "0.6309523809523809\n",
      "batch_idx: 21\n",
      "0.625\n",
      "batch_idx: 22\n",
      "0.6304347826086957\n",
      "batch_idx: 23\n",
      "0.6267361111111112\n",
      "batch_idx: 24\n",
      "0.6233333333333333\n",
      "Training Epoch: 163, total loss: 46.280899\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.625\n",
      "batch_idx: 4\n",
      "0.6583333333333333\n",
      "batch_idx: 5\n",
      "0.6458333333333334\n",
      "batch_idx: 6\n",
      "0.6666666666666666\n",
      "batch_idx: 7\n",
      "0.6510416666666666\n",
      "batch_idx: 8\n",
      "0.6435185185185185\n",
      "batch_idx: 9\n",
      "0.6458333333333334\n",
      "batch_idx: 10\n",
      "0.6590909090909091\n",
      "batch_idx: 11\n",
      "0.6597222222222222\n",
      "batch_idx: 12\n",
      "0.6506410256410257\n",
      "batch_idx: 13\n",
      "0.6488095238095238\n",
      "batch_idx: 14\n",
      "0.6444444444444445\n",
      "batch_idx: 15\n",
      "0.6432291666666666\n",
      "batch_idx: 16\n",
      "0.6372549019607843\n",
      "batch_idx: 17\n",
      "0.6388888888888888\n",
      "batch_idx: 18\n",
      "0.6447368421052632\n",
      "batch_idx: 19\n",
      "0.63125\n",
      "batch_idx: 20\n",
      "0.6329365079365079\n",
      "batch_idx: 21\n",
      "0.6382575757575758\n",
      "batch_idx: 22\n",
      "0.6358695652173914\n",
      "batch_idx: 23\n",
      "0.6388888888888888\n",
      "batch_idx: 24\n",
      "0.635\n",
      "Training Epoch: 164, total loss: 46.003747\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.5416666666666666\n",
      "batch_idx: 2\n",
      "0.5972222222222222\n",
      "batch_idx: 3\n",
      "0.5520833333333334\n",
      "batch_idx: 4\n",
      "0.5583333333333333\n",
      "batch_idx: 5\n",
      "0.5972222222222222\n",
      "batch_idx: 6\n",
      "0.625\n",
      "batch_idx: 7\n",
      "0.6458333333333334\n",
      "batch_idx: 8\n",
      "0.6481481481481481\n",
      "batch_idx: 9\n",
      "0.6208333333333333\n",
      "batch_idx: 10\n",
      "0.6401515151515151\n",
      "batch_idx: 11\n",
      "0.6284722222222222\n",
      "batch_idx: 12\n",
      "0.625\n",
      "batch_idx: 13\n",
      "0.6279761904761905\n",
      "batch_idx: 14\n",
      "0.6333333333333333\n",
      "batch_idx: 15\n",
      "0.6354166666666666\n",
      "batch_idx: 16\n",
      "0.6397058823529411\n",
      "batch_idx: 17\n",
      "0.6412037037037037\n",
      "batch_idx: 18\n",
      "0.6447368421052632\n",
      "batch_idx: 19\n",
      "0.64375\n",
      "batch_idx: 20\n",
      "0.6428571428571429\n",
      "batch_idx: 21\n",
      "0.634469696969697\n",
      "batch_idx: 22\n",
      "0.6340579710144928\n",
      "batch_idx: 23\n",
      "0.6302083333333334\n",
      "batch_idx: 24\n",
      "0.63\n",
      "Training Epoch: 165, total loss: 46.193905\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.5416666666666666\n",
      "batch_idx: 2\n",
      "0.5833333333333334\n",
      "batch_idx: 3\n",
      "0.5729166666666666\n",
      "batch_idx: 4\n",
      "0.575\n",
      "batch_idx: 5\n",
      "0.5902777777777778\n",
      "batch_idx: 6\n",
      "0.6071428571428571\n",
      "batch_idx: 7\n",
      "0.6145833333333334\n",
      "batch_idx: 8\n",
      "0.6157407407407407\n",
      "batch_idx: 9\n",
      "0.6125\n",
      "batch_idx: 10\n",
      "0.6098484848484849\n",
      "batch_idx: 11\n",
      "0.5972222222222222\n",
      "batch_idx: 12\n",
      "0.5961538461538461\n",
      "batch_idx: 13\n",
      "0.6071428571428571\n",
      "batch_idx: 14\n",
      "0.6138888888888889\n",
      "batch_idx: 15\n",
      "0.6197916666666666\n",
      "batch_idx: 16\n",
      "0.6225490196078431\n",
      "batch_idx: 17\n",
      "0.6296296296296297\n",
      "batch_idx: 18\n",
      "0.6228070175438597\n",
      "batch_idx: 19\n",
      "0.6229166666666667\n",
      "batch_idx: 20\n",
      "0.626984126984127\n",
      "batch_idx: 21\n",
      "0.6193181818181818\n",
      "batch_idx: 22\n",
      "0.6195652173913043\n",
      "batch_idx: 23\n",
      "0.6111111111111112\n",
      "batch_idx: 24\n",
      "0.6133333333333333\n",
      "Training Epoch: 166, total loss: 46.441602\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.6770833333333334\n",
      "batch_idx: 4\n",
      "0.6833333333333333\n",
      "batch_idx: 5\n",
      "0.6805555555555556\n",
      "batch_idx: 6\n",
      "0.6666666666666666\n",
      "batch_idx: 7\n",
      "0.65625\n",
      "batch_idx: 8\n",
      "0.6481481481481481\n",
      "batch_idx: 9\n",
      "0.6375\n",
      "batch_idx: 10\n",
      "0.6439393939393939\n",
      "batch_idx: 11\n",
      "0.6493055555555556\n",
      "batch_idx: 12\n",
      "0.6474358974358975\n",
      "batch_idx: 13\n",
      "0.6458333333333334\n",
      "batch_idx: 14\n",
      "0.6388888888888888\n",
      "batch_idx: 15\n",
      "0.640625\n",
      "batch_idx: 16\n",
      "0.6397058823529411\n",
      "batch_idx: 17\n",
      "0.6342592592592593\n",
      "batch_idx: 18\n",
      "0.6359649122807017\n",
      "batch_idx: 19\n",
      "0.6375\n",
      "batch_idx: 20\n",
      "0.6309523809523809\n",
      "batch_idx: 21\n",
      "0.6268939393939394\n",
      "batch_idx: 22\n",
      "0.6213768115942029\n",
      "batch_idx: 23\n",
      "0.6163194444444444\n",
      "batch_idx: 24\n",
      "0.6133333333333333\n",
      "Training Epoch: 167, total loss: 46.407102\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.6041666666666666\n",
      "batch_idx: 4\n",
      "0.5833333333333334\n",
      "batch_idx: 5\n",
      "0.6111111111111112\n",
      "batch_idx: 6\n",
      "0.625\n",
      "batch_idx: 7\n",
      "0.6197916666666666\n",
      "batch_idx: 8\n",
      "0.5925925925925926\n",
      "batch_idx: 9\n",
      "0.6\n",
      "batch_idx: 10\n",
      "0.6060606060606061\n",
      "batch_idx: 11\n",
      "0.6076388888888888\n",
      "batch_idx: 12\n",
      "0.6057692307692307\n",
      "batch_idx: 13\n",
      "0.6190476190476191\n",
      "batch_idx: 14\n",
      "0.6055555555555555\n",
      "batch_idx: 15\n",
      "0.6067708333333334\n",
      "batch_idx: 16\n",
      "0.6078431372549019\n",
      "batch_idx: 17\n",
      "0.6018518518518519\n",
      "batch_idx: 18\n",
      "0.6052631578947368\n",
      "batch_idx: 19\n",
      "0.6041666666666666\n",
      "batch_idx: 20\n",
      "0.6091269841269841\n",
      "batch_idx: 21\n",
      "0.6060606060606061\n",
      "batch_idx: 22\n",
      "0.6014492753623188\n",
      "batch_idx: 23\n",
      "0.5972222222222222\n",
      "batch_idx: 24\n",
      "0.6016666666666667\n",
      "Training Epoch: 168, total loss: 46.633644\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6666666666666666\n",
      "batch_idx: 6\n",
      "0.6666666666666666\n",
      "batch_idx: 7\n",
      "0.6614583333333334\n",
      "batch_idx: 8\n",
      "0.6388888888888888\n",
      "batch_idx: 9\n",
      "0.65\n",
      "batch_idx: 10\n",
      "0.6477272727272727\n",
      "batch_idx: 11\n",
      "0.6388888888888888\n",
      "batch_idx: 12\n",
      "0.6346153846153846\n",
      "batch_idx: 13\n",
      "0.6309523809523809\n",
      "batch_idx: 14\n",
      "0.6277777777777778\n",
      "batch_idx: 15\n",
      "0.6223958333333334\n",
      "batch_idx: 16\n",
      "0.6348039215686274\n",
      "batch_idx: 17\n",
      "0.6412037037037037\n",
      "batch_idx: 18\n",
      "0.6447368421052632\n",
      "batch_idx: 19\n",
      "0.6416666666666667\n",
      "batch_idx: 20\n",
      "0.6428571428571429\n",
      "batch_idx: 21\n",
      "0.6420454545454546\n",
      "batch_idx: 22\n",
      "0.6413043478260869\n",
      "batch_idx: 23\n",
      "0.6423611111111112\n",
      "batch_idx: 24\n",
      "0.635\n",
      "Training Epoch: 169, total loss: 46.082030\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7\n",
      "batch_idx: 5\n",
      "0.7013888888888888\n",
      "batch_idx: 6\n",
      "0.7142857142857143\n",
      "batch_idx: 7\n",
      "0.7083333333333334\n",
      "batch_idx: 8\n",
      "0.7083333333333334\n",
      "batch_idx: 9\n",
      "0.7083333333333334\n",
      "batch_idx: 10\n",
      "0.7045454545454546\n",
      "batch_idx: 11\n",
      "0.7013888888888888\n",
      "batch_idx: 12\n",
      "0.6923076923076923\n",
      "batch_idx: 13\n",
      "0.6904761904761905\n",
      "batch_idx: 14\n",
      "0.675\n",
      "batch_idx: 15\n",
      "0.6744791666666666\n",
      "batch_idx: 16\n",
      "0.6642156862745098\n",
      "batch_idx: 17\n",
      "0.6481481481481481\n",
      "batch_idx: 18\n",
      "0.6469298245614035\n",
      "batch_idx: 19\n",
      "0.6395833333333333\n",
      "batch_idx: 20\n",
      "0.6408730158730159\n",
      "batch_idx: 21\n",
      "0.6401515151515151\n",
      "batch_idx: 22\n",
      "0.6376811594202898\n",
      "batch_idx: 23\n",
      "0.6423611111111112\n",
      "batch_idx: 24\n",
      "0.6466666666666666\n",
      "Training Epoch: 170, total loss: 45.972435\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.5833333333333334\n",
      "batch_idx: 3\n",
      "0.6145833333333334\n",
      "batch_idx: 4\n",
      "0.6083333333333333\n",
      "batch_idx: 5\n",
      "0.6041666666666666\n",
      "batch_idx: 6\n",
      "0.6309523809523809\n",
      "batch_idx: 7\n",
      "0.6354166666666666\n",
      "batch_idx: 8\n",
      "0.6111111111111112\n",
      "batch_idx: 9\n",
      "0.6041666666666666\n",
      "batch_idx: 10\n",
      "0.5984848484848485\n",
      "batch_idx: 11\n",
      "0.6006944444444444\n",
      "batch_idx: 12\n",
      "0.6153846153846154\n",
      "batch_idx: 13\n",
      "0.6160714285714286\n",
      "batch_idx: 14\n",
      "0.6138888888888889\n",
      "batch_idx: 15\n",
      "0.6197916666666666\n",
      "batch_idx: 16\n",
      "0.6200980392156863\n",
      "batch_idx: 17\n",
      "0.6226851851851852\n",
      "batch_idx: 18\n",
      "0.618421052631579\n",
      "batch_idx: 19\n",
      "0.6145833333333334\n",
      "batch_idx: 20\n",
      "0.6091269841269841\n",
      "batch_idx: 21\n",
      "0.6098484848484849\n",
      "batch_idx: 22\n",
      "0.6068840579710145\n",
      "batch_idx: 23\n",
      "0.6076388888888888\n",
      "batch_idx: 24\n",
      "0.605\n",
      "Training Epoch: 171, total loss: 46.357242\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.7\n",
      "batch_idx: 5\n",
      "0.6527777777777778\n",
      "batch_idx: 6\n",
      "0.6428571428571429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 7\n",
      "0.625\n",
      "batch_idx: 8\n",
      "0.6296296296296297\n",
      "batch_idx: 9\n",
      "0.6416666666666667\n",
      "batch_idx: 10\n",
      "0.6439393939393939\n",
      "batch_idx: 11\n",
      "0.6319444444444444\n",
      "batch_idx: 12\n",
      "0.6314102564102564\n",
      "batch_idx: 13\n",
      "0.6309523809523809\n",
      "batch_idx: 14\n",
      "0.625\n",
      "batch_idx: 15\n",
      "0.6197916666666666\n",
      "batch_idx: 16\n",
      "0.6274509803921569\n",
      "batch_idx: 17\n",
      "0.625\n",
      "batch_idx: 18\n",
      "0.625\n",
      "batch_idx: 19\n",
      "0.61875\n",
      "batch_idx: 20\n",
      "0.6150793650793651\n",
      "batch_idx: 21\n",
      "0.6079545454545454\n",
      "batch_idx: 22\n",
      "0.6014492753623188\n",
      "batch_idx: 23\n",
      "0.6041666666666666\n",
      "batch_idx: 24\n",
      "0.605\n",
      "Training Epoch: 172, total loss: 46.738073\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.5972222222222222\n",
      "batch_idx: 3\n",
      "0.6145833333333334\n",
      "batch_idx: 4\n",
      "0.65\n",
      "batch_idx: 5\n",
      "0.6319444444444444\n",
      "batch_idx: 6\n",
      "0.6369047619047619\n",
      "batch_idx: 7\n",
      "0.6302083333333334\n",
      "batch_idx: 8\n",
      "0.6388888888888888\n",
      "batch_idx: 9\n",
      "0.6458333333333334\n",
      "batch_idx: 10\n",
      "0.6325757575757576\n",
      "batch_idx: 11\n",
      "0.6319444444444444\n",
      "batch_idx: 12\n",
      "0.625\n",
      "batch_idx: 13\n",
      "0.625\n",
      "batch_idx: 14\n",
      "0.6222222222222222\n",
      "batch_idx: 15\n",
      "0.6223958333333334\n",
      "batch_idx: 16\n",
      "0.6200980392156863\n",
      "batch_idx: 17\n",
      "0.6273148148148148\n",
      "batch_idx: 18\n",
      "0.6293859649122807\n",
      "batch_idx: 19\n",
      "0.63125\n",
      "batch_idx: 20\n",
      "0.6369047619047619\n",
      "batch_idx: 21\n",
      "0.6325757575757576\n",
      "batch_idx: 22\n",
      "0.6304347826086957\n",
      "batch_idx: 23\n",
      "0.625\n",
      "batch_idx: 24\n",
      "0.625\n",
      "Training Epoch: 173, total loss: 46.290001\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.6770833333333334\n",
      "batch_idx: 4\n",
      "0.65\n",
      "batch_idx: 5\n",
      "0.6041666666666666\n",
      "batch_idx: 6\n",
      "0.6071428571428571\n",
      "batch_idx: 7\n",
      "0.6197916666666666\n",
      "batch_idx: 8\n",
      "0.6157407407407407\n",
      "batch_idx: 9\n",
      "0.6166666666666667\n",
      "batch_idx: 10\n",
      "0.6060606060606061\n",
      "batch_idx: 11\n",
      "0.6041666666666666\n",
      "batch_idx: 12\n",
      "0.6121794871794872\n",
      "batch_idx: 13\n",
      "0.6101190476190477\n",
      "batch_idx: 14\n",
      "0.625\n",
      "batch_idx: 15\n",
      "0.6223958333333334\n",
      "batch_idx: 16\n",
      "0.6274509803921569\n",
      "batch_idx: 17\n",
      "0.6226851851851852\n",
      "batch_idx: 18\n",
      "0.6206140350877193\n",
      "batch_idx: 19\n",
      "0.6229166666666667\n",
      "batch_idx: 20\n",
      "0.623015873015873\n",
      "batch_idx: 21\n",
      "0.625\n",
      "batch_idx: 22\n",
      "0.6286231884057971\n",
      "batch_idx: 23\n",
      "0.6336805555555556\n",
      "batch_idx: 24\n",
      "0.6266666666666667\n",
      "Training Epoch: 174, total loss: 46.222023\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.6875\n",
      "batch_idx: 6\n",
      "0.7142857142857143\n",
      "batch_idx: 7\n",
      "0.6875\n",
      "batch_idx: 8\n",
      "0.6805555555555556\n",
      "batch_idx: 9\n",
      "0.6875\n",
      "batch_idx: 10\n",
      "0.6818181818181818\n",
      "batch_idx: 11\n",
      "0.6840277777777778\n",
      "batch_idx: 12\n",
      "0.6730769230769231\n",
      "batch_idx: 13\n",
      "0.6488095238095238\n",
      "batch_idx: 14\n",
      "0.65\n",
      "batch_idx: 15\n",
      "0.6458333333333334\n",
      "batch_idx: 16\n",
      "0.6446078431372549\n",
      "batch_idx: 17\n",
      "0.6365740740740741\n",
      "batch_idx: 18\n",
      "0.6293859649122807\n",
      "batch_idx: 19\n",
      "0.6291666666666667\n",
      "batch_idx: 20\n",
      "0.6369047619047619\n",
      "batch_idx: 21\n",
      "0.6325757575757576\n",
      "batch_idx: 22\n",
      "0.6340579710144928\n",
      "batch_idx: 23\n",
      "0.6354166666666666\n",
      "batch_idx: 24\n",
      "0.6383333333333333\n",
      "Training Epoch: 175, total loss: 46.050307\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.5625\n",
      "batch_idx: 2\n",
      "0.5277777777777778\n",
      "batch_idx: 3\n",
      "0.5625\n",
      "batch_idx: 4\n",
      "0.5833333333333334\n",
      "batch_idx: 5\n",
      "0.5763888888888888\n",
      "batch_idx: 6\n",
      "0.5952380952380952\n",
      "batch_idx: 7\n",
      "0.5989583333333334\n",
      "batch_idx: 8\n",
      "0.5879629629629629\n",
      "batch_idx: 9\n",
      "0.5875\n",
      "batch_idx: 10\n",
      "0.5984848484848485\n",
      "batch_idx: 11\n",
      "0.6111111111111112\n",
      "batch_idx: 12\n",
      "0.625\n",
      "batch_idx: 13\n",
      "0.6279761904761905\n",
      "batch_idx: 14\n",
      "0.6361111111111111\n",
      "batch_idx: 15\n",
      "0.6380208333333334\n",
      "batch_idx: 16\n",
      "0.6348039215686274\n",
      "batch_idx: 17\n",
      "0.6273148148148148\n",
      "batch_idx: 18\n",
      "0.6228070175438597\n",
      "batch_idx: 19\n",
      "0.63125\n",
      "batch_idx: 20\n",
      "0.628968253968254\n",
      "batch_idx: 21\n",
      "0.6268939393939394\n",
      "batch_idx: 22\n",
      "0.6286231884057971\n",
      "batch_idx: 23\n",
      "0.625\n",
      "batch_idx: 24\n",
      "0.6283333333333333\n",
      "Training Epoch: 176, total loss: 46.269688\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.5972222222222222\n",
      "batch_idx: 3\n",
      "0.5625\n",
      "batch_idx: 4\n",
      "0.55\n",
      "batch_idx: 5\n",
      "0.5763888888888888\n",
      "batch_idx: 6\n",
      "0.5892857142857143\n",
      "batch_idx: 7\n",
      "0.5885416666666666\n",
      "batch_idx: 8\n",
      "0.6018518518518519\n",
      "batch_idx: 9\n",
      "0.6041666666666666\n",
      "batch_idx: 10\n",
      "0.6022727272727273\n",
      "batch_idx: 11\n",
      "0.5972222222222222\n",
      "batch_idx: 12\n",
      "0.5897435897435898\n",
      "batch_idx: 13\n",
      "0.5952380952380952\n",
      "batch_idx: 14\n",
      "0.6\n",
      "batch_idx: 15\n",
      "0.5963541666666666\n",
      "batch_idx: 16\n",
      "0.6004901960784313\n",
      "batch_idx: 17\n",
      "0.6018518518518519\n",
      "batch_idx: 18\n",
      "0.5986842105263158\n",
      "batch_idx: 19\n",
      "0.6\n",
      "batch_idx: 20\n",
      "0.5992063492063492\n",
      "batch_idx: 21\n",
      "0.6041666666666666\n",
      "batch_idx: 22\n",
      "0.5996376811594203\n",
      "batch_idx: 23\n",
      "0.5989583333333334\n",
      "batch_idx: 24\n",
      "0.5966666666666667\n",
      "Training Epoch: 177, total loss: 46.722914\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.5833333333333334\n",
      "batch_idx: 3\n",
      "0.5833333333333334\n",
      "batch_idx: 4\n",
      "0.6333333333333333\n",
      "batch_idx: 5\n",
      "0.6458333333333334\n",
      "batch_idx: 6\n",
      "0.6309523809523809\n",
      "batch_idx: 7\n",
      "0.625\n",
      "batch_idx: 8\n",
      "0.6296296296296297\n",
      "batch_idx: 9\n",
      "0.6083333333333333\n",
      "batch_idx: 10\n",
      "0.5946969696969697\n",
      "batch_idx: 11\n",
      "0.6041666666666666\n",
      "batch_idx: 12\n",
      "0.6025641025641025\n",
      "batch_idx: 13\n",
      "0.6011904761904762\n",
      "batch_idx: 14\n",
      "0.6027777777777777\n",
      "batch_idx: 15\n",
      "0.5963541666666666\n",
      "batch_idx: 16\n",
      "0.5882352941176471\n",
      "batch_idx: 17\n",
      "0.5787037037037037\n",
      "batch_idx: 18\n",
      "0.5833333333333334\n",
      "batch_idx: 19\n",
      "0.5791666666666667\n",
      "batch_idx: 20\n",
      "0.5793650793650794\n",
      "batch_idx: 21\n",
      "0.5795454545454546\n",
      "batch_idx: 22\n",
      "0.5815217391304348\n",
      "batch_idx: 23\n",
      "0.5850694444444444\n",
      "batch_idx: 24\n",
      "0.5883333333333334\n",
      "Training Epoch: 178, total loss: 46.953229\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.4583333333333333\n",
      "batch_idx: 2\n",
      "0.5555555555555556\n",
      "batch_idx: 3\n",
      "0.5833333333333334\n",
      "batch_idx: 4\n",
      "0.5666666666666667\n",
      "batch_idx: 5\n",
      "0.5763888888888888\n",
      "batch_idx: 6\n",
      "0.5773809523809523\n",
      "batch_idx: 7\n",
      "0.59375\n",
      "batch_idx: 8\n",
      "0.6018518518518519\n",
      "batch_idx: 9\n",
      "0.6208333333333333\n",
      "batch_idx: 10\n",
      "0.6287878787878788\n",
      "batch_idx: 11\n",
      "0.6284722222222222\n",
      "batch_idx: 12\n",
      "0.6346153846153846\n",
      "batch_idx: 13\n",
      "0.6309523809523809\n",
      "batch_idx: 14\n",
      "0.6305555555555555\n",
      "batch_idx: 15\n",
      "0.6328125\n",
      "batch_idx: 16\n",
      "0.6421568627450981\n",
      "batch_idx: 17\n",
      "0.6365740740740741\n",
      "batch_idx: 18\n",
      "0.6359649122807017\n",
      "batch_idx: 19\n",
      "0.63125\n",
      "batch_idx: 20\n",
      "0.6349206349206349\n",
      "batch_idx: 21\n",
      "0.6325757575757576\n",
      "batch_idx: 22\n",
      "0.6322463768115942\n",
      "batch_idx: 23\n",
      "0.6284722222222222\n",
      "batch_idx: 24\n",
      "0.63\n",
      "Training Epoch: 179, total loss: 46.121291\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.5416666666666666\n",
      "batch_idx: 2\n",
      "0.5972222222222222\n",
      "batch_idx: 3\n",
      "0.6145833333333334\n",
      "batch_idx: 4\n",
      "0.6083333333333333\n",
      "batch_idx: 5\n",
      "0.625\n",
      "batch_idx: 6\n",
      "0.6071428571428571\n",
      "batch_idx: 7\n",
      "0.6041666666666666\n",
      "batch_idx: 8\n",
      "0.6111111111111112\n",
      "batch_idx: 9\n",
      "0.6083333333333333\n",
      "batch_idx: 10\n",
      "0.6022727272727273\n",
      "batch_idx: 11\n",
      "0.6180555555555556\n",
      "batch_idx: 12\n",
      "0.6185897435897436\n",
      "batch_idx: 13\n",
      "0.6279761904761905\n",
      "batch_idx: 14\n",
      "0.6222222222222222\n",
      "batch_idx: 15\n",
      "0.6328125\n",
      "batch_idx: 16\n",
      "0.6372549019607843\n",
      "batch_idx: 17\n",
      "0.6435185185185185\n",
      "batch_idx: 18\n",
      "0.6403508771929824\n",
      "batch_idx: 19\n",
      "0.6375\n",
      "batch_idx: 20\n",
      "0.6309523809523809\n",
      "batch_idx: 21\n",
      "0.6231060606060606\n",
      "batch_idx: 22\n",
      "0.6268115942028986\n",
      "batch_idx: 23\n",
      "0.6215277777777778\n",
      "batch_idx: 24\n",
      "0.6166666666666667\n",
      "Training Epoch: 180, total loss: 46.515754\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.5416666666666666\n",
      "batch_idx: 2\n",
      "0.5\n",
      "batch_idx: 3\n",
      "0.5520833333333334\n",
      "batch_idx: 4\n",
      "0.5833333333333334\n",
      "batch_idx: 5\n",
      "0.5486111111111112\n",
      "batch_idx: 6\n",
      "0.5595238095238095\n",
      "batch_idx: 7\n",
      "0.578125\n",
      "batch_idx: 8\n",
      "0.5833333333333334\n",
      "batch_idx: 9\n",
      "0.5833333333333334\n",
      "batch_idx: 10\n",
      "0.5833333333333334\n",
      "batch_idx: 11\n",
      "0.5763888888888888\n",
      "batch_idx: 12\n",
      "0.5673076923076923\n",
      "batch_idx: 13\n",
      "0.5714285714285714\n",
      "batch_idx: 14\n",
      "0.5722222222222222\n",
      "batch_idx: 15\n",
      "0.5729166666666666\n",
      "batch_idx: 16\n",
      "0.5808823529411765\n",
      "batch_idx: 17\n",
      "0.5810185185185185\n",
      "batch_idx: 18\n",
      "0.581140350877193\n",
      "batch_idx: 19\n",
      "0.5770833333333333\n",
      "batch_idx: 20\n",
      "0.5813492063492064\n",
      "batch_idx: 21\n",
      "0.5776515151515151\n",
      "batch_idx: 22\n",
      "0.5833333333333334\n",
      "batch_idx: 23\n",
      "0.5833333333333334\n",
      "batch_idx: 24\n",
      "0.5816666666666667\n",
      "Training Epoch: 181, total loss: 46.956236\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.5625\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.6458333333333334\n",
      "batch_idx: 4\n",
      "0.6666666666666666\n",
      "batch_idx: 5\n",
      "0.6597222222222222\n",
      "batch_idx: 6\n",
      "0.6547619047619048\n",
      "batch_idx: 7\n",
      "0.6458333333333334\n",
      "batch_idx: 8\n",
      "0.6620370370370371\n",
      "batch_idx: 9\n",
      "0.6541666666666667\n",
      "batch_idx: 10\n",
      "0.6628787878787878\n",
      "batch_idx: 11\n",
      "0.6493055555555556\n",
      "batch_idx: 12\n",
      "0.6538461538461539\n",
      "batch_idx: 13\n",
      "0.6339285714285714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 14\n",
      "0.6388888888888888\n",
      "batch_idx: 15\n",
      "0.6354166666666666\n",
      "batch_idx: 16\n",
      "0.6299019607843137\n",
      "batch_idx: 17\n",
      "0.6273148148148148\n",
      "batch_idx: 18\n",
      "0.631578947368421\n",
      "batch_idx: 19\n",
      "0.625\n",
      "batch_idx: 20\n",
      "0.628968253968254\n",
      "batch_idx: 21\n",
      "0.625\n",
      "batch_idx: 22\n",
      "0.6195652173913043\n",
      "batch_idx: 23\n",
      "0.6197916666666666\n",
      "batch_idx: 24\n",
      "0.6266666666666667\n",
      "Training Epoch: 182, total loss: 46.208503\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.625\n",
      "batch_idx: 4\n",
      "0.65\n",
      "batch_idx: 5\n",
      "0.625\n",
      "batch_idx: 6\n",
      "0.6190476190476191\n",
      "batch_idx: 7\n",
      "0.6041666666666666\n",
      "batch_idx: 8\n",
      "0.6064814814814815\n",
      "batch_idx: 9\n",
      "0.6041666666666666\n",
      "batch_idx: 10\n",
      "0.5909090909090909\n",
      "batch_idx: 11\n",
      "0.6006944444444444\n",
      "batch_idx: 12\n",
      "0.5993589743589743\n",
      "batch_idx: 13\n",
      "0.6011904761904762\n",
      "batch_idx: 14\n",
      "0.6\n",
      "batch_idx: 15\n",
      "0.5859375\n",
      "batch_idx: 16\n",
      "0.5857843137254902\n",
      "batch_idx: 17\n",
      "0.5856481481481481\n",
      "batch_idx: 18\n",
      "0.5855263157894737\n",
      "batch_idx: 19\n",
      "0.5895833333333333\n",
      "batch_idx: 20\n",
      "0.5992063492063492\n",
      "batch_idx: 21\n",
      "0.6003787878787878\n",
      "batch_idx: 22\n",
      "0.6068840579710145\n",
      "batch_idx: 23\n",
      "0.609375\n",
      "batch_idx: 24\n",
      "0.6116666666666667\n",
      "Training Epoch: 183, total loss: 46.302698\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.5972222222222222\n",
      "batch_idx: 3\n",
      "0.6145833333333334\n",
      "batch_idx: 4\n",
      "0.6083333333333333\n",
      "batch_idx: 5\n",
      "0.5833333333333334\n",
      "batch_idx: 6\n",
      "0.5952380952380952\n",
      "batch_idx: 7\n",
      "0.609375\n",
      "batch_idx: 8\n",
      "0.6203703703703703\n",
      "batch_idx: 9\n",
      "0.6291666666666667\n",
      "batch_idx: 10\n",
      "0.6212121212121212\n",
      "batch_idx: 11\n",
      "0.6215277777777778\n",
      "batch_idx: 12\n",
      "0.625\n",
      "batch_idx: 13\n",
      "0.6279761904761905\n",
      "batch_idx: 14\n",
      "0.6222222222222222\n",
      "batch_idx: 15\n",
      "0.6197916666666666\n",
      "batch_idx: 16\n",
      "0.6225490196078431\n",
      "batch_idx: 17\n",
      "0.6111111111111112\n",
      "batch_idx: 18\n",
      "0.6096491228070176\n",
      "batch_idx: 19\n",
      "0.5979166666666667\n",
      "batch_idx: 20\n",
      "0.6111111111111112\n",
      "batch_idx: 21\n",
      "0.6022727272727273\n",
      "batch_idx: 22\n",
      "0.6086956521739131\n",
      "batch_idx: 23\n",
      "0.6128472222222222\n",
      "batch_idx: 24\n",
      "0.615\n",
      "Training Epoch: 184, total loss: 46.287586\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.6388888888888888\n",
      "batch_idx: 3\n",
      "0.65625\n",
      "batch_idx: 4\n",
      "0.6416666666666667\n",
      "batch_idx: 5\n",
      "0.6458333333333334\n",
      "batch_idx: 6\n",
      "0.6488095238095238\n",
      "batch_idx: 7\n",
      "0.6510416666666666\n",
      "batch_idx: 8\n",
      "0.6666666666666666\n",
      "batch_idx: 9\n",
      "0.65\n",
      "batch_idx: 10\n",
      "0.6401515151515151\n",
      "batch_idx: 11\n",
      "0.6493055555555556\n",
      "batch_idx: 12\n",
      "0.6506410256410257\n",
      "batch_idx: 13\n",
      "0.6636904761904762\n",
      "batch_idx: 14\n",
      "0.6611111111111111\n",
      "batch_idx: 15\n",
      "0.65625\n",
      "batch_idx: 16\n",
      "0.6495098039215687\n",
      "batch_idx: 17\n",
      "0.6481481481481481\n",
      "batch_idx: 18\n",
      "0.6491228070175439\n",
      "batch_idx: 19\n",
      "0.6541666666666667\n",
      "batch_idx: 20\n",
      "0.6507936507936508\n",
      "batch_idx: 21\n",
      "0.6534090909090909\n",
      "batch_idx: 22\n",
      "0.6503623188405797\n",
      "batch_idx: 23\n",
      "0.6545138888888888\n",
      "batch_idx: 24\n",
      "0.655\n",
      "Training Epoch: 185, total loss: 45.692535\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.6833333333333333\n",
      "batch_idx: 5\n",
      "0.6875\n",
      "batch_idx: 6\n",
      "0.6845238095238095\n",
      "batch_idx: 7\n",
      "0.6875\n",
      "batch_idx: 8\n",
      "0.6944444444444444\n",
      "batch_idx: 9\n",
      "0.6708333333333333\n",
      "batch_idx: 10\n",
      "0.6590909090909091\n",
      "batch_idx: 11\n",
      "0.6701388888888888\n",
      "batch_idx: 12\n",
      "0.6730769230769231\n",
      "batch_idx: 13\n",
      "0.6755952380952381\n",
      "batch_idx: 14\n",
      "0.6722222222222223\n",
      "batch_idx: 15\n",
      "0.6614583333333334\n",
      "batch_idx: 16\n",
      "0.6568627450980392\n",
      "batch_idx: 17\n",
      "0.6574074074074074\n",
      "batch_idx: 18\n",
      "0.6513157894736842\n",
      "batch_idx: 19\n",
      "0.6541666666666667\n",
      "batch_idx: 20\n",
      "0.6527777777777778\n",
      "batch_idx: 21\n",
      "0.6571969696969697\n",
      "batch_idx: 22\n",
      "0.6539855072463768\n",
      "batch_idx: 23\n",
      "0.6545138888888888\n",
      "batch_idx: 24\n",
      "0.6516666666666666\n",
      "Training Epoch: 186, total loss: 45.612917\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.6354166666666666\n",
      "batch_idx: 4\n",
      "0.6416666666666667\n",
      "batch_idx: 5\n",
      "0.6111111111111112\n",
      "batch_idx: 6\n",
      "0.6130952380952381\n",
      "batch_idx: 7\n",
      "0.6197916666666666\n",
      "batch_idx: 8\n",
      "0.6064814814814815\n",
      "batch_idx: 9\n",
      "0.6041666666666666\n",
      "batch_idx: 10\n",
      "0.6060606060606061\n",
      "batch_idx: 11\n",
      "0.6180555555555556\n",
      "batch_idx: 12\n",
      "0.6057692307692307\n",
      "batch_idx: 13\n",
      "0.6190476190476191\n",
      "batch_idx: 14\n",
      "0.6111111111111112\n",
      "batch_idx: 15\n",
      "0.609375\n",
      "batch_idx: 16\n",
      "0.6053921568627451\n",
      "batch_idx: 17\n",
      "0.6041666666666666\n",
      "batch_idx: 18\n",
      "0.6074561403508771\n",
      "batch_idx: 19\n",
      "0.5979166666666667\n",
      "batch_idx: 20\n",
      "0.5972222222222222\n",
      "batch_idx: 21\n",
      "0.5984848484848485\n",
      "batch_idx: 22\n",
      "0.605072463768116\n",
      "batch_idx: 23\n",
      "0.6041666666666666\n",
      "batch_idx: 24\n",
      "0.6066666666666667\n",
      "Training Epoch: 187, total loss: 46.534922\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.6354166666666666\n",
      "batch_idx: 4\n",
      "0.6416666666666667\n",
      "batch_idx: 5\n",
      "0.6597222222222222\n",
      "batch_idx: 6\n",
      "0.6488095238095238\n",
      "batch_idx: 7\n",
      "0.6354166666666666\n",
      "batch_idx: 8\n",
      "0.6388888888888888\n",
      "batch_idx: 9\n",
      "0.6333333333333333\n",
      "batch_idx: 10\n",
      "0.6401515151515151\n",
      "batch_idx: 11\n",
      "0.6354166666666666\n",
      "batch_idx: 12\n",
      "0.6346153846153846\n",
      "batch_idx: 13\n",
      "0.6428571428571429\n",
      "batch_idx: 14\n",
      "0.6444444444444445\n",
      "batch_idx: 15\n",
      "0.6484375\n",
      "batch_idx: 16\n",
      "0.6495098039215687\n",
      "batch_idx: 17\n",
      "0.6435185185185185\n",
      "batch_idx: 18\n",
      "0.6381578947368421\n",
      "batch_idx: 19\n",
      "0.6333333333333333\n",
      "batch_idx: 20\n",
      "0.621031746031746\n",
      "batch_idx: 21\n",
      "0.6287878787878788\n",
      "batch_idx: 22\n",
      "0.625\n",
      "batch_idx: 23\n",
      "0.6197916666666666\n",
      "batch_idx: 24\n",
      "0.62\n",
      "Training Epoch: 188, total loss: 46.243508\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.65625\n",
      "batch_idx: 4\n",
      "0.6666666666666666\n",
      "batch_idx: 5\n",
      "0.6805555555555556\n",
      "batch_idx: 6\n",
      "0.6607142857142857\n",
      "batch_idx: 7\n",
      "0.640625\n",
      "batch_idx: 8\n",
      "0.6388888888888888\n",
      "batch_idx: 9\n",
      "0.625\n",
      "batch_idx: 10\n",
      "0.6022727272727273\n",
      "batch_idx: 11\n",
      "0.5902777777777778\n",
      "batch_idx: 12\n",
      "0.5801282051282052\n",
      "batch_idx: 13\n",
      "0.5714285714285714\n",
      "batch_idx: 14\n",
      "0.5805555555555556\n",
      "batch_idx: 15\n",
      "0.5833333333333334\n",
      "batch_idx: 16\n",
      "0.5931372549019608\n",
      "batch_idx: 17\n",
      "0.5879629629629629\n",
      "batch_idx: 18\n",
      "0.5986842105263158\n",
      "batch_idx: 19\n",
      "0.5958333333333333\n",
      "batch_idx: 20\n",
      "0.5972222222222222\n",
      "batch_idx: 21\n",
      "0.6079545454545454\n",
      "batch_idx: 22\n",
      "0.6123188405797102\n",
      "batch_idx: 23\n",
      "0.6180555555555556\n",
      "batch_idx: 24\n",
      "0.615\n",
      "Training Epoch: 189, total loss: 46.491465\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.65625\n",
      "batch_idx: 4\n",
      "0.6166666666666667\n",
      "batch_idx: 5\n",
      "0.6388888888888888\n",
      "batch_idx: 6\n",
      "0.6428571428571429\n",
      "batch_idx: 7\n",
      "0.6458333333333334\n",
      "batch_idx: 8\n",
      "0.6342592592592593\n",
      "batch_idx: 9\n",
      "0.625\n",
      "batch_idx: 10\n",
      "0.6477272727272727\n",
      "batch_idx: 11\n",
      "0.6597222222222222\n",
      "batch_idx: 12\n",
      "0.657051282051282\n",
      "batch_idx: 13\n",
      "0.6517857142857143\n",
      "batch_idx: 14\n",
      "0.6472222222222223\n",
      "batch_idx: 15\n",
      "0.6458333333333334\n",
      "batch_idx: 16\n",
      "0.6397058823529411\n",
      "batch_idx: 17\n",
      "0.6365740740740741\n",
      "batch_idx: 18\n",
      "0.6293859649122807\n",
      "batch_idx: 19\n",
      "0.63125\n",
      "batch_idx: 20\n",
      "0.628968253968254\n",
      "batch_idx: 21\n",
      "0.625\n",
      "batch_idx: 22\n",
      "0.6268115942028986\n",
      "batch_idx: 23\n",
      "0.6267361111111112\n",
      "batch_idx: 24\n",
      "0.6316666666666667\n",
      "Training Epoch: 190, total loss: 46.003776\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.7\n",
      "batch_idx: 5\n",
      "0.6597222222222222\n",
      "batch_idx: 6\n",
      "0.6488095238095238\n",
      "batch_idx: 7\n",
      "0.6458333333333334\n",
      "batch_idx: 8\n",
      "0.6342592592592593\n",
      "batch_idx: 9\n",
      "0.6458333333333334\n",
      "batch_idx: 10\n",
      "0.6515151515151515\n",
      "batch_idx: 11\n",
      "0.6493055555555556\n",
      "batch_idx: 12\n",
      "0.657051282051282\n",
      "batch_idx: 13\n",
      "0.6458333333333334\n",
      "batch_idx: 14\n",
      "0.6472222222222223\n",
      "batch_idx: 15\n",
      "0.6328125\n",
      "batch_idx: 16\n",
      "0.6372549019607843\n",
      "batch_idx: 17\n",
      "0.625\n",
      "batch_idx: 18\n",
      "0.618421052631579\n",
      "batch_idx: 19\n",
      "0.625\n",
      "batch_idx: 20\n",
      "0.628968253968254\n",
      "batch_idx: 21\n",
      "0.625\n",
      "batch_idx: 22\n",
      "0.6268115942028986\n",
      "batch_idx: 23\n",
      "0.6336805555555556\n",
      "batch_idx: 24\n",
      "0.6316666666666667\n",
      "Training Epoch: 191, total loss: 45.964746\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.5416666666666666\n",
      "batch_idx: 2\n",
      "0.5972222222222222\n",
      "batch_idx: 3\n",
      "0.5729166666666666\n",
      "batch_idx: 4\n",
      "0.5833333333333334\n",
      "batch_idx: 5\n",
      "0.6041666666666666\n",
      "batch_idx: 6\n",
      "0.6071428571428571\n",
      "batch_idx: 7\n",
      "0.609375\n",
      "batch_idx: 8\n",
      "0.6064814814814815\n",
      "batch_idx: 9\n",
      "0.6125\n",
      "batch_idx: 10\n",
      "0.6174242424242424\n",
      "batch_idx: 11\n",
      "0.6284722222222222\n",
      "batch_idx: 12\n",
      "0.625\n",
      "batch_idx: 13\n",
      "0.6130952380952381\n",
      "batch_idx: 14\n",
      "0.6194444444444445\n",
      "batch_idx: 15\n",
      "0.6119791666666666\n",
      "batch_idx: 16\n",
      "0.6078431372549019\n",
      "batch_idx: 17\n",
      "0.6041666666666666\n",
      "batch_idx: 18\n",
      "0.6052631578947368\n",
      "batch_idx: 19\n",
      "0.60625\n",
      "batch_idx: 20\n",
      "0.6071428571428571\n",
      "batch_idx: 21\n",
      "0.6079545454545454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 22\n",
      "0.5996376811594203\n",
      "batch_idx: 23\n",
      "0.5972222222222222\n",
      "batch_idx: 24\n",
      "0.595\n",
      "Training Epoch: 192, total loss: 46.749353\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.6388888888888888\n",
      "batch_idx: 3\n",
      "0.6770833333333334\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.6666666666666666\n",
      "batch_idx: 6\n",
      "0.6726190476190477\n",
      "batch_idx: 7\n",
      "0.671875\n",
      "batch_idx: 8\n",
      "0.6759259259259259\n",
      "batch_idx: 9\n",
      "0.675\n",
      "batch_idx: 10\n",
      "0.6628787878787878\n",
      "batch_idx: 11\n",
      "0.6527777777777778\n",
      "batch_idx: 12\n",
      "0.657051282051282\n",
      "batch_idx: 13\n",
      "0.6577380952380952\n",
      "batch_idx: 14\n",
      "0.65\n",
      "batch_idx: 15\n",
      "0.6510416666666666\n",
      "batch_idx: 16\n",
      "0.6568627450980392\n",
      "batch_idx: 17\n",
      "0.6504629629629629\n",
      "batch_idx: 18\n",
      "0.6447368421052632\n",
      "batch_idx: 19\n",
      "0.6458333333333334\n",
      "batch_idx: 20\n",
      "0.6488095238095238\n",
      "batch_idx: 21\n",
      "0.6571969696969697\n",
      "batch_idx: 22\n",
      "0.6612318840579711\n",
      "batch_idx: 23\n",
      "0.6614583333333334\n",
      "batch_idx: 24\n",
      "0.6566666666666666\n",
      "Training Epoch: 193, total loss: 45.622136\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6666666666666666\n",
      "batch_idx: 6\n",
      "0.6785714285714286\n",
      "batch_idx: 7\n",
      "0.65625\n",
      "batch_idx: 8\n",
      "0.6435185185185185\n",
      "batch_idx: 9\n",
      "0.6416666666666667\n",
      "batch_idx: 10\n",
      "0.6477272727272727\n",
      "batch_idx: 11\n",
      "0.6527777777777778\n",
      "batch_idx: 12\n",
      "0.657051282051282\n",
      "batch_idx: 13\n",
      "0.6547619047619048\n",
      "batch_idx: 14\n",
      "0.6416666666666667\n",
      "batch_idx: 15\n",
      "0.6276041666666666\n",
      "batch_idx: 16\n",
      "0.6225490196078431\n",
      "batch_idx: 17\n",
      "0.6180555555555556\n",
      "batch_idx: 18\n",
      "0.618421052631579\n",
      "batch_idx: 19\n",
      "0.6166666666666667\n",
      "batch_idx: 20\n",
      "0.625\n",
      "batch_idx: 21\n",
      "0.625\n",
      "batch_idx: 22\n",
      "0.6358695652173914\n",
      "batch_idx: 23\n",
      "0.6371527777777778\n",
      "batch_idx: 24\n",
      "0.6383333333333333\n",
      "Training Epoch: 194, total loss: 45.899036\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.65625\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6805555555555556\n",
      "batch_idx: 6\n",
      "0.6845238095238095\n",
      "batch_idx: 7\n",
      "0.6510416666666666\n",
      "batch_idx: 8\n",
      "0.6435185185185185\n",
      "batch_idx: 9\n",
      "0.6458333333333334\n",
      "batch_idx: 10\n",
      "0.6401515151515151\n",
      "batch_idx: 11\n",
      "0.6493055555555556\n",
      "batch_idx: 12\n",
      "0.6506410256410257\n",
      "batch_idx: 13\n",
      "0.6428571428571429\n",
      "batch_idx: 14\n",
      "0.6361111111111111\n",
      "batch_idx: 15\n",
      "0.6276041666666666\n",
      "batch_idx: 16\n",
      "0.6323529411764706\n",
      "batch_idx: 17\n",
      "0.6319444444444444\n",
      "batch_idx: 18\n",
      "0.6469298245614035\n",
      "batch_idx: 19\n",
      "0.6479166666666667\n",
      "batch_idx: 20\n",
      "0.6448412698412699\n",
      "batch_idx: 21\n",
      "0.6401515151515151\n",
      "batch_idx: 22\n",
      "0.6394927536231884\n",
      "batch_idx: 23\n",
      "0.6371527777777778\n",
      "batch_idx: 24\n",
      "0.6383333333333333\n",
      "Training Epoch: 195, total loss: 45.966213\n",
      "batch_idx: 0\n",
      "0.4166666666666667\n",
      "batch_idx: 1\n",
      "0.4791666666666667\n",
      "batch_idx: 2\n",
      "0.5972222222222222\n",
      "batch_idx: 3\n",
      "0.6041666666666666\n",
      "batch_idx: 4\n",
      "0.6083333333333333\n",
      "batch_idx: 5\n",
      "0.6388888888888888\n",
      "batch_idx: 6\n",
      "0.6190476190476191\n",
      "batch_idx: 7\n",
      "0.6354166666666666\n",
      "batch_idx: 8\n",
      "0.6620370370370371\n",
      "batch_idx: 9\n",
      "0.6625\n",
      "batch_idx: 10\n",
      "0.6515151515151515\n",
      "batch_idx: 11\n",
      "0.6493055555555556\n",
      "batch_idx: 12\n",
      "0.6538461538461539\n",
      "batch_idx: 13\n",
      "0.6607142857142857\n",
      "batch_idx: 14\n",
      "0.6666666666666666\n",
      "batch_idx: 15\n",
      "0.6744791666666666\n",
      "batch_idx: 16\n",
      "0.6617647058823529\n",
      "batch_idx: 17\n",
      "0.6481481481481481\n",
      "batch_idx: 18\n",
      "0.6469298245614035\n",
      "batch_idx: 19\n",
      "0.6416666666666667\n",
      "batch_idx: 20\n",
      "0.6527777777777778\n",
      "batch_idx: 21\n",
      "0.6590909090909091\n",
      "batch_idx: 22\n",
      "0.6521739130434783\n",
      "batch_idx: 23\n",
      "0.6475694444444444\n",
      "batch_idx: 24\n",
      "0.6466666666666666\n",
      "Training Epoch: 196, total loss: 45.907154\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.5972222222222222\n",
      "batch_idx: 3\n",
      "0.6354166666666666\n",
      "batch_idx: 4\n",
      "0.6583333333333333\n",
      "batch_idx: 5\n",
      "0.6666666666666666\n",
      "batch_idx: 6\n",
      "0.6607142857142857\n",
      "batch_idx: 7\n",
      "0.6666666666666666\n",
      "batch_idx: 8\n",
      "0.6620370370370371\n",
      "batch_idx: 9\n",
      "0.65\n",
      "batch_idx: 10\n",
      "0.6477272727272727\n",
      "batch_idx: 11\n",
      "0.6493055555555556\n",
      "batch_idx: 12\n",
      "0.6506410256410257\n",
      "batch_idx: 13\n",
      "0.6488095238095238\n",
      "batch_idx: 14\n",
      "0.6444444444444445\n",
      "batch_idx: 15\n",
      "0.6432291666666666\n",
      "batch_idx: 16\n",
      "0.6495098039215687\n",
      "batch_idx: 17\n",
      "0.6504629629629629\n",
      "batch_idx: 18\n",
      "0.6425438596491229\n",
      "batch_idx: 19\n",
      "0.63125\n",
      "batch_idx: 20\n",
      "0.625\n",
      "batch_idx: 21\n",
      "0.6193181818181818\n",
      "batch_idx: 22\n",
      "0.6213768115942029\n",
      "batch_idx: 23\n",
      "0.6180555555555556\n",
      "batch_idx: 24\n",
      "0.6133333333333333\n",
      "Training Epoch: 197, total loss: 46.331307\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.6458333333333334\n",
      "batch_idx: 4\n",
      "0.6333333333333333\n",
      "batch_idx: 5\n",
      "0.6111111111111112\n",
      "batch_idx: 6\n",
      "0.6130952380952381\n",
      "batch_idx: 7\n",
      "0.625\n",
      "batch_idx: 8\n",
      "0.6064814814814815\n",
      "batch_idx: 9\n",
      "0.6291666666666667\n",
      "batch_idx: 10\n",
      "0.6174242424242424\n",
      "batch_idx: 11\n",
      "0.625\n",
      "batch_idx: 12\n",
      "0.6410256410256411\n",
      "batch_idx: 13\n",
      "0.6279761904761905\n",
      "batch_idx: 14\n",
      "0.6277777777777778\n",
      "batch_idx: 15\n",
      "0.6328125\n",
      "batch_idx: 16\n",
      "0.6299019607843137\n",
      "batch_idx: 17\n",
      "0.6226851851851852\n",
      "batch_idx: 18\n",
      "0.6140350877192983\n",
      "batch_idx: 19\n",
      "0.61875\n",
      "batch_idx: 20\n",
      "0.6150793650793651\n",
      "batch_idx: 21\n",
      "0.615530303030303\n",
      "batch_idx: 22\n",
      "0.6195652173913043\n",
      "batch_idx: 23\n",
      "0.6180555555555556\n",
      "batch_idx: 24\n",
      "0.6166666666666667\n",
      "Training Epoch: 198, total loss: 46.201093\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.5694444444444444\n",
      "batch_idx: 3\n",
      "0.6145833333333334\n",
      "batch_idx: 4\n",
      "0.5833333333333334\n",
      "batch_idx: 5\n",
      "0.625\n",
      "batch_idx: 6\n",
      "0.6190476190476191\n",
      "batch_idx: 7\n",
      "0.6197916666666666\n",
      "batch_idx: 8\n",
      "0.6157407407407407\n",
      "batch_idx: 9\n",
      "0.625\n",
      "batch_idx: 10\n",
      "0.6287878787878788\n",
      "batch_idx: 11\n",
      "0.6111111111111112\n",
      "batch_idx: 12\n",
      "0.6089743589743589\n",
      "batch_idx: 13\n",
      "0.6160714285714286\n",
      "batch_idx: 14\n",
      "0.6111111111111112\n",
      "batch_idx: 15\n",
      "0.6197916666666666\n",
      "batch_idx: 16\n",
      "0.6225490196078431\n",
      "batch_idx: 17\n",
      "0.6157407407407407\n",
      "batch_idx: 18\n",
      "0.6162280701754386\n",
      "batch_idx: 19\n",
      "0.6104166666666667\n",
      "batch_idx: 20\n",
      "0.6111111111111112\n",
      "batch_idx: 21\n",
      "0.6117424242424242\n",
      "batch_idx: 22\n",
      "0.6177536231884058\n",
      "batch_idx: 23\n",
      "0.6232638888888888\n",
      "batch_idx: 24\n",
      "0.6283333333333333\n",
      "Training Epoch: 199, total loss: 46.082331\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.65625\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6875\n",
      "batch_idx: 6\n",
      "0.6785714285714286\n",
      "batch_idx: 7\n",
      "0.6927083333333334\n",
      "batch_idx: 8\n",
      "0.7037037037037037\n",
      "batch_idx: 9\n",
      "0.6916666666666667\n",
      "batch_idx: 10\n",
      "0.6893939393939394\n",
      "batch_idx: 11\n",
      "0.6875\n",
      "batch_idx: 12\n",
      "0.6858974358974359\n",
      "batch_idx: 13\n",
      "0.6785714285714286\n",
      "batch_idx: 14\n",
      "0.6777777777777778\n",
      "batch_idx: 15\n",
      "0.6692708333333334\n",
      "batch_idx: 16\n",
      "0.6642156862745098\n",
      "batch_idx: 17\n",
      "0.6689814814814815\n",
      "batch_idx: 18\n",
      "0.668859649122807\n",
      "batch_idx: 19\n",
      "0.6708333333333333\n",
      "batch_idx: 20\n",
      "0.6785714285714286\n",
      "batch_idx: 21\n",
      "0.6742424242424242\n",
      "batch_idx: 22\n",
      "0.6757246376811594\n",
      "batch_idx: 23\n",
      "0.6736111111111112\n",
      "batch_idx: 24\n",
      "0.6666666666666666\n",
      "Training Epoch: 200, total loss: 45.377337\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.6770833333333334\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.6944444444444444\n",
      "batch_idx: 6\n",
      "0.6845238095238095\n",
      "batch_idx: 7\n",
      "0.703125\n",
      "batch_idx: 8\n",
      "0.6990740740740741\n",
      "batch_idx: 9\n",
      "0.7\n",
      "batch_idx: 10\n",
      "0.7045454545454546\n",
      "batch_idx: 11\n",
      "0.6944444444444444\n",
      "batch_idx: 12\n",
      "0.7019230769230769\n",
      "batch_idx: 13\n",
      "0.6904761904761905\n",
      "batch_idx: 14\n",
      "0.6888888888888889\n",
      "batch_idx: 15\n",
      "0.6901041666666666\n",
      "batch_idx: 16\n",
      "0.6838235294117647\n",
      "batch_idx: 17\n",
      "0.6898148148148148\n",
      "batch_idx: 18\n",
      "0.6864035087719298\n",
      "batch_idx: 19\n",
      "0.6833333333333333\n",
      "batch_idx: 20\n",
      "0.6825396825396826\n",
      "batch_idx: 21\n",
      "0.6856060606060606\n",
      "batch_idx: 22\n",
      "0.6920289855072463\n",
      "batch_idx: 23\n",
      "0.6944444444444444\n",
      "batch_idx: 24\n",
      "0.6983333333333334\n",
      "Training Epoch: 201, total loss: 44.905484\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.6666666666666666\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.7152777777777778\n",
      "batch_idx: 6\n",
      "0.6904761904761905\n",
      "batch_idx: 7\n",
      "0.6822916666666666\n",
      "batch_idx: 8\n",
      "0.6944444444444444\n",
      "batch_idx: 9\n",
      "0.6708333333333333\n",
      "batch_idx: 10\n",
      "0.6666666666666666\n",
      "batch_idx: 11\n",
      "0.6631944444444444\n",
      "batch_idx: 12\n",
      "0.6538461538461539\n",
      "batch_idx: 13\n",
      "0.6577380952380952\n",
      "batch_idx: 14\n",
      "0.6666666666666666\n",
      "batch_idx: 15\n",
      "0.6640625\n",
      "batch_idx: 16\n",
      "0.6568627450980392\n",
      "batch_idx: 17\n",
      "0.6666666666666666\n",
      "batch_idx: 18\n",
      "0.6600877192982456\n",
      "batch_idx: 19\n",
      "0.6645833333333333\n",
      "batch_idx: 20\n",
      "0.6607142857142857\n",
      "batch_idx: 21\n",
      "0.6590909090909091\n",
      "batch_idx: 22\n",
      "0.657608695652174\n",
      "batch_idx: 23\n",
      "0.6597222222222222\n",
      "batch_idx: 24\n",
      "0.66\n",
      "Training Epoch: 202, total loss: 45.545013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.6666666666666666\n",
      "batch_idx: 5\n",
      "0.6875\n",
      "batch_idx: 6\n",
      "0.6904761904761905\n",
      "batch_idx: 7\n",
      "0.6927083333333334\n",
      "batch_idx: 8\n",
      "0.6805555555555556\n",
      "batch_idx: 9\n",
      "0.6541666666666667\n",
      "batch_idx: 10\n",
      "0.6553030303030303\n",
      "batch_idx: 11\n",
      "0.6631944444444444\n",
      "batch_idx: 12\n",
      "0.6666666666666666\n",
      "batch_idx: 13\n",
      "0.6607142857142857\n",
      "batch_idx: 14\n",
      "0.6527777777777778\n",
      "batch_idx: 15\n",
      "0.65625\n",
      "batch_idx: 16\n",
      "0.6519607843137255\n",
      "batch_idx: 17\n",
      "0.6458333333333334\n",
      "batch_idx: 18\n",
      "0.6513157894736842\n",
      "batch_idx: 19\n",
      "0.65625\n",
      "batch_idx: 20\n",
      "0.6507936507936508\n",
      "batch_idx: 21\n",
      "0.6553030303030303\n",
      "batch_idx: 22\n",
      "0.6539855072463768\n",
      "batch_idx: 23\n",
      "0.6493055555555556\n",
      "batch_idx: 24\n",
      "0.6533333333333333\n",
      "Training Epoch: 203, total loss: 45.609422\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.6583333333333333\n",
      "batch_idx: 5\n",
      "0.6666666666666666\n",
      "batch_idx: 6\n",
      "0.6964285714285714\n",
      "batch_idx: 7\n",
      "0.6822916666666666\n",
      "batch_idx: 8\n",
      "0.6805555555555556\n",
      "batch_idx: 9\n",
      "0.6875\n",
      "batch_idx: 10\n",
      "0.6742424242424242\n",
      "batch_idx: 11\n",
      "0.6736111111111112\n",
      "batch_idx: 12\n",
      "0.6826923076923077\n",
      "batch_idx: 13\n",
      "0.6755952380952381\n",
      "batch_idx: 14\n",
      "0.6694444444444444\n",
      "batch_idx: 15\n",
      "0.671875\n",
      "batch_idx: 16\n",
      "0.6740196078431373\n",
      "batch_idx: 17\n",
      "0.6736111111111112\n",
      "batch_idx: 18\n",
      "0.6776315789473685\n",
      "batch_idx: 19\n",
      "0.68125\n",
      "batch_idx: 20\n",
      "0.6845238095238095\n",
      "batch_idx: 21\n",
      "0.6818181818181818\n",
      "batch_idx: 22\n",
      "0.6811594202898551\n",
      "batch_idx: 23\n",
      "0.6822916666666666\n",
      "batch_idx: 24\n",
      "0.6783333333333333\n",
      "Training Epoch: 204, total loss: 45.218994\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.6388888888888888\n",
      "batch_idx: 3\n",
      "0.65625\n",
      "batch_idx: 4\n",
      "0.6333333333333333\n",
      "batch_idx: 5\n",
      "0.625\n",
      "batch_idx: 6\n",
      "0.6309523809523809\n",
      "batch_idx: 7\n",
      "0.6458333333333334\n",
      "batch_idx: 8\n",
      "0.6435185185185185\n",
      "batch_idx: 9\n",
      "0.6333333333333333\n",
      "batch_idx: 10\n",
      "0.6325757575757576\n",
      "batch_idx: 11\n",
      "0.6423611111111112\n",
      "batch_idx: 12\n",
      "0.6410256410256411\n",
      "batch_idx: 13\n",
      "0.6488095238095238\n",
      "batch_idx: 14\n",
      "0.6611111111111111\n",
      "batch_idx: 15\n",
      "0.6692708333333334\n",
      "batch_idx: 16\n",
      "0.6764705882352942\n",
      "batch_idx: 17\n",
      "0.6851851851851852\n",
      "batch_idx: 18\n",
      "0.6885964912280702\n",
      "batch_idx: 19\n",
      "0.6895833333333333\n",
      "batch_idx: 20\n",
      "0.6865079365079365\n",
      "batch_idx: 21\n",
      "0.6761363636363636\n",
      "batch_idx: 22\n",
      "0.6739130434782609\n",
      "batch_idx: 23\n",
      "0.6736111111111112\n",
      "batch_idx: 24\n",
      "0.6683333333333333\n",
      "Training Epoch: 205, total loss: 45.308861\n",
      "batch_idx: 0\n",
      "0.4166666666666667\n",
      "batch_idx: 1\n",
      "0.5625\n",
      "batch_idx: 2\n",
      "0.5833333333333334\n",
      "batch_idx: 3\n",
      "0.625\n",
      "batch_idx: 4\n",
      "0.625\n",
      "batch_idx: 5\n",
      "0.625\n",
      "batch_idx: 6\n",
      "0.6309523809523809\n",
      "batch_idx: 7\n",
      "0.640625\n",
      "batch_idx: 8\n",
      "0.6296296296296297\n",
      "batch_idx: 9\n",
      "0.6291666666666667\n",
      "batch_idx: 10\n",
      "0.6363636363636364\n",
      "batch_idx: 11\n",
      "0.6319444444444444\n",
      "batch_idx: 12\n",
      "0.6378205128205128\n",
      "batch_idx: 13\n",
      "0.6428571428571429\n",
      "batch_idx: 14\n",
      "0.6361111111111111\n",
      "batch_idx: 15\n",
      "0.6458333333333334\n",
      "batch_idx: 16\n",
      "0.6323529411764706\n",
      "batch_idx: 17\n",
      "0.6388888888888888\n",
      "batch_idx: 18\n",
      "0.6425438596491229\n",
      "batch_idx: 19\n",
      "0.6479166666666667\n",
      "batch_idx: 20\n",
      "0.6587301587301587\n",
      "batch_idx: 21\n",
      "0.6534090909090909\n",
      "batch_idx: 22\n",
      "0.6539855072463768\n",
      "batch_idx: 23\n",
      "0.65625\n",
      "batch_idx: 24\n",
      "0.6583333333333333\n",
      "Training Epoch: 206, total loss: 45.853250\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.5972222222222222\n",
      "batch_idx: 3\n",
      "0.6458333333333334\n",
      "batch_idx: 4\n",
      "0.6333333333333333\n",
      "batch_idx: 5\n",
      "0.6458333333333334\n",
      "batch_idx: 6\n",
      "0.6488095238095238\n",
      "batch_idx: 7\n",
      "0.6510416666666666\n",
      "batch_idx: 8\n",
      "0.6527777777777778\n",
      "batch_idx: 9\n",
      "0.6458333333333334\n",
      "batch_idx: 10\n",
      "0.6401515151515151\n",
      "batch_idx: 11\n",
      "0.6493055555555556\n",
      "batch_idx: 12\n",
      "0.6474358974358975\n",
      "batch_idx: 13\n",
      "0.6458333333333334\n",
      "batch_idx: 14\n",
      "0.6527777777777778\n",
      "batch_idx: 15\n",
      "0.6484375\n",
      "batch_idx: 16\n",
      "0.6495098039215687\n",
      "batch_idx: 17\n",
      "0.6388888888888888\n",
      "batch_idx: 18\n",
      "0.6381578947368421\n",
      "batch_idx: 19\n",
      "0.6416666666666667\n",
      "batch_idx: 20\n",
      "0.6388888888888888\n",
      "batch_idx: 21\n",
      "0.6458333333333334\n",
      "batch_idx: 22\n",
      "0.6413043478260869\n",
      "batch_idx: 23\n",
      "0.6440972222222222\n",
      "batch_idx: 24\n",
      "0.64\n",
      "Training Epoch: 207, total loss: 45.865873\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.5972222222222222\n",
      "batch_idx: 3\n",
      "0.59375\n",
      "batch_idx: 4\n",
      "0.625\n",
      "batch_idx: 5\n",
      "0.6041666666666666\n",
      "batch_idx: 6\n",
      "0.5952380952380952\n",
      "batch_idx: 7\n",
      "0.5989583333333334\n",
      "batch_idx: 8\n",
      "0.6064814814814815\n",
      "batch_idx: 9\n",
      "0.6291666666666667\n",
      "batch_idx: 10\n",
      "0.6363636363636364\n",
      "batch_idx: 11\n",
      "0.6284722222222222\n",
      "batch_idx: 12\n",
      "0.6185897435897436\n",
      "batch_idx: 13\n",
      "0.6190476190476191\n",
      "batch_idx: 14\n",
      "0.6138888888888889\n",
      "batch_idx: 15\n",
      "0.6197916666666666\n",
      "batch_idx: 16\n",
      "0.6151960784313726\n",
      "batch_idx: 17\n",
      "0.6180555555555556\n",
      "batch_idx: 18\n",
      "0.625\n",
      "batch_idx: 19\n",
      "0.6270833333333333\n",
      "batch_idx: 20\n",
      "0.626984126984127\n",
      "batch_idx: 21\n",
      "0.6325757575757576\n",
      "batch_idx: 22\n",
      "0.6268115942028986\n",
      "batch_idx: 23\n",
      "0.6284722222222222\n",
      "batch_idx: 24\n",
      "0.6233333333333333\n",
      "Training Epoch: 208, total loss: 46.151546\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.7142857142857143\n",
      "batch_idx: 7\n",
      "0.6770833333333334\n",
      "batch_idx: 8\n",
      "0.6851851851851852\n",
      "batch_idx: 9\n",
      "0.6875\n",
      "batch_idx: 10\n",
      "0.6818181818181818\n",
      "batch_idx: 11\n",
      "0.6770833333333334\n",
      "batch_idx: 12\n",
      "0.6698717948717948\n",
      "batch_idx: 13\n",
      "0.6696428571428571\n",
      "batch_idx: 14\n",
      "0.6638888888888889\n",
      "batch_idx: 15\n",
      "0.6666666666666666\n",
      "batch_idx: 16\n",
      "0.6568627450980392\n",
      "batch_idx: 17\n",
      "0.6550925925925926\n",
      "batch_idx: 18\n",
      "0.6535087719298246\n",
      "batch_idx: 19\n",
      "0.6583333333333333\n",
      "batch_idx: 20\n",
      "0.6567460317460317\n",
      "batch_idx: 21\n",
      "0.6685606060606061\n",
      "batch_idx: 22\n",
      "0.6666666666666666\n",
      "batch_idx: 23\n",
      "0.6684027777777778\n",
      "batch_idx: 24\n",
      "0.6616666666666666\n",
      "Training Epoch: 209, total loss: 45.521577\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7083333333333334\n",
      "batch_idx: 7\n",
      "0.7135416666666666\n",
      "batch_idx: 8\n",
      "0.6990740740740741\n",
      "batch_idx: 9\n",
      "0.6875\n",
      "batch_idx: 10\n",
      "0.6856060606060606\n",
      "batch_idx: 11\n",
      "0.6840277777777778\n",
      "batch_idx: 12\n",
      "0.6891025641025641\n",
      "batch_idx: 13\n",
      "0.6785714285714286\n",
      "batch_idx: 14\n",
      "0.6861111111111111\n",
      "batch_idx: 15\n",
      "0.6875\n",
      "batch_idx: 16\n",
      "0.6813725490196079\n",
      "batch_idx: 17\n",
      "0.6759259259259259\n",
      "batch_idx: 18\n",
      "0.6754385964912281\n",
      "batch_idx: 19\n",
      "0.6770833333333334\n",
      "batch_idx: 20\n",
      "0.6706349206349206\n",
      "batch_idx: 21\n",
      "0.6723484848484849\n",
      "batch_idx: 22\n",
      "0.6739130434782609\n",
      "batch_idx: 23\n",
      "0.6770833333333334\n",
      "batch_idx: 24\n",
      "0.67\n",
      "Training Epoch: 210, total loss: 45.556900\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.5416666666666666\n",
      "batch_idx: 2\n",
      "0.5416666666666666\n",
      "batch_idx: 3\n",
      "0.5625\n",
      "batch_idx: 4\n",
      "0.5916666666666667\n",
      "batch_idx: 5\n",
      "0.5833333333333334\n",
      "batch_idx: 6\n",
      "0.5833333333333334\n",
      "batch_idx: 7\n",
      "0.5833333333333334\n",
      "batch_idx: 8\n",
      "0.6018518518518519\n",
      "batch_idx: 9\n",
      "0.5958333333333333\n",
      "batch_idx: 10\n",
      "0.6022727272727273\n",
      "batch_idx: 11\n",
      "0.6111111111111112\n",
      "batch_idx: 12\n",
      "0.6089743589743589\n",
      "batch_idx: 13\n",
      "0.6190476190476191\n",
      "batch_idx: 14\n",
      "0.6194444444444445\n",
      "batch_idx: 15\n",
      "0.6171875\n",
      "batch_idx: 16\n",
      "0.625\n",
      "batch_idx: 17\n",
      "0.6273148148148148\n",
      "batch_idx: 18\n",
      "0.6271929824561403\n",
      "batch_idx: 19\n",
      "0.6270833333333333\n",
      "batch_idx: 20\n",
      "0.628968253968254\n",
      "batch_idx: 21\n",
      "0.625\n",
      "batch_idx: 22\n",
      "0.625\n",
      "batch_idx: 23\n",
      "0.6267361111111112\n",
      "batch_idx: 24\n",
      "0.6316666666666667\n",
      "Training Epoch: 211, total loss: 45.951370\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.7013888888888888\n",
      "batch_idx: 6\n",
      "0.6785714285714286\n",
      "batch_idx: 7\n",
      "0.6875\n",
      "batch_idx: 8\n",
      "0.6851851851851852\n",
      "batch_idx: 9\n",
      "0.675\n",
      "batch_idx: 10\n",
      "0.6818181818181818\n",
      "batch_idx: 11\n",
      "0.6736111111111112\n",
      "batch_idx: 12\n",
      "0.6730769230769231\n",
      "batch_idx: 13\n",
      "0.6666666666666666\n",
      "batch_idx: 14\n",
      "0.6666666666666666\n",
      "batch_idx: 15\n",
      "0.6614583333333334\n",
      "batch_idx: 16\n",
      "0.6666666666666666\n",
      "batch_idx: 17\n",
      "0.6620370370370371\n",
      "batch_idx: 18\n",
      "0.6644736842105263\n",
      "batch_idx: 19\n",
      "0.65625\n",
      "batch_idx: 20\n",
      "0.6607142857142857\n",
      "batch_idx: 21\n",
      "0.6553030303030303\n",
      "batch_idx: 22\n",
      "0.6521739130434783\n",
      "batch_idx: 23\n",
      "0.6527777777777778\n",
      "batch_idx: 24\n",
      "0.6516666666666666\n",
      "Training Epoch: 212, total loss: 45.639897\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7083333333333334\n",
      "batch_idx: 6\n",
      "0.6785714285714286\n",
      "batch_idx: 7\n",
      "0.671875\n",
      "batch_idx: 8\n",
      "0.6805555555555556\n",
      "batch_idx: 9\n",
      "0.6875\n",
      "batch_idx: 10\n",
      "0.6856060606060606\n",
      "batch_idx: 11\n",
      "0.6770833333333334\n",
      "batch_idx: 12\n",
      "0.6666666666666666\n",
      "batch_idx: 13\n",
      "0.6636904761904762\n",
      "batch_idx: 14\n",
      "0.6638888888888889\n",
      "batch_idx: 15\n",
      "0.6640625\n",
      "batch_idx: 16\n",
      "0.6642156862745098\n",
      "batch_idx: 17\n",
      "0.6620370370370371\n",
      "batch_idx: 18\n",
      "0.6666666666666666\n",
      "batch_idx: 19\n",
      "0.6645833333333333\n",
      "batch_idx: 20\n",
      "0.6607142857142857\n",
      "batch_idx: 21\n",
      "0.6515151515151515\n",
      "batch_idx: 22\n",
      "0.657608695652174\n",
      "batch_idx: 23\n",
      "0.6493055555555556\n",
      "batch_idx: 24\n",
      "0.6516666666666666\n",
      "Training Epoch: 213, total loss: 45.750625\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.6666666666666666\n",
      "batch_idx: 5\n",
      "0.6944444444444444\n",
      "batch_idx: 6\n",
      "0.7083333333333334\n",
      "batch_idx: 7\n",
      "0.6822916666666666\n",
      "batch_idx: 8\n",
      "0.6851851851851852\n",
      "batch_idx: 9\n",
      "0.6708333333333333\n",
      "batch_idx: 10\n",
      "0.6628787878787878\n",
      "batch_idx: 11\n",
      "0.6666666666666666\n",
      "batch_idx: 12\n",
      "0.6634615384615384\n",
      "batch_idx: 13\n",
      "0.6666666666666666\n",
      "batch_idx: 14\n",
      "0.6583333333333333\n",
      "batch_idx: 15\n",
      "0.6588541666666666\n",
      "batch_idx: 16\n",
      "0.6593137254901961\n",
      "batch_idx: 17\n",
      "0.6620370370370371\n",
      "batch_idx: 18\n",
      "0.6578947368421053\n",
      "batch_idx: 19\n",
      "0.65\n",
      "batch_idx: 20\n",
      "0.6547619047619048\n",
      "batch_idx: 21\n",
      "0.6553030303030303\n",
      "batch_idx: 22\n",
      "0.6485507246376812\n",
      "batch_idx: 23\n",
      "0.6527777777777778\n",
      "batch_idx: 24\n",
      "0.65\n",
      "Training Epoch: 214, total loss: 45.767294\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.6458333333333334\n",
      "batch_idx: 4\n",
      "0.65\n",
      "batch_idx: 5\n",
      "0.6458333333333334\n",
      "batch_idx: 6\n",
      "0.6488095238095238\n",
      "batch_idx: 7\n",
      "0.640625\n",
      "batch_idx: 8\n",
      "0.6342592592592593\n",
      "batch_idx: 9\n",
      "0.6416666666666667\n",
      "batch_idx: 10\n",
      "0.6401515151515151\n",
      "batch_idx: 11\n",
      "0.6319444444444444\n",
      "batch_idx: 12\n",
      "0.6282051282051282\n",
      "batch_idx: 13\n",
      "0.6339285714285714\n",
      "batch_idx: 14\n",
      "0.6333333333333333\n",
      "batch_idx: 15\n",
      "0.6276041666666666\n",
      "batch_idx: 16\n",
      "0.6348039215686274\n",
      "batch_idx: 17\n",
      "0.6388888888888888\n",
      "batch_idx: 18\n",
      "0.6337719298245614\n",
      "batch_idx: 19\n",
      "0.6333333333333333\n",
      "batch_idx: 20\n",
      "0.6309523809523809\n",
      "batch_idx: 21\n",
      "0.6363636363636364\n",
      "batch_idx: 22\n",
      "0.6358695652173914\n",
      "batch_idx: 23\n",
      "0.6336805555555556\n",
      "batch_idx: 24\n",
      "0.63\n",
      "Training Epoch: 215, total loss: 46.043512\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.6770833333333334\n",
      "batch_idx: 4\n",
      "0.6333333333333333\n",
      "batch_idx: 5\n",
      "0.6597222222222222\n",
      "batch_idx: 6\n",
      "0.6607142857142857\n",
      "batch_idx: 7\n",
      "0.6458333333333334\n",
      "batch_idx: 8\n",
      "0.6481481481481481\n",
      "batch_idx: 9\n",
      "0.6583333333333333\n",
      "batch_idx: 10\n",
      "0.6553030303030303\n",
      "batch_idx: 11\n",
      "0.6597222222222222\n",
      "batch_idx: 12\n",
      "0.6634615384615384\n",
      "batch_idx: 13\n",
      "0.6547619047619048\n",
      "batch_idx: 14\n",
      "0.6583333333333333\n",
      "batch_idx: 15\n",
      "0.65625\n",
      "batch_idx: 16\n",
      "0.6544117647058824\n",
      "batch_idx: 17\n",
      "0.6597222222222222\n",
      "batch_idx: 18\n",
      "0.6600877192982456\n",
      "batch_idx: 19\n",
      "0.6625\n",
      "batch_idx: 20\n",
      "0.6587301587301587\n",
      "batch_idx: 21\n",
      "0.6590909090909091\n",
      "batch_idx: 22\n",
      "0.657608695652174\n",
      "batch_idx: 23\n",
      "0.6597222222222222\n",
      "batch_idx: 24\n",
      "0.6616666666666666\n",
      "Training Epoch: 216, total loss: 45.622010\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.625\n",
      "batch_idx: 4\n",
      "0.6416666666666667\n",
      "batch_idx: 5\n",
      "0.6388888888888888\n",
      "batch_idx: 6\n",
      "0.6130952380952381\n",
      "batch_idx: 7\n",
      "0.6197916666666666\n",
      "batch_idx: 8\n",
      "0.625\n",
      "batch_idx: 9\n",
      "0.6416666666666667\n",
      "batch_idx: 10\n",
      "0.6325757575757576\n",
      "batch_idx: 11\n",
      "0.6319444444444444\n",
      "batch_idx: 12\n",
      "0.6282051282051282\n",
      "batch_idx: 13\n",
      "0.6279761904761905\n",
      "batch_idx: 14\n",
      "0.6305555555555555\n",
      "batch_idx: 15\n",
      "0.6354166666666666\n",
      "batch_idx: 16\n",
      "0.6421568627450981\n",
      "batch_idx: 17\n",
      "0.6412037037037037\n",
      "batch_idx: 18\n",
      "0.6491228070175439\n",
      "batch_idx: 19\n",
      "0.6416666666666667\n",
      "batch_idx: 20\n",
      "0.6349206349206349\n",
      "batch_idx: 21\n",
      "0.6363636363636364\n",
      "batch_idx: 22\n",
      "0.6394927536231884\n",
      "batch_idx: 23\n",
      "0.6371527777777778\n",
      "batch_idx: 24\n",
      "0.635\n",
      "Training Epoch: 217, total loss: 45.938520\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6597222222222222\n",
      "batch_idx: 6\n",
      "0.6726190476190477\n",
      "batch_idx: 7\n",
      "0.6666666666666666\n",
      "batch_idx: 8\n",
      "0.6805555555555556\n",
      "batch_idx: 9\n",
      "0.6708333333333333\n",
      "batch_idx: 10\n",
      "0.6666666666666666\n",
      "batch_idx: 11\n",
      "0.6701388888888888\n",
      "batch_idx: 12\n",
      "0.6730769230769231\n",
      "batch_idx: 13\n",
      "0.6845238095238095\n",
      "batch_idx: 14\n",
      "0.6944444444444444\n",
      "batch_idx: 15\n",
      "0.6901041666666666\n",
      "batch_idx: 16\n",
      "0.678921568627451\n",
      "batch_idx: 17\n",
      "0.6782407407407407\n",
      "batch_idx: 18\n",
      "0.6732456140350878\n",
      "batch_idx: 19\n",
      "0.675\n",
      "batch_idx: 20\n",
      "0.6706349206349206\n",
      "batch_idx: 21\n",
      "0.6590909090909091\n",
      "batch_idx: 22\n",
      "0.6539855072463768\n",
      "batch_idx: 23\n",
      "0.6527777777777778\n",
      "batch_idx: 24\n",
      "0.6483333333333333\n",
      "Training Epoch: 218, total loss: 45.780112\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6388888888888888\n",
      "batch_idx: 3\n",
      "0.6354166666666666\n",
      "batch_idx: 4\n",
      "0.6416666666666667\n",
      "batch_idx: 5\n",
      "0.6388888888888888\n",
      "batch_idx: 6\n",
      "0.6309523809523809\n",
      "batch_idx: 7\n",
      "0.609375\n",
      "batch_idx: 8\n",
      "0.6296296296296297\n",
      "batch_idx: 9\n",
      "0.6416666666666667\n",
      "batch_idx: 10\n",
      "0.6363636363636364\n",
      "batch_idx: 11\n",
      "0.6354166666666666\n",
      "batch_idx: 12\n",
      "0.6346153846153846\n",
      "batch_idx: 13\n",
      "0.6309523809523809\n",
      "batch_idx: 14\n",
      "0.6333333333333333\n",
      "batch_idx: 15\n",
      "0.6354166666666666\n",
      "batch_idx: 16\n",
      "0.6225490196078431\n",
      "batch_idx: 17\n",
      "0.6111111111111112\n",
      "batch_idx: 18\n",
      "0.6096491228070176\n",
      "batch_idx: 19\n",
      "0.6145833333333334\n",
      "batch_idx: 20\n",
      "0.6111111111111112\n",
      "batch_idx: 21\n",
      "0.6117424242424242\n",
      "batch_idx: 22\n",
      "0.6105072463768116\n",
      "batch_idx: 23\n",
      "0.6128472222222222\n",
      "batch_idx: 24\n",
      "0.6116666666666667\n",
      "Training Epoch: 219, total loss: 46.316168\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.6833333333333333\n",
      "batch_idx: 5\n",
      "0.6736111111111112\n",
      "batch_idx: 6\n",
      "0.6726190476190477\n",
      "batch_idx: 7\n",
      "0.65625\n",
      "batch_idx: 8\n",
      "0.6620370370370371\n",
      "batch_idx: 9\n",
      "0.6541666666666667\n",
      "batch_idx: 10\n",
      "0.6477272727272727\n",
      "batch_idx: 11\n",
      "0.6493055555555556\n",
      "batch_idx: 12\n",
      "0.657051282051282\n",
      "batch_idx: 13\n",
      "0.6607142857142857\n",
      "batch_idx: 14\n",
      "0.6583333333333333\n",
      "batch_idx: 15\n",
      "0.6510416666666666\n",
      "batch_idx: 16\n",
      "0.6544117647058824\n",
      "batch_idx: 17\n",
      "0.6504629629629629\n",
      "batch_idx: 18\n",
      "0.6513157894736842\n",
      "batch_idx: 19\n",
      "0.64375\n",
      "batch_idx: 20\n",
      "0.6408730158730159\n",
      "batch_idx: 21\n",
      "0.6420454545454546\n",
      "batch_idx: 22\n",
      "0.6431159420289855\n",
      "batch_idx: 23\n",
      "0.6440972222222222\n",
      "batch_idx: 24\n",
      "0.6433333333333333\n",
      "Training Epoch: 220, total loss: 45.911231\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.65625\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6805555555555556\n",
      "batch_idx: 6\n",
      "0.6547619047619048\n",
      "batch_idx: 7\n",
      "0.65625\n",
      "batch_idx: 8\n",
      "0.6388888888888888\n",
      "batch_idx: 9\n",
      "0.6208333333333333\n",
      "batch_idx: 10\n",
      "0.6098484848484849\n",
      "batch_idx: 11\n",
      "0.6180555555555556\n",
      "batch_idx: 12\n",
      "0.6057692307692307\n",
      "batch_idx: 13\n",
      "0.6130952380952381\n",
      "batch_idx: 14\n",
      "0.6083333333333333\n",
      "batch_idx: 15\n",
      "0.5989583333333334\n",
      "batch_idx: 16\n",
      "0.6053921568627451\n",
      "batch_idx: 17\n",
      "0.6064814814814815\n",
      "batch_idx: 18\n",
      "0.6052631578947368\n",
      "batch_idx: 19\n",
      "0.6083333333333333\n",
      "batch_idx: 20\n",
      "0.6091269841269841\n",
      "batch_idx: 21\n",
      "0.6098484848484849\n",
      "batch_idx: 22\n",
      "0.6105072463768116\n",
      "batch_idx: 23\n",
      "0.6076388888888888\n",
      "batch_idx: 24\n",
      "0.6083333333333333\n",
      "Training Epoch: 221, total loss: 46.523016\n",
      "batch_idx: 0\n",
      "0.4166666666666667\n",
      "batch_idx: 1\n",
      "0.5416666666666666\n",
      "batch_idx: 2\n",
      "0.5972222222222222\n",
      "batch_idx: 3\n",
      "0.625\n",
      "batch_idx: 4\n",
      "0.6333333333333333\n",
      "batch_idx: 5\n",
      "0.6527777777777778\n",
      "batch_idx: 6\n",
      "0.6488095238095238\n",
      "batch_idx: 7\n",
      "0.6458333333333334\n",
      "batch_idx: 8\n",
      "0.6620370370370371\n",
      "batch_idx: 9\n",
      "0.675\n",
      "batch_idx: 10\n",
      "0.6704545454545454\n",
      "batch_idx: 11\n",
      "0.6840277777777778\n",
      "batch_idx: 12\n",
      "0.6794871794871795\n",
      "batch_idx: 13\n",
      "0.6726190476190477\n",
      "batch_idx: 14\n",
      "0.6694444444444444\n",
      "batch_idx: 15\n",
      "0.6640625\n",
      "batch_idx: 16\n",
      "0.6666666666666666\n",
      "batch_idx: 17\n",
      "0.6550925925925926\n",
      "batch_idx: 18\n",
      "0.6666666666666666\n",
      "batch_idx: 19\n",
      "0.6625\n",
      "batch_idx: 20\n",
      "0.6666666666666666\n",
      "batch_idx: 21\n",
      "0.6666666666666666\n",
      "batch_idx: 22\n",
      "0.6630434782608695\n",
      "batch_idx: 23\n",
      "0.6631944444444444\n",
      "batch_idx: 24\n",
      "0.66\n",
      "Training Epoch: 222, total loss: 45.680264\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.6458333333333334\n",
      "batch_idx: 4\n",
      "0.6583333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 5\n",
      "0.6388888888888888\n",
      "batch_idx: 6\n",
      "0.6547619047619048\n",
      "batch_idx: 7\n",
      "0.6510416666666666\n",
      "batch_idx: 8\n",
      "0.6574074074074074\n",
      "batch_idx: 9\n",
      "0.6583333333333333\n",
      "batch_idx: 10\n",
      "0.6628787878787878\n",
      "batch_idx: 11\n",
      "0.6666666666666666\n",
      "batch_idx: 12\n",
      "0.6474358974358975\n",
      "batch_idx: 13\n",
      "0.6398809523809523\n",
      "batch_idx: 14\n",
      "0.6222222222222222\n",
      "batch_idx: 15\n",
      "0.6223958333333334\n",
      "batch_idx: 16\n",
      "0.625\n",
      "batch_idx: 17\n",
      "0.6226851851851852\n",
      "batch_idx: 18\n",
      "0.6293859649122807\n",
      "batch_idx: 19\n",
      "0.6291666666666667\n",
      "batch_idx: 20\n",
      "0.621031746031746\n",
      "batch_idx: 21\n",
      "0.6231060606060606\n",
      "batch_idx: 22\n",
      "0.6195652173913043\n",
      "batch_idx: 23\n",
      "0.6232638888888888\n",
      "batch_idx: 24\n",
      "0.63\n",
      "Training Epoch: 223, total loss: 45.849421\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.6458333333333334\n",
      "batch_idx: 4\n",
      "0.6666666666666666\n",
      "batch_idx: 5\n",
      "0.6597222222222222\n",
      "batch_idx: 6\n",
      "0.6666666666666666\n",
      "batch_idx: 7\n",
      "0.6666666666666666\n",
      "batch_idx: 8\n",
      "0.6805555555555556\n",
      "batch_idx: 9\n",
      "0.6916666666666667\n",
      "batch_idx: 10\n",
      "0.6856060606060606\n",
      "batch_idx: 11\n",
      "0.7013888888888888\n",
      "batch_idx: 12\n",
      "0.6858974358974359\n",
      "batch_idx: 13\n",
      "0.6755952380952381\n",
      "batch_idx: 14\n",
      "0.6833333333333333\n",
      "batch_idx: 15\n",
      "0.6744791666666666\n",
      "batch_idx: 16\n",
      "0.6862745098039216\n",
      "batch_idx: 17\n",
      "0.6782407407407407\n",
      "batch_idx: 18\n",
      "0.6666666666666666\n",
      "batch_idx: 19\n",
      "0.6729166666666667\n",
      "batch_idx: 20\n",
      "0.6746031746031746\n",
      "batch_idx: 21\n",
      "0.6666666666666666\n",
      "batch_idx: 22\n",
      "0.6684782608695652\n",
      "batch_idx: 23\n",
      "0.6631944444444444\n",
      "batch_idx: 24\n",
      "0.6666666666666666\n",
      "Training Epoch: 224, total loss: 45.447747\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.7\n",
      "batch_idx: 5\n",
      "0.6944444444444444\n",
      "batch_idx: 6\n",
      "0.7202380952380952\n",
      "batch_idx: 7\n",
      "0.703125\n",
      "batch_idx: 8\n",
      "0.6851851851851852\n",
      "batch_idx: 9\n",
      "0.7041666666666667\n",
      "batch_idx: 10\n",
      "0.7159090909090909\n",
      "batch_idx: 11\n",
      "0.7118055555555556\n",
      "batch_idx: 12\n",
      "0.7051282051282052\n",
      "batch_idx: 13\n",
      "0.7113095238095238\n",
      "batch_idx: 14\n",
      "0.7111111111111111\n",
      "batch_idx: 15\n",
      "0.7057291666666666\n",
      "batch_idx: 16\n",
      "0.7107843137254902\n",
      "batch_idx: 17\n",
      "0.7129629629629629\n",
      "batch_idx: 18\n",
      "0.7105263157894737\n",
      "batch_idx: 19\n",
      "0.70625\n",
      "batch_idx: 20\n",
      "0.7003968253968254\n",
      "batch_idx: 21\n",
      "0.7026515151515151\n",
      "batch_idx: 22\n",
      "0.6920289855072463\n",
      "batch_idx: 23\n",
      "0.6996527777777778\n",
      "batch_idx: 24\n",
      "0.6966666666666667\n",
      "Training Epoch: 225, total loss: 44.884713\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.7152777777777778\n",
      "batch_idx: 6\n",
      "0.7083333333333334\n",
      "batch_idx: 7\n",
      "0.6927083333333334\n",
      "batch_idx: 8\n",
      "0.6805555555555556\n",
      "batch_idx: 9\n",
      "0.6791666666666667\n",
      "batch_idx: 10\n",
      "0.6893939393939394\n",
      "batch_idx: 11\n",
      "0.6840277777777778\n",
      "batch_idx: 12\n",
      "0.6891025641025641\n",
      "batch_idx: 13\n",
      "0.6755952380952381\n",
      "batch_idx: 14\n",
      "0.6666666666666666\n",
      "batch_idx: 15\n",
      "0.6692708333333334\n",
      "batch_idx: 16\n",
      "0.6593137254901961\n",
      "batch_idx: 17\n",
      "0.6689814814814815\n",
      "batch_idx: 18\n",
      "0.6557017543859649\n",
      "batch_idx: 19\n",
      "0.65625\n",
      "batch_idx: 20\n",
      "0.6607142857142857\n",
      "batch_idx: 21\n",
      "0.6553030303030303\n",
      "batch_idx: 22\n",
      "0.6557971014492754\n",
      "batch_idx: 23\n",
      "0.6684027777777778\n",
      "batch_idx: 24\n",
      "0.6666666666666666\n",
      "Training Epoch: 226, total loss: 45.279502\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.6964285714285714\n",
      "batch_idx: 7\n",
      "0.6822916666666666\n",
      "batch_idx: 8\n",
      "0.6666666666666666\n",
      "batch_idx: 9\n",
      "0.6708333333333333\n",
      "batch_idx: 10\n",
      "0.6704545454545454\n",
      "batch_idx: 11\n",
      "0.6701388888888888\n",
      "batch_idx: 12\n",
      "0.6602564102564102\n",
      "batch_idx: 13\n",
      "0.6577380952380952\n",
      "batch_idx: 14\n",
      "0.6611111111111111\n",
      "batch_idx: 15\n",
      "0.6588541666666666\n",
      "batch_idx: 16\n",
      "0.6642156862745098\n",
      "batch_idx: 17\n",
      "0.6620370370370371\n",
      "batch_idx: 18\n",
      "0.668859649122807\n",
      "batch_idx: 19\n",
      "0.6604166666666667\n",
      "batch_idx: 20\n",
      "0.6567460317460317\n",
      "batch_idx: 21\n",
      "0.6534090909090909\n",
      "batch_idx: 22\n",
      "0.6557971014492754\n",
      "batch_idx: 23\n",
      "0.6597222222222222\n",
      "batch_idx: 24\n",
      "0.6583333333333333\n",
      "Training Epoch: 227, total loss: 45.613366\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.6458333333333334\n",
      "batch_idx: 4\n",
      "0.6666666666666666\n",
      "batch_idx: 5\n",
      "0.6527777777777778\n",
      "batch_idx: 6\n",
      "0.6488095238095238\n",
      "batch_idx: 7\n",
      "0.6510416666666666\n",
      "batch_idx: 8\n",
      "0.6481481481481481\n",
      "batch_idx: 9\n",
      "0.6541666666666667\n",
      "batch_idx: 10\n",
      "0.6590909090909091\n",
      "batch_idx: 11\n",
      "0.65625\n",
      "batch_idx: 12\n",
      "0.6634615384615384\n",
      "batch_idx: 13\n",
      "0.6577380952380952\n",
      "batch_idx: 14\n",
      "0.6611111111111111\n",
      "batch_idx: 15\n",
      "0.6536458333333334\n",
      "batch_idx: 16\n",
      "0.6568627450980392\n",
      "batch_idx: 17\n",
      "0.6550925925925926\n",
      "batch_idx: 18\n",
      "0.6622807017543859\n",
      "batch_idx: 19\n",
      "0.65625\n",
      "batch_idx: 20\n",
      "0.6527777777777778\n",
      "batch_idx: 21\n",
      "0.6496212121212122\n",
      "batch_idx: 22\n",
      "0.6413043478260869\n",
      "batch_idx: 23\n",
      "0.6440972222222222\n",
      "batch_idx: 24\n",
      "0.6466666666666666\n",
      "Training Epoch: 228, total loss: 45.724216\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.6388888888888888\n",
      "batch_idx: 3\n",
      "0.6354166666666666\n",
      "batch_idx: 4\n",
      "0.625\n",
      "batch_idx: 5\n",
      "0.6319444444444444\n",
      "batch_idx: 6\n",
      "0.6428571428571429\n",
      "batch_idx: 7\n",
      "0.6666666666666666\n",
      "batch_idx: 8\n",
      "0.6712962962962963\n",
      "batch_idx: 9\n",
      "0.6666666666666666\n",
      "batch_idx: 10\n",
      "0.6666666666666666\n",
      "batch_idx: 11\n",
      "0.6527777777777778\n",
      "batch_idx: 12\n",
      "0.6474358974358975\n",
      "batch_idx: 13\n",
      "0.6488095238095238\n",
      "batch_idx: 14\n",
      "0.6416666666666667\n",
      "batch_idx: 15\n",
      "0.6380208333333334\n",
      "batch_idx: 16\n",
      "0.6421568627450981\n",
      "batch_idx: 17\n",
      "0.6504629629629629\n",
      "batch_idx: 18\n",
      "0.6557017543859649\n",
      "batch_idx: 19\n",
      "0.6625\n",
      "batch_idx: 20\n",
      "0.6607142857142857\n",
      "batch_idx: 21\n",
      "0.6590909090909091\n",
      "batch_idx: 22\n",
      "0.6557971014492754\n",
      "batch_idx: 23\n",
      "0.6579861111111112\n",
      "batch_idx: 24\n",
      "0.6583333333333333\n",
      "Training Epoch: 229, total loss: 45.569613\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.5\n",
      "batch_idx: 2\n",
      "0.4861111111111111\n",
      "batch_idx: 3\n",
      "0.46875\n",
      "batch_idx: 4\n",
      "0.49166666666666664\n",
      "batch_idx: 5\n",
      "0.5277777777777778\n",
      "batch_idx: 6\n",
      "0.5535714285714286\n",
      "batch_idx: 7\n",
      "0.5677083333333334\n",
      "batch_idx: 8\n",
      "0.5648148148148148\n",
      "batch_idx: 9\n",
      "0.575\n",
      "batch_idx: 10\n",
      "0.5833333333333334\n",
      "batch_idx: 11\n",
      "0.6041666666666666\n",
      "batch_idx: 12\n",
      "0.5961538461538461\n",
      "batch_idx: 13\n",
      "0.6011904761904762\n",
      "batch_idx: 14\n",
      "0.6\n",
      "batch_idx: 15\n",
      "0.5989583333333334\n",
      "batch_idx: 16\n",
      "0.5955882352941176\n",
      "batch_idx: 17\n",
      "0.5972222222222222\n",
      "batch_idx: 18\n",
      "0.5964912280701754\n",
      "batch_idx: 19\n",
      "0.5979166666666667\n",
      "batch_idx: 20\n",
      "0.5972222222222222\n",
      "batch_idx: 21\n",
      "0.6022727272727273\n",
      "batch_idx: 22\n",
      "0.6032608695652174\n",
      "batch_idx: 23\n",
      "0.6059027777777778\n",
      "batch_idx: 24\n",
      "0.6083333333333333\n",
      "Training Epoch: 230, total loss: 46.464318\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7013888888888888\n",
      "batch_idx: 6\n",
      "0.6547619047619048\n",
      "batch_idx: 7\n",
      "0.6666666666666666\n",
      "batch_idx: 8\n",
      "0.6759259259259259\n",
      "batch_idx: 9\n",
      "0.675\n",
      "batch_idx: 10\n",
      "0.6818181818181818\n",
      "batch_idx: 11\n",
      "0.6909722222222222\n",
      "batch_idx: 12\n",
      "0.6955128205128205\n",
      "batch_idx: 13\n",
      "0.6845238095238095\n",
      "batch_idx: 14\n",
      "0.6833333333333333\n",
      "batch_idx: 15\n",
      "0.6796875\n",
      "batch_idx: 16\n",
      "0.678921568627451\n",
      "batch_idx: 17\n",
      "0.6736111111111112\n",
      "batch_idx: 18\n",
      "0.6732456140350878\n",
      "batch_idx: 19\n",
      "0.6708333333333333\n",
      "batch_idx: 20\n",
      "0.6746031746031746\n",
      "batch_idx: 21\n",
      "0.6742424242424242\n",
      "batch_idx: 22\n",
      "0.6757246376811594\n",
      "batch_idx: 23\n",
      "0.6753472222222222\n",
      "batch_idx: 24\n",
      "0.675\n",
      "Training Epoch: 231, total loss: 45.268729\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.6388888888888888\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.7\n",
      "batch_idx: 5\n",
      "0.6875\n",
      "batch_idx: 6\n",
      "0.6845238095238095\n",
      "batch_idx: 7\n",
      "0.6875\n",
      "batch_idx: 8\n",
      "0.6620370370370371\n",
      "batch_idx: 9\n",
      "0.6583333333333333\n",
      "batch_idx: 10\n",
      "0.6515151515151515\n",
      "batch_idx: 11\n",
      "0.6493055555555556\n",
      "batch_idx: 12\n",
      "0.6474358974358975\n",
      "batch_idx: 13\n",
      "0.6547619047619048\n",
      "batch_idx: 14\n",
      "0.6666666666666666\n",
      "batch_idx: 15\n",
      "0.6614583333333334\n",
      "batch_idx: 16\n",
      "0.6691176470588235\n",
      "batch_idx: 17\n",
      "0.6689814814814815\n",
      "batch_idx: 18\n",
      "0.6710526315789473\n",
      "batch_idx: 19\n",
      "0.675\n",
      "batch_idx: 20\n",
      "0.6726190476190477\n",
      "batch_idx: 21\n",
      "0.6666666666666666\n",
      "batch_idx: 22\n",
      "0.6666666666666666\n",
      "batch_idx: 23\n",
      "0.6684027777777778\n",
      "batch_idx: 24\n",
      "0.6716666666666666\n",
      "Training Epoch: 232, total loss: 45.484447\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.6770833333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6527777777777778\n",
      "batch_idx: 6\n",
      "0.6726190476190477\n",
      "batch_idx: 7\n",
      "0.6822916666666666\n",
      "batch_idx: 8\n",
      "0.6759259259259259\n",
      "batch_idx: 9\n",
      "0.6791666666666667\n",
      "batch_idx: 10\n",
      "0.6704545454545454\n",
      "batch_idx: 11\n",
      "0.6701388888888888\n",
      "batch_idx: 12\n",
      "0.6698717948717948\n",
      "batch_idx: 13\n",
      "0.6666666666666666\n",
      "batch_idx: 14\n",
      "0.675\n",
      "batch_idx: 15\n",
      "0.6822916666666666\n",
      "batch_idx: 16\n",
      "0.6862745098039216\n",
      "batch_idx: 17\n",
      "0.6828703703703703\n",
      "batch_idx: 18\n",
      "0.6754385964912281\n",
      "batch_idx: 19\n",
      "0.675\n",
      "batch_idx: 20\n",
      "0.6785714285714286\n",
      "batch_idx: 21\n",
      "0.6818181818181818\n",
      "batch_idx: 22\n",
      "0.6757246376811594\n",
      "batch_idx: 23\n",
      "0.6684027777777778\n",
      "batch_idx: 24\n",
      "0.665\n",
      "Training Epoch: 233, total loss: 45.417318\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.6770833333333334\n",
      "batch_idx: 4\n",
      "0.6416666666666667\n",
      "batch_idx: 5\n",
      "0.6736111111111112\n",
      "batch_idx: 6\n",
      "0.6666666666666666\n",
      "batch_idx: 7\n",
      "0.6666666666666666\n",
      "batch_idx: 8\n",
      "0.6944444444444444\n",
      "batch_idx: 9\n",
      "0.6833333333333333\n",
      "batch_idx: 10\n",
      "0.6856060606060606\n",
      "batch_idx: 11\n",
      "0.6666666666666666\n",
      "batch_idx: 12\n",
      "0.6634615384615384\n",
      "batch_idx: 13\n",
      "0.6577380952380952\n",
      "batch_idx: 14\n",
      "0.6611111111111111\n",
      "batch_idx: 15\n",
      "0.671875\n",
      "batch_idx: 16\n",
      "0.678921568627451\n",
      "batch_idx: 17\n",
      "0.6828703703703703\n",
      "batch_idx: 18\n",
      "0.6732456140350878\n",
      "batch_idx: 19\n",
      "0.6708333333333333\n",
      "batch_idx: 20\n",
      "0.6626984126984127\n",
      "batch_idx: 21\n",
      "0.6571969696969697\n",
      "batch_idx: 22\n",
      "0.6539855072463768\n",
      "batch_idx: 23\n",
      "0.6527777777777778\n",
      "batch_idx: 24\n",
      "0.6516666666666666\n",
      "Training Epoch: 234, total loss: 45.760815\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.5625\n",
      "batch_idx: 2\n",
      "0.6388888888888888\n",
      "batch_idx: 3\n",
      "0.6354166666666666\n",
      "batch_idx: 4\n",
      "0.6333333333333333\n",
      "batch_idx: 5\n",
      "0.6458333333333334\n",
      "batch_idx: 6\n",
      "0.6369047619047619\n",
      "batch_idx: 7\n",
      "0.6458333333333334\n",
      "batch_idx: 8\n",
      "0.6527777777777778\n",
      "batch_idx: 9\n",
      "0.6541666666666667\n",
      "batch_idx: 10\n",
      "0.6553030303030303\n",
      "batch_idx: 11\n",
      "0.6527777777777778\n",
      "batch_idx: 12\n",
      "0.6506410256410257\n",
      "batch_idx: 13\n",
      "0.6517857142857143\n",
      "batch_idx: 14\n",
      "0.6527777777777778\n",
      "batch_idx: 15\n",
      "0.6510416666666666\n",
      "batch_idx: 16\n",
      "0.6470588235294118\n",
      "batch_idx: 17\n",
      "0.6550925925925926\n",
      "batch_idx: 18\n",
      "0.6513157894736842\n",
      "batch_idx: 19\n",
      "0.6458333333333334\n",
      "batch_idx: 20\n",
      "0.6428571428571429\n",
      "batch_idx: 21\n",
      "0.6477272727272727\n",
      "batch_idx: 22\n",
      "0.644927536231884\n",
      "batch_idx: 23\n",
      "0.640625\n",
      "batch_idx: 24\n",
      "0.6416666666666667\n",
      "Training Epoch: 235, total loss: 45.871271\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.6458333333333334\n",
      "batch_idx: 4\n",
      "0.6416666666666667\n",
      "batch_idx: 5\n",
      "0.6388888888888888\n",
      "batch_idx: 6\n",
      "0.6666666666666666\n",
      "batch_idx: 7\n",
      "0.6510416666666666\n",
      "batch_idx: 8\n",
      "0.6342592592592593\n",
      "batch_idx: 9\n",
      "0.6375\n",
      "batch_idx: 10\n",
      "0.6477272727272727\n",
      "batch_idx: 11\n",
      "0.6597222222222222\n",
      "batch_idx: 12\n",
      "0.6602564102564102\n",
      "batch_idx: 13\n",
      "0.6577380952380952\n",
      "batch_idx: 14\n",
      "0.6666666666666666\n",
      "batch_idx: 15\n",
      "0.6692708333333334\n",
      "batch_idx: 16\n",
      "0.6617647058823529\n",
      "batch_idx: 17\n",
      "0.6574074074074074\n",
      "batch_idx: 18\n",
      "0.6622807017543859\n",
      "batch_idx: 19\n",
      "0.6645833333333333\n",
      "batch_idx: 20\n",
      "0.6666666666666666\n",
      "batch_idx: 21\n",
      "0.6704545454545454\n",
      "batch_idx: 22\n",
      "0.6684782608695652\n",
      "batch_idx: 23\n",
      "0.6684027777777778\n",
      "batch_idx: 24\n",
      "0.67\n",
      "Training Epoch: 236, total loss: 45.307850\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.6388888888888888\n",
      "batch_idx: 3\n",
      "0.6041666666666666\n",
      "batch_idx: 4\n",
      "0.6166666666666667\n",
      "batch_idx: 5\n",
      "0.6111111111111112\n",
      "batch_idx: 6\n",
      "0.6369047619047619\n",
      "batch_idx: 7\n",
      "0.640625\n",
      "batch_idx: 8\n",
      "0.6342592592592593\n",
      "batch_idx: 9\n",
      "0.6333333333333333\n",
      "batch_idx: 10\n",
      "0.6401515151515151\n",
      "batch_idx: 11\n",
      "0.6493055555555556\n",
      "batch_idx: 12\n",
      "0.6506410256410257\n",
      "batch_idx: 13\n",
      "0.6458333333333334\n",
      "batch_idx: 14\n",
      "0.6472222222222223\n",
      "batch_idx: 15\n",
      "0.6510416666666666\n",
      "batch_idx: 16\n",
      "0.6568627450980392\n",
      "batch_idx: 17\n",
      "0.6689814814814815\n",
      "batch_idx: 18\n",
      "0.668859649122807\n",
      "batch_idx: 19\n",
      "0.66875\n",
      "batch_idx: 20\n",
      "0.6726190476190477\n",
      "batch_idx: 21\n",
      "0.6761363636363636\n",
      "batch_idx: 22\n",
      "0.6793478260869565\n",
      "batch_idx: 23\n",
      "0.6753472222222222\n",
      "batch_idx: 24\n",
      "0.6716666666666666\n",
      "Training Epoch: 237, total loss: 45.406367\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.6944444444444444\n",
      "batch_idx: 6\n",
      "0.6904761904761905\n",
      "batch_idx: 7\n",
      "0.6979166666666666\n",
      "batch_idx: 8\n",
      "0.6712962962962963\n",
      "batch_idx: 9\n",
      "0.6791666666666667\n",
      "batch_idx: 10\n",
      "0.6742424242424242\n",
      "batch_idx: 11\n",
      "0.6770833333333334\n",
      "batch_idx: 12\n",
      "0.6762820512820513\n",
      "batch_idx: 13\n",
      "0.6666666666666666\n",
      "batch_idx: 14\n",
      "0.675\n",
      "batch_idx: 15\n",
      "0.6770833333333334\n",
      "batch_idx: 16\n",
      "0.6740196078431373\n",
      "batch_idx: 17\n",
      "0.6759259259259259\n",
      "batch_idx: 18\n",
      "0.6776315789473685\n",
      "batch_idx: 19\n",
      "0.6770833333333334\n",
      "batch_idx: 20\n",
      "0.6825396825396826\n",
      "batch_idx: 21\n",
      "0.6856060606060606\n",
      "batch_idx: 22\n",
      "0.6884057971014492\n",
      "batch_idx: 23\n",
      "0.6961805555555556\n",
      "batch_idx: 24\n",
      "0.69\n",
      "Training Epoch: 238, total loss: 44.994135\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.6875\n",
      "batch_idx: 6\n",
      "0.6726190476190477\n",
      "batch_idx: 7\n",
      "0.6614583333333334\n",
      "batch_idx: 8\n",
      "0.6620370370370371\n",
      "batch_idx: 9\n",
      "0.6708333333333333\n",
      "batch_idx: 10\n",
      "0.6590909090909091\n",
      "batch_idx: 11\n",
      "0.6666666666666666\n",
      "batch_idx: 12\n",
      "0.6634615384615384\n",
      "batch_idx: 13\n",
      "0.6547619047619048\n",
      "batch_idx: 14\n",
      "0.6583333333333333\n",
      "batch_idx: 15\n",
      "0.6640625\n",
      "batch_idx: 16\n",
      "0.6617647058823529\n",
      "batch_idx: 17\n",
      "0.6666666666666666\n",
      "batch_idx: 18\n",
      "0.6644736842105263\n",
      "batch_idx: 19\n",
      "0.66875\n",
      "batch_idx: 20\n",
      "0.6706349206349206\n",
      "batch_idx: 21\n",
      "0.6761363636363636\n",
      "batch_idx: 22\n",
      "0.6702898550724637\n",
      "batch_idx: 23\n",
      "0.671875\n",
      "batch_idx: 24\n",
      "0.67\n",
      "Training Epoch: 239, total loss: 45.348623\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.6666666666666666\n",
      "batch_idx: 4\n",
      "0.6666666666666666\n",
      "batch_idx: 5\n",
      "0.6736111111111112\n",
      "batch_idx: 6\n",
      "0.6726190476190477\n",
      "batch_idx: 7\n",
      "0.6666666666666666\n",
      "batch_idx: 8\n",
      "0.6574074074074074\n",
      "batch_idx: 9\n",
      "0.6708333333333333\n",
      "batch_idx: 10\n",
      "0.6666666666666666\n",
      "batch_idx: 11\n",
      "0.6631944444444444\n",
      "batch_idx: 12\n",
      "0.6698717948717948\n",
      "batch_idx: 13\n",
      "0.6666666666666666\n",
      "batch_idx: 14\n",
      "0.6611111111111111\n",
      "batch_idx: 15\n",
      "0.6588541666666666\n",
      "batch_idx: 16\n",
      "0.6642156862745098\n",
      "batch_idx: 17\n",
      "0.6574074074074074\n",
      "batch_idx: 18\n",
      "0.6535087719298246\n",
      "batch_idx: 19\n",
      "0.6541666666666667\n",
      "batch_idx: 20\n",
      "0.6468253968253969\n",
      "batch_idx: 21\n",
      "0.6439393939393939\n",
      "batch_idx: 22\n",
      "0.6431159420289855\n",
      "batch_idx: 23\n",
      "0.6527777777777778\n",
      "batch_idx: 24\n",
      "0.6616666666666666\n",
      "Training Epoch: 240, total loss: 45.348859\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.65625\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6666666666666666\n",
      "batch_idx: 6\n",
      "0.6904761904761905\n",
      "batch_idx: 7\n",
      "0.671875\n",
      "batch_idx: 8\n",
      "0.6620370370370371\n",
      "batch_idx: 9\n",
      "0.6625\n",
      "batch_idx: 10\n",
      "0.6628787878787878\n",
      "batch_idx: 11\n",
      "0.6631944444444444\n",
      "batch_idx: 12\n",
      "0.6506410256410257\n",
      "batch_idx: 13\n",
      "0.6517857142857143\n",
      "batch_idx: 14\n",
      "0.6527777777777778\n",
      "batch_idx: 15\n",
      "0.65625\n",
      "batch_idx: 16\n",
      "0.6544117647058824\n",
      "batch_idx: 17\n",
      "0.6574074074074074\n",
      "batch_idx: 18\n",
      "0.6535087719298246\n",
      "batch_idx: 19\n",
      "0.6520833333333333\n",
      "batch_idx: 20\n",
      "0.6547619047619048\n",
      "batch_idx: 21\n",
      "0.6534090909090909\n",
      "batch_idx: 22\n",
      "0.657608695652174\n",
      "batch_idx: 23\n",
      "0.6597222222222222\n",
      "batch_idx: 24\n",
      "0.6583333333333333\n",
      "Training Epoch: 241, total loss: 45.584439\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.6666666666666666\n",
      "batch_idx: 4\n",
      "0.7\n",
      "batch_idx: 5\n",
      "0.6875\n",
      "batch_idx: 6\n",
      "0.6666666666666666\n",
      "batch_idx: 7\n",
      "0.6666666666666666\n",
      "batch_idx: 8\n",
      "0.6666666666666666\n",
      "batch_idx: 9\n",
      "0.675\n",
      "batch_idx: 10\n",
      "0.6818181818181818\n",
      "batch_idx: 11\n",
      "0.6875\n",
      "batch_idx: 12\n",
      "0.6923076923076923\n",
      "batch_idx: 13\n",
      "0.6904761904761905\n",
      "batch_idx: 14\n",
      "0.7027777777777777\n",
      "batch_idx: 15\n",
      "0.703125\n",
      "batch_idx: 16\n",
      "0.7009803921568627\n",
      "batch_idx: 17\n",
      "0.6990740740740741\n",
      "batch_idx: 18\n",
      "0.706140350877193\n",
      "batch_idx: 19\n",
      "0.7083333333333334\n",
      "batch_idx: 20\n",
      "0.7103174603174603\n",
      "batch_idx: 21\n",
      "0.7159090909090909\n",
      "batch_idx: 22\n",
      "0.7137681159420289\n",
      "batch_idx: 23\n",
      "0.703125\n",
      "batch_idx: 24\n",
      "0.6966666666666667\n",
      "Training Epoch: 242, total loss: 44.930049\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.6805555555555556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 6\n",
      "0.6904761904761905\n",
      "batch_idx: 7\n",
      "0.703125\n",
      "batch_idx: 8\n",
      "0.6851851851851852\n",
      "batch_idx: 9\n",
      "0.6875\n",
      "batch_idx: 10\n",
      "0.6856060606060606\n",
      "batch_idx: 11\n",
      "0.7013888888888888\n",
      "batch_idx: 12\n",
      "0.717948717948718\n",
      "batch_idx: 13\n",
      "0.7172619047619048\n",
      "batch_idx: 14\n",
      "0.7222222222222222\n",
      "batch_idx: 15\n",
      "0.7239583333333334\n",
      "batch_idx: 16\n",
      "0.7132352941176471\n",
      "batch_idx: 17\n",
      "0.7060185185185185\n",
      "batch_idx: 18\n",
      "0.7039473684210527\n",
      "batch_idx: 19\n",
      "0.7\n",
      "batch_idx: 20\n",
      "0.6944444444444444\n",
      "batch_idx: 21\n",
      "0.6893939393939394\n",
      "batch_idx: 22\n",
      "0.6865942028985508\n",
      "batch_idx: 23\n",
      "0.6892361111111112\n",
      "batch_idx: 24\n",
      "0.69\n",
      "Training Epoch: 243, total loss: 44.951758\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.7013888888888888\n",
      "batch_idx: 6\n",
      "0.6845238095238095\n",
      "batch_idx: 7\n",
      "0.6927083333333334\n",
      "batch_idx: 8\n",
      "0.6898148148148148\n",
      "batch_idx: 9\n",
      "0.7041666666666667\n",
      "batch_idx: 10\n",
      "0.7007575757575758\n",
      "batch_idx: 11\n",
      "0.6944444444444444\n",
      "batch_idx: 12\n",
      "0.6987179487179487\n",
      "batch_idx: 13\n",
      "0.6994047619047619\n",
      "batch_idx: 14\n",
      "0.6944444444444444\n",
      "batch_idx: 15\n",
      "0.703125\n",
      "batch_idx: 16\n",
      "0.7034313725490197\n",
      "batch_idx: 17\n",
      "0.7037037037037037\n",
      "batch_idx: 18\n",
      "0.706140350877193\n",
      "batch_idx: 19\n",
      "0.70625\n",
      "batch_idx: 20\n",
      "0.7083333333333334\n",
      "batch_idx: 21\n",
      "0.7083333333333334\n",
      "batch_idx: 22\n",
      "0.7101449275362319\n",
      "batch_idx: 23\n",
      "0.7083333333333334\n",
      "batch_idx: 24\n",
      "0.7\n",
      "Training Epoch: 244, total loss: 44.637265\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.6111111111111112\n",
      "batch_idx: 3\n",
      "0.65625\n",
      "batch_idx: 4\n",
      "0.6416666666666667\n",
      "batch_idx: 5\n",
      "0.6527777777777778\n",
      "batch_idx: 6\n",
      "0.6666666666666666\n",
      "batch_idx: 7\n",
      "0.6666666666666666\n",
      "batch_idx: 8\n",
      "0.6712962962962963\n",
      "batch_idx: 9\n",
      "0.6708333333333333\n",
      "batch_idx: 10\n",
      "0.6742424242424242\n",
      "batch_idx: 11\n",
      "0.6666666666666666\n",
      "batch_idx: 12\n",
      "0.6762820512820513\n",
      "batch_idx: 13\n",
      "0.6666666666666666\n",
      "batch_idx: 14\n",
      "0.6555555555555556\n",
      "batch_idx: 15\n",
      "0.6588541666666666\n",
      "batch_idx: 16\n",
      "0.6617647058823529\n",
      "batch_idx: 17\n",
      "0.6666666666666666\n",
      "batch_idx: 18\n",
      "0.668859649122807\n",
      "batch_idx: 19\n",
      "0.6625\n",
      "batch_idx: 20\n",
      "0.6626984126984127\n",
      "batch_idx: 21\n",
      "0.6666666666666666\n",
      "batch_idx: 22\n",
      "0.6757246376811594\n",
      "batch_idx: 23\n",
      "0.6822916666666666\n",
      "batch_idx: 24\n",
      "0.685\n",
      "Training Epoch: 245, total loss: 44.872414\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.5625\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.6666666666666666\n",
      "batch_idx: 4\n",
      "0.6666666666666666\n",
      "batch_idx: 5\n",
      "0.6944444444444444\n",
      "batch_idx: 6\n",
      "0.7142857142857143\n",
      "batch_idx: 7\n",
      "0.7083333333333334\n",
      "batch_idx: 8\n",
      "0.7129629629629629\n",
      "batch_idx: 9\n",
      "0.7041666666666667\n",
      "batch_idx: 10\n",
      "0.7007575757575758\n",
      "batch_idx: 11\n",
      "0.7083333333333334\n",
      "batch_idx: 12\n",
      "0.7115384615384616\n",
      "batch_idx: 13\n",
      "0.7113095238095238\n",
      "batch_idx: 14\n",
      "0.7055555555555556\n",
      "batch_idx: 15\n",
      "0.703125\n",
      "batch_idx: 16\n",
      "0.6887254901960784\n",
      "batch_idx: 17\n",
      "0.6898148148148148\n",
      "batch_idx: 18\n",
      "0.6929824561403509\n",
      "batch_idx: 19\n",
      "0.6916666666666667\n",
      "batch_idx: 20\n",
      "0.6884920634920635\n",
      "batch_idx: 21\n",
      "0.6893939393939394\n",
      "batch_idx: 22\n",
      "0.6938405797101449\n",
      "batch_idx: 23\n",
      "0.7013888888888888\n",
      "batch_idx: 24\n",
      "0.7\n",
      "Training Epoch: 246, total loss: 44.737918\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.6458333333333334\n",
      "batch_idx: 4\n",
      "0.6583333333333333\n",
      "batch_idx: 5\n",
      "0.6944444444444444\n",
      "batch_idx: 6\n",
      "0.6726190476190477\n",
      "batch_idx: 7\n",
      "0.6614583333333334\n",
      "batch_idx: 8\n",
      "0.6759259259259259\n",
      "batch_idx: 9\n",
      "0.6875\n",
      "batch_idx: 10\n",
      "0.6818181818181818\n",
      "batch_idx: 11\n",
      "0.6736111111111112\n",
      "batch_idx: 12\n",
      "0.6858974358974359\n",
      "batch_idx: 13\n",
      "0.6904761904761905\n",
      "batch_idx: 14\n",
      "0.6861111111111111\n",
      "batch_idx: 15\n",
      "0.6875\n",
      "batch_idx: 16\n",
      "0.6862745098039216\n",
      "batch_idx: 17\n",
      "0.6898148148148148\n",
      "batch_idx: 18\n",
      "0.6929824561403509\n",
      "batch_idx: 19\n",
      "0.69375\n",
      "batch_idx: 20\n",
      "0.6984126984126984\n",
      "batch_idx: 21\n",
      "0.6931818181818182\n",
      "batch_idx: 22\n",
      "0.6938405797101449\n",
      "batch_idx: 23\n",
      "0.6979166666666666\n",
      "batch_idx: 24\n",
      "0.6966666666666667\n",
      "Training Epoch: 247, total loss: 44.820388\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.7\n",
      "batch_idx: 5\n",
      "0.6736111111111112\n",
      "batch_idx: 6\n",
      "0.6785714285714286\n",
      "batch_idx: 7\n",
      "0.671875\n",
      "batch_idx: 8\n",
      "0.6574074074074074\n",
      "batch_idx: 9\n",
      "0.675\n",
      "batch_idx: 10\n",
      "0.6856060606060606\n",
      "batch_idx: 11\n",
      "0.6875\n",
      "batch_idx: 12\n",
      "0.6794871794871795\n",
      "batch_idx: 13\n",
      "0.6815476190476191\n",
      "batch_idx: 14\n",
      "0.6722222222222223\n",
      "batch_idx: 15\n",
      "0.6692708333333334\n",
      "batch_idx: 16\n",
      "0.678921568627451\n",
      "batch_idx: 17\n",
      "0.6875\n",
      "batch_idx: 18\n",
      "0.6820175438596491\n",
      "batch_idx: 19\n",
      "0.6875\n",
      "batch_idx: 20\n",
      "0.6944444444444444\n",
      "batch_idx: 21\n",
      "0.6893939393939394\n",
      "batch_idx: 22\n",
      "0.697463768115942\n",
      "batch_idx: 23\n",
      "0.6927083333333334\n",
      "batch_idx: 24\n",
      "0.695\n",
      "Training Epoch: 248, total loss: 44.832146\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.71875\n",
      "batch_idx: 8\n",
      "0.7083333333333334\n",
      "batch_idx: 9\n",
      "0.7\n",
      "batch_idx: 10\n",
      "0.7007575757575758\n",
      "batch_idx: 11\n",
      "0.6875\n",
      "batch_idx: 12\n",
      "0.6955128205128205\n",
      "batch_idx: 13\n",
      "0.6815476190476191\n",
      "batch_idx: 14\n",
      "0.6694444444444444\n",
      "batch_idx: 15\n",
      "0.6588541666666666\n",
      "batch_idx: 16\n",
      "0.6617647058823529\n",
      "batch_idx: 17\n",
      "0.6736111111111112\n",
      "batch_idx: 18\n",
      "0.668859649122807\n",
      "batch_idx: 19\n",
      "0.6708333333333333\n",
      "batch_idx: 20\n",
      "0.6765873015873016\n",
      "batch_idx: 21\n",
      "0.6799242424242424\n",
      "batch_idx: 22\n",
      "0.6865942028985508\n",
      "batch_idx: 23\n",
      "0.6892361111111112\n",
      "batch_idx: 24\n",
      "0.6933333333333334\n",
      "Training Epoch: 249, total loss: 44.845508\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6875\n",
      "batch_idx: 6\n",
      "0.7142857142857143\n",
      "batch_idx: 7\n",
      "0.6979166666666666\n",
      "batch_idx: 8\n",
      "0.7037037037037037\n",
      "batch_idx: 9\n",
      "0.6791666666666667\n",
      "batch_idx: 10\n",
      "0.6893939393939394\n",
      "batch_idx: 11\n",
      "0.6909722222222222\n",
      "batch_idx: 12\n",
      "0.6955128205128205\n",
      "batch_idx: 13\n",
      "0.7023809523809523\n",
      "batch_idx: 14\n",
      "0.6944444444444444\n",
      "batch_idx: 15\n",
      "0.6953125\n",
      "batch_idx: 16\n",
      "0.6838235294117647\n",
      "batch_idx: 17\n",
      "0.6898148148148148\n",
      "batch_idx: 18\n",
      "0.6929824561403509\n",
      "batch_idx: 19\n",
      "0.6875\n",
      "batch_idx: 20\n",
      "0.6904761904761905\n",
      "batch_idx: 21\n",
      "0.6912878787878788\n",
      "batch_idx: 22\n",
      "0.6956521739130435\n",
      "batch_idx: 23\n",
      "0.6979166666666666\n",
      "batch_idx: 24\n",
      "0.705\n",
      "Training Epoch: 250, total loss: 44.519302\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.6770833333333334\n",
      "batch_idx: 4\n",
      "0.6833333333333333\n",
      "batch_idx: 5\n",
      "0.6944444444444444\n",
      "batch_idx: 6\n",
      "0.6845238095238095\n",
      "batch_idx: 7\n",
      "0.6822916666666666\n",
      "batch_idx: 8\n",
      "0.6666666666666666\n",
      "batch_idx: 9\n",
      "0.6833333333333333\n",
      "batch_idx: 10\n",
      "0.6666666666666666\n",
      "batch_idx: 11\n",
      "0.6527777777777778\n",
      "batch_idx: 12\n",
      "0.6506410256410257\n",
      "batch_idx: 13\n",
      "0.6488095238095238\n",
      "batch_idx: 14\n",
      "0.6611111111111111\n",
      "batch_idx: 15\n",
      "0.6640625\n",
      "batch_idx: 16\n",
      "0.6617647058823529\n",
      "batch_idx: 17\n",
      "0.6504629629629629\n",
      "batch_idx: 18\n",
      "0.6535087719298246\n",
      "batch_idx: 19\n",
      "0.65625\n",
      "batch_idx: 20\n",
      "0.6607142857142857\n",
      "batch_idx: 21\n",
      "0.6609848484848485\n",
      "batch_idx: 22\n",
      "0.6612318840579711\n",
      "batch_idx: 23\n",
      "0.6545138888888888\n",
      "batch_idx: 24\n",
      "0.6583333333333333\n",
      "Training Epoch: 251, total loss: 45.367976\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7\n",
      "batch_idx: 5\n",
      "0.6875\n",
      "batch_idx: 6\n",
      "0.6964285714285714\n",
      "batch_idx: 7\n",
      "0.6979166666666666\n",
      "batch_idx: 8\n",
      "0.7083333333333334\n",
      "batch_idx: 9\n",
      "0.7041666666666667\n",
      "batch_idx: 10\n",
      "0.696969696969697\n",
      "batch_idx: 11\n",
      "0.6909722222222222\n",
      "batch_idx: 12\n",
      "0.6923076923076923\n",
      "batch_idx: 13\n",
      "0.6964285714285714\n",
      "batch_idx: 14\n",
      "0.6944444444444444\n",
      "batch_idx: 15\n",
      "0.6979166666666666\n",
      "batch_idx: 16\n",
      "0.6911764705882353\n",
      "batch_idx: 17\n",
      "0.6828703703703703\n",
      "batch_idx: 18\n",
      "0.6864035087719298\n",
      "batch_idx: 19\n",
      "0.6833333333333333\n",
      "batch_idx: 20\n",
      "0.6865079365079365\n",
      "batch_idx: 21\n",
      "0.6893939393939394\n",
      "batch_idx: 22\n",
      "0.6884057971014492\n",
      "batch_idx: 23\n",
      "0.6822916666666666\n",
      "batch_idx: 24\n",
      "0.6783333333333333\n",
      "Training Epoch: 252, total loss: 45.060895\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.6770833333333334\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.7013888888888888\n",
      "batch_idx: 6\n",
      "0.6904761904761905\n",
      "batch_idx: 7\n",
      "0.6770833333333334\n",
      "batch_idx: 8\n",
      "0.6944444444444444\n",
      "batch_idx: 9\n",
      "0.6791666666666667\n",
      "batch_idx: 10\n",
      "0.6856060606060606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 11\n",
      "0.6909722222222222\n",
      "batch_idx: 12\n",
      "0.7115384615384616\n",
      "batch_idx: 13\n",
      "0.6964285714285714\n",
      "batch_idx: 14\n",
      "0.6944444444444444\n",
      "batch_idx: 15\n",
      "0.6875\n",
      "batch_idx: 16\n",
      "0.6985294117647058\n",
      "batch_idx: 17\n",
      "0.7037037037037037\n",
      "batch_idx: 18\n",
      "0.706140350877193\n",
      "batch_idx: 19\n",
      "0.7020833333333333\n",
      "batch_idx: 20\n",
      "0.7023809523809523\n",
      "batch_idx: 21\n",
      "0.6988636363636364\n",
      "batch_idx: 22\n",
      "0.6884057971014492\n",
      "batch_idx: 23\n",
      "0.6909722222222222\n",
      "batch_idx: 24\n",
      "0.695\n",
      "Training Epoch: 253, total loss: 44.819788\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7440476190476191\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7546296296296297\n",
      "batch_idx: 9\n",
      "0.7666666666666667\n",
      "batch_idx: 10\n",
      "0.7651515151515151\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.7756410256410257\n",
      "batch_idx: 13\n",
      "0.7678571428571429\n",
      "batch_idx: 14\n",
      "0.7583333333333333\n",
      "batch_idx: 15\n",
      "0.7421875\n",
      "batch_idx: 16\n",
      "0.7377450980392157\n",
      "batch_idx: 17\n",
      "0.7361111111111112\n",
      "batch_idx: 18\n",
      "0.7368421052631579\n",
      "batch_idx: 19\n",
      "0.7395833333333334\n",
      "batch_idx: 20\n",
      "0.7341269841269841\n",
      "batch_idx: 21\n",
      "0.7367424242424242\n",
      "batch_idx: 22\n",
      "0.7355072463768116\n",
      "batch_idx: 23\n",
      "0.7309027777777778\n",
      "batch_idx: 24\n",
      "0.7233333333333334\n",
      "Training Epoch: 254, total loss: 44.338715\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7314814814814815\n",
      "batch_idx: 9\n",
      "0.7333333333333333\n",
      "batch_idx: 10\n",
      "0.7386363636363636\n",
      "batch_idx: 11\n",
      "0.7395833333333334\n",
      "batch_idx: 12\n",
      "0.7307692307692307\n",
      "batch_idx: 13\n",
      "0.7261904761904762\n",
      "batch_idx: 14\n",
      "0.7222222222222222\n",
      "batch_idx: 15\n",
      "0.7213541666666666\n",
      "batch_idx: 16\n",
      "0.7230392156862745\n",
      "batch_idx: 17\n",
      "0.7245370370370371\n",
      "batch_idx: 18\n",
      "0.7171052631578947\n",
      "batch_idx: 19\n",
      "0.7145833333333333\n",
      "batch_idx: 20\n",
      "0.7142857142857143\n",
      "batch_idx: 21\n",
      "0.7102272727272727\n",
      "batch_idx: 22\n",
      "0.7119565217391305\n",
      "batch_idx: 23\n",
      "0.7152777777777778\n",
      "batch_idx: 24\n",
      "0.715\n",
      "Training Epoch: 255, total loss: 44.502934\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7440476190476191\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.7314814814814815\n",
      "batch_idx: 9\n",
      "0.7375\n",
      "batch_idx: 10\n",
      "0.7196969696969697\n",
      "batch_idx: 11\n",
      "0.7256944444444444\n",
      "batch_idx: 12\n",
      "0.7147435897435898\n",
      "batch_idx: 13\n",
      "0.6964285714285714\n",
      "batch_idx: 14\n",
      "0.6916666666666667\n",
      "batch_idx: 15\n",
      "0.6953125\n",
      "batch_idx: 16\n",
      "0.6862745098039216\n",
      "batch_idx: 17\n",
      "0.6805555555555556\n",
      "batch_idx: 18\n",
      "0.6754385964912281\n",
      "batch_idx: 19\n",
      "0.6770833333333334\n",
      "batch_idx: 20\n",
      "0.6646825396825397\n",
      "batch_idx: 21\n",
      "0.6704545454545454\n",
      "batch_idx: 22\n",
      "0.6721014492753623\n",
      "batch_idx: 23\n",
      "0.6701388888888888\n",
      "batch_idx: 24\n",
      "0.6683333333333333\n",
      "Training Epoch: 256, total loss: 45.129047\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.6770833333333334\n",
      "batch_idx: 4\n",
      "0.6583333333333333\n",
      "batch_idx: 5\n",
      "0.6736111111111112\n",
      "batch_idx: 6\n",
      "0.6785714285714286\n",
      "batch_idx: 7\n",
      "0.6614583333333334\n",
      "batch_idx: 8\n",
      "0.6481481481481481\n",
      "batch_idx: 9\n",
      "0.6541666666666667\n",
      "batch_idx: 10\n",
      "0.6666666666666666\n",
      "batch_idx: 11\n",
      "0.6701388888888888\n",
      "batch_idx: 12\n",
      "0.6762820512820513\n",
      "batch_idx: 13\n",
      "0.6845238095238095\n",
      "batch_idx: 14\n",
      "0.6861111111111111\n",
      "batch_idx: 15\n",
      "0.6901041666666666\n",
      "batch_idx: 16\n",
      "0.6887254901960784\n",
      "batch_idx: 17\n",
      "0.6851851851851852\n",
      "batch_idx: 18\n",
      "0.6885964912280702\n",
      "batch_idx: 19\n",
      "0.68125\n",
      "batch_idx: 20\n",
      "0.6884920634920635\n",
      "batch_idx: 21\n",
      "0.6893939393939394\n",
      "batch_idx: 22\n",
      "0.6920289855072463\n",
      "batch_idx: 23\n",
      "0.6875\n",
      "batch_idx: 24\n",
      "0.69\n",
      "Training Epoch: 257, total loss: 44.988031\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.6770833333333334\n",
      "batch_idx: 4\n",
      "0.7\n",
      "batch_idx: 5\n",
      "0.6944444444444444\n",
      "batch_idx: 6\n",
      "0.6845238095238095\n",
      "batch_idx: 7\n",
      "0.6875\n",
      "batch_idx: 8\n",
      "0.6805555555555556\n",
      "batch_idx: 9\n",
      "0.6833333333333333\n",
      "batch_idx: 10\n",
      "0.6818181818181818\n",
      "batch_idx: 11\n",
      "0.6840277777777778\n",
      "batch_idx: 12\n",
      "0.6794871794871795\n",
      "batch_idx: 13\n",
      "0.6964285714285714\n",
      "batch_idx: 14\n",
      "0.6944444444444444\n",
      "batch_idx: 15\n",
      "0.6875\n",
      "batch_idx: 16\n",
      "0.678921568627451\n",
      "batch_idx: 17\n",
      "0.6828703703703703\n",
      "batch_idx: 18\n",
      "0.6929824561403509\n",
      "batch_idx: 19\n",
      "0.6916666666666667\n",
      "batch_idx: 20\n",
      "0.6944444444444444\n",
      "batch_idx: 21\n",
      "0.6988636363636364\n",
      "batch_idx: 22\n",
      "0.6992753623188406\n",
      "batch_idx: 23\n",
      "0.703125\n",
      "batch_idx: 24\n",
      "0.705\n",
      "Training Epoch: 258, total loss: 44.609906\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.7083333333333334\n",
      "batch_idx: 6\n",
      "0.7083333333333334\n",
      "batch_idx: 7\n",
      "0.7291666666666666\n",
      "batch_idx: 8\n",
      "0.7222222222222222\n",
      "batch_idx: 9\n",
      "0.7333333333333333\n",
      "batch_idx: 10\n",
      "0.7348484848484849\n",
      "batch_idx: 11\n",
      "0.7326388888888888\n",
      "batch_idx: 12\n",
      "0.7275641025641025\n",
      "batch_idx: 13\n",
      "0.7321428571428571\n",
      "batch_idx: 14\n",
      "0.725\n",
      "batch_idx: 15\n",
      "0.7265625\n",
      "batch_idx: 16\n",
      "0.7303921568627451\n",
      "batch_idx: 17\n",
      "0.7245370370370371\n",
      "batch_idx: 18\n",
      "0.7214912280701754\n",
      "batch_idx: 19\n",
      "0.7208333333333333\n",
      "batch_idx: 20\n",
      "0.7182539682539683\n",
      "batch_idx: 21\n",
      "0.7159090909090909\n",
      "batch_idx: 22\n",
      "0.7083333333333334\n",
      "batch_idx: 23\n",
      "0.6996527777777778\n",
      "batch_idx: 24\n",
      "0.7016666666666667\n",
      "Training Epoch: 259, total loss: 44.711197\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.6458333333333334\n",
      "batch_idx: 4\n",
      "0.65\n",
      "batch_idx: 5\n",
      "0.6805555555555556\n",
      "batch_idx: 6\n",
      "0.6964285714285714\n",
      "batch_idx: 7\n",
      "0.703125\n",
      "batch_idx: 8\n",
      "0.6944444444444444\n",
      "batch_idx: 9\n",
      "0.6958333333333333\n",
      "batch_idx: 10\n",
      "0.6893939393939394\n",
      "batch_idx: 11\n",
      "0.6909722222222222\n",
      "batch_idx: 12\n",
      "0.6794871794871795\n",
      "batch_idx: 13\n",
      "0.6815476190476191\n",
      "batch_idx: 14\n",
      "0.6888888888888889\n",
      "batch_idx: 15\n",
      "0.6953125\n",
      "batch_idx: 16\n",
      "0.6936274509803921\n",
      "batch_idx: 17\n",
      "0.7013888888888888\n",
      "batch_idx: 18\n",
      "0.7039473684210527\n",
      "batch_idx: 19\n",
      "0.7020833333333333\n",
      "batch_idx: 20\n",
      "0.6964285714285714\n",
      "batch_idx: 21\n",
      "0.6931818181818182\n",
      "batch_idx: 22\n",
      "0.6920289855072463\n",
      "batch_idx: 23\n",
      "0.6875\n",
      "batch_idx: 24\n",
      "0.685\n",
      "Training Epoch: 260, total loss: 45.155536\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.625\n",
      "batch_idx: 4\n",
      "0.625\n",
      "batch_idx: 5\n",
      "0.6319444444444444\n",
      "batch_idx: 6\n",
      "0.6428571428571429\n",
      "batch_idx: 7\n",
      "0.640625\n",
      "batch_idx: 8\n",
      "0.6435185185185185\n",
      "batch_idx: 9\n",
      "0.65\n",
      "batch_idx: 10\n",
      "0.6666666666666666\n",
      "batch_idx: 11\n",
      "0.6770833333333334\n",
      "batch_idx: 12\n",
      "0.6730769230769231\n",
      "batch_idx: 13\n",
      "0.6607142857142857\n",
      "batch_idx: 14\n",
      "0.6583333333333333\n",
      "batch_idx: 15\n",
      "0.6614583333333334\n",
      "batch_idx: 16\n",
      "0.6593137254901961\n",
      "batch_idx: 17\n",
      "0.6597222222222222\n",
      "batch_idx: 18\n",
      "0.6622807017543859\n",
      "batch_idx: 19\n",
      "0.6583333333333333\n",
      "batch_idx: 20\n",
      "0.6626984126984127\n",
      "batch_idx: 21\n",
      "0.6685606060606061\n",
      "batch_idx: 22\n",
      "0.6702898550724637\n",
      "batch_idx: 23\n",
      "0.6666666666666666\n",
      "batch_idx: 24\n",
      "0.665\n",
      "Training Epoch: 261, total loss: 45.400852\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.5416666666666666\n",
      "batch_idx: 2\n",
      "0.5972222222222222\n",
      "batch_idx: 3\n",
      "0.6354166666666666\n",
      "batch_idx: 4\n",
      "0.6416666666666667\n",
      "batch_idx: 5\n",
      "0.6527777777777778\n",
      "batch_idx: 6\n",
      "0.6607142857142857\n",
      "batch_idx: 7\n",
      "0.6510416666666666\n",
      "batch_idx: 8\n",
      "0.6527777777777778\n",
      "batch_idx: 9\n",
      "0.675\n",
      "batch_idx: 10\n",
      "0.6742424242424242\n",
      "batch_idx: 11\n",
      "0.6840277777777778\n",
      "batch_idx: 12\n",
      "0.6923076923076923\n",
      "batch_idx: 13\n",
      "0.6934523809523809\n",
      "batch_idx: 14\n",
      "0.6944444444444444\n",
      "batch_idx: 15\n",
      "0.6953125\n",
      "batch_idx: 16\n",
      "0.6936274509803921\n",
      "batch_idx: 17\n",
      "0.7060185185185185\n",
      "batch_idx: 18\n",
      "0.7017543859649122\n",
      "batch_idx: 19\n",
      "0.7083333333333334\n",
      "batch_idx: 20\n",
      "0.7043650793650794\n",
      "batch_idx: 21\n",
      "0.7007575757575758\n",
      "batch_idx: 22\n",
      "0.697463768115942\n",
      "batch_idx: 23\n",
      "0.7013888888888888\n",
      "batch_idx: 24\n",
      "0.7033333333333334\n",
      "Training Epoch: 262, total loss: 44.666751\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.6666666666666666\n",
      "batch_idx: 6\n",
      "0.6666666666666666\n",
      "batch_idx: 7\n",
      "0.6614583333333334\n",
      "batch_idx: 8\n",
      "0.6527777777777778\n",
      "batch_idx: 9\n",
      "0.6458333333333334\n",
      "batch_idx: 10\n",
      "0.6477272727272727\n",
      "batch_idx: 11\n",
      "0.6597222222222222\n",
      "batch_idx: 12\n",
      "0.6538461538461539\n",
      "batch_idx: 13\n",
      "0.6517857142857143\n",
      "batch_idx: 14\n",
      "0.65\n",
      "batch_idx: 15\n",
      "0.6536458333333334\n",
      "batch_idx: 16\n",
      "0.6642156862745098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 17\n",
      "0.6689814814814815\n",
      "batch_idx: 18\n",
      "0.6776315789473685\n",
      "batch_idx: 19\n",
      "0.6770833333333334\n",
      "batch_idx: 20\n",
      "0.6825396825396826\n",
      "batch_idx: 21\n",
      "0.6875\n",
      "batch_idx: 22\n",
      "0.6884057971014492\n",
      "batch_idx: 23\n",
      "0.6979166666666666\n",
      "batch_idx: 24\n",
      "0.6983333333333334\n",
      "Training Epoch: 263, total loss: 44.769043\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.6458333333333334\n",
      "batch_idx: 4\n",
      "0.6166666666666667\n",
      "batch_idx: 5\n",
      "0.625\n",
      "batch_idx: 6\n",
      "0.6190476190476191\n",
      "batch_idx: 7\n",
      "0.609375\n",
      "batch_idx: 8\n",
      "0.6064814814814815\n",
      "batch_idx: 9\n",
      "0.6041666666666666\n",
      "batch_idx: 10\n",
      "0.6136363636363636\n",
      "batch_idx: 11\n",
      "0.6111111111111112\n",
      "batch_idx: 12\n",
      "0.6057692307692307\n",
      "batch_idx: 13\n",
      "0.6101190476190477\n",
      "batch_idx: 14\n",
      "0.6055555555555555\n",
      "batch_idx: 15\n",
      "0.6223958333333334\n",
      "batch_idx: 16\n",
      "0.6323529411764706\n",
      "batch_idx: 17\n",
      "0.6388888888888888\n",
      "batch_idx: 18\n",
      "0.6447368421052632\n",
      "batch_idx: 19\n",
      "0.64375\n",
      "batch_idx: 20\n",
      "0.6388888888888888\n",
      "batch_idx: 21\n",
      "0.6363636363636364\n",
      "batch_idx: 22\n",
      "0.6358695652173914\n",
      "batch_idx: 23\n",
      "0.6354166666666666\n",
      "batch_idx: 24\n",
      "0.6316666666666667\n",
      "Training Epoch: 264, total loss: 45.862481\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.6388888888888888\n",
      "batch_idx: 3\n",
      "0.625\n",
      "batch_idx: 4\n",
      "0.6416666666666667\n",
      "batch_idx: 5\n",
      "0.6666666666666666\n",
      "batch_idx: 6\n",
      "0.6607142857142857\n",
      "batch_idx: 7\n",
      "0.6666666666666666\n",
      "batch_idx: 8\n",
      "0.6620370370370371\n",
      "batch_idx: 9\n",
      "0.675\n",
      "batch_idx: 10\n",
      "0.6590909090909091\n",
      "batch_idx: 11\n",
      "0.6597222222222222\n",
      "batch_idx: 12\n",
      "0.6602564102564102\n",
      "batch_idx: 13\n",
      "0.6547619047619048\n",
      "batch_idx: 14\n",
      "0.6583333333333333\n",
      "batch_idx: 15\n",
      "0.6640625\n",
      "batch_idx: 16\n",
      "0.6568627450980392\n",
      "batch_idx: 17\n",
      "0.6574074074074074\n",
      "batch_idx: 18\n",
      "0.6578947368421053\n",
      "batch_idx: 19\n",
      "0.6666666666666666\n",
      "batch_idx: 20\n",
      "0.6646825396825397\n",
      "batch_idx: 21\n",
      "0.6609848484848485\n",
      "batch_idx: 22\n",
      "0.6557971014492754\n",
      "batch_idx: 23\n",
      "0.6614583333333334\n",
      "batch_idx: 24\n",
      "0.6616666666666666\n",
      "Training Epoch: 265, total loss: 45.521731\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.6666666666666666\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6875\n",
      "batch_idx: 6\n",
      "0.7023809523809523\n",
      "batch_idx: 7\n",
      "0.6927083333333334\n",
      "batch_idx: 8\n",
      "0.7037037037037037\n",
      "batch_idx: 9\n",
      "0.725\n",
      "batch_idx: 10\n",
      "0.7272727272727273\n",
      "batch_idx: 11\n",
      "0.7291666666666666\n",
      "batch_idx: 12\n",
      "0.7211538461538461\n",
      "batch_idx: 13\n",
      "0.7172619047619048\n",
      "batch_idx: 14\n",
      "0.7138888888888889\n",
      "batch_idx: 15\n",
      "0.7135416666666666\n",
      "batch_idx: 16\n",
      "0.7156862745098039\n",
      "batch_idx: 17\n",
      "0.7060185185185185\n",
      "batch_idx: 18\n",
      "0.7105263157894737\n",
      "batch_idx: 19\n",
      "0.7104166666666667\n",
      "batch_idx: 20\n",
      "0.7142857142857143\n",
      "batch_idx: 21\n",
      "0.7140151515151515\n",
      "batch_idx: 22\n",
      "0.7101449275362319\n",
      "batch_idx: 23\n",
      "0.7170138888888888\n",
      "batch_idx: 24\n",
      "0.7166666666666667\n",
      "Training Epoch: 266, total loss: 44.509675\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.7083333333333334\n",
      "batch_idx: 6\n",
      "0.7023809523809523\n",
      "batch_idx: 7\n",
      "0.6927083333333334\n",
      "batch_idx: 8\n",
      "0.7037037037037037\n",
      "batch_idx: 9\n",
      "0.6916666666666667\n",
      "batch_idx: 10\n",
      "0.6931818181818182\n",
      "batch_idx: 11\n",
      "0.7048611111111112\n",
      "batch_idx: 12\n",
      "0.7147435897435898\n",
      "batch_idx: 13\n",
      "0.7172619047619048\n",
      "batch_idx: 14\n",
      "0.7111111111111111\n",
      "batch_idx: 15\n",
      "0.7109375\n",
      "batch_idx: 16\n",
      "0.7107843137254902\n",
      "batch_idx: 17\n",
      "0.7083333333333334\n",
      "batch_idx: 18\n",
      "0.7083333333333334\n",
      "batch_idx: 19\n",
      "0.7041666666666667\n",
      "batch_idx: 20\n",
      "0.7003968253968254\n",
      "batch_idx: 21\n",
      "0.7026515151515151\n",
      "batch_idx: 22\n",
      "0.6956521739130435\n",
      "batch_idx: 23\n",
      "0.6875\n",
      "batch_idx: 24\n",
      "0.6883333333333334\n",
      "Training Epoch: 267, total loss: 45.024745\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7361111111111112\n",
      "batch_idx: 6\n",
      "0.7142857142857143\n",
      "batch_idx: 7\n",
      "0.7239583333333334\n",
      "batch_idx: 8\n",
      "0.7175925925925926\n",
      "batch_idx: 9\n",
      "0.7166666666666667\n",
      "batch_idx: 10\n",
      "0.7159090909090909\n",
      "batch_idx: 11\n",
      "0.7118055555555556\n",
      "batch_idx: 12\n",
      "0.7051282051282052\n",
      "batch_idx: 13\n",
      "0.6964285714285714\n",
      "batch_idx: 14\n",
      "0.6944444444444444\n",
      "batch_idx: 15\n",
      "0.6979166666666666\n",
      "batch_idx: 16\n",
      "0.7083333333333334\n",
      "batch_idx: 17\n",
      "0.7152777777777778\n",
      "batch_idx: 18\n",
      "0.7083333333333334\n",
      "batch_idx: 19\n",
      "0.70625\n",
      "batch_idx: 20\n",
      "0.7063492063492064\n",
      "batch_idx: 21\n",
      "0.7121212121212122\n",
      "batch_idx: 22\n",
      "0.7101449275362319\n",
      "batch_idx: 23\n",
      "0.7100694444444444\n",
      "batch_idx: 24\n",
      "0.705\n",
      "Training Epoch: 268, total loss: 44.693559\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.6805555555555556\n",
      "batch_idx: 6\n",
      "0.6904761904761905\n",
      "batch_idx: 7\n",
      "0.703125\n",
      "batch_idx: 8\n",
      "0.6990740740740741\n",
      "batch_idx: 9\n",
      "0.7083333333333334\n",
      "batch_idx: 10\n",
      "0.7159090909090909\n",
      "batch_idx: 11\n",
      "0.7118055555555556\n",
      "batch_idx: 12\n",
      "0.7083333333333334\n",
      "batch_idx: 13\n",
      "0.7113095238095238\n",
      "batch_idx: 14\n",
      "0.7083333333333334\n",
      "batch_idx: 15\n",
      "0.6979166666666666\n",
      "batch_idx: 16\n",
      "0.7034313725490197\n",
      "batch_idx: 17\n",
      "0.6990740740740741\n",
      "batch_idx: 18\n",
      "0.7017543859649122\n",
      "batch_idx: 19\n",
      "0.7104166666666667\n",
      "batch_idx: 20\n",
      "0.7103174603174603\n",
      "batch_idx: 21\n",
      "0.7064393939393939\n",
      "batch_idx: 22\n",
      "0.7010869565217391\n",
      "batch_idx: 23\n",
      "0.6927083333333334\n",
      "batch_idx: 24\n",
      "0.6916666666666667\n",
      "Training Epoch: 269, total loss: 45.096066\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.5972222222222222\n",
      "batch_idx: 3\n",
      "0.59375\n",
      "batch_idx: 4\n",
      "0.625\n",
      "batch_idx: 5\n",
      "0.6388888888888888\n",
      "batch_idx: 6\n",
      "0.6309523809523809\n",
      "batch_idx: 7\n",
      "0.6302083333333334\n",
      "batch_idx: 8\n",
      "0.625\n",
      "batch_idx: 9\n",
      "0.625\n",
      "batch_idx: 10\n",
      "0.6136363636363636\n",
      "batch_idx: 11\n",
      "0.6284722222222222\n",
      "batch_idx: 12\n",
      "0.6378205128205128\n",
      "batch_idx: 13\n",
      "0.6458333333333334\n",
      "batch_idx: 14\n",
      "0.65\n",
      "batch_idx: 15\n",
      "0.6432291666666666\n",
      "batch_idx: 16\n",
      "0.6446078431372549\n",
      "batch_idx: 17\n",
      "0.6504629629629629\n",
      "batch_idx: 18\n",
      "0.6513157894736842\n",
      "batch_idx: 19\n",
      "0.6541666666666667\n",
      "batch_idx: 20\n",
      "0.6587301587301587\n",
      "batch_idx: 21\n",
      "0.6571969696969697\n",
      "batch_idx: 22\n",
      "0.6557971014492754\n",
      "batch_idx: 23\n",
      "0.6597222222222222\n",
      "batch_idx: 24\n",
      "0.66\n",
      "Training Epoch: 270, total loss: 45.427364\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.6388888888888888\n",
      "batch_idx: 3\n",
      "0.6770833333333334\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.7152777777777778\n",
      "batch_idx: 6\n",
      "0.7202380952380952\n",
      "batch_idx: 7\n",
      "0.6979166666666666\n",
      "batch_idx: 8\n",
      "0.6851851851851852\n",
      "batch_idx: 9\n",
      "0.6916666666666667\n",
      "batch_idx: 10\n",
      "0.7007575757575758\n",
      "batch_idx: 11\n",
      "0.7013888888888888\n",
      "batch_idx: 12\n",
      "0.7083333333333334\n",
      "batch_idx: 13\n",
      "0.7142857142857143\n",
      "batch_idx: 14\n",
      "0.7083333333333334\n",
      "batch_idx: 15\n",
      "0.7083333333333334\n",
      "batch_idx: 16\n",
      "0.7009803921568627\n",
      "batch_idx: 17\n",
      "0.7013888888888888\n",
      "batch_idx: 18\n",
      "0.6951754385964912\n",
      "batch_idx: 19\n",
      "0.68125\n",
      "batch_idx: 20\n",
      "0.6785714285714286\n",
      "batch_idx: 21\n",
      "0.6742424242424242\n",
      "batch_idx: 22\n",
      "0.6648550724637681\n",
      "batch_idx: 23\n",
      "0.6614583333333334\n",
      "batch_idx: 24\n",
      "0.6666666666666666\n",
      "Training Epoch: 271, total loss: 45.265911\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.7083333333333334\n",
      "batch_idx: 6\n",
      "0.7023809523809523\n",
      "batch_idx: 7\n",
      "0.7083333333333334\n",
      "batch_idx: 8\n",
      "0.6944444444444444\n",
      "batch_idx: 9\n",
      "0.6833333333333333\n",
      "batch_idx: 10\n",
      "0.6818181818181818\n",
      "batch_idx: 11\n",
      "0.6736111111111112\n",
      "batch_idx: 12\n",
      "0.6730769230769231\n",
      "batch_idx: 13\n",
      "0.6696428571428571\n",
      "batch_idx: 14\n",
      "0.6666666666666666\n",
      "batch_idx: 15\n",
      "0.6692708333333334\n",
      "batch_idx: 16\n",
      "0.6862745098039216\n",
      "batch_idx: 17\n",
      "0.6898148148148148\n",
      "batch_idx: 18\n",
      "0.6929824561403509\n",
      "batch_idx: 19\n",
      "0.7\n",
      "batch_idx: 20\n",
      "0.7043650793650794\n",
      "batch_idx: 21\n",
      "0.7007575757575758\n",
      "batch_idx: 22\n",
      "0.7010869565217391\n",
      "batch_idx: 23\n",
      "0.6979166666666666\n",
      "batch_idx: 24\n",
      "0.7016666666666667\n",
      "Training Epoch: 272, total loss: 44.739541\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.6666666666666666\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6597222222222222\n",
      "batch_idx: 6\n",
      "0.6666666666666666\n",
      "batch_idx: 7\n",
      "0.6927083333333334\n",
      "batch_idx: 8\n",
      "0.6898148148148148\n",
      "batch_idx: 9\n",
      "0.7\n",
      "batch_idx: 10\n",
      "0.6931818181818182\n",
      "batch_idx: 11\n",
      "0.6944444444444444\n",
      "batch_idx: 12\n",
      "0.6987179487179487\n",
      "batch_idx: 13\n",
      "0.7023809523809523\n",
      "batch_idx: 14\n",
      "0.7027777777777777\n",
      "batch_idx: 15\n",
      "0.6953125\n",
      "batch_idx: 16\n",
      "0.6862745098039216\n",
      "batch_idx: 17\n",
      "0.6805555555555556\n",
      "batch_idx: 18\n",
      "0.6842105263157895\n",
      "batch_idx: 19\n",
      "0.68125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 20\n",
      "0.6765873015873016\n",
      "batch_idx: 21\n",
      "0.6723484848484849\n",
      "batch_idx: 22\n",
      "0.6702898550724637\n",
      "batch_idx: 23\n",
      "0.6736111111111112\n",
      "batch_idx: 24\n",
      "0.68\n",
      "Training Epoch: 273, total loss: 45.240006\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7321428571428571\n",
      "batch_idx: 7\n",
      "0.7395833333333334\n",
      "batch_idx: 8\n",
      "0.7222222222222222\n",
      "batch_idx: 9\n",
      "0.7125\n",
      "batch_idx: 10\n",
      "0.7159090909090909\n",
      "batch_idx: 11\n",
      "0.71875\n",
      "batch_idx: 12\n",
      "0.7115384615384616\n",
      "batch_idx: 13\n",
      "0.7053571428571429\n",
      "batch_idx: 14\n",
      "0.7111111111111111\n",
      "batch_idx: 15\n",
      "0.7057291666666666\n",
      "batch_idx: 16\n",
      "0.7083333333333334\n",
      "batch_idx: 17\n",
      "0.7152777777777778\n",
      "batch_idx: 18\n",
      "0.7127192982456141\n",
      "batch_idx: 19\n",
      "0.7104166666666667\n",
      "batch_idx: 20\n",
      "0.7083333333333334\n",
      "batch_idx: 21\n",
      "0.7102272727272727\n",
      "batch_idx: 22\n",
      "0.7083333333333334\n",
      "batch_idx: 23\n",
      "0.7100694444444444\n",
      "batch_idx: 24\n",
      "0.7066666666666667\n",
      "Training Epoch: 274, total loss: 44.649862\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.6833333333333333\n",
      "batch_idx: 5\n",
      "0.7013888888888888\n",
      "batch_idx: 6\n",
      "0.6845238095238095\n",
      "batch_idx: 7\n",
      "0.6666666666666666\n",
      "batch_idx: 8\n",
      "0.6759259259259259\n",
      "batch_idx: 9\n",
      "0.6916666666666667\n",
      "batch_idx: 10\n",
      "0.7007575757575758\n",
      "batch_idx: 11\n",
      "0.6909722222222222\n",
      "batch_idx: 12\n",
      "0.7019230769230769\n",
      "batch_idx: 13\n",
      "0.6875\n",
      "batch_idx: 14\n",
      "0.6916666666666667\n",
      "batch_idx: 15\n",
      "0.6822916666666666\n",
      "batch_idx: 16\n",
      "0.6838235294117647\n",
      "batch_idx: 17\n",
      "0.6898148148148148\n",
      "batch_idx: 18\n",
      "0.6951754385964912\n",
      "batch_idx: 19\n",
      "0.6958333333333333\n",
      "batch_idx: 20\n",
      "0.6984126984126984\n",
      "batch_idx: 21\n",
      "0.7007575757575758\n",
      "batch_idx: 22\n",
      "0.7101449275362319\n",
      "batch_idx: 23\n",
      "0.7100694444444444\n",
      "batch_idx: 24\n",
      "0.7083333333333334\n",
      "Training Epoch: 275, total loss: 44.507030\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7202380952380952\n",
      "batch_idx: 7\n",
      "0.7291666666666666\n",
      "batch_idx: 8\n",
      "0.7175925925925926\n",
      "batch_idx: 9\n",
      "0.7041666666666667\n",
      "batch_idx: 10\n",
      "0.7083333333333334\n",
      "batch_idx: 11\n",
      "0.7118055555555556\n",
      "batch_idx: 12\n",
      "0.7019230769230769\n",
      "batch_idx: 13\n",
      "0.7053571428571429\n",
      "batch_idx: 14\n",
      "0.6972222222222222\n",
      "batch_idx: 15\n",
      "0.7057291666666666\n",
      "batch_idx: 16\n",
      "0.6862745098039216\n",
      "batch_idx: 17\n",
      "0.6875\n",
      "batch_idx: 18\n",
      "0.6820175438596491\n",
      "batch_idx: 19\n",
      "0.6791666666666667\n",
      "batch_idx: 20\n",
      "0.6805555555555556\n",
      "batch_idx: 21\n",
      "0.6761363636363636\n",
      "batch_idx: 22\n",
      "0.6847826086956522\n",
      "batch_idx: 23\n",
      "0.6840277777777778\n",
      "batch_idx: 24\n",
      "0.685\n",
      "Training Epoch: 276, total loss: 44.929420\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.7142857142857143\n",
      "batch_idx: 7\n",
      "0.703125\n",
      "batch_idx: 8\n",
      "0.7037037037037037\n",
      "batch_idx: 9\n",
      "0.675\n",
      "batch_idx: 10\n",
      "0.678030303030303\n",
      "batch_idx: 11\n",
      "0.6736111111111112\n",
      "batch_idx: 12\n",
      "0.6730769230769231\n",
      "batch_idx: 13\n",
      "0.6815476190476191\n",
      "batch_idx: 14\n",
      "0.6833333333333333\n",
      "batch_idx: 15\n",
      "0.6875\n",
      "batch_idx: 16\n",
      "0.6838235294117647\n",
      "batch_idx: 17\n",
      "0.6875\n",
      "batch_idx: 18\n",
      "0.6842105263157895\n",
      "batch_idx: 19\n",
      "0.6916666666666667\n",
      "batch_idx: 20\n",
      "0.7003968253968254\n",
      "batch_idx: 21\n",
      "0.7007575757575758\n",
      "batch_idx: 22\n",
      "0.7065217391304348\n",
      "batch_idx: 23\n",
      "0.7100694444444444\n",
      "batch_idx: 24\n",
      "0.7133333333333334\n",
      "Training Epoch: 277, total loss: 44.516980\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.725\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.71875\n",
      "batch_idx: 8\n",
      "0.7129629629629629\n",
      "batch_idx: 9\n",
      "0.7083333333333334\n",
      "batch_idx: 10\n",
      "0.7196969696969697\n",
      "batch_idx: 11\n",
      "0.7083333333333334\n",
      "batch_idx: 12\n",
      "0.7051282051282052\n",
      "batch_idx: 13\n",
      "0.7023809523809523\n",
      "batch_idx: 14\n",
      "0.7138888888888889\n",
      "batch_idx: 15\n",
      "0.7161458333333334\n",
      "batch_idx: 16\n",
      "0.7156862745098039\n",
      "batch_idx: 17\n",
      "0.7083333333333334\n",
      "batch_idx: 18\n",
      "0.7127192982456141\n",
      "batch_idx: 19\n",
      "0.7020833333333333\n",
      "batch_idx: 20\n",
      "0.7103174603174603\n",
      "batch_idx: 21\n",
      "0.7083333333333334\n",
      "batch_idx: 22\n",
      "0.7101449275362319\n",
      "batch_idx: 23\n",
      "0.7013888888888888\n",
      "batch_idx: 24\n",
      "0.7066666666666667\n",
      "Training Epoch: 278, total loss: 44.600060\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.725\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.7291666666666666\n",
      "batch_idx: 8\n",
      "0.7361111111111112\n",
      "batch_idx: 9\n",
      "0.7291666666666666\n",
      "batch_idx: 10\n",
      "0.7159090909090909\n",
      "batch_idx: 11\n",
      "0.7013888888888888\n",
      "batch_idx: 12\n",
      "0.7051282051282052\n",
      "batch_idx: 13\n",
      "0.7053571428571429\n",
      "batch_idx: 14\n",
      "0.7\n",
      "batch_idx: 15\n",
      "0.6979166666666666\n",
      "batch_idx: 16\n",
      "0.696078431372549\n",
      "batch_idx: 17\n",
      "0.7037037037037037\n",
      "batch_idx: 18\n",
      "0.6929824561403509\n",
      "batch_idx: 19\n",
      "0.6958333333333333\n",
      "batch_idx: 20\n",
      "0.6964285714285714\n",
      "batch_idx: 21\n",
      "0.6931818181818182\n",
      "batch_idx: 22\n",
      "0.6938405797101449\n",
      "batch_idx: 23\n",
      "0.6927083333333334\n",
      "batch_idx: 24\n",
      "0.6966666666666667\n",
      "Training Epoch: 279, total loss: 44.700796\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.6944444444444444\n",
      "batch_idx: 6\n",
      "0.6964285714285714\n",
      "batch_idx: 7\n",
      "0.7083333333333334\n",
      "batch_idx: 8\n",
      "0.6990740740740741\n",
      "batch_idx: 9\n",
      "0.7\n",
      "batch_idx: 10\n",
      "0.6931818181818182\n",
      "batch_idx: 11\n",
      "0.6979166666666666\n",
      "batch_idx: 12\n",
      "0.6987179487179487\n",
      "batch_idx: 13\n",
      "0.6964285714285714\n",
      "batch_idx: 14\n",
      "0.6888888888888889\n",
      "batch_idx: 15\n",
      "0.6979166666666666\n",
      "batch_idx: 16\n",
      "0.696078431372549\n",
      "batch_idx: 17\n",
      "0.6967592592592593\n",
      "batch_idx: 18\n",
      "0.6951754385964912\n",
      "batch_idx: 19\n",
      "0.6958333333333333\n",
      "batch_idx: 20\n",
      "0.6825396825396826\n",
      "batch_idx: 21\n",
      "0.6742424242424242\n",
      "batch_idx: 22\n",
      "0.677536231884058\n",
      "batch_idx: 23\n",
      "0.6770833333333334\n",
      "batch_idx: 24\n",
      "0.6816666666666666\n",
      "Training Epoch: 280, total loss: 45.102062\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.5833333333333334\n",
      "batch_idx: 3\n",
      "0.625\n",
      "batch_idx: 4\n",
      "0.6666666666666666\n",
      "batch_idx: 5\n",
      "0.6458333333333334\n",
      "batch_idx: 6\n",
      "0.6488095238095238\n",
      "batch_idx: 7\n",
      "0.6458333333333334\n",
      "batch_idx: 8\n",
      "0.6759259259259259\n",
      "batch_idx: 9\n",
      "0.6833333333333333\n",
      "batch_idx: 10\n",
      "0.678030303030303\n",
      "batch_idx: 11\n",
      "0.6770833333333334\n",
      "batch_idx: 12\n",
      "0.6634615384615384\n",
      "batch_idx: 13\n",
      "0.6696428571428571\n",
      "batch_idx: 14\n",
      "0.6694444444444444\n",
      "batch_idx: 15\n",
      "0.6640625\n",
      "batch_idx: 16\n",
      "0.678921568627451\n",
      "batch_idx: 17\n",
      "0.6805555555555556\n",
      "batch_idx: 18\n",
      "0.6929824561403509\n",
      "batch_idx: 19\n",
      "0.69375\n",
      "batch_idx: 20\n",
      "0.6984126984126984\n",
      "batch_idx: 21\n",
      "0.7026515151515151\n",
      "batch_idx: 22\n",
      "0.7065217391304348\n",
      "batch_idx: 23\n",
      "0.7135416666666666\n",
      "batch_idx: 24\n",
      "0.7\n",
      "Training Epoch: 281, total loss: 44.670773\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.6833333333333333\n",
      "batch_idx: 5\n",
      "0.6944444444444444\n",
      "batch_idx: 6\n",
      "0.7023809523809523\n",
      "batch_idx: 7\n",
      "0.7135416666666666\n",
      "batch_idx: 8\n",
      "0.7083333333333334\n",
      "batch_idx: 9\n",
      "0.725\n",
      "batch_idx: 10\n",
      "0.7196969696969697\n",
      "batch_idx: 11\n",
      "0.71875\n",
      "batch_idx: 12\n",
      "0.7051282051282052\n",
      "batch_idx: 13\n",
      "0.7053571428571429\n",
      "batch_idx: 14\n",
      "0.7027777777777777\n",
      "batch_idx: 15\n",
      "0.7005208333333334\n",
      "batch_idx: 16\n",
      "0.7107843137254902\n",
      "batch_idx: 17\n",
      "0.7013888888888888\n",
      "batch_idx: 18\n",
      "0.6929824561403509\n",
      "batch_idx: 19\n",
      "0.69375\n",
      "batch_idx: 20\n",
      "0.6845238095238095\n",
      "batch_idx: 21\n",
      "0.6875\n",
      "batch_idx: 22\n",
      "0.6847826086956522\n",
      "batch_idx: 23\n",
      "0.6927083333333334\n",
      "batch_idx: 24\n",
      "0.69\n",
      "Training Epoch: 282, total loss: 44.858290\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.7013888888888888\n",
      "batch_idx: 6\n",
      "0.7261904761904762\n",
      "batch_idx: 7\n",
      "0.7135416666666666\n",
      "batch_idx: 8\n",
      "0.6990740740740741\n",
      "batch_idx: 9\n",
      "0.7041666666666667\n",
      "batch_idx: 10\n",
      "0.7045454545454546\n",
      "batch_idx: 11\n",
      "0.7152777777777778\n",
      "batch_idx: 12\n",
      "0.7147435897435898\n",
      "batch_idx: 13\n",
      "0.7202380952380952\n",
      "batch_idx: 14\n",
      "0.7194444444444444\n",
      "batch_idx: 15\n",
      "0.7239583333333334\n",
      "batch_idx: 16\n",
      "0.7254901960784313\n",
      "batch_idx: 17\n",
      "0.7245370370370371\n",
      "batch_idx: 18\n",
      "0.7258771929824561\n",
      "batch_idx: 19\n",
      "0.7229166666666667\n",
      "batch_idx: 20\n",
      "0.7202380952380952\n",
      "batch_idx: 21\n",
      "0.7159090909090909\n",
      "batch_idx: 22\n",
      "0.7119565217391305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 23\n",
      "0.7118055555555556\n",
      "batch_idx: 24\n",
      "0.71\n",
      "Training Epoch: 283, total loss: 44.571801\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7361111111111112\n",
      "batch_idx: 9\n",
      "0.7125\n",
      "batch_idx: 10\n",
      "0.7007575757575758\n",
      "batch_idx: 11\n",
      "0.7013888888888888\n",
      "batch_idx: 12\n",
      "0.7019230769230769\n",
      "batch_idx: 13\n",
      "0.7023809523809523\n",
      "batch_idx: 14\n",
      "0.7083333333333334\n",
      "batch_idx: 15\n",
      "0.7057291666666666\n",
      "batch_idx: 16\n",
      "0.7058823529411765\n",
      "batch_idx: 17\n",
      "0.7106481481481481\n",
      "batch_idx: 18\n",
      "0.7105263157894737\n",
      "batch_idx: 19\n",
      "0.7125\n",
      "batch_idx: 20\n",
      "0.7043650793650794\n",
      "batch_idx: 21\n",
      "0.7026515151515151\n",
      "batch_idx: 22\n",
      "0.7065217391304348\n",
      "batch_idx: 23\n",
      "0.7118055555555556\n",
      "batch_idx: 24\n",
      "0.7166666666666667\n",
      "Training Epoch: 284, total loss: 44.336522\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.7152777777777778\n",
      "batch_idx: 6\n",
      "0.6726190476190477\n",
      "batch_idx: 7\n",
      "0.671875\n",
      "batch_idx: 8\n",
      "0.6759259259259259\n",
      "batch_idx: 9\n",
      "0.7\n",
      "batch_idx: 10\n",
      "0.6931818181818182\n",
      "batch_idx: 11\n",
      "0.6875\n",
      "batch_idx: 12\n",
      "0.7019230769230769\n",
      "batch_idx: 13\n",
      "0.7023809523809523\n",
      "batch_idx: 14\n",
      "0.7111111111111111\n",
      "batch_idx: 15\n",
      "0.7161458333333334\n",
      "batch_idx: 16\n",
      "0.7205882352941176\n",
      "batch_idx: 17\n",
      "0.7245370370370371\n",
      "batch_idx: 18\n",
      "0.7214912280701754\n",
      "batch_idx: 19\n",
      "0.71875\n",
      "batch_idx: 20\n",
      "0.7103174603174603\n",
      "batch_idx: 21\n",
      "0.7102272727272727\n",
      "batch_idx: 22\n",
      "0.7137681159420289\n",
      "batch_idx: 23\n",
      "0.7100694444444444\n",
      "batch_idx: 24\n",
      "0.71\n",
      "Training Epoch: 285, total loss: 44.549357\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.7083333333333334\n",
      "batch_idx: 7\n",
      "0.6979166666666666\n",
      "batch_idx: 8\n",
      "0.7129629629629629\n",
      "batch_idx: 9\n",
      "0.6875\n",
      "batch_idx: 10\n",
      "0.6818181818181818\n",
      "batch_idx: 11\n",
      "0.6840277777777778\n",
      "batch_idx: 12\n",
      "0.6923076923076923\n",
      "batch_idx: 13\n",
      "0.6964285714285714\n",
      "batch_idx: 14\n",
      "0.7\n",
      "batch_idx: 15\n",
      "0.7005208333333334\n",
      "batch_idx: 16\n",
      "0.696078431372549\n",
      "batch_idx: 17\n",
      "0.6944444444444444\n",
      "batch_idx: 18\n",
      "0.6885964912280702\n",
      "batch_idx: 19\n",
      "0.6916666666666667\n",
      "batch_idx: 20\n",
      "0.6964285714285714\n",
      "batch_idx: 21\n",
      "0.7045454545454546\n",
      "batch_idx: 22\n",
      "0.7047101449275363\n",
      "batch_idx: 23\n",
      "0.7100694444444444\n",
      "batch_idx: 24\n",
      "0.71\n",
      "Training Epoch: 286, total loss: 44.592005\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7261904761904762\n",
      "batch_idx: 7\n",
      "0.7083333333333334\n",
      "batch_idx: 8\n",
      "0.7083333333333334\n",
      "batch_idx: 9\n",
      "0.7083333333333334\n",
      "batch_idx: 10\n",
      "0.7083333333333334\n",
      "batch_idx: 11\n",
      "0.6979166666666666\n",
      "batch_idx: 12\n",
      "0.7019230769230769\n",
      "batch_idx: 13\n",
      "0.7113095238095238\n",
      "batch_idx: 14\n",
      "0.7166666666666667\n",
      "batch_idx: 15\n",
      "0.7213541666666666\n",
      "batch_idx: 16\n",
      "0.7156862745098039\n",
      "batch_idx: 17\n",
      "0.7175925925925926\n",
      "batch_idx: 18\n",
      "0.7258771929824561\n",
      "batch_idx: 19\n",
      "0.7354166666666667\n",
      "batch_idx: 20\n",
      "0.7380952380952381\n",
      "batch_idx: 21\n",
      "0.740530303030303\n",
      "batch_idx: 22\n",
      "0.7427536231884058\n",
      "batch_idx: 23\n",
      "0.7378472222222222\n",
      "batch_idx: 24\n",
      "0.7366666666666667\n",
      "Training Epoch: 287, total loss: 44.056234\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.7\n",
      "batch_idx: 5\n",
      "0.6944444444444444\n",
      "batch_idx: 6\n",
      "0.6964285714285714\n",
      "batch_idx: 7\n",
      "0.6875\n",
      "batch_idx: 8\n",
      "0.6944444444444444\n",
      "batch_idx: 9\n",
      "0.6833333333333333\n",
      "batch_idx: 10\n",
      "0.6856060606060606\n",
      "batch_idx: 11\n",
      "0.6736111111111112\n",
      "batch_idx: 12\n",
      "0.6698717948717948\n",
      "batch_idx: 13\n",
      "0.6815476190476191\n",
      "batch_idx: 14\n",
      "0.675\n",
      "batch_idx: 15\n",
      "0.6796875\n",
      "batch_idx: 16\n",
      "0.6666666666666666\n",
      "batch_idx: 17\n",
      "0.6736111111111112\n",
      "batch_idx: 18\n",
      "0.6842105263157895\n",
      "batch_idx: 19\n",
      "0.68125\n",
      "batch_idx: 20\n",
      "0.6865079365079365\n",
      "batch_idx: 21\n",
      "0.6912878787878788\n",
      "batch_idx: 22\n",
      "0.6865942028985508\n",
      "batch_idx: 23\n",
      "0.6909722222222222\n",
      "batch_idx: 24\n",
      "0.695\n",
      "Training Epoch: 288, total loss: 44.733826\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.6833333333333333\n",
      "batch_idx: 5\n",
      "0.7152777777777778\n",
      "batch_idx: 6\n",
      "0.6547619047619048\n",
      "batch_idx: 7\n",
      "0.6614583333333334\n",
      "batch_idx: 8\n",
      "0.6712962962962963\n",
      "batch_idx: 9\n",
      "0.6666666666666666\n",
      "batch_idx: 10\n",
      "0.6742424242424242\n",
      "batch_idx: 11\n",
      "0.6736111111111112\n",
      "batch_idx: 12\n",
      "0.6826923076923077\n",
      "batch_idx: 13\n",
      "0.6904761904761905\n",
      "batch_idx: 14\n",
      "0.6944444444444444\n",
      "batch_idx: 15\n",
      "0.6927083333333334\n",
      "batch_idx: 16\n",
      "0.6936274509803921\n",
      "batch_idx: 17\n",
      "0.7013888888888888\n",
      "batch_idx: 18\n",
      "0.706140350877193\n",
      "batch_idx: 19\n",
      "0.6979166666666666\n",
      "batch_idx: 20\n",
      "0.7043650793650794\n",
      "batch_idx: 21\n",
      "0.7064393939393939\n",
      "batch_idx: 22\n",
      "0.7083333333333334\n",
      "batch_idx: 23\n",
      "0.7065972222222222\n",
      "batch_idx: 24\n",
      "0.705\n",
      "Training Epoch: 289, total loss: 44.644640\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7559523809523809\n",
      "batch_idx: 7\n",
      "0.71875\n",
      "batch_idx: 8\n",
      "0.7129629629629629\n",
      "batch_idx: 9\n",
      "0.7041666666666667\n",
      "batch_idx: 10\n",
      "0.7045454545454546\n",
      "batch_idx: 11\n",
      "0.6909722222222222\n",
      "batch_idx: 12\n",
      "0.7019230769230769\n",
      "batch_idx: 13\n",
      "0.6964285714285714\n",
      "batch_idx: 14\n",
      "0.7\n",
      "batch_idx: 15\n",
      "0.7109375\n",
      "batch_idx: 16\n",
      "0.7132352941176471\n",
      "batch_idx: 17\n",
      "0.7106481481481481\n",
      "batch_idx: 18\n",
      "0.7127192982456141\n",
      "batch_idx: 19\n",
      "0.7145833333333333\n",
      "batch_idx: 20\n",
      "0.7142857142857143\n",
      "batch_idx: 21\n",
      "0.7102272727272727\n",
      "batch_idx: 22\n",
      "0.7047101449275363\n",
      "batch_idx: 23\n",
      "0.7013888888888888\n",
      "batch_idx: 24\n",
      "0.7033333333333334\n",
      "Training Epoch: 290, total loss: 44.603853\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.6458333333333334\n",
      "batch_idx: 4\n",
      "0.6416666666666667\n",
      "batch_idx: 5\n",
      "0.6597222222222222\n",
      "batch_idx: 6\n",
      "0.6726190476190477\n",
      "batch_idx: 7\n",
      "0.6927083333333334\n",
      "batch_idx: 8\n",
      "0.6898148148148148\n",
      "batch_idx: 9\n",
      "0.7\n",
      "batch_idx: 10\n",
      "0.7007575757575758\n",
      "batch_idx: 11\n",
      "0.6979166666666666\n",
      "batch_idx: 12\n",
      "0.7051282051282052\n",
      "batch_idx: 13\n",
      "0.7142857142857143\n",
      "batch_idx: 14\n",
      "0.7166666666666667\n",
      "batch_idx: 15\n",
      "0.7135416666666666\n",
      "batch_idx: 16\n",
      "0.7083333333333334\n",
      "batch_idx: 17\n",
      "0.7013888888888888\n",
      "batch_idx: 18\n",
      "0.6907894736842105\n",
      "batch_idx: 19\n",
      "0.6958333333333333\n",
      "batch_idx: 20\n",
      "0.6904761904761905\n",
      "batch_idx: 21\n",
      "0.6931818181818182\n",
      "batch_idx: 22\n",
      "0.7010869565217391\n",
      "batch_idx: 23\n",
      "0.7013888888888888\n",
      "batch_idx: 24\n",
      "0.7033333333333334\n",
      "Training Epoch: 291, total loss: 44.654364\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.6354166666666666\n",
      "batch_idx: 4\n",
      "0.6666666666666666\n",
      "batch_idx: 5\n",
      "0.6805555555555556\n",
      "batch_idx: 6\n",
      "0.6904761904761905\n",
      "batch_idx: 7\n",
      "0.7135416666666666\n",
      "batch_idx: 8\n",
      "0.7407407407407407\n",
      "batch_idx: 9\n",
      "0.725\n",
      "batch_idx: 10\n",
      "0.7196969696969697\n",
      "batch_idx: 11\n",
      "0.7118055555555556\n",
      "batch_idx: 12\n",
      "0.7083333333333334\n",
      "batch_idx: 13\n",
      "0.7053571428571429\n",
      "batch_idx: 14\n",
      "0.7083333333333334\n",
      "batch_idx: 15\n",
      "0.71875\n",
      "batch_idx: 16\n",
      "0.7083333333333334\n",
      "batch_idx: 17\n",
      "0.7152777777777778\n",
      "batch_idx: 18\n",
      "0.7083333333333334\n",
      "batch_idx: 19\n",
      "0.7104166666666667\n",
      "batch_idx: 20\n",
      "0.7182539682539683\n",
      "batch_idx: 21\n",
      "0.7178030303030303\n",
      "batch_idx: 22\n",
      "0.7228260869565217\n",
      "batch_idx: 23\n",
      "0.7222222222222222\n",
      "batch_idx: 24\n",
      "0.7216666666666667\n",
      "Training Epoch: 292, total loss: 44.294613\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7083333333333334\n",
      "batch_idx: 6\n",
      "0.7202380952380952\n",
      "batch_idx: 7\n",
      "0.7239583333333334\n",
      "batch_idx: 8\n",
      "0.7407407407407407\n",
      "batch_idx: 9\n",
      "0.7458333333333333\n",
      "batch_idx: 10\n",
      "0.75\n",
      "batch_idx: 11\n",
      "0.7465277777777778\n",
      "batch_idx: 12\n",
      "0.7403846153846154\n",
      "batch_idx: 13\n",
      "0.7291666666666666\n",
      "batch_idx: 14\n",
      "0.7166666666666667\n",
      "batch_idx: 15\n",
      "0.7083333333333334\n",
      "batch_idx: 16\n",
      "0.7034313725490197\n",
      "batch_idx: 17\n",
      "0.7083333333333334\n",
      "batch_idx: 18\n",
      "0.7083333333333334\n",
      "batch_idx: 19\n",
      "0.7020833333333333\n",
      "batch_idx: 20\n",
      "0.7083333333333334\n",
      "batch_idx: 21\n",
      "0.7045454545454546\n",
      "batch_idx: 22\n",
      "0.7010869565217391\n",
      "batch_idx: 23\n",
      "0.6996527777777778\n",
      "batch_idx: 24\n",
      "0.7033333333333334\n",
      "Training Epoch: 293, total loss: 44.674311\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.725\n",
      "batch_idx: 5\n",
      "0.7013888888888888\n",
      "batch_idx: 6\n",
      "0.7261904761904762\n",
      "batch_idx: 7\n",
      "0.7395833333333334\n",
      "batch_idx: 8\n",
      "0.7361111111111112\n",
      "batch_idx: 9\n",
      "0.7291666666666666\n",
      "batch_idx: 10\n",
      "0.7272727272727273\n",
      "batch_idx: 11\n",
      "0.7152777777777778\n",
      "batch_idx: 12\n",
      "0.717948717948718\n",
      "batch_idx: 13\n",
      "0.7172619047619048\n",
      "batch_idx: 14\n",
      "0.7111111111111111\n",
      "batch_idx: 15\n",
      "0.7135416666666666\n",
      "batch_idx: 16\n",
      "0.7156862745098039\n",
      "batch_idx: 17\n",
      "0.7175925925925926\n",
      "batch_idx: 18\n",
      "0.7127192982456141\n",
      "batch_idx: 19\n",
      "0.7104166666666667\n",
      "batch_idx: 20\n",
      "0.7103174603174603\n",
      "batch_idx: 21\n",
      "0.7121212121212122\n",
      "batch_idx: 22\n",
      "0.7028985507246377\n",
      "batch_idx: 23\n",
      "0.7048611111111112\n",
      "batch_idx: 24\n",
      "0.7016666666666667\n",
      "Training Epoch: 294, total loss: 44.784846\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.7013888888888888\n",
      "batch_idx: 6\n",
      "0.6904761904761905\n",
      "batch_idx: 7\n",
      "0.6822916666666666\n",
      "batch_idx: 8\n",
      "0.6990740740740741\n",
      "batch_idx: 9\n",
      "0.7041666666666667\n",
      "batch_idx: 10\n",
      "0.7007575757575758\n",
      "batch_idx: 11\n",
      "0.7083333333333334\n",
      "batch_idx: 12\n",
      "0.7147435897435898\n",
      "batch_idx: 13\n",
      "0.7083333333333334\n",
      "batch_idx: 14\n",
      "0.7166666666666667\n",
      "batch_idx: 15\n",
      "0.7291666666666666\n",
      "batch_idx: 16\n",
      "0.7279411764705882\n",
      "batch_idx: 17\n",
      "0.7175925925925926\n",
      "batch_idx: 18\n",
      "0.7171052631578947\n",
      "batch_idx: 19\n",
      "0.7208333333333333\n",
      "batch_idx: 20\n",
      "0.7281746031746031\n",
      "batch_idx: 21\n",
      "0.7215909090909091\n",
      "batch_idx: 22\n",
      "0.7228260869565217\n",
      "batch_idx: 23\n",
      "0.7222222222222222\n",
      "batch_idx: 24\n",
      "0.7233333333333334\n",
      "Training Epoch: 295, total loss: 44.273344\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.6354166666666666\n",
      "batch_idx: 4\n",
      "0.6666666666666666\n",
      "batch_idx: 5\n",
      "0.6666666666666666\n",
      "batch_idx: 6\n",
      "0.6547619047619048\n",
      "batch_idx: 7\n",
      "0.6666666666666666\n",
      "batch_idx: 8\n",
      "0.6574074074074074\n",
      "batch_idx: 9\n",
      "0.6708333333333333\n",
      "batch_idx: 10\n",
      "0.6856060606060606\n",
      "batch_idx: 11\n",
      "0.6875\n",
      "batch_idx: 12\n",
      "0.6826923076923077\n",
      "batch_idx: 13\n",
      "0.6904761904761905\n",
      "batch_idx: 14\n",
      "0.6916666666666667\n",
      "batch_idx: 15\n",
      "0.6848958333333334\n",
      "batch_idx: 16\n",
      "0.6691176470588235\n",
      "batch_idx: 17\n",
      "0.6689814814814815\n",
      "batch_idx: 18\n",
      "0.668859649122807\n",
      "batch_idx: 19\n",
      "0.6729166666666667\n",
      "batch_idx: 20\n",
      "0.6706349206349206\n",
      "batch_idx: 21\n",
      "0.6799242424242424\n",
      "batch_idx: 22\n",
      "0.677536231884058\n",
      "batch_idx: 23\n",
      "0.6875\n",
      "batch_idx: 24\n",
      "0.685\n",
      "Training Epoch: 296, total loss: 44.990900\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.7083333333333334\n",
      "batch_idx: 6\n",
      "0.7083333333333334\n",
      "batch_idx: 7\n",
      "0.6822916666666666\n",
      "batch_idx: 8\n",
      "0.6666666666666666\n",
      "batch_idx: 9\n",
      "0.6833333333333333\n",
      "batch_idx: 10\n",
      "0.6818181818181818\n",
      "batch_idx: 11\n",
      "0.6805555555555556\n",
      "batch_idx: 12\n",
      "0.6698717948717948\n",
      "batch_idx: 13\n",
      "0.6785714285714286\n",
      "batch_idx: 14\n",
      "0.675\n",
      "batch_idx: 15\n",
      "0.6822916666666666\n",
      "batch_idx: 16\n",
      "0.6887254901960784\n",
      "batch_idx: 17\n",
      "0.6944444444444444\n",
      "batch_idx: 18\n",
      "0.7039473684210527\n",
      "batch_idx: 19\n",
      "0.7020833333333333\n",
      "batch_idx: 20\n",
      "0.7003968253968254\n",
      "batch_idx: 21\n",
      "0.7083333333333334\n",
      "batch_idx: 22\n",
      "0.7083333333333334\n",
      "batch_idx: 23\n",
      "0.6979166666666666\n",
      "batch_idx: 24\n",
      "0.695\n",
      "Training Epoch: 297, total loss: 44.797660\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.6458333333333334\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6875\n",
      "batch_idx: 6\n",
      "0.7202380952380952\n",
      "batch_idx: 7\n",
      "0.6875\n",
      "batch_idx: 8\n",
      "0.6944444444444444\n",
      "batch_idx: 9\n",
      "0.6958333333333333\n",
      "batch_idx: 10\n",
      "0.7083333333333334\n",
      "batch_idx: 11\n",
      "0.71875\n",
      "batch_idx: 12\n",
      "0.717948717948718\n",
      "batch_idx: 13\n",
      "0.7232142857142857\n",
      "batch_idx: 14\n",
      "0.7361111111111112\n",
      "batch_idx: 15\n",
      "0.7265625\n",
      "batch_idx: 16\n",
      "0.7254901960784313\n",
      "batch_idx: 17\n",
      "0.7199074074074074\n",
      "batch_idx: 18\n",
      "0.7171052631578947\n",
      "batch_idx: 19\n",
      "0.7166666666666667\n",
      "batch_idx: 20\n",
      "0.7103174603174603\n",
      "batch_idx: 21\n",
      "0.7159090909090909\n",
      "batch_idx: 22\n",
      "0.7192028985507246\n",
      "batch_idx: 23\n",
      "0.7239583333333334\n",
      "batch_idx: 24\n",
      "0.72\n",
      "Training Epoch: 298, total loss: 44.274829\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.75\n",
      "batch_idx: 9\n",
      "0.7333333333333333\n",
      "batch_idx: 10\n",
      "0.7348484848484849\n",
      "batch_idx: 11\n",
      "0.7291666666666666\n",
      "batch_idx: 12\n",
      "0.7275641025641025\n",
      "batch_idx: 13\n",
      "0.7142857142857143\n",
      "batch_idx: 14\n",
      "0.7166666666666667\n",
      "batch_idx: 15\n",
      "0.7083333333333334\n",
      "batch_idx: 16\n",
      "0.7156862745098039\n",
      "batch_idx: 17\n",
      "0.7083333333333334\n",
      "batch_idx: 18\n",
      "0.7149122807017544\n",
      "batch_idx: 19\n",
      "0.7145833333333333\n",
      "batch_idx: 20\n",
      "0.7103174603174603\n",
      "batch_idx: 21\n",
      "0.7064393939393939\n",
      "batch_idx: 22\n",
      "0.7047101449275363\n",
      "batch_idx: 23\n",
      "0.7135416666666666\n",
      "batch_idx: 24\n",
      "0.7166666666666667\n",
      "Training Epoch: 299, total loss: 44.362554\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7321428571428571\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.7407407407407407\n",
      "batch_idx: 9\n",
      "0.7458333333333333\n",
      "batch_idx: 10\n",
      "0.7424242424242424\n",
      "batch_idx: 11\n",
      "0.7430555555555556\n",
      "batch_idx: 12\n",
      "0.7243589743589743\n",
      "batch_idx: 13\n",
      "0.7261904761904762\n",
      "batch_idx: 14\n",
      "0.7222222222222222\n",
      "batch_idx: 15\n",
      "0.71875\n",
      "batch_idx: 16\n",
      "0.7230392156862745\n",
      "batch_idx: 17\n",
      "0.7175925925925926\n",
      "batch_idx: 18\n",
      "0.7214912280701754\n",
      "batch_idx: 19\n",
      "0.7229166666666667\n",
      "batch_idx: 20\n",
      "0.7182539682539683\n",
      "batch_idx: 21\n",
      "0.7215909090909091\n",
      "batch_idx: 22\n",
      "0.730072463768116\n",
      "batch_idx: 23\n",
      "0.7291666666666666\n",
      "batch_idx: 24\n",
      "0.7266666666666667\n",
      "Training Epoch: 300, total loss: 44.151446\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.7291666666666666\n",
      "batch_idx: 8\n",
      "0.7268518518518519\n",
      "batch_idx: 9\n",
      "0.7375\n",
      "batch_idx: 10\n",
      "0.7424242424242424\n",
      "batch_idx: 11\n",
      "0.7430555555555556\n",
      "batch_idx: 12\n",
      "0.7403846153846154\n",
      "batch_idx: 13\n",
      "0.7291666666666666\n",
      "batch_idx: 14\n",
      "0.7138888888888889\n",
      "batch_idx: 15\n",
      "0.7083333333333334\n",
      "batch_idx: 16\n",
      "0.7107843137254902\n",
      "batch_idx: 17\n",
      "0.7083333333333334\n",
      "batch_idx: 18\n",
      "0.7105263157894737\n",
      "batch_idx: 19\n",
      "0.7083333333333334\n",
      "batch_idx: 20\n",
      "0.7003968253968254\n",
      "batch_idx: 21\n",
      "0.696969696969697\n",
      "batch_idx: 22\n",
      "0.6920289855072463\n",
      "batch_idx: 23\n",
      "0.6875\n",
      "batch_idx: 24\n",
      "0.6866666666666666\n",
      "Training Epoch: 301, total loss: 45.063959\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.625\n",
      "batch_idx: 4\n",
      "0.65\n",
      "batch_idx: 5\n",
      "0.6527777777777778\n",
      "batch_idx: 6\n",
      "0.6785714285714286\n",
      "batch_idx: 7\n",
      "0.703125\n",
      "batch_idx: 8\n",
      "0.7083333333333334\n",
      "batch_idx: 9\n",
      "0.7208333333333333\n",
      "batch_idx: 10\n",
      "0.7272727272727273\n",
      "batch_idx: 11\n",
      "0.7256944444444444\n",
      "batch_idx: 12\n",
      "0.7275641025641025\n",
      "batch_idx: 13\n",
      "0.7291666666666666\n",
      "batch_idx: 14\n",
      "0.7222222222222222\n",
      "batch_idx: 15\n",
      "0.7213541666666666\n",
      "batch_idx: 16\n",
      "0.7254901960784313\n",
      "batch_idx: 17\n",
      "0.7222222222222222\n",
      "batch_idx: 18\n",
      "0.7236842105263158\n",
      "batch_idx: 19\n",
      "0.7208333333333333\n",
      "batch_idx: 20\n",
      "0.7242063492063492\n",
      "batch_idx: 21\n",
      "0.7272727272727273\n",
      "batch_idx: 22\n",
      "0.730072463768116\n",
      "batch_idx: 23\n",
      "0.7309027777777778\n",
      "batch_idx: 24\n",
      "0.72\n",
      "Training Epoch: 302, total loss: 44.398987\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6388888888888888\n",
      "batch_idx: 6\n",
      "0.6547619047619048\n",
      "batch_idx: 7\n",
      "0.640625\n",
      "batch_idx: 8\n",
      "0.6296296296296297\n",
      "batch_idx: 9\n",
      "0.65\n",
      "batch_idx: 10\n",
      "0.6553030303030303\n",
      "batch_idx: 11\n",
      "0.6631944444444444\n",
      "batch_idx: 12\n",
      "0.6666666666666666\n",
      "batch_idx: 13\n",
      "0.6755952380952381\n",
      "batch_idx: 14\n",
      "0.6861111111111111\n",
      "batch_idx: 15\n",
      "0.6901041666666666\n",
      "batch_idx: 16\n",
      "0.6813725490196079\n",
      "batch_idx: 17\n",
      "0.6875\n",
      "batch_idx: 18\n",
      "0.6929824561403509\n",
      "batch_idx: 19\n",
      "0.6854166666666667\n",
      "batch_idx: 20\n",
      "0.6904761904761905\n",
      "batch_idx: 21\n",
      "0.6950757575757576\n",
      "batch_idx: 22\n",
      "0.7010869565217391\n",
      "batch_idx: 23\n",
      "0.703125\n",
      "batch_idx: 24\n",
      "0.7033333333333334\n",
      "Training Epoch: 303, total loss: 44.686365\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.6736111111111112\n",
      "batch_idx: 6\n",
      "0.6904761904761905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 7\n",
      "0.6875\n",
      "batch_idx: 8\n",
      "0.7083333333333334\n",
      "batch_idx: 9\n",
      "0.7125\n",
      "batch_idx: 10\n",
      "0.7121212121212122\n",
      "batch_idx: 11\n",
      "0.7048611111111112\n",
      "batch_idx: 12\n",
      "0.7051282051282052\n",
      "batch_idx: 13\n",
      "0.6964285714285714\n",
      "batch_idx: 14\n",
      "0.6944444444444444\n",
      "batch_idx: 15\n",
      "0.6875\n",
      "batch_idx: 16\n",
      "0.6862745098039216\n",
      "batch_idx: 17\n",
      "0.6875\n",
      "batch_idx: 18\n",
      "0.6885964912280702\n",
      "batch_idx: 19\n",
      "0.7\n",
      "batch_idx: 20\n",
      "0.7083333333333334\n",
      "batch_idx: 21\n",
      "0.7045454545454546\n",
      "batch_idx: 22\n",
      "0.697463768115942\n",
      "batch_idx: 23\n",
      "0.6996527777777778\n",
      "batch_idx: 24\n",
      "0.6983333333333334\n",
      "Training Epoch: 304, total loss: 44.684501\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.7152777777777778\n",
      "batch_idx: 6\n",
      "0.7083333333333334\n",
      "batch_idx: 7\n",
      "0.6979166666666666\n",
      "batch_idx: 8\n",
      "0.6898148148148148\n",
      "batch_idx: 9\n",
      "0.6708333333333333\n",
      "batch_idx: 10\n",
      "0.6856060606060606\n",
      "batch_idx: 11\n",
      "0.6875\n",
      "batch_idx: 12\n",
      "0.6730769230769231\n",
      "batch_idx: 13\n",
      "0.6815476190476191\n",
      "batch_idx: 14\n",
      "0.6805555555555556\n",
      "batch_idx: 15\n",
      "0.6822916666666666\n",
      "batch_idx: 16\n",
      "0.6838235294117647\n",
      "batch_idx: 17\n",
      "0.6782407407407407\n",
      "batch_idx: 18\n",
      "0.6820175438596491\n",
      "batch_idx: 19\n",
      "0.6791666666666667\n",
      "batch_idx: 20\n",
      "0.6765873015873016\n",
      "batch_idx: 21\n",
      "0.6799242424242424\n",
      "batch_idx: 22\n",
      "0.6847826086956522\n",
      "batch_idx: 23\n",
      "0.6875\n",
      "batch_idx: 24\n",
      "0.6916666666666667\n",
      "Training Epoch: 305, total loss: 44.872044\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.6944444444444444\n",
      "batch_idx: 6\n",
      "0.6726190476190477\n",
      "batch_idx: 7\n",
      "0.6822916666666666\n",
      "batch_idx: 8\n",
      "0.6944444444444444\n",
      "batch_idx: 9\n",
      "0.6916666666666667\n",
      "batch_idx: 10\n",
      "0.678030303030303\n",
      "batch_idx: 11\n",
      "0.6875\n",
      "batch_idx: 12\n",
      "0.6891025641025641\n",
      "batch_idx: 13\n",
      "0.6845238095238095\n",
      "batch_idx: 14\n",
      "0.6888888888888889\n",
      "batch_idx: 15\n",
      "0.6822916666666666\n",
      "batch_idx: 16\n",
      "0.6813725490196079\n",
      "batch_idx: 17\n",
      "0.6782407407407407\n",
      "batch_idx: 18\n",
      "0.6776315789473685\n",
      "batch_idx: 19\n",
      "0.6791666666666667\n",
      "batch_idx: 20\n",
      "0.6785714285714286\n",
      "batch_idx: 21\n",
      "0.678030303030303\n",
      "batch_idx: 22\n",
      "0.6793478260869565\n",
      "batch_idx: 23\n",
      "0.6770833333333334\n",
      "batch_idx: 24\n",
      "0.6816666666666666\n",
      "Training Epoch: 306, total loss: 44.893630\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.7361111111111112\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.7239583333333334\n",
      "batch_idx: 8\n",
      "0.7222222222222222\n",
      "batch_idx: 9\n",
      "0.725\n",
      "batch_idx: 10\n",
      "0.7196969696969697\n",
      "batch_idx: 11\n",
      "0.71875\n",
      "batch_idx: 12\n",
      "0.717948717948718\n",
      "batch_idx: 13\n",
      "0.7113095238095238\n",
      "batch_idx: 14\n",
      "0.7138888888888889\n",
      "batch_idx: 15\n",
      "0.71875\n",
      "batch_idx: 16\n",
      "0.7107843137254902\n",
      "batch_idx: 17\n",
      "0.7106481481481481\n",
      "batch_idx: 18\n",
      "0.7105263157894737\n",
      "batch_idx: 19\n",
      "0.7083333333333334\n",
      "batch_idx: 20\n",
      "0.7043650793650794\n",
      "batch_idx: 21\n",
      "0.7064393939393939\n",
      "batch_idx: 22\n",
      "0.7047101449275363\n",
      "batch_idx: 23\n",
      "0.7083333333333334\n",
      "batch_idx: 24\n",
      "0.7133333333333334\n",
      "Training Epoch: 307, total loss: 44.464715\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7440476190476191\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7361111111111112\n",
      "batch_idx: 9\n",
      "0.7541666666666667\n",
      "batch_idx: 10\n",
      "0.75\n",
      "batch_idx: 11\n",
      "0.7430555555555556\n",
      "batch_idx: 12\n",
      "0.7435897435897436\n",
      "batch_idx: 13\n",
      "0.7410714285714286\n",
      "batch_idx: 14\n",
      "0.7222222222222222\n",
      "batch_idx: 15\n",
      "0.7161458333333334\n",
      "batch_idx: 16\n",
      "0.7181372549019608\n",
      "batch_idx: 17\n",
      "0.7222222222222222\n",
      "batch_idx: 18\n",
      "0.7280701754385965\n",
      "batch_idx: 19\n",
      "0.7291666666666666\n",
      "batch_idx: 20\n",
      "0.7341269841269841\n",
      "batch_idx: 21\n",
      "0.7329545454545454\n",
      "batch_idx: 22\n",
      "0.7373188405797102\n",
      "batch_idx: 23\n",
      "0.7378472222222222\n",
      "batch_idx: 24\n",
      "0.74\n",
      "Training Epoch: 308, total loss: 43.973703\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.7361111111111112\n",
      "batch_idx: 6\n",
      "0.7440476190476191\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7361111111111112\n",
      "batch_idx: 9\n",
      "0.7375\n",
      "batch_idx: 10\n",
      "0.7575757575757576\n",
      "batch_idx: 11\n",
      "0.75\n",
      "batch_idx: 12\n",
      "0.7596153846153846\n",
      "batch_idx: 13\n",
      "0.75\n",
      "batch_idx: 14\n",
      "0.7416666666666667\n",
      "batch_idx: 15\n",
      "0.7447916666666666\n",
      "batch_idx: 16\n",
      "0.7426470588235294\n",
      "batch_idx: 17\n",
      "0.7361111111111112\n",
      "batch_idx: 18\n",
      "0.7368421052631579\n",
      "batch_idx: 19\n",
      "0.7375\n",
      "batch_idx: 20\n",
      "0.7440476190476191\n",
      "batch_idx: 21\n",
      "0.7443181818181818\n",
      "batch_idx: 22\n",
      "0.7427536231884058\n",
      "batch_idx: 23\n",
      "0.7447916666666666\n",
      "batch_idx: 24\n",
      "0.745\n",
      "Training Epoch: 309, total loss: 43.931404\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.7152777777777778\n",
      "batch_idx: 6\n",
      "0.7321428571428571\n",
      "batch_idx: 7\n",
      "0.7395833333333334\n",
      "batch_idx: 8\n",
      "0.7453703703703703\n",
      "batch_idx: 9\n",
      "0.7458333333333333\n",
      "batch_idx: 10\n",
      "0.7537878787878788\n",
      "batch_idx: 11\n",
      "0.7465277777777778\n",
      "batch_idx: 12\n",
      "0.7467948717948718\n",
      "batch_idx: 13\n",
      "0.7351190476190477\n",
      "batch_idx: 14\n",
      "0.7305555555555555\n",
      "batch_idx: 15\n",
      "0.7265625\n",
      "batch_idx: 16\n",
      "0.7303921568627451\n",
      "batch_idx: 17\n",
      "0.7199074074074074\n",
      "batch_idx: 18\n",
      "0.7171052631578947\n",
      "batch_idx: 19\n",
      "0.7229166666666667\n",
      "batch_idx: 20\n",
      "0.7123015873015873\n",
      "batch_idx: 21\n",
      "0.7215909090909091\n",
      "batch_idx: 22\n",
      "0.7246376811594203\n",
      "batch_idx: 23\n",
      "0.7274305555555556\n",
      "batch_idx: 24\n",
      "0.725\n",
      "Training Epoch: 310, total loss: 44.086482\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7440476190476191\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7546296296296297\n",
      "batch_idx: 9\n",
      "0.7583333333333333\n",
      "batch_idx: 10\n",
      "0.75\n",
      "batch_idx: 11\n",
      "0.7326388888888888\n",
      "batch_idx: 12\n",
      "0.7435897435897436\n",
      "batch_idx: 13\n",
      "0.7410714285714286\n",
      "batch_idx: 14\n",
      "0.7388888888888889\n",
      "batch_idx: 15\n",
      "0.7421875\n",
      "batch_idx: 16\n",
      "0.7377450980392157\n",
      "batch_idx: 17\n",
      "0.7384259259259259\n",
      "batch_idx: 18\n",
      "0.7324561403508771\n",
      "batch_idx: 19\n",
      "0.7270833333333333\n",
      "batch_idx: 20\n",
      "0.7261904761904762\n",
      "batch_idx: 21\n",
      "0.7272727272727273\n",
      "batch_idx: 22\n",
      "0.7228260869565217\n",
      "batch_idx: 23\n",
      "0.7204861111111112\n",
      "batch_idx: 24\n",
      "0.7133333333333334\n",
      "Training Epoch: 311, total loss: 44.318375\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.7321428571428571\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7407407407407407\n",
      "batch_idx: 9\n",
      "0.75\n",
      "batch_idx: 10\n",
      "0.7537878787878788\n",
      "batch_idx: 11\n",
      "0.7638888888888888\n",
      "batch_idx: 12\n",
      "0.75\n",
      "batch_idx: 13\n",
      "0.7440476190476191\n",
      "batch_idx: 14\n",
      "0.7388888888888889\n",
      "batch_idx: 15\n",
      "0.7369791666666666\n",
      "batch_idx: 16\n",
      "0.7377450980392157\n",
      "batch_idx: 17\n",
      "0.7314814814814815\n",
      "batch_idx: 18\n",
      "0.7280701754385965\n",
      "batch_idx: 19\n",
      "0.73125\n",
      "batch_idx: 20\n",
      "0.7361111111111112\n",
      "batch_idx: 21\n",
      "0.7367424242424242\n",
      "batch_idx: 22\n",
      "0.7373188405797102\n",
      "batch_idx: 23\n",
      "0.7309027777777778\n",
      "batch_idx: 24\n",
      "0.7333333333333333\n",
      "Training Epoch: 312, total loss: 44.062027\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7559523809523809\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.7638888888888888\n",
      "batch_idx: 9\n",
      "0.7625\n",
      "batch_idx: 10\n",
      "0.7727272727272727\n",
      "batch_idx: 11\n",
      "0.7743055555555556\n",
      "batch_idx: 12\n",
      "0.7724358974358975\n",
      "batch_idx: 13\n",
      "0.7738095238095238\n",
      "batch_idx: 14\n",
      "0.7666666666666667\n",
      "batch_idx: 15\n",
      "0.7526041666666666\n",
      "batch_idx: 16\n",
      "0.7377450980392157\n",
      "batch_idx: 17\n",
      "0.7268518518518519\n",
      "batch_idx: 18\n",
      "0.7368421052631579\n",
      "batch_idx: 19\n",
      "0.7395833333333334\n",
      "batch_idx: 20\n",
      "0.7400793650793651\n",
      "batch_idx: 21\n",
      "0.740530303030303\n",
      "batch_idx: 22\n",
      "0.7409420289855072\n",
      "batch_idx: 23\n",
      "0.7361111111111112\n",
      "batch_idx: 24\n",
      "0.735\n",
      "Training Epoch: 313, total loss: 43.918359\n",
      "batch_idx: 0\n",
      "0.4583333333333333\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.6388888888888888\n",
      "batch_idx: 3\n",
      "0.6458333333333334\n",
      "batch_idx: 4\n",
      "0.6666666666666666\n",
      "batch_idx: 5\n",
      "0.6666666666666666\n",
      "batch_idx: 6\n",
      "0.6488095238095238\n",
      "batch_idx: 7\n",
      "0.6458333333333334\n",
      "batch_idx: 8\n",
      "0.6759259259259259\n",
      "batch_idx: 9\n",
      "0.6625\n",
      "batch_idx: 10\n",
      "0.6704545454545454\n",
      "batch_idx: 11\n",
      "0.6423611111111112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 12\n",
      "0.6410256410256411\n",
      "batch_idx: 13\n",
      "0.6547619047619048\n",
      "batch_idx: 14\n",
      "0.6527777777777778\n",
      "batch_idx: 15\n",
      "0.65625\n",
      "batch_idx: 16\n",
      "0.6568627450980392\n",
      "batch_idx: 17\n",
      "0.6689814814814815\n",
      "batch_idx: 18\n",
      "0.668859649122807\n",
      "batch_idx: 19\n",
      "0.66875\n",
      "batch_idx: 20\n",
      "0.6726190476190477\n",
      "batch_idx: 21\n",
      "0.6723484848484849\n",
      "batch_idx: 22\n",
      "0.6757246376811594\n",
      "batch_idx: 23\n",
      "0.6805555555555556\n",
      "batch_idx: 24\n",
      "0.685\n",
      "Training Epoch: 314, total loss: 45.122049\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.7395833333333334\n",
      "batch_idx: 8\n",
      "0.7175925925925926\n",
      "batch_idx: 9\n",
      "0.7208333333333333\n",
      "batch_idx: 10\n",
      "0.7234848484848485\n",
      "batch_idx: 11\n",
      "0.7256944444444444\n",
      "batch_idx: 12\n",
      "0.7243589743589743\n",
      "batch_idx: 13\n",
      "0.7261904761904762\n",
      "batch_idx: 14\n",
      "0.7277777777777777\n",
      "batch_idx: 15\n",
      "0.7291666666666666\n",
      "batch_idx: 16\n",
      "0.7303921568627451\n",
      "batch_idx: 17\n",
      "0.7245370370370371\n",
      "batch_idx: 18\n",
      "0.7149122807017544\n",
      "batch_idx: 19\n",
      "0.7145833333333333\n",
      "batch_idx: 20\n",
      "0.7162698412698413\n",
      "batch_idx: 21\n",
      "0.7083333333333334\n",
      "batch_idx: 22\n",
      "0.7119565217391305\n",
      "batch_idx: 23\n",
      "0.7118055555555556\n",
      "batch_idx: 24\n",
      "0.7083333333333334\n",
      "Training Epoch: 315, total loss: 44.558184\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.725\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.7261904761904762\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7314814814814815\n",
      "batch_idx: 9\n",
      "0.7333333333333333\n",
      "batch_idx: 10\n",
      "0.7310606060606061\n",
      "batch_idx: 11\n",
      "0.7326388888888888\n",
      "batch_idx: 12\n",
      "0.717948717948718\n",
      "batch_idx: 13\n",
      "0.7142857142857143\n",
      "batch_idx: 14\n",
      "0.7055555555555556\n",
      "batch_idx: 15\n",
      "0.6979166666666666\n",
      "batch_idx: 16\n",
      "0.7107843137254902\n",
      "batch_idx: 17\n",
      "0.7175925925925926\n",
      "batch_idx: 18\n",
      "0.7192982456140351\n",
      "batch_idx: 19\n",
      "0.71875\n",
      "batch_idx: 20\n",
      "0.7222222222222222\n",
      "batch_idx: 21\n",
      "0.7253787878787878\n",
      "batch_idx: 22\n",
      "0.730072463768116\n",
      "batch_idx: 23\n",
      "0.7291666666666666\n",
      "batch_idx: 24\n",
      "0.73\n",
      "Training Epoch: 316, total loss: 44.172626\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7447916666666666\n",
      "batch_idx: 8\n",
      "0.7546296296296297\n",
      "batch_idx: 9\n",
      "0.7541666666666667\n",
      "batch_idx: 10\n",
      "0.7575757575757576\n",
      "batch_idx: 11\n",
      "0.7430555555555556\n",
      "batch_idx: 12\n",
      "0.7564102564102564\n",
      "batch_idx: 13\n",
      "0.7559523809523809\n",
      "batch_idx: 14\n",
      "0.7583333333333333\n",
      "batch_idx: 15\n",
      "0.7473958333333334\n",
      "batch_idx: 16\n",
      "0.7450980392156863\n",
      "batch_idx: 17\n",
      "0.7453703703703703\n",
      "batch_idx: 18\n",
      "0.7456140350877193\n",
      "batch_idx: 19\n",
      "0.7479166666666667\n",
      "batch_idx: 20\n",
      "0.746031746031746\n",
      "batch_idx: 21\n",
      "0.7367424242424242\n",
      "batch_idx: 22\n",
      "0.7336956521739131\n",
      "batch_idx: 23\n",
      "0.734375\n",
      "batch_idx: 24\n",
      "0.73\n",
      "Training Epoch: 317, total loss: 44.110860\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.703125\n",
      "batch_idx: 8\n",
      "0.7083333333333334\n",
      "batch_idx: 9\n",
      "0.7166666666666667\n",
      "batch_idx: 10\n",
      "0.7121212121212122\n",
      "batch_idx: 11\n",
      "0.7118055555555556\n",
      "batch_idx: 12\n",
      "0.717948717948718\n",
      "batch_idx: 13\n",
      "0.7113095238095238\n",
      "batch_idx: 14\n",
      "0.725\n",
      "batch_idx: 15\n",
      "0.7317708333333334\n",
      "batch_idx: 16\n",
      "0.7377450980392157\n",
      "batch_idx: 17\n",
      "0.7407407407407407\n",
      "batch_idx: 18\n",
      "0.7412280701754386\n",
      "batch_idx: 19\n",
      "0.7416666666666667\n",
      "batch_idx: 20\n",
      "0.7440476190476191\n",
      "batch_idx: 21\n",
      "0.7386363636363636\n",
      "batch_idx: 22\n",
      "0.7282608695652174\n",
      "batch_idx: 23\n",
      "0.7256944444444444\n",
      "batch_idx: 24\n",
      "0.7266666666666667\n",
      "Training Epoch: 318, total loss: 44.214536\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7731481481481481\n",
      "batch_idx: 9\n",
      "0.7625\n",
      "batch_idx: 10\n",
      "0.7575757575757576\n",
      "batch_idx: 11\n",
      "0.7465277777777778\n",
      "batch_idx: 12\n",
      "0.7467948717948718\n",
      "batch_idx: 13\n",
      "0.7440476190476191\n",
      "batch_idx: 14\n",
      "0.7305555555555555\n",
      "batch_idx: 15\n",
      "0.7291666666666666\n",
      "batch_idx: 16\n",
      "0.7328431372549019\n",
      "batch_idx: 17\n",
      "0.7291666666666666\n",
      "batch_idx: 18\n",
      "0.7368421052631579\n",
      "batch_idx: 19\n",
      "0.7333333333333333\n",
      "batch_idx: 20\n",
      "0.7301587301587301\n",
      "batch_idx: 21\n",
      "0.7253787878787878\n",
      "batch_idx: 22\n",
      "0.7282608695652174\n",
      "batch_idx: 23\n",
      "0.7326388888888888\n",
      "batch_idx: 24\n",
      "0.7333333333333333\n",
      "Training Epoch: 319, total loss: 44.163687\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.65625\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6875\n",
      "batch_idx: 6\n",
      "0.7023809523809523\n",
      "batch_idx: 7\n",
      "0.6927083333333334\n",
      "batch_idx: 8\n",
      "0.7083333333333334\n",
      "batch_idx: 9\n",
      "0.7083333333333334\n",
      "batch_idx: 10\n",
      "0.7083333333333334\n",
      "batch_idx: 11\n",
      "0.7222222222222222\n",
      "batch_idx: 12\n",
      "0.717948717948718\n",
      "batch_idx: 13\n",
      "0.7053571428571429\n",
      "batch_idx: 14\n",
      "0.7111111111111111\n",
      "batch_idx: 15\n",
      "0.7083333333333334\n",
      "batch_idx: 16\n",
      "0.7083333333333334\n",
      "batch_idx: 17\n",
      "0.7152777777777778\n",
      "batch_idx: 18\n",
      "0.7105263157894737\n",
      "batch_idx: 19\n",
      "0.7104166666666667\n",
      "batch_idx: 20\n",
      "0.7123015873015873\n",
      "batch_idx: 21\n",
      "0.7121212121212122\n",
      "batch_idx: 22\n",
      "0.7155797101449275\n",
      "batch_idx: 23\n",
      "0.7083333333333334\n",
      "batch_idx: 24\n",
      "0.7166666666666667\n",
      "Training Epoch: 320, total loss: 44.416770\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.6111111111111112\n",
      "batch_idx: 3\n",
      "0.65625\n",
      "batch_idx: 4\n",
      "0.6583333333333333\n",
      "batch_idx: 5\n",
      "0.6736111111111112\n",
      "batch_idx: 6\n",
      "0.6785714285714286\n",
      "batch_idx: 7\n",
      "0.703125\n",
      "batch_idx: 8\n",
      "0.7129629629629629\n",
      "batch_idx: 9\n",
      "0.725\n",
      "batch_idx: 10\n",
      "0.7348484848484849\n",
      "batch_idx: 11\n",
      "0.7361111111111112\n",
      "batch_idx: 12\n",
      "0.7339743589743589\n",
      "batch_idx: 13\n",
      "0.7380952380952381\n",
      "batch_idx: 14\n",
      "0.7388888888888889\n",
      "batch_idx: 15\n",
      "0.7369791666666666\n",
      "batch_idx: 16\n",
      "0.7377450980392157\n",
      "batch_idx: 17\n",
      "0.7361111111111112\n",
      "batch_idx: 18\n",
      "0.7368421052631579\n",
      "batch_idx: 19\n",
      "0.7375\n",
      "batch_idx: 20\n",
      "0.7301587301587301\n",
      "batch_idx: 21\n",
      "0.7291666666666666\n",
      "batch_idx: 22\n",
      "0.7282608695652174\n",
      "batch_idx: 23\n",
      "0.7291666666666666\n",
      "batch_idx: 24\n",
      "0.7183333333333334\n",
      "Training Epoch: 321, total loss: 44.383230\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7321428571428571\n",
      "batch_idx: 7\n",
      "0.7239583333333334\n",
      "batch_idx: 8\n",
      "0.7407407407407407\n",
      "batch_idx: 9\n",
      "0.7416666666666667\n",
      "batch_idx: 10\n",
      "0.7462121212121212\n",
      "batch_idx: 11\n",
      "0.7569444444444444\n",
      "batch_idx: 12\n",
      "0.7532051282051282\n",
      "batch_idx: 13\n",
      "0.7440476190476191\n",
      "batch_idx: 14\n",
      "0.75\n",
      "batch_idx: 15\n",
      "0.7526041666666666\n",
      "batch_idx: 16\n",
      "0.7475490196078431\n",
      "batch_idx: 17\n",
      "0.75\n",
      "batch_idx: 18\n",
      "0.7521929824561403\n",
      "batch_idx: 19\n",
      "0.7541666666666667\n",
      "batch_idx: 20\n",
      "0.748015873015873\n",
      "batch_idx: 21\n",
      "0.740530303030303\n",
      "batch_idx: 22\n",
      "0.7318840579710145\n",
      "batch_idx: 23\n",
      "0.7291666666666666\n",
      "batch_idx: 24\n",
      "0.73\n",
      "Training Epoch: 322, total loss: 44.113160\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.6833333333333333\n",
      "batch_idx: 5\n",
      "0.6736111111111112\n",
      "batch_idx: 6\n",
      "0.6785714285714286\n",
      "batch_idx: 7\n",
      "0.6875\n",
      "batch_idx: 8\n",
      "0.6944444444444444\n",
      "batch_idx: 9\n",
      "0.6875\n",
      "batch_idx: 10\n",
      "0.6931818181818182\n",
      "batch_idx: 11\n",
      "0.7013888888888888\n",
      "batch_idx: 12\n",
      "0.6923076923076923\n",
      "batch_idx: 13\n",
      "0.7023809523809523\n",
      "batch_idx: 14\n",
      "0.7083333333333334\n",
      "batch_idx: 15\n",
      "0.7083333333333334\n",
      "batch_idx: 16\n",
      "0.7156862745098039\n",
      "batch_idx: 17\n",
      "0.7175925925925926\n",
      "batch_idx: 18\n",
      "0.7171052631578947\n",
      "batch_idx: 19\n",
      "0.71875\n",
      "batch_idx: 20\n",
      "0.7142857142857143\n",
      "batch_idx: 21\n",
      "0.7121212121212122\n",
      "batch_idx: 22\n",
      "0.7101449275362319\n",
      "batch_idx: 23\n",
      "0.7065972222222222\n",
      "batch_idx: 24\n",
      "0.71\n",
      "Training Epoch: 323, total loss: 44.498125\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.7638888888888888\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.7692307692307693\n",
      "batch_idx: 13\n",
      "0.7619047619047619\n",
      "batch_idx: 14\n",
      "0.75\n",
      "batch_idx: 15\n",
      "0.7447916666666666\n",
      "batch_idx: 16\n",
      "0.7352941176470589\n",
      "batch_idx: 17\n",
      "0.7384259259259259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 18\n",
      "0.7456140350877193\n",
      "batch_idx: 19\n",
      "0.73125\n",
      "batch_idx: 20\n",
      "0.7242063492063492\n",
      "batch_idx: 21\n",
      "0.7253787878787878\n",
      "batch_idx: 22\n",
      "0.7246376811594203\n",
      "batch_idx: 23\n",
      "0.7204861111111112\n",
      "batch_idx: 24\n",
      "0.72\n",
      "Training Epoch: 324, total loss: 44.342946\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.7440476190476191\n",
      "batch_idx: 7\n",
      "0.7395833333333334\n",
      "batch_idx: 8\n",
      "0.75\n",
      "batch_idx: 9\n",
      "0.7458333333333333\n",
      "batch_idx: 10\n",
      "0.7234848484848485\n",
      "batch_idx: 11\n",
      "0.7256944444444444\n",
      "batch_idx: 12\n",
      "0.7275641025641025\n",
      "batch_idx: 13\n",
      "0.7261904761904762\n",
      "batch_idx: 14\n",
      "0.7333333333333333\n",
      "batch_idx: 15\n",
      "0.7161458333333334\n",
      "batch_idx: 16\n",
      "0.7156862745098039\n",
      "batch_idx: 17\n",
      "0.7222222222222222\n",
      "batch_idx: 18\n",
      "0.7192982456140351\n",
      "batch_idx: 19\n",
      "0.7229166666666667\n",
      "batch_idx: 20\n",
      "0.7202380952380952\n",
      "batch_idx: 21\n",
      "0.7140151515151515\n",
      "batch_idx: 22\n",
      "0.7137681159420289\n",
      "batch_idx: 23\n",
      "0.7170138888888888\n",
      "batch_idx: 24\n",
      "0.7133333333333334\n",
      "Training Epoch: 325, total loss: 44.506717\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.7083333333333334\n",
      "batch_idx: 6\n",
      "0.7202380952380952\n",
      "batch_idx: 7\n",
      "0.7135416666666666\n",
      "batch_idx: 8\n",
      "0.6990740740740741\n",
      "batch_idx: 9\n",
      "0.7\n",
      "batch_idx: 10\n",
      "0.7159090909090909\n",
      "batch_idx: 11\n",
      "0.71875\n",
      "batch_idx: 12\n",
      "0.7147435897435898\n",
      "batch_idx: 13\n",
      "0.7113095238095238\n",
      "batch_idx: 14\n",
      "0.7166666666666667\n",
      "batch_idx: 15\n",
      "0.7057291666666666\n",
      "batch_idx: 16\n",
      "0.7009803921568627\n",
      "batch_idx: 17\n",
      "0.7013888888888888\n",
      "batch_idx: 18\n",
      "0.6973684210526315\n",
      "batch_idx: 19\n",
      "0.7\n",
      "batch_idx: 20\n",
      "0.6904761904761905\n",
      "batch_idx: 21\n",
      "0.6893939393939394\n",
      "batch_idx: 22\n",
      "0.6865942028985508\n",
      "batch_idx: 23\n",
      "0.6875\n",
      "batch_idx: 24\n",
      "0.69\n",
      "Training Epoch: 326, total loss: 45.044317\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.6666666666666666\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.6736111111111112\n",
      "batch_idx: 6\n",
      "0.6845238095238095\n",
      "batch_idx: 7\n",
      "0.6875\n",
      "batch_idx: 8\n",
      "0.6990740740740741\n",
      "batch_idx: 9\n",
      "0.7\n",
      "batch_idx: 10\n",
      "0.7121212121212122\n",
      "batch_idx: 11\n",
      "0.6979166666666666\n",
      "batch_idx: 12\n",
      "0.6891025641025641\n",
      "batch_idx: 13\n",
      "0.6964285714285714\n",
      "batch_idx: 14\n",
      "0.6944444444444444\n",
      "batch_idx: 15\n",
      "0.6901041666666666\n",
      "batch_idx: 16\n",
      "0.6936274509803921\n",
      "batch_idx: 17\n",
      "0.6875\n",
      "batch_idx: 18\n",
      "0.6929824561403509\n",
      "batch_idx: 19\n",
      "0.6895833333333333\n",
      "batch_idx: 20\n",
      "0.6904761904761905\n",
      "batch_idx: 21\n",
      "0.6875\n",
      "batch_idx: 22\n",
      "0.6884057971014492\n",
      "batch_idx: 23\n",
      "0.6840277777777778\n",
      "batch_idx: 24\n",
      "0.6816666666666666\n",
      "Training Epoch: 327, total loss: 45.077398\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.7152777777777778\n",
      "batch_idx: 6\n",
      "0.6964285714285714\n",
      "batch_idx: 7\n",
      "0.6979166666666666\n",
      "batch_idx: 8\n",
      "0.6898148148148148\n",
      "batch_idx: 9\n",
      "0.6916666666666667\n",
      "batch_idx: 10\n",
      "0.6893939393939394\n",
      "batch_idx: 11\n",
      "0.6840277777777778\n",
      "batch_idx: 12\n",
      "0.6698717948717948\n",
      "batch_idx: 13\n",
      "0.6696428571428571\n",
      "batch_idx: 14\n",
      "0.675\n",
      "batch_idx: 15\n",
      "0.6796875\n",
      "batch_idx: 16\n",
      "0.6740196078431373\n",
      "batch_idx: 17\n",
      "0.6736111111111112\n",
      "batch_idx: 18\n",
      "0.668859649122807\n",
      "batch_idx: 19\n",
      "0.66875\n",
      "batch_idx: 20\n",
      "0.6726190476190477\n",
      "batch_idx: 21\n",
      "0.6723484848484849\n",
      "batch_idx: 22\n",
      "0.6757246376811594\n",
      "batch_idx: 23\n",
      "0.6736111111111112\n",
      "batch_idx: 24\n",
      "0.6716666666666666\n",
      "Training Epoch: 328, total loss: 45.179246\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7152777777777778\n",
      "batch_idx: 6\n",
      "0.6845238095238095\n",
      "batch_idx: 7\n",
      "0.6979166666666666\n",
      "batch_idx: 8\n",
      "0.6898148148148148\n",
      "batch_idx: 9\n",
      "0.7\n",
      "batch_idx: 10\n",
      "0.696969696969697\n",
      "batch_idx: 11\n",
      "0.6979166666666666\n",
      "batch_idx: 12\n",
      "0.6987179487179487\n",
      "batch_idx: 13\n",
      "0.6934523809523809\n",
      "batch_idx: 14\n",
      "0.6972222222222222\n",
      "batch_idx: 15\n",
      "0.703125\n",
      "batch_idx: 16\n",
      "0.7058823529411765\n",
      "batch_idx: 17\n",
      "0.7106481481481481\n",
      "batch_idx: 18\n",
      "0.7236842105263158\n",
      "batch_idx: 19\n",
      "0.7270833333333333\n",
      "batch_idx: 20\n",
      "0.7222222222222222\n",
      "batch_idx: 21\n",
      "0.7272727272727273\n",
      "batch_idx: 22\n",
      "0.730072463768116\n",
      "batch_idx: 23\n",
      "0.7309027777777778\n",
      "batch_idx: 24\n",
      "0.7283333333333334\n",
      "Training Epoch: 329, total loss: 44.132519\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.5625\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.6875\n",
      "batch_idx: 6\n",
      "0.6785714285714286\n",
      "batch_idx: 7\n",
      "0.6822916666666666\n",
      "batch_idx: 8\n",
      "0.6851851851851852\n",
      "batch_idx: 9\n",
      "0.6833333333333333\n",
      "batch_idx: 10\n",
      "0.678030303030303\n",
      "batch_idx: 11\n",
      "0.6701388888888888\n",
      "batch_idx: 12\n",
      "0.6826923076923077\n",
      "batch_idx: 13\n",
      "0.6815476190476191\n",
      "batch_idx: 14\n",
      "0.6777777777777778\n",
      "batch_idx: 15\n",
      "0.6744791666666666\n",
      "batch_idx: 16\n",
      "0.6715686274509803\n",
      "batch_idx: 17\n",
      "0.6736111111111112\n",
      "batch_idx: 18\n",
      "0.6776315789473685\n",
      "batch_idx: 19\n",
      "0.6791666666666667\n",
      "batch_idx: 20\n",
      "0.6845238095238095\n",
      "batch_idx: 21\n",
      "0.6875\n",
      "batch_idx: 22\n",
      "0.6938405797101449\n",
      "batch_idx: 23\n",
      "0.6979166666666666\n",
      "batch_idx: 24\n",
      "0.6966666666666667\n",
      "Training Epoch: 330, total loss: 44.858982\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.6666666666666666\n",
      "batch_idx: 6\n",
      "0.6785714285714286\n",
      "batch_idx: 7\n",
      "0.6927083333333334\n",
      "batch_idx: 8\n",
      "0.6990740740740741\n",
      "batch_idx: 9\n",
      "0.7041666666666667\n",
      "batch_idx: 10\n",
      "0.7083333333333334\n",
      "batch_idx: 11\n",
      "0.7048611111111112\n",
      "batch_idx: 12\n",
      "0.7051282051282052\n",
      "batch_idx: 13\n",
      "0.7023809523809523\n",
      "batch_idx: 14\n",
      "0.6861111111111111\n",
      "batch_idx: 15\n",
      "0.6901041666666666\n",
      "batch_idx: 16\n",
      "0.6985294117647058\n",
      "batch_idx: 17\n",
      "0.7013888888888888\n",
      "batch_idx: 18\n",
      "0.706140350877193\n",
      "batch_idx: 19\n",
      "0.7041666666666667\n",
      "batch_idx: 20\n",
      "0.7063492063492064\n",
      "batch_idx: 21\n",
      "0.7121212121212122\n",
      "batch_idx: 22\n",
      "0.7101449275362319\n",
      "batch_idx: 23\n",
      "0.7100694444444444\n",
      "batch_idx: 24\n",
      "0.7133333333333334\n",
      "Training Epoch: 331, total loss: 44.465214\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.7202380952380952\n",
      "batch_idx: 7\n",
      "0.71875\n",
      "batch_idx: 8\n",
      "0.7222222222222222\n",
      "batch_idx: 9\n",
      "0.7041666666666667\n",
      "batch_idx: 10\n",
      "0.7083333333333334\n",
      "batch_idx: 11\n",
      "0.7013888888888888\n",
      "batch_idx: 12\n",
      "0.7019230769230769\n",
      "batch_idx: 13\n",
      "0.6934523809523809\n",
      "batch_idx: 14\n",
      "0.6861111111111111\n",
      "batch_idx: 15\n",
      "0.6927083333333334\n",
      "batch_idx: 16\n",
      "0.6936274509803921\n",
      "batch_idx: 17\n",
      "0.6851851851851852\n",
      "batch_idx: 18\n",
      "0.6776315789473685\n",
      "batch_idx: 19\n",
      "0.6791666666666667\n",
      "batch_idx: 20\n",
      "0.6785714285714286\n",
      "batch_idx: 21\n",
      "0.6742424242424242\n",
      "batch_idx: 22\n",
      "0.6739130434782609\n",
      "batch_idx: 23\n",
      "0.6770833333333334\n",
      "batch_idx: 24\n",
      "0.6833333333333333\n",
      "Training Epoch: 332, total loss: 44.982292\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.5694444444444444\n",
      "batch_idx: 3\n",
      "0.6041666666666666\n",
      "batch_idx: 4\n",
      "0.6166666666666667\n",
      "batch_idx: 5\n",
      "0.6319444444444444\n",
      "batch_idx: 6\n",
      "0.6547619047619048\n",
      "batch_idx: 7\n",
      "0.6770833333333334\n",
      "batch_idx: 8\n",
      "0.6805555555555556\n",
      "batch_idx: 9\n",
      "0.6875\n",
      "batch_idx: 10\n",
      "0.6931818181818182\n",
      "batch_idx: 11\n",
      "0.6875\n",
      "batch_idx: 12\n",
      "0.6955128205128205\n",
      "batch_idx: 13\n",
      "0.6815476190476191\n",
      "batch_idx: 14\n",
      "0.6888888888888889\n",
      "batch_idx: 15\n",
      "0.6875\n",
      "batch_idx: 16\n",
      "0.6838235294117647\n",
      "batch_idx: 17\n",
      "0.6875\n",
      "batch_idx: 18\n",
      "0.6907894736842105\n",
      "batch_idx: 19\n",
      "0.6833333333333333\n",
      "batch_idx: 20\n",
      "0.6904761904761905\n",
      "batch_idx: 21\n",
      "0.6818181818181818\n",
      "batch_idx: 22\n",
      "0.6884057971014492\n",
      "batch_idx: 23\n",
      "0.6805555555555556\n",
      "batch_idx: 24\n",
      "0.6766666666666666\n",
      "Training Epoch: 333, total loss: 44.995862\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7447916666666666\n",
      "batch_idx: 8\n",
      "0.7268518518518519\n",
      "batch_idx: 9\n",
      "0.725\n",
      "batch_idx: 10\n",
      "0.7196969696969697\n",
      "batch_idx: 11\n",
      "0.7256944444444444\n",
      "batch_idx: 12\n",
      "0.717948717948718\n",
      "batch_idx: 13\n",
      "0.7172619047619048\n",
      "batch_idx: 14\n",
      "0.7166666666666667\n",
      "batch_idx: 15\n",
      "0.7083333333333334\n",
      "batch_idx: 16\n",
      "0.7132352941176471\n",
      "batch_idx: 17\n",
      "0.7060185185185185\n",
      "batch_idx: 18\n",
      "0.7083333333333334\n",
      "batch_idx: 19\n",
      "0.70625\n",
      "batch_idx: 20\n",
      "0.7142857142857143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 21\n",
      "0.7140151515151515\n",
      "batch_idx: 22\n",
      "0.7155797101449275\n",
      "batch_idx: 23\n",
      "0.7152777777777778\n",
      "batch_idx: 24\n",
      "0.705\n",
      "Training Epoch: 334, total loss: 44.674457\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.6666666666666666\n",
      "batch_idx: 4\n",
      "0.65\n",
      "batch_idx: 5\n",
      "0.6944444444444444\n",
      "batch_idx: 6\n",
      "0.6845238095238095\n",
      "batch_idx: 7\n",
      "0.6979166666666666\n",
      "batch_idx: 8\n",
      "0.6759259259259259\n",
      "batch_idx: 9\n",
      "0.6791666666666667\n",
      "batch_idx: 10\n",
      "0.6628787878787878\n",
      "batch_idx: 11\n",
      "0.6631944444444444\n",
      "batch_idx: 12\n",
      "0.6698717948717948\n",
      "batch_idx: 13\n",
      "0.6696428571428571\n",
      "batch_idx: 14\n",
      "0.6638888888888889\n",
      "batch_idx: 15\n",
      "0.65625\n",
      "batch_idx: 16\n",
      "0.6593137254901961\n",
      "batch_idx: 17\n",
      "0.6620370370370371\n",
      "batch_idx: 18\n",
      "0.6732456140350878\n",
      "batch_idx: 19\n",
      "0.675\n",
      "batch_idx: 20\n",
      "0.6706349206349206\n",
      "batch_idx: 21\n",
      "0.6704545454545454\n",
      "batch_idx: 22\n",
      "0.6721014492753623\n",
      "batch_idx: 23\n",
      "0.6753472222222222\n",
      "batch_idx: 24\n",
      "0.6833333333333333\n",
      "Training Epoch: 335, total loss: 45.039487\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7361111111111112\n",
      "batch_idx: 6\n",
      "0.7202380952380952\n",
      "batch_idx: 7\n",
      "0.7239583333333334\n",
      "batch_idx: 8\n",
      "0.7129629629629629\n",
      "batch_idx: 9\n",
      "0.7166666666666667\n",
      "batch_idx: 10\n",
      "0.7196969696969697\n",
      "batch_idx: 11\n",
      "0.7291666666666666\n",
      "batch_idx: 12\n",
      "0.7403846153846154\n",
      "batch_idx: 13\n",
      "0.7351190476190477\n",
      "batch_idx: 14\n",
      "0.7361111111111112\n",
      "batch_idx: 15\n",
      "0.7291666666666666\n",
      "batch_idx: 16\n",
      "0.7328431372549019\n",
      "batch_idx: 17\n",
      "0.7291666666666666\n",
      "batch_idx: 18\n",
      "0.7236842105263158\n",
      "batch_idx: 19\n",
      "0.7291666666666666\n",
      "batch_idx: 20\n",
      "0.7182539682539683\n",
      "batch_idx: 21\n",
      "0.7102272727272727\n",
      "batch_idx: 22\n",
      "0.7065217391304348\n",
      "batch_idx: 23\n",
      "0.7013888888888888\n",
      "batch_idx: 24\n",
      "0.705\n",
      "Training Epoch: 336, total loss: 44.615710\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.75\n",
      "batch_idx: 9\n",
      "0.7541666666666667\n",
      "batch_idx: 10\n",
      "0.7613636363636364\n",
      "batch_idx: 11\n",
      "0.7534722222222222\n",
      "batch_idx: 12\n",
      "0.7403846153846154\n",
      "batch_idx: 13\n",
      "0.7440476190476191\n",
      "batch_idx: 14\n",
      "0.7416666666666667\n",
      "batch_idx: 15\n",
      "0.7239583333333334\n",
      "batch_idx: 16\n",
      "0.7181372549019608\n",
      "batch_idx: 17\n",
      "0.7129629629629629\n",
      "batch_idx: 18\n",
      "0.7149122807017544\n",
      "batch_idx: 19\n",
      "0.7166666666666667\n",
      "batch_idx: 20\n",
      "0.7182539682539683\n",
      "batch_idx: 21\n",
      "0.7272727272727273\n",
      "batch_idx: 22\n",
      "0.7264492753623188\n",
      "batch_idx: 23\n",
      "0.7309027777777778\n",
      "batch_idx: 24\n",
      "0.7266666666666667\n",
      "Training Epoch: 337, total loss: 44.241370\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7870370370370371\n",
      "batch_idx: 9\n",
      "0.7625\n",
      "batch_idx: 10\n",
      "0.7689393939393939\n",
      "batch_idx: 11\n",
      "0.7604166666666666\n",
      "batch_idx: 12\n",
      "0.7628205128205128\n",
      "batch_idx: 13\n",
      "0.7589285714285714\n",
      "batch_idx: 14\n",
      "0.7472222222222222\n",
      "batch_idx: 15\n",
      "0.7395833333333334\n",
      "batch_idx: 16\n",
      "0.7401960784313726\n",
      "batch_idx: 17\n",
      "0.7384259259259259\n",
      "batch_idx: 18\n",
      "0.7390350877192983\n",
      "batch_idx: 19\n",
      "0.725\n",
      "batch_idx: 20\n",
      "0.7202380952380952\n",
      "batch_idx: 21\n",
      "0.7140151515151515\n",
      "batch_idx: 22\n",
      "0.7192028985507246\n",
      "batch_idx: 23\n",
      "0.7135416666666666\n",
      "batch_idx: 24\n",
      "0.7083333333333334\n",
      "Training Epoch: 338, total loss: 44.583617\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.65625\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6736111111111112\n",
      "batch_idx: 6\n",
      "0.7023809523809523\n",
      "batch_idx: 7\n",
      "0.6979166666666666\n",
      "batch_idx: 8\n",
      "0.6990740740740741\n",
      "batch_idx: 9\n",
      "0.6958333333333333\n",
      "batch_idx: 10\n",
      "0.7007575757575758\n",
      "batch_idx: 11\n",
      "0.7048611111111112\n",
      "batch_idx: 12\n",
      "0.6987179487179487\n",
      "batch_idx: 13\n",
      "0.7083333333333334\n",
      "batch_idx: 14\n",
      "0.7111111111111111\n",
      "batch_idx: 15\n",
      "0.71875\n",
      "batch_idx: 16\n",
      "0.7230392156862745\n",
      "batch_idx: 17\n",
      "0.7337962962962963\n",
      "batch_idx: 18\n",
      "0.7258771929824561\n",
      "batch_idx: 19\n",
      "0.7270833333333333\n",
      "batch_idx: 20\n",
      "0.7321428571428571\n",
      "batch_idx: 21\n",
      "0.7215909090909091\n",
      "batch_idx: 22\n",
      "0.7137681159420289\n",
      "batch_idx: 23\n",
      "0.7118055555555556\n",
      "batch_idx: 24\n",
      "0.7116666666666667\n",
      "Training Epoch: 339, total loss: 44.461282\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.7083333333333334\n",
      "batch_idx: 6\n",
      "0.6904761904761905\n",
      "batch_idx: 7\n",
      "0.7083333333333334\n",
      "batch_idx: 8\n",
      "0.7083333333333334\n",
      "batch_idx: 9\n",
      "0.6958333333333333\n",
      "batch_idx: 10\n",
      "0.7007575757575758\n",
      "batch_idx: 11\n",
      "0.6979166666666666\n",
      "batch_idx: 12\n",
      "0.7083333333333334\n",
      "batch_idx: 13\n",
      "0.7202380952380952\n",
      "batch_idx: 14\n",
      "0.725\n",
      "batch_idx: 15\n",
      "0.7213541666666666\n",
      "batch_idx: 16\n",
      "0.7279411764705882\n",
      "batch_idx: 17\n",
      "0.7245370370370371\n",
      "batch_idx: 18\n",
      "0.7236842105263158\n",
      "batch_idx: 19\n",
      "0.71875\n",
      "batch_idx: 20\n",
      "0.7162698412698413\n",
      "batch_idx: 21\n",
      "0.7196969696969697\n",
      "batch_idx: 22\n",
      "0.7155797101449275\n",
      "batch_idx: 23\n",
      "0.71875\n",
      "batch_idx: 24\n",
      "0.7133333333333334\n",
      "Training Epoch: 340, total loss: 44.349013\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7440476190476191\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7361111111111112\n",
      "batch_idx: 9\n",
      "0.7458333333333333\n",
      "batch_idx: 10\n",
      "0.7272727272727273\n",
      "batch_idx: 11\n",
      "0.7361111111111112\n",
      "batch_idx: 12\n",
      "0.7371794871794872\n",
      "batch_idx: 13\n",
      "0.7291666666666666\n",
      "batch_idx: 14\n",
      "0.7083333333333334\n",
      "batch_idx: 15\n",
      "0.7135416666666666\n",
      "batch_idx: 16\n",
      "0.7132352941176471\n",
      "batch_idx: 17\n",
      "0.7152777777777778\n",
      "batch_idx: 18\n",
      "0.7214912280701754\n",
      "batch_idx: 19\n",
      "0.725\n",
      "batch_idx: 20\n",
      "0.7261904761904762\n",
      "batch_idx: 21\n",
      "0.7215909090909091\n",
      "batch_idx: 22\n",
      "0.7155797101449275\n",
      "batch_idx: 23\n",
      "0.7135416666666666\n",
      "batch_idx: 24\n",
      "0.7166666666666667\n",
      "Training Epoch: 341, total loss: 44.440299\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.6833333333333333\n",
      "batch_idx: 5\n",
      "0.6944444444444444\n",
      "batch_idx: 6\n",
      "0.6845238095238095\n",
      "batch_idx: 7\n",
      "0.6979166666666666\n",
      "batch_idx: 8\n",
      "0.7175925925925926\n",
      "batch_idx: 9\n",
      "0.7291666666666666\n",
      "batch_idx: 10\n",
      "0.7310606060606061\n",
      "batch_idx: 11\n",
      "0.7326388888888888\n",
      "batch_idx: 12\n",
      "0.7275641025641025\n",
      "batch_idx: 13\n",
      "0.7321428571428571\n",
      "batch_idx: 14\n",
      "0.7277777777777777\n",
      "batch_idx: 15\n",
      "0.7317708333333334\n",
      "batch_idx: 16\n",
      "0.7279411764705882\n",
      "batch_idx: 17\n",
      "0.7245370370370371\n",
      "batch_idx: 18\n",
      "0.7171052631578947\n",
      "batch_idx: 19\n",
      "0.7125\n",
      "batch_idx: 20\n",
      "0.7023809523809523\n",
      "batch_idx: 21\n",
      "0.7064393939393939\n",
      "batch_idx: 22\n",
      "0.7137681159420289\n",
      "batch_idx: 23\n",
      "0.71875\n",
      "batch_idx: 24\n",
      "0.7183333333333334\n",
      "Training Epoch: 342, total loss: 44.472816\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7395833333333334\n",
      "batch_idx: 8\n",
      "0.75\n",
      "batch_idx: 9\n",
      "0.7583333333333333\n",
      "batch_idx: 10\n",
      "0.75\n",
      "batch_idx: 11\n",
      "0.75\n",
      "batch_idx: 12\n",
      "0.7628205128205128\n",
      "batch_idx: 13\n",
      "0.7589285714285714\n",
      "batch_idx: 14\n",
      "0.75\n",
      "batch_idx: 15\n",
      "0.7447916666666666\n",
      "batch_idx: 16\n",
      "0.7401960784313726\n",
      "batch_idx: 17\n",
      "0.75\n",
      "batch_idx: 18\n",
      "0.7478070175438597\n",
      "batch_idx: 19\n",
      "0.7458333333333333\n",
      "batch_idx: 20\n",
      "0.7420634920634921\n",
      "batch_idx: 21\n",
      "0.7424242424242424\n",
      "batch_idx: 22\n",
      "0.7409420289855072\n",
      "batch_idx: 23\n",
      "0.7361111111111112\n",
      "batch_idx: 24\n",
      "0.7266666666666667\n",
      "Training Epoch: 343, total loss: 44.249787\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.7202380952380952\n",
      "batch_idx: 7\n",
      "0.7135416666666666\n",
      "batch_idx: 8\n",
      "0.7129629629629629\n",
      "batch_idx: 9\n",
      "0.7208333333333333\n",
      "batch_idx: 10\n",
      "0.7272727272727273\n",
      "batch_idx: 11\n",
      "0.7430555555555556\n",
      "batch_idx: 12\n",
      "0.7435897435897436\n",
      "batch_idx: 13\n",
      "0.7380952380952381\n",
      "batch_idx: 14\n",
      "0.7388888888888889\n",
      "batch_idx: 15\n",
      "0.7447916666666666\n",
      "batch_idx: 16\n",
      "0.7475490196078431\n",
      "batch_idx: 17\n",
      "0.75\n",
      "batch_idx: 18\n",
      "0.7521929824561403\n",
      "batch_idx: 19\n",
      "0.7479166666666667\n",
      "batch_idx: 20\n",
      "0.751984126984127\n",
      "batch_idx: 21\n",
      "0.7462121212121212\n",
      "batch_idx: 22\n",
      "0.7481884057971014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 23\n",
      "0.7517361111111112\n",
      "batch_idx: 24\n",
      "0.7516666666666667\n",
      "Training Epoch: 344, total loss: 43.738512\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.7361111111111112\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.75\n",
      "batch_idx: 9\n",
      "0.7291666666666666\n",
      "batch_idx: 10\n",
      "0.7272727272727273\n",
      "batch_idx: 11\n",
      "0.7222222222222222\n",
      "batch_idx: 12\n",
      "0.7275641025641025\n",
      "batch_idx: 13\n",
      "0.7232142857142857\n",
      "batch_idx: 14\n",
      "0.7194444444444444\n",
      "batch_idx: 15\n",
      "0.7291666666666666\n",
      "batch_idx: 16\n",
      "0.7279411764705882\n",
      "batch_idx: 17\n",
      "0.7361111111111112\n",
      "batch_idx: 18\n",
      "0.7324561403508771\n",
      "batch_idx: 19\n",
      "0.7291666666666666\n",
      "batch_idx: 20\n",
      "0.7361111111111112\n",
      "batch_idx: 21\n",
      "0.7424242424242424\n",
      "batch_idx: 22\n",
      "0.7409420289855072\n",
      "batch_idx: 23\n",
      "0.7447916666666666\n",
      "batch_idx: 24\n",
      "0.7466666666666667\n",
      "Training Epoch: 345, total loss: 43.756164\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.7013888888888888\n",
      "batch_idx: 6\n",
      "0.7142857142857143\n",
      "batch_idx: 7\n",
      "0.71875\n",
      "batch_idx: 8\n",
      "0.7222222222222222\n",
      "batch_idx: 9\n",
      "0.725\n",
      "batch_idx: 10\n",
      "0.7348484848484849\n",
      "batch_idx: 11\n",
      "0.7361111111111112\n",
      "batch_idx: 12\n",
      "0.7339743589743589\n",
      "batch_idx: 13\n",
      "0.7321428571428571\n",
      "batch_idx: 14\n",
      "0.7333333333333333\n",
      "batch_idx: 15\n",
      "0.7395833333333334\n",
      "batch_idx: 16\n",
      "0.7450980392156863\n",
      "batch_idx: 17\n",
      "0.7453703703703703\n",
      "batch_idx: 18\n",
      "0.7390350877192983\n",
      "batch_idx: 19\n",
      "0.7333333333333333\n",
      "batch_idx: 20\n",
      "0.7301587301587301\n",
      "batch_idx: 21\n",
      "0.7215909090909091\n",
      "batch_idx: 22\n",
      "0.730072463768116\n",
      "batch_idx: 23\n",
      "0.7239583333333334\n",
      "batch_idx: 24\n",
      "0.73\n",
      "Training Epoch: 346, total loss: 44.078542\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7261904761904762\n",
      "batch_idx: 7\n",
      "0.7083333333333334\n",
      "batch_idx: 8\n",
      "0.6944444444444444\n",
      "batch_idx: 9\n",
      "0.6875\n",
      "batch_idx: 10\n",
      "0.6818181818181818\n",
      "batch_idx: 11\n",
      "0.6909722222222222\n",
      "batch_idx: 12\n",
      "0.6794871794871795\n",
      "batch_idx: 13\n",
      "0.6785714285714286\n",
      "batch_idx: 14\n",
      "0.6916666666666667\n",
      "batch_idx: 15\n",
      "0.6927083333333334\n",
      "batch_idx: 16\n",
      "0.6838235294117647\n",
      "batch_idx: 17\n",
      "0.6828703703703703\n",
      "batch_idx: 18\n",
      "0.6798245614035088\n",
      "batch_idx: 19\n",
      "0.6854166666666667\n",
      "batch_idx: 20\n",
      "0.6884920634920635\n",
      "batch_idx: 21\n",
      "0.6875\n",
      "batch_idx: 22\n",
      "0.6938405797101449\n",
      "batch_idx: 23\n",
      "0.6927083333333334\n",
      "batch_idx: 24\n",
      "0.6933333333333334\n",
      "Training Epoch: 347, total loss: 44.754752\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.7083333333333334\n",
      "batch_idx: 7\n",
      "0.71875\n",
      "batch_idx: 8\n",
      "0.7129629629629629\n",
      "batch_idx: 9\n",
      "0.7291666666666666\n",
      "batch_idx: 10\n",
      "0.7083333333333334\n",
      "batch_idx: 11\n",
      "0.7222222222222222\n",
      "batch_idx: 12\n",
      "0.7147435897435898\n",
      "batch_idx: 13\n",
      "0.7202380952380952\n",
      "batch_idx: 14\n",
      "0.725\n",
      "batch_idx: 15\n",
      "0.7161458333333334\n",
      "batch_idx: 16\n",
      "0.7132352941176471\n",
      "batch_idx: 17\n",
      "0.7199074074074074\n",
      "batch_idx: 18\n",
      "0.7149122807017544\n",
      "batch_idx: 19\n",
      "0.7083333333333334\n",
      "batch_idx: 20\n",
      "0.7123015873015873\n",
      "batch_idx: 21\n",
      "0.7102272727272727\n",
      "batch_idx: 22\n",
      "0.7101449275362319\n",
      "batch_idx: 23\n",
      "0.7118055555555556\n",
      "batch_idx: 24\n",
      "0.7116666666666667\n",
      "Training Epoch: 348, total loss: 44.534930\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.7\n",
      "batch_idx: 5\n",
      "0.6944444444444444\n",
      "batch_idx: 6\n",
      "0.7083333333333334\n",
      "batch_idx: 7\n",
      "0.7239583333333334\n",
      "batch_idx: 8\n",
      "0.7314814814814815\n",
      "batch_idx: 9\n",
      "0.7166666666666667\n",
      "batch_idx: 10\n",
      "0.7196969696969697\n",
      "batch_idx: 11\n",
      "0.71875\n",
      "batch_idx: 12\n",
      "0.7211538461538461\n",
      "batch_idx: 13\n",
      "0.7321428571428571\n",
      "batch_idx: 14\n",
      "0.7388888888888889\n",
      "batch_idx: 15\n",
      "0.734375\n",
      "batch_idx: 16\n",
      "0.7401960784313726\n",
      "batch_idx: 17\n",
      "0.7430555555555556\n",
      "batch_idx: 18\n",
      "0.7412280701754386\n",
      "batch_idx: 19\n",
      "0.7458333333333333\n",
      "batch_idx: 20\n",
      "0.746031746031746\n",
      "batch_idx: 21\n",
      "0.7518939393939394\n",
      "batch_idx: 22\n",
      "0.7590579710144928\n",
      "batch_idx: 23\n",
      "0.7552083333333334\n",
      "batch_idx: 24\n",
      "0.7533333333333333\n",
      "Training Epoch: 349, total loss: 43.865887\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.7791666666666667\n",
      "batch_idx: 10\n",
      "0.7765151515151515\n",
      "batch_idx: 11\n",
      "0.7777777777777778\n",
      "batch_idx: 12\n",
      "0.7628205128205128\n",
      "batch_idx: 13\n",
      "0.7767857142857143\n",
      "batch_idx: 14\n",
      "0.7777777777777778\n",
      "batch_idx: 15\n",
      "0.7734375\n",
      "batch_idx: 16\n",
      "0.7524509803921569\n",
      "batch_idx: 17\n",
      "0.7546296296296297\n",
      "batch_idx: 18\n",
      "0.7456140350877193\n",
      "batch_idx: 19\n",
      "0.7458333333333333\n",
      "batch_idx: 20\n",
      "0.748015873015873\n",
      "batch_idx: 21\n",
      "0.7462121212121212\n",
      "batch_idx: 22\n",
      "0.7445652173913043\n",
      "batch_idx: 23\n",
      "0.75\n",
      "batch_idx: 24\n",
      "0.7483333333333333\n",
      "Training Epoch: 350, total loss: 43.918675\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.725\n",
      "batch_idx: 5\n",
      "0.7013888888888888\n",
      "batch_idx: 6\n",
      "0.7023809523809523\n",
      "batch_idx: 7\n",
      "0.7083333333333334\n",
      "batch_idx: 8\n",
      "0.7037037037037037\n",
      "batch_idx: 9\n",
      "0.7083333333333334\n",
      "batch_idx: 10\n",
      "0.7196969696969697\n",
      "batch_idx: 11\n",
      "0.71875\n",
      "batch_idx: 12\n",
      "0.7211538461538461\n",
      "batch_idx: 13\n",
      "0.7232142857142857\n",
      "batch_idx: 14\n",
      "0.7166666666666667\n",
      "batch_idx: 15\n",
      "0.7135416666666666\n",
      "batch_idx: 16\n",
      "0.7107843137254902\n",
      "batch_idx: 17\n",
      "0.7199074074074074\n",
      "batch_idx: 18\n",
      "0.7236842105263158\n",
      "batch_idx: 19\n",
      "0.7229166666666667\n",
      "batch_idx: 20\n",
      "0.7242063492063492\n",
      "batch_idx: 21\n",
      "0.7196969696969697\n",
      "batch_idx: 22\n",
      "0.7155797101449275\n",
      "batch_idx: 23\n",
      "0.7170138888888888\n",
      "batch_idx: 24\n",
      "0.72\n",
      "Training Epoch: 351, total loss: 44.236412\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.7152777777777778\n",
      "batch_idx: 6\n",
      "0.7142857142857143\n",
      "batch_idx: 7\n",
      "0.7239583333333334\n",
      "batch_idx: 8\n",
      "0.7268518518518519\n",
      "batch_idx: 9\n",
      "0.7333333333333333\n",
      "batch_idx: 10\n",
      "0.7234848484848485\n",
      "batch_idx: 11\n",
      "0.7222222222222222\n",
      "batch_idx: 12\n",
      "0.717948717948718\n",
      "batch_idx: 13\n",
      "0.7083333333333334\n",
      "batch_idx: 14\n",
      "0.6972222222222222\n",
      "batch_idx: 15\n",
      "0.6979166666666666\n",
      "batch_idx: 16\n",
      "0.7034313725490197\n",
      "batch_idx: 17\n",
      "0.7060185185185185\n",
      "batch_idx: 18\n",
      "0.7039473684210527\n",
      "batch_idx: 19\n",
      "0.70625\n",
      "batch_idx: 20\n",
      "0.7162698412698413\n",
      "batch_idx: 21\n",
      "0.7140151515151515\n",
      "batch_idx: 22\n",
      "0.7119565217391305\n",
      "batch_idx: 23\n",
      "0.7170138888888888\n",
      "batch_idx: 24\n",
      "0.715\n",
      "Training Epoch: 352, total loss: 44.334846\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.6770833333333334\n",
      "batch_idx: 4\n",
      "0.6833333333333333\n",
      "batch_idx: 5\n",
      "0.6944444444444444\n",
      "batch_idx: 6\n",
      "0.7142857142857143\n",
      "batch_idx: 7\n",
      "0.71875\n",
      "batch_idx: 8\n",
      "0.7268518518518519\n",
      "batch_idx: 9\n",
      "0.7208333333333333\n",
      "batch_idx: 10\n",
      "0.7196969696969697\n",
      "batch_idx: 11\n",
      "0.7222222222222222\n",
      "batch_idx: 12\n",
      "0.7115384615384616\n",
      "batch_idx: 13\n",
      "0.7202380952380952\n",
      "batch_idx: 14\n",
      "0.725\n",
      "batch_idx: 15\n",
      "0.7291666666666666\n",
      "batch_idx: 16\n",
      "0.7254901960784313\n",
      "batch_idx: 17\n",
      "0.7291666666666666\n",
      "batch_idx: 18\n",
      "0.7324561403508771\n",
      "batch_idx: 19\n",
      "0.7333333333333333\n",
      "batch_idx: 20\n",
      "0.7361111111111112\n",
      "batch_idx: 21\n",
      "0.7348484848484849\n",
      "batch_idx: 22\n",
      "0.7391304347826086\n",
      "batch_idx: 23\n",
      "0.7413194444444444\n",
      "batch_idx: 24\n",
      "0.7416666666666667\n",
      "Training Epoch: 353, total loss: 43.946278\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.7\n",
      "batch_idx: 5\n",
      "0.6805555555555556\n",
      "batch_idx: 6\n",
      "0.6845238095238095\n",
      "batch_idx: 7\n",
      "0.71875\n",
      "batch_idx: 8\n",
      "0.7129629629629629\n",
      "batch_idx: 9\n",
      "0.7291666666666666\n",
      "batch_idx: 10\n",
      "0.7234848484848485\n",
      "batch_idx: 11\n",
      "0.7118055555555556\n",
      "batch_idx: 12\n",
      "0.7051282051282052\n",
      "batch_idx: 13\n",
      "0.7053571428571429\n",
      "batch_idx: 14\n",
      "0.7083333333333334\n",
      "batch_idx: 15\n",
      "0.7057291666666666\n",
      "batch_idx: 16\n",
      "0.7009803921568627\n",
      "batch_idx: 17\n",
      "0.7106481481481481\n",
      "batch_idx: 18\n",
      "0.7105263157894737\n",
      "batch_idx: 19\n",
      "0.7104166666666667\n",
      "batch_idx: 20\n",
      "0.7123015873015873\n",
      "batch_idx: 21\n",
      "0.7159090909090909\n",
      "batch_idx: 22\n",
      "0.7065217391304348\n",
      "batch_idx: 23\n",
      "0.7100694444444444\n",
      "batch_idx: 24\n",
      "0.7066666666666667\n",
      "Training Epoch: 354, total loss: 44.526092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.725\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.71875\n",
      "batch_idx: 8\n",
      "0.7314814814814815\n",
      "batch_idx: 9\n",
      "0.725\n",
      "batch_idx: 10\n",
      "0.7196969696969697\n",
      "batch_idx: 11\n",
      "0.71875\n",
      "batch_idx: 12\n",
      "0.7211538461538461\n",
      "batch_idx: 13\n",
      "0.7261904761904762\n",
      "batch_idx: 14\n",
      "0.7194444444444444\n",
      "batch_idx: 15\n",
      "0.7161458333333334\n",
      "batch_idx: 16\n",
      "0.7205882352941176\n",
      "batch_idx: 17\n",
      "0.7013888888888888\n",
      "batch_idx: 18\n",
      "0.7017543859649122\n",
      "batch_idx: 19\n",
      "0.7041666666666667\n",
      "batch_idx: 20\n",
      "0.7083333333333334\n",
      "batch_idx: 21\n",
      "0.7045454545454546\n",
      "batch_idx: 22\n",
      "0.7028985507246377\n",
      "batch_idx: 23\n",
      "0.7083333333333334\n",
      "batch_idx: 24\n",
      "0.7116666666666667\n",
      "Training Epoch: 355, total loss: 44.543332\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7321428571428571\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7407407407407407\n",
      "batch_idx: 9\n",
      "0.7333333333333333\n",
      "batch_idx: 10\n",
      "0.7348484848484849\n",
      "batch_idx: 11\n",
      "0.7361111111111112\n",
      "batch_idx: 12\n",
      "0.7307692307692307\n",
      "batch_idx: 13\n",
      "0.7261904761904762\n",
      "batch_idx: 14\n",
      "0.7166666666666667\n",
      "batch_idx: 15\n",
      "0.7109375\n",
      "batch_idx: 16\n",
      "0.7083333333333334\n",
      "batch_idx: 17\n",
      "0.6990740740740741\n",
      "batch_idx: 18\n",
      "0.6995614035087719\n",
      "batch_idx: 19\n",
      "0.7041666666666667\n",
      "batch_idx: 20\n",
      "0.7023809523809523\n",
      "batch_idx: 21\n",
      "0.7064393939393939\n",
      "batch_idx: 22\n",
      "0.7047101449275363\n",
      "batch_idx: 23\n",
      "0.7065972222222222\n",
      "batch_idx: 24\n",
      "0.7066666666666667\n",
      "Training Epoch: 356, total loss: 44.627915\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.6388888888888888\n",
      "batch_idx: 3\n",
      "0.6666666666666666\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.6944444444444444\n",
      "batch_idx: 6\n",
      "0.6964285714285714\n",
      "batch_idx: 7\n",
      "0.6927083333333334\n",
      "batch_idx: 8\n",
      "0.6990740740740741\n",
      "batch_idx: 9\n",
      "0.7125\n",
      "batch_idx: 10\n",
      "0.7121212121212122\n",
      "batch_idx: 11\n",
      "0.7152777777777778\n",
      "batch_idx: 12\n",
      "0.7051282051282052\n",
      "batch_idx: 13\n",
      "0.7083333333333334\n",
      "batch_idx: 14\n",
      "0.7083333333333334\n",
      "batch_idx: 15\n",
      "0.7109375\n",
      "batch_idx: 16\n",
      "0.7132352941176471\n",
      "batch_idx: 17\n",
      "0.7152777777777778\n",
      "batch_idx: 18\n",
      "0.7236842105263158\n",
      "batch_idx: 19\n",
      "0.73125\n",
      "batch_idx: 20\n",
      "0.7321428571428571\n",
      "batch_idx: 21\n",
      "0.7291666666666666\n",
      "batch_idx: 22\n",
      "0.7318840579710145\n",
      "batch_idx: 23\n",
      "0.7326388888888888\n",
      "batch_idx: 24\n",
      "0.7333333333333333\n",
      "Training Epoch: 357, total loss: 44.177669\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7440476190476191\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.7407407407407407\n",
      "batch_idx: 9\n",
      "0.7208333333333333\n",
      "batch_idx: 10\n",
      "0.7386363636363636\n",
      "batch_idx: 11\n",
      "0.7430555555555556\n",
      "batch_idx: 12\n",
      "0.7403846153846154\n",
      "batch_idx: 13\n",
      "0.7470238095238095\n",
      "batch_idx: 14\n",
      "0.7416666666666667\n",
      "batch_idx: 15\n",
      "0.7369791666666666\n",
      "batch_idx: 16\n",
      "0.7279411764705882\n",
      "batch_idx: 17\n",
      "0.7291666666666666\n",
      "batch_idx: 18\n",
      "0.7280701754385965\n",
      "batch_idx: 19\n",
      "0.7270833333333333\n",
      "batch_idx: 20\n",
      "0.7261904761904762\n",
      "batch_idx: 21\n",
      "0.7291666666666666\n",
      "batch_idx: 22\n",
      "0.7355072463768116\n",
      "batch_idx: 23\n",
      "0.7378472222222222\n",
      "batch_idx: 24\n",
      "0.7383333333333333\n",
      "Training Epoch: 358, total loss: 44.014871\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7552083333333334\n",
      "batch_idx: 8\n",
      "0.7453703703703703\n",
      "batch_idx: 9\n",
      "0.7416666666666667\n",
      "batch_idx: 10\n",
      "0.7462121212121212\n",
      "batch_idx: 11\n",
      "0.7395833333333334\n",
      "batch_idx: 12\n",
      "0.7339743589743589\n",
      "batch_idx: 13\n",
      "0.7380952380952381\n",
      "batch_idx: 14\n",
      "0.7333333333333333\n",
      "batch_idx: 15\n",
      "0.7161458333333334\n",
      "batch_idx: 16\n",
      "0.7107843137254902\n",
      "batch_idx: 17\n",
      "0.7129629629629629\n",
      "batch_idx: 18\n",
      "0.7192982456140351\n",
      "batch_idx: 19\n",
      "0.7270833333333333\n",
      "batch_idx: 20\n",
      "0.7182539682539683\n",
      "batch_idx: 21\n",
      "0.7140151515151515\n",
      "batch_idx: 22\n",
      "0.7083333333333334\n",
      "batch_idx: 23\n",
      "0.7118055555555556\n",
      "batch_idx: 24\n",
      "0.7083333333333334\n",
      "Training Epoch: 359, total loss: 44.561511\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.75\n",
      "batch_idx: 9\n",
      "0.75\n",
      "batch_idx: 10\n",
      "0.7462121212121212\n",
      "batch_idx: 11\n",
      "0.7256944444444444\n",
      "batch_idx: 12\n",
      "0.7307692307692307\n",
      "batch_idx: 13\n",
      "0.7321428571428571\n",
      "batch_idx: 14\n",
      "0.7222222222222222\n",
      "batch_idx: 15\n",
      "0.7291666666666666\n",
      "batch_idx: 16\n",
      "0.7352941176470589\n",
      "batch_idx: 17\n",
      "0.7384259259259259\n",
      "batch_idx: 18\n",
      "0.7412280701754386\n",
      "batch_idx: 19\n",
      "0.74375\n",
      "batch_idx: 20\n",
      "0.746031746031746\n",
      "batch_idx: 21\n",
      "0.7462121212121212\n",
      "batch_idx: 22\n",
      "0.7427536231884058\n",
      "batch_idx: 23\n",
      "0.7413194444444444\n",
      "batch_idx: 24\n",
      "0.74\n",
      "Training Epoch: 360, total loss: 43.963530\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.6833333333333333\n",
      "batch_idx: 5\n",
      "0.7013888888888888\n",
      "batch_idx: 6\n",
      "0.7083333333333334\n",
      "batch_idx: 7\n",
      "0.71875\n",
      "batch_idx: 8\n",
      "0.7222222222222222\n",
      "batch_idx: 9\n",
      "0.7083333333333334\n",
      "batch_idx: 10\n",
      "0.7045454545454546\n",
      "batch_idx: 11\n",
      "0.7152777777777778\n",
      "batch_idx: 12\n",
      "0.6987179487179487\n",
      "batch_idx: 13\n",
      "0.7053571428571429\n",
      "batch_idx: 14\n",
      "0.7055555555555556\n",
      "batch_idx: 15\n",
      "0.7083333333333334\n",
      "batch_idx: 16\n",
      "0.7156862745098039\n",
      "batch_idx: 17\n",
      "0.7083333333333334\n",
      "batch_idx: 18\n",
      "0.7127192982456141\n",
      "batch_idx: 19\n",
      "0.7229166666666667\n",
      "batch_idx: 20\n",
      "0.7162698412698413\n",
      "batch_idx: 21\n",
      "0.7140151515151515\n",
      "batch_idx: 22\n",
      "0.7137681159420289\n",
      "batch_idx: 23\n",
      "0.7170138888888888\n",
      "batch_idx: 24\n",
      "0.7166666666666667\n",
      "Training Epoch: 361, total loss: 44.278527\n",
      "batch_idx: 0\n",
      "0.4583333333333333\n",
      "batch_idx: 1\n",
      "0.5416666666666666\n",
      "batch_idx: 2\n",
      "0.5555555555555556\n",
      "batch_idx: 3\n",
      "0.6041666666666666\n",
      "batch_idx: 4\n",
      "0.6\n",
      "batch_idx: 5\n",
      "0.6111111111111112\n",
      "batch_idx: 6\n",
      "0.6309523809523809\n",
      "batch_idx: 7\n",
      "0.65625\n",
      "batch_idx: 8\n",
      "0.6574074074074074\n",
      "batch_idx: 9\n",
      "0.6666666666666666\n",
      "batch_idx: 10\n",
      "0.6666666666666666\n",
      "batch_idx: 11\n",
      "0.6631944444444444\n",
      "batch_idx: 12\n",
      "0.6602564102564102\n",
      "batch_idx: 13\n",
      "0.6785714285714286\n",
      "batch_idx: 14\n",
      "0.6833333333333333\n",
      "batch_idx: 15\n",
      "0.6927083333333334\n",
      "batch_idx: 16\n",
      "0.696078431372549\n",
      "batch_idx: 17\n",
      "0.7013888888888888\n",
      "batch_idx: 18\n",
      "0.7039473684210527\n",
      "batch_idx: 19\n",
      "0.7083333333333334\n",
      "batch_idx: 20\n",
      "0.7103174603174603\n",
      "batch_idx: 21\n",
      "0.7121212121212122\n",
      "batch_idx: 22\n",
      "0.7065217391304348\n",
      "batch_idx: 23\n",
      "0.7013888888888888\n",
      "batch_idx: 24\n",
      "0.705\n",
      "Training Epoch: 362, total loss: 44.681365\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.75\n",
      "batch_idx: 9\n",
      "0.7333333333333333\n",
      "batch_idx: 10\n",
      "0.7272727272727273\n",
      "batch_idx: 11\n",
      "0.7256944444444444\n",
      "batch_idx: 12\n",
      "0.7275641025641025\n",
      "batch_idx: 13\n",
      "0.7321428571428571\n",
      "batch_idx: 14\n",
      "0.7222222222222222\n",
      "batch_idx: 15\n",
      "0.7213541666666666\n",
      "batch_idx: 16\n",
      "0.7303921568627451\n",
      "batch_idx: 17\n",
      "0.7314814814814815\n",
      "batch_idx: 18\n",
      "0.7346491228070176\n",
      "batch_idx: 19\n",
      "0.7354166666666667\n",
      "batch_idx: 20\n",
      "0.7361111111111112\n",
      "batch_idx: 21\n",
      "0.7386363636363636\n",
      "batch_idx: 22\n",
      "0.7336956521739131\n",
      "batch_idx: 23\n",
      "0.7274305555555556\n",
      "batch_idx: 24\n",
      "0.7266666666666667\n",
      "Training Epoch: 363, total loss: 44.076895\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.7708333333333334\n",
      "batch_idx: 10\n",
      "0.7689393939393939\n",
      "batch_idx: 11\n",
      "0.7534722222222222\n",
      "batch_idx: 12\n",
      "0.75\n",
      "batch_idx: 13\n",
      "0.75\n",
      "batch_idx: 14\n",
      "0.7388888888888889\n",
      "batch_idx: 15\n",
      "0.7395833333333334\n",
      "batch_idx: 16\n",
      "0.7352941176470589\n",
      "batch_idx: 17\n",
      "0.7361111111111112\n",
      "batch_idx: 18\n",
      "0.7324561403508771\n",
      "batch_idx: 19\n",
      "0.7354166666666667\n",
      "batch_idx: 20\n",
      "0.7321428571428571\n",
      "batch_idx: 21\n",
      "0.7329545454545454\n",
      "batch_idx: 22\n",
      "0.7409420289855072\n",
      "batch_idx: 23\n",
      "0.7430555555555556\n",
      "batch_idx: 24\n",
      "0.7333333333333333\n",
      "Training Epoch: 364, total loss: 44.080493\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.7321428571428571\n",
      "batch_idx: 7\n",
      "0.7291666666666666\n",
      "batch_idx: 8\n",
      "0.7314814814814815\n",
      "batch_idx: 9\n",
      "0.725\n",
      "batch_idx: 10\n",
      "0.7234848484848485\n",
      "batch_idx: 11\n",
      "0.7395833333333334\n",
      "batch_idx: 12\n",
      "0.7307692307692307\n",
      "batch_idx: 13\n",
      "0.7202380952380952\n",
      "batch_idx: 14\n",
      "0.7166666666666667\n",
      "batch_idx: 15\n",
      "0.7057291666666666\n",
      "batch_idx: 16\n",
      "0.7083333333333334\n",
      "batch_idx: 17\n",
      "0.7060185185185185\n",
      "batch_idx: 18\n",
      "0.7039473684210527\n",
      "batch_idx: 19\n",
      "0.7041666666666667\n",
      "batch_idx: 20\n",
      "0.6964285714285714\n",
      "batch_idx: 21\n",
      "0.696969696969697\n",
      "batch_idx: 22\n",
      "0.697463768115942\n",
      "batch_idx: 23\n",
      "0.6944444444444444\n",
      "batch_idx: 24\n",
      "0.7\n",
      "Training Epoch: 365, total loss: 44.793845\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.6583333333333333\n",
      "batch_idx: 5\n",
      "0.6180555555555556\n",
      "batch_idx: 6\n",
      "0.6309523809523809\n",
      "batch_idx: 7\n",
      "0.6614583333333334\n",
      "batch_idx: 8\n",
      "0.6851851851851852\n",
      "batch_idx: 9\n",
      "0.6833333333333333\n",
      "batch_idx: 10\n",
      "0.696969696969697\n",
      "batch_idx: 11\n",
      "0.7083333333333334\n",
      "batch_idx: 12\n",
      "0.7115384615384616\n",
      "batch_idx: 13\n",
      "0.7142857142857143\n",
      "batch_idx: 14\n",
      "0.7\n",
      "batch_idx: 15\n",
      "0.6848958333333334\n",
      "batch_idx: 16\n",
      "0.6862745098039216\n",
      "batch_idx: 17\n",
      "0.6921296296296297\n",
      "batch_idx: 18\n",
      "0.6929824561403509\n",
      "batch_idx: 19\n",
      "0.6916666666666667\n",
      "batch_idx: 20\n",
      "0.6964285714285714\n",
      "batch_idx: 21\n",
      "0.6912878787878788\n",
      "batch_idx: 22\n",
      "0.6847826086956522\n",
      "batch_idx: 23\n",
      "0.6840277777777778\n",
      "batch_idx: 24\n",
      "0.68\n",
      "Training Epoch: 366, total loss: 45.033020\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7261904761904762\n",
      "batch_idx: 7\n",
      "0.7239583333333334\n",
      "batch_idx: 8\n",
      "0.7222222222222222\n",
      "batch_idx: 9\n",
      "0.7291666666666666\n",
      "batch_idx: 10\n",
      "0.7348484848484849\n",
      "batch_idx: 11\n",
      "0.75\n",
      "batch_idx: 12\n",
      "0.7564102564102564\n",
      "batch_idx: 13\n",
      "0.7529761904761905\n",
      "batch_idx: 14\n",
      "0.7527777777777778\n",
      "batch_idx: 15\n",
      "0.7473958333333334\n",
      "batch_idx: 16\n",
      "0.7450980392156863\n",
      "batch_idx: 17\n",
      "0.7453703703703703\n",
      "batch_idx: 18\n",
      "0.743421052631579\n",
      "batch_idx: 19\n",
      "0.7354166666666667\n",
      "batch_idx: 20\n",
      "0.7361111111111112\n",
      "batch_idx: 21\n",
      "0.7367424242424242\n",
      "batch_idx: 22\n",
      "0.7391304347826086\n",
      "batch_idx: 23\n",
      "0.7378472222222222\n",
      "batch_idx: 24\n",
      "0.74\n",
      "Training Epoch: 367, total loss: 44.062269\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7083333333333334\n",
      "batch_idx: 6\n",
      "0.7083333333333334\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7037037037037037\n",
      "batch_idx: 9\n",
      "0.7041666666666667\n",
      "batch_idx: 10\n",
      "0.6931818181818182\n",
      "batch_idx: 11\n",
      "0.6909722222222222\n",
      "batch_idx: 12\n",
      "0.6987179487179487\n",
      "batch_idx: 13\n",
      "0.7023809523809523\n",
      "batch_idx: 14\n",
      "0.6916666666666667\n",
      "batch_idx: 15\n",
      "0.6875\n",
      "batch_idx: 16\n",
      "0.6838235294117647\n",
      "batch_idx: 17\n",
      "0.6805555555555556\n",
      "batch_idx: 18\n",
      "0.6820175438596491\n",
      "batch_idx: 19\n",
      "0.68125\n",
      "batch_idx: 20\n",
      "0.6884920634920635\n",
      "batch_idx: 21\n",
      "0.6875\n",
      "batch_idx: 22\n",
      "0.6865942028985508\n",
      "batch_idx: 23\n",
      "0.6822916666666666\n",
      "batch_idx: 24\n",
      "0.6833333333333333\n",
      "Training Epoch: 368, total loss: 44.856905\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7261904761904762\n",
      "batch_idx: 7\n",
      "0.71875\n",
      "batch_idx: 8\n",
      "0.7314814814814815\n",
      "batch_idx: 9\n",
      "0.7416666666666667\n",
      "batch_idx: 10\n",
      "0.7234848484848485\n",
      "batch_idx: 11\n",
      "0.7222222222222222\n",
      "batch_idx: 12\n",
      "0.7243589743589743\n",
      "batch_idx: 13\n",
      "0.7172619047619048\n",
      "batch_idx: 14\n",
      "0.725\n",
      "batch_idx: 15\n",
      "0.7239583333333334\n",
      "batch_idx: 16\n",
      "0.7132352941176471\n",
      "batch_idx: 17\n",
      "0.7106481481481481\n",
      "batch_idx: 18\n",
      "0.7017543859649122\n",
      "batch_idx: 19\n",
      "0.7083333333333334\n",
      "batch_idx: 20\n",
      "0.7182539682539683\n",
      "batch_idx: 21\n",
      "0.7196969696969697\n",
      "batch_idx: 22\n",
      "0.717391304347826\n",
      "batch_idx: 23\n",
      "0.7118055555555556\n",
      "batch_idx: 24\n",
      "0.71\n",
      "Training Epoch: 369, total loss: 44.539884\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.7152777777777778\n",
      "batch_idx: 6\n",
      "0.7261904761904762\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7222222222222222\n",
      "batch_idx: 9\n",
      "0.7333333333333333\n",
      "batch_idx: 10\n",
      "0.7234848484848485\n",
      "batch_idx: 11\n",
      "0.7361111111111112\n",
      "batch_idx: 12\n",
      "0.7307692307692307\n",
      "batch_idx: 13\n",
      "0.7380952380952381\n",
      "batch_idx: 14\n",
      "0.7333333333333333\n",
      "batch_idx: 15\n",
      "0.734375\n",
      "batch_idx: 16\n",
      "0.7303921568627451\n",
      "batch_idx: 17\n",
      "0.7314814814814815\n",
      "batch_idx: 18\n",
      "0.7280701754385965\n",
      "batch_idx: 19\n",
      "0.7333333333333333\n",
      "batch_idx: 20\n",
      "0.7341269841269841\n",
      "batch_idx: 21\n",
      "0.7348484848484849\n",
      "batch_idx: 22\n",
      "0.7409420289855072\n",
      "batch_idx: 23\n",
      "0.7309027777777778\n",
      "batch_idx: 24\n",
      "0.73\n",
      "Training Epoch: 370, total loss: 44.433707\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.725\n",
      "batch_idx: 5\n",
      "0.7013888888888888\n",
      "batch_idx: 6\n",
      "0.6964285714285714\n",
      "batch_idx: 7\n",
      "0.703125\n",
      "batch_idx: 8\n",
      "0.7129629629629629\n",
      "batch_idx: 9\n",
      "0.7208333333333333\n",
      "batch_idx: 10\n",
      "0.7272727272727273\n",
      "batch_idx: 11\n",
      "0.7256944444444444\n",
      "batch_idx: 12\n",
      "0.7243589743589743\n",
      "batch_idx: 13\n",
      "0.7351190476190477\n",
      "batch_idx: 14\n",
      "0.7388888888888889\n",
      "batch_idx: 15\n",
      "0.734375\n",
      "batch_idx: 16\n",
      "0.7377450980392157\n",
      "batch_idx: 17\n",
      "0.7152777777777778\n",
      "batch_idx: 18\n",
      "0.7149122807017544\n",
      "batch_idx: 19\n",
      "0.7229166666666667\n",
      "batch_idx: 20\n",
      "0.7281746031746031\n",
      "batch_idx: 21\n",
      "0.7348484848484849\n",
      "batch_idx: 22\n",
      "0.7336956521739131\n",
      "batch_idx: 23\n",
      "0.734375\n",
      "batch_idx: 24\n",
      "0.735\n",
      "Training Epoch: 371, total loss: 44.180289\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7261904761904762\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7361111111111112\n",
      "batch_idx: 9\n",
      "0.7291666666666666\n",
      "batch_idx: 10\n",
      "0.7310606060606061\n",
      "batch_idx: 11\n",
      "0.7291666666666666\n",
      "batch_idx: 12\n",
      "0.7339743589743589\n",
      "batch_idx: 13\n",
      "0.7351190476190477\n",
      "batch_idx: 14\n",
      "0.7416666666666667\n",
      "batch_idx: 15\n",
      "0.7395833333333334\n",
      "batch_idx: 16\n",
      "0.7426470588235294\n",
      "batch_idx: 17\n",
      "0.7314814814814815\n",
      "batch_idx: 18\n",
      "0.7324561403508771\n",
      "batch_idx: 19\n",
      "0.73125\n",
      "batch_idx: 20\n",
      "0.7301587301587301\n",
      "batch_idx: 21\n",
      "0.7348484848484849\n",
      "batch_idx: 22\n",
      "0.7336956521739131\n",
      "batch_idx: 23\n",
      "0.7326388888888888\n",
      "batch_idx: 24\n",
      "0.7383333333333333\n",
      "Training Epoch: 372, total loss: 43.974183\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7552083333333334\n",
      "batch_idx: 8\n",
      "0.7546296296296297\n",
      "batch_idx: 9\n",
      "0.7583333333333333\n",
      "batch_idx: 10\n",
      "0.75\n",
      "batch_idx: 11\n",
      "0.75\n",
      "batch_idx: 12\n",
      "0.7435897435897436\n",
      "batch_idx: 13\n",
      "0.7380952380952381\n",
      "batch_idx: 14\n",
      "0.7361111111111112\n",
      "batch_idx: 15\n",
      "0.7369791666666666\n",
      "batch_idx: 16\n",
      "0.7352941176470589\n",
      "batch_idx: 17\n",
      "0.7407407407407407\n",
      "batch_idx: 18\n",
      "0.7346491228070176\n",
      "batch_idx: 19\n",
      "0.7354166666666667\n",
      "batch_idx: 20\n",
      "0.7400793650793651\n",
      "batch_idx: 21\n",
      "0.7367424242424242\n",
      "batch_idx: 22\n",
      "0.7336956521739131\n",
      "batch_idx: 23\n",
      "0.7309027777777778\n",
      "batch_idx: 24\n",
      "0.7316666666666667\n",
      "Training Epoch: 373, total loss: 44.253884\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.7013888888888888\n",
      "batch_idx: 6\n",
      "0.7083333333333334\n",
      "batch_idx: 7\n",
      "0.7291666666666666\n",
      "batch_idx: 8\n",
      "0.7129629629629629\n",
      "batch_idx: 9\n",
      "0.7125\n",
      "batch_idx: 10\n",
      "0.7196969696969697\n",
      "batch_idx: 11\n",
      "0.7152777777777778\n",
      "batch_idx: 12\n",
      "0.7115384615384616\n",
      "batch_idx: 13\n",
      "0.7053571428571429\n",
      "batch_idx: 14\n",
      "0.7083333333333334\n",
      "batch_idx: 15\n",
      "0.6953125\n",
      "batch_idx: 16\n",
      "0.6887254901960784\n",
      "batch_idx: 17\n",
      "0.6851851851851852\n",
      "batch_idx: 18\n",
      "0.6907894736842105\n",
      "batch_idx: 19\n",
      "0.6916666666666667\n",
      "batch_idx: 20\n",
      "0.6944444444444444\n",
      "batch_idx: 21\n",
      "0.6931818181818182\n",
      "batch_idx: 22\n",
      "0.6938405797101449\n",
      "batch_idx: 23\n",
      "0.6909722222222222\n",
      "batch_idx: 24\n",
      "0.69\n",
      "Training Epoch: 374, total loss: 44.784312\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.7395833333333334\n",
      "batch_idx: 8\n",
      "0.7268518518518519\n",
      "batch_idx: 9\n",
      "0.7333333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 10\n",
      "0.7121212121212122\n",
      "batch_idx: 11\n",
      "0.7152777777777778\n",
      "batch_idx: 12\n",
      "0.7211538461538461\n",
      "batch_idx: 13\n",
      "0.7202380952380952\n",
      "batch_idx: 14\n",
      "0.7166666666666667\n",
      "batch_idx: 15\n",
      "0.7109375\n",
      "batch_idx: 16\n",
      "0.7107843137254902\n",
      "batch_idx: 17\n",
      "0.7175925925925926\n",
      "batch_idx: 18\n",
      "0.706140350877193\n",
      "batch_idx: 19\n",
      "0.7104166666666667\n",
      "batch_idx: 20\n",
      "0.7182539682539683\n",
      "batch_idx: 21\n",
      "0.7178030303030303\n",
      "batch_idx: 22\n",
      "0.7155797101449275\n",
      "batch_idx: 23\n",
      "0.7152777777777778\n",
      "batch_idx: 24\n",
      "0.7183333333333334\n",
      "Training Epoch: 375, total loss: 44.251427\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.782051282051282\n",
      "batch_idx: 13\n",
      "0.7797619047619048\n",
      "batch_idx: 14\n",
      "0.7833333333333333\n",
      "batch_idx: 15\n",
      "0.7760416666666666\n",
      "batch_idx: 16\n",
      "0.7769607843137255\n",
      "batch_idx: 17\n",
      "0.7777777777777778\n",
      "batch_idx: 18\n",
      "0.7785087719298246\n",
      "batch_idx: 19\n",
      "0.76875\n",
      "batch_idx: 20\n",
      "0.7698412698412699\n",
      "batch_idx: 21\n",
      "0.7746212121212122\n",
      "batch_idx: 22\n",
      "0.769927536231884\n",
      "batch_idx: 23\n",
      "0.7621527777777778\n",
      "batch_idx: 24\n",
      "0.7566666666666667\n",
      "Training Epoch: 376, total loss: 43.796373\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7083333333333334\n",
      "batch_idx: 9\n",
      "0.7041666666666667\n",
      "batch_idx: 10\n",
      "0.696969696969697\n",
      "batch_idx: 11\n",
      "0.6944444444444444\n",
      "batch_idx: 12\n",
      "0.6891025641025641\n",
      "batch_idx: 13\n",
      "0.6994047619047619\n",
      "batch_idx: 14\n",
      "0.7083333333333334\n",
      "batch_idx: 15\n",
      "0.71875\n",
      "batch_idx: 16\n",
      "0.7205882352941176\n",
      "batch_idx: 17\n",
      "0.7152777777777778\n",
      "batch_idx: 18\n",
      "0.7192982456140351\n",
      "batch_idx: 19\n",
      "0.7229166666666667\n",
      "batch_idx: 20\n",
      "0.7242063492063492\n",
      "batch_idx: 21\n",
      "0.7234848484848485\n",
      "batch_idx: 22\n",
      "0.730072463768116\n",
      "batch_idx: 23\n",
      "0.7309027777777778\n",
      "batch_idx: 24\n",
      "0.7266666666666667\n",
      "Training Epoch: 377, total loss: 44.189867\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7361111111111112\n",
      "batch_idx: 6\n",
      "0.7083333333333334\n",
      "batch_idx: 7\n",
      "0.7083333333333334\n",
      "batch_idx: 8\n",
      "0.7175925925925926\n",
      "batch_idx: 9\n",
      "0.7166666666666667\n",
      "batch_idx: 10\n",
      "0.7121212121212122\n",
      "batch_idx: 11\n",
      "0.7222222222222222\n",
      "batch_idx: 12\n",
      "0.7307692307692307\n",
      "batch_idx: 13\n",
      "0.7291666666666666\n",
      "batch_idx: 14\n",
      "0.7305555555555555\n",
      "batch_idx: 15\n",
      "0.7369791666666666\n",
      "batch_idx: 16\n",
      "0.7377450980392157\n",
      "batch_idx: 17\n",
      "0.7384259259259259\n",
      "batch_idx: 18\n",
      "0.7368421052631579\n",
      "batch_idx: 19\n",
      "0.7416666666666667\n",
      "batch_idx: 20\n",
      "0.7380952380952381\n",
      "batch_idx: 21\n",
      "0.740530303030303\n",
      "batch_idx: 22\n",
      "0.7463768115942029\n",
      "batch_idx: 23\n",
      "0.7465277777777778\n",
      "batch_idx: 24\n",
      "0.75\n",
      "Training Epoch: 378, total loss: 43.772226\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7261904761904762\n",
      "batch_idx: 7\n",
      "0.7395833333333334\n",
      "batch_idx: 8\n",
      "0.7361111111111112\n",
      "batch_idx: 9\n",
      "0.7333333333333333\n",
      "batch_idx: 10\n",
      "0.7424242424242424\n",
      "batch_idx: 11\n",
      "0.7430555555555556\n",
      "batch_idx: 12\n",
      "0.7403846153846154\n",
      "batch_idx: 13\n",
      "0.7410714285714286\n",
      "batch_idx: 14\n",
      "0.7472222222222222\n",
      "batch_idx: 15\n",
      "0.7421875\n",
      "batch_idx: 16\n",
      "0.7450980392156863\n",
      "batch_idx: 17\n",
      "0.7453703703703703\n",
      "batch_idx: 18\n",
      "0.7390350877192983\n",
      "batch_idx: 19\n",
      "0.7458333333333333\n",
      "batch_idx: 20\n",
      "0.7380952380952381\n",
      "batch_idx: 21\n",
      "0.7386363636363636\n",
      "batch_idx: 22\n",
      "0.7355072463768116\n",
      "batch_idx: 23\n",
      "0.7378472222222222\n",
      "batch_idx: 24\n",
      "0.7383333333333333\n",
      "Training Epoch: 379, total loss: 44.086747\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.6770833333333334\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.6805555555555556\n",
      "batch_idx: 6\n",
      "0.7023809523809523\n",
      "batch_idx: 7\n",
      "0.6979166666666666\n",
      "batch_idx: 8\n",
      "0.7129629629629629\n",
      "batch_idx: 9\n",
      "0.7125\n",
      "batch_idx: 10\n",
      "0.7159090909090909\n",
      "batch_idx: 11\n",
      "0.7256944444444444\n",
      "batch_idx: 12\n",
      "0.7243589743589743\n",
      "batch_idx: 13\n",
      "0.7232142857142857\n",
      "batch_idx: 14\n",
      "0.725\n",
      "batch_idx: 15\n",
      "0.7213541666666666\n",
      "batch_idx: 16\n",
      "0.7230392156862745\n",
      "batch_idx: 17\n",
      "0.7245370370370371\n",
      "batch_idx: 18\n",
      "0.7236842105263158\n",
      "batch_idx: 19\n",
      "0.725\n",
      "batch_idx: 20\n",
      "0.7182539682539683\n",
      "batch_idx: 21\n",
      "0.7196969696969697\n",
      "batch_idx: 22\n",
      "0.7155797101449275\n",
      "batch_idx: 23\n",
      "0.7204861111111112\n",
      "batch_idx: 24\n",
      "0.7166666666666667\n",
      "Training Epoch: 380, total loss: 44.445122\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.5\n",
      "batch_idx: 2\n",
      "0.5416666666666666\n",
      "batch_idx: 3\n",
      "0.5729166666666666\n",
      "batch_idx: 4\n",
      "0.575\n",
      "batch_idx: 5\n",
      "0.5763888888888888\n",
      "batch_idx: 6\n",
      "0.5892857142857143\n",
      "batch_idx: 7\n",
      "0.6145833333333334\n",
      "batch_idx: 8\n",
      "0.6203703703703703\n",
      "batch_idx: 9\n",
      "0.6291666666666667\n",
      "batch_idx: 10\n",
      "0.6136363636363636\n",
      "batch_idx: 11\n",
      "0.6354166666666666\n",
      "batch_idx: 12\n",
      "0.6442307692307693\n",
      "batch_idx: 13\n",
      "0.6547619047619048\n",
      "batch_idx: 14\n",
      "0.6611111111111111\n",
      "batch_idx: 15\n",
      "0.6744791666666666\n",
      "batch_idx: 16\n",
      "0.6813725490196079\n",
      "batch_idx: 17\n",
      "0.6736111111111112\n",
      "batch_idx: 18\n",
      "0.6754385964912281\n",
      "batch_idx: 19\n",
      "0.6791666666666667\n",
      "batch_idx: 20\n",
      "0.6746031746031746\n",
      "batch_idx: 21\n",
      "0.678030303030303\n",
      "batch_idx: 22\n",
      "0.6739130434782609\n",
      "batch_idx: 23\n",
      "0.6805555555555556\n",
      "batch_idx: 24\n",
      "0.6833333333333333\n",
      "Training Epoch: 381, total loss: 44.887930\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.6666666666666666\n",
      "batch_idx: 5\n",
      "0.6527777777777778\n",
      "batch_idx: 6\n",
      "0.6726190476190477\n",
      "batch_idx: 7\n",
      "0.6666666666666666\n",
      "batch_idx: 8\n",
      "0.6805555555555556\n",
      "batch_idx: 9\n",
      "0.6958333333333333\n",
      "batch_idx: 10\n",
      "0.7007575757575758\n",
      "batch_idx: 11\n",
      "0.6979166666666666\n",
      "batch_idx: 12\n",
      "0.7019230769230769\n",
      "batch_idx: 13\n",
      "0.6994047619047619\n",
      "batch_idx: 14\n",
      "0.7\n",
      "batch_idx: 15\n",
      "0.7083333333333334\n",
      "batch_idx: 16\n",
      "0.7058823529411765\n",
      "batch_idx: 17\n",
      "0.7060185185185185\n",
      "batch_idx: 18\n",
      "0.7127192982456141\n",
      "batch_idx: 19\n",
      "0.7145833333333333\n",
      "batch_idx: 20\n",
      "0.7162698412698413\n",
      "batch_idx: 21\n",
      "0.7159090909090909\n",
      "batch_idx: 22\n",
      "0.7228260869565217\n",
      "batch_idx: 23\n",
      "0.7239583333333334\n",
      "batch_idx: 24\n",
      "0.7216666666666667\n",
      "Training Epoch: 382, total loss: 44.281737\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.7152777777777778\n",
      "batch_idx: 6\n",
      "0.7202380952380952\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7037037037037037\n",
      "batch_idx: 9\n",
      "0.6958333333333333\n",
      "batch_idx: 10\n",
      "0.696969696969697\n",
      "batch_idx: 11\n",
      "0.6944444444444444\n",
      "batch_idx: 12\n",
      "0.7019230769230769\n",
      "batch_idx: 13\n",
      "0.6964285714285714\n",
      "batch_idx: 14\n",
      "0.6888888888888889\n",
      "batch_idx: 15\n",
      "0.6979166666666666\n",
      "batch_idx: 16\n",
      "0.6862745098039216\n",
      "batch_idx: 17\n",
      "0.6875\n",
      "batch_idx: 18\n",
      "0.6885964912280702\n",
      "batch_idx: 19\n",
      "0.6895833333333333\n",
      "batch_idx: 20\n",
      "0.6944444444444444\n",
      "batch_idx: 21\n",
      "0.696969696969697\n",
      "batch_idx: 22\n",
      "0.6956521739130435\n",
      "batch_idx: 23\n",
      "0.7013888888888888\n",
      "batch_idx: 24\n",
      "0.7083333333333334\n",
      "Training Epoch: 383, total loss: 44.496305\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.7202380952380952\n",
      "batch_idx: 7\n",
      "0.7239583333333334\n",
      "batch_idx: 8\n",
      "0.7175925925925926\n",
      "batch_idx: 9\n",
      "0.7041666666666667\n",
      "batch_idx: 10\n",
      "0.7083333333333334\n",
      "batch_idx: 11\n",
      "0.7083333333333334\n",
      "batch_idx: 12\n",
      "0.717948717948718\n",
      "batch_idx: 13\n",
      "0.7083333333333334\n",
      "batch_idx: 14\n",
      "0.7166666666666667\n",
      "batch_idx: 15\n",
      "0.7213541666666666\n",
      "batch_idx: 16\n",
      "0.7205882352941176\n",
      "batch_idx: 17\n",
      "0.7199074074074074\n",
      "batch_idx: 18\n",
      "0.7280701754385965\n",
      "batch_idx: 19\n",
      "0.7270833333333333\n",
      "batch_idx: 20\n",
      "0.7281746031746031\n",
      "batch_idx: 21\n",
      "0.7348484848484849\n",
      "batch_idx: 22\n",
      "0.7373188405797102\n",
      "batch_idx: 23\n",
      "0.7291666666666666\n",
      "batch_idx: 24\n",
      "0.7316666666666667\n",
      "Training Epoch: 384, total loss: 44.113442\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.6388888888888888\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.7395833333333334\n",
      "batch_idx: 8\n",
      "0.7268518518518519\n",
      "batch_idx: 9\n",
      "0.725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 10\n",
      "0.7272727272727273\n",
      "batch_idx: 11\n",
      "0.7222222222222222\n",
      "batch_idx: 12\n",
      "0.7307692307692307\n",
      "batch_idx: 13\n",
      "0.7321428571428571\n",
      "batch_idx: 14\n",
      "0.7388888888888889\n",
      "batch_idx: 15\n",
      "0.7473958333333334\n",
      "batch_idx: 16\n",
      "0.7450980392156863\n",
      "batch_idx: 17\n",
      "0.7430555555555556\n",
      "batch_idx: 18\n",
      "0.7346491228070176\n",
      "batch_idx: 19\n",
      "0.73125\n",
      "batch_idx: 20\n",
      "0.7361111111111112\n",
      "batch_idx: 21\n",
      "0.7386363636363636\n",
      "batch_idx: 22\n",
      "0.7391304347826086\n",
      "batch_idx: 23\n",
      "0.734375\n",
      "batch_idx: 24\n",
      "0.7333333333333333\n",
      "Training Epoch: 385, total loss: 43.889412\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.7638888888888888\n",
      "batch_idx: 9\n",
      "0.7625\n",
      "batch_idx: 10\n",
      "0.7462121212121212\n",
      "batch_idx: 11\n",
      "0.7430555555555556\n",
      "batch_idx: 12\n",
      "0.7564102564102564\n",
      "batch_idx: 13\n",
      "0.7619047619047619\n",
      "batch_idx: 14\n",
      "0.7611111111111111\n",
      "batch_idx: 15\n",
      "0.765625\n",
      "batch_idx: 16\n",
      "0.7598039215686274\n",
      "batch_idx: 17\n",
      "0.7569444444444444\n",
      "batch_idx: 18\n",
      "0.756578947368421\n",
      "batch_idx: 19\n",
      "0.7395833333333334\n",
      "batch_idx: 20\n",
      "0.7321428571428571\n",
      "batch_idx: 21\n",
      "0.7329545454545454\n",
      "batch_idx: 22\n",
      "0.7373188405797102\n",
      "batch_idx: 23\n",
      "0.7361111111111112\n",
      "batch_idx: 24\n",
      "0.735\n",
      "Training Epoch: 386, total loss: 44.017264\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.6805555555555556\n",
      "batch_idx: 6\n",
      "0.6904761904761905\n",
      "batch_idx: 7\n",
      "0.6927083333333334\n",
      "batch_idx: 8\n",
      "0.7037037037037037\n",
      "batch_idx: 9\n",
      "0.7083333333333334\n",
      "batch_idx: 10\n",
      "0.696969696969697\n",
      "batch_idx: 11\n",
      "0.6944444444444444\n",
      "batch_idx: 12\n",
      "0.7019230769230769\n",
      "batch_idx: 13\n",
      "0.6994047619047619\n",
      "batch_idx: 14\n",
      "0.6944444444444444\n",
      "batch_idx: 15\n",
      "0.6901041666666666\n",
      "batch_idx: 16\n",
      "0.6936274509803921\n",
      "batch_idx: 17\n",
      "0.6944444444444444\n",
      "batch_idx: 18\n",
      "0.7039473684210527\n",
      "batch_idx: 19\n",
      "0.6979166666666666\n",
      "batch_idx: 20\n",
      "0.7043650793650794\n",
      "batch_idx: 21\n",
      "0.7026515151515151\n",
      "batch_idx: 22\n",
      "0.7028985507246377\n",
      "batch_idx: 23\n",
      "0.7013888888888888\n",
      "batch_idx: 24\n",
      "0.705\n",
      "Training Epoch: 387, total loss: 44.509485\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.7440476190476191\n",
      "batch_idx: 7\n",
      "0.7552083333333334\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.75\n",
      "batch_idx: 10\n",
      "0.7424242424242424\n",
      "batch_idx: 11\n",
      "0.7465277777777778\n",
      "batch_idx: 12\n",
      "0.7467948717948718\n",
      "batch_idx: 13\n",
      "0.7470238095238095\n",
      "batch_idx: 14\n",
      "0.7361111111111112\n",
      "batch_idx: 15\n",
      "0.734375\n",
      "batch_idx: 16\n",
      "0.7254901960784313\n",
      "batch_idx: 17\n",
      "0.7175925925925926\n",
      "batch_idx: 18\n",
      "0.7171052631578947\n",
      "batch_idx: 19\n",
      "0.71875\n",
      "batch_idx: 20\n",
      "0.7202380952380952\n",
      "batch_idx: 21\n",
      "0.7196969696969697\n",
      "batch_idx: 22\n",
      "0.717391304347826\n",
      "batch_idx: 23\n",
      "0.7135416666666666\n",
      "batch_idx: 24\n",
      "0.7166666666666667\n",
      "Training Epoch: 388, total loss: 44.522037\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.7\n",
      "batch_idx: 5\n",
      "0.7013888888888888\n",
      "batch_idx: 6\n",
      "0.6964285714285714\n",
      "batch_idx: 7\n",
      "0.6979166666666666\n",
      "batch_idx: 8\n",
      "0.7268518518518519\n",
      "batch_idx: 9\n",
      "0.7125\n",
      "batch_idx: 10\n",
      "0.7007575757575758\n",
      "batch_idx: 11\n",
      "0.7013888888888888\n",
      "batch_idx: 12\n",
      "0.6923076923076923\n",
      "batch_idx: 13\n",
      "0.6964285714285714\n",
      "batch_idx: 14\n",
      "0.6972222222222222\n",
      "batch_idx: 15\n",
      "0.703125\n",
      "batch_idx: 16\n",
      "0.7083333333333334\n",
      "batch_idx: 17\n",
      "0.7083333333333334\n",
      "batch_idx: 18\n",
      "0.7171052631578947\n",
      "batch_idx: 19\n",
      "0.7166666666666667\n",
      "batch_idx: 20\n",
      "0.7182539682539683\n",
      "batch_idx: 21\n",
      "0.7178030303030303\n",
      "batch_idx: 22\n",
      "0.717391304347826\n",
      "batch_idx: 23\n",
      "0.7152777777777778\n",
      "batch_idx: 24\n",
      "0.7183333333333334\n",
      "Training Epoch: 389, total loss: 44.342106\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.725\n",
      "batch_idx: 5\n",
      "0.7083333333333334\n",
      "batch_idx: 6\n",
      "0.7142857142857143\n",
      "batch_idx: 7\n",
      "0.703125\n",
      "batch_idx: 8\n",
      "0.7175925925925926\n",
      "batch_idx: 9\n",
      "0.7208333333333333\n",
      "batch_idx: 10\n",
      "0.7348484848484849\n",
      "batch_idx: 11\n",
      "0.7256944444444444\n",
      "batch_idx: 12\n",
      "0.7307692307692307\n",
      "batch_idx: 13\n",
      "0.7380952380952381\n",
      "batch_idx: 14\n",
      "0.7388888888888889\n",
      "batch_idx: 15\n",
      "0.7239583333333334\n",
      "batch_idx: 16\n",
      "0.7279411764705882\n",
      "batch_idx: 17\n",
      "0.7314814814814815\n",
      "batch_idx: 18\n",
      "0.7368421052631579\n",
      "batch_idx: 19\n",
      "0.74375\n",
      "batch_idx: 20\n",
      "0.7420634920634921\n",
      "batch_idx: 21\n",
      "0.7348484848484849\n",
      "batch_idx: 22\n",
      "0.7318840579710145\n",
      "batch_idx: 23\n",
      "0.7395833333333334\n",
      "batch_idx: 24\n",
      "0.735\n",
      "Training Epoch: 390, total loss: 44.132716\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.725\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7202380952380952\n",
      "batch_idx: 7\n",
      "0.7291666666666666\n",
      "batch_idx: 8\n",
      "0.7268518518518519\n",
      "batch_idx: 9\n",
      "0.7166666666666667\n",
      "batch_idx: 10\n",
      "0.7272727272727273\n",
      "batch_idx: 11\n",
      "0.7326388888888888\n",
      "batch_idx: 12\n",
      "0.7243589743589743\n",
      "batch_idx: 13\n",
      "0.7321428571428571\n",
      "batch_idx: 14\n",
      "0.7361111111111112\n",
      "batch_idx: 15\n",
      "0.7291666666666666\n",
      "batch_idx: 16\n",
      "0.7303921568627451\n",
      "batch_idx: 17\n",
      "0.7361111111111112\n",
      "batch_idx: 18\n",
      "0.7390350877192983\n",
      "batch_idx: 19\n",
      "0.7375\n",
      "batch_idx: 20\n",
      "0.7361111111111112\n",
      "batch_idx: 21\n",
      "0.7348484848484849\n",
      "batch_idx: 22\n",
      "0.7409420289855072\n",
      "batch_idx: 23\n",
      "0.7326388888888888\n",
      "batch_idx: 24\n",
      "0.735\n",
      "Training Epoch: 391, total loss: 43.991184\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7152777777777778\n",
      "batch_idx: 6\n",
      "0.6964285714285714\n",
      "batch_idx: 7\n",
      "0.703125\n",
      "batch_idx: 8\n",
      "0.6990740740740741\n",
      "batch_idx: 9\n",
      "0.6958333333333333\n",
      "batch_idx: 10\n",
      "0.6893939393939394\n",
      "batch_idx: 11\n",
      "0.6944444444444444\n",
      "batch_idx: 12\n",
      "0.6923076923076923\n",
      "batch_idx: 13\n",
      "0.6934523809523809\n",
      "batch_idx: 14\n",
      "0.7\n",
      "batch_idx: 15\n",
      "0.7109375\n",
      "batch_idx: 16\n",
      "0.7107843137254902\n",
      "batch_idx: 17\n",
      "0.7060185185185185\n",
      "batch_idx: 18\n",
      "0.7105263157894737\n",
      "batch_idx: 19\n",
      "0.7104166666666667\n",
      "batch_idx: 20\n",
      "0.7162698412698413\n",
      "batch_idx: 21\n",
      "0.7159090909090909\n",
      "batch_idx: 22\n",
      "0.7155797101449275\n",
      "batch_idx: 23\n",
      "0.7100694444444444\n",
      "batch_idx: 24\n",
      "0.71\n",
      "Training Epoch: 392, total loss: 44.528771\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7731481481481481\n",
      "batch_idx: 9\n",
      "0.7791666666666667\n",
      "batch_idx: 10\n",
      "0.7651515151515151\n",
      "batch_idx: 11\n",
      "0.7638888888888888\n",
      "batch_idx: 12\n",
      "0.7724358974358975\n",
      "batch_idx: 13\n",
      "0.7619047619047619\n",
      "batch_idx: 14\n",
      "0.7555555555555555\n",
      "batch_idx: 15\n",
      "0.7630208333333334\n",
      "batch_idx: 16\n",
      "0.7450980392156863\n",
      "batch_idx: 17\n",
      "0.7430555555555556\n",
      "batch_idx: 18\n",
      "0.743421052631579\n",
      "batch_idx: 19\n",
      "0.7395833333333334\n",
      "batch_idx: 20\n",
      "0.7261904761904762\n",
      "batch_idx: 21\n",
      "0.7178030303030303\n",
      "batch_idx: 22\n",
      "0.7119565217391305\n",
      "batch_idx: 23\n",
      "0.7152777777777778\n",
      "batch_idx: 24\n",
      "0.7116666666666667\n",
      "Training Epoch: 393, total loss: 44.523426\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7261904761904762\n",
      "batch_idx: 7\n",
      "0.7239583333333334\n",
      "batch_idx: 8\n",
      "0.7222222222222222\n",
      "batch_idx: 9\n",
      "0.7125\n",
      "batch_idx: 10\n",
      "0.7234848484848485\n",
      "batch_idx: 11\n",
      "0.7361111111111112\n",
      "batch_idx: 12\n",
      "0.7435897435897436\n",
      "batch_idx: 13\n",
      "0.7410714285714286\n",
      "batch_idx: 14\n",
      "0.7388888888888889\n",
      "batch_idx: 15\n",
      "0.7265625\n",
      "batch_idx: 16\n",
      "0.7181372549019608\n",
      "batch_idx: 17\n",
      "0.7152777777777778\n",
      "batch_idx: 18\n",
      "0.706140350877193\n",
      "batch_idx: 19\n",
      "0.7083333333333334\n",
      "batch_idx: 20\n",
      "0.7063492063492064\n",
      "batch_idx: 21\n",
      "0.7121212121212122\n",
      "batch_idx: 22\n",
      "0.717391304347826\n",
      "batch_idx: 23\n",
      "0.7170138888888888\n",
      "batch_idx: 24\n",
      "0.7166666666666667\n",
      "Training Epoch: 394, total loss: 44.565747\n",
      "batch_idx: 0\n",
      "0.5\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.6458333333333334\n",
      "batch_idx: 4\n",
      "0.6416666666666667\n",
      "batch_idx: 5\n",
      "0.6458333333333334\n",
      "batch_idx: 6\n",
      "0.6607142857142857\n",
      "batch_idx: 7\n",
      "0.671875\n",
      "batch_idx: 8\n",
      "0.6620370370370371\n",
      "batch_idx: 9\n",
      "0.6583333333333333\n",
      "batch_idx: 10\n",
      "0.6439393939393939\n",
      "batch_idx: 11\n",
      "0.6493055555555556\n",
      "batch_idx: 12\n",
      "0.6538461538461539\n",
      "batch_idx: 13\n",
      "0.6607142857142857\n",
      "batch_idx: 14\n",
      "0.6694444444444444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 15\n",
      "0.671875\n",
      "batch_idx: 16\n",
      "0.678921568627451\n",
      "batch_idx: 17\n",
      "0.6875\n",
      "batch_idx: 18\n",
      "0.6951754385964912\n",
      "batch_idx: 19\n",
      "0.69375\n",
      "batch_idx: 20\n",
      "0.6944444444444444\n",
      "batch_idx: 21\n",
      "0.7007575757575758\n",
      "batch_idx: 22\n",
      "0.7028985507246377\n",
      "batch_idx: 23\n",
      "0.7048611111111112\n",
      "batch_idx: 24\n",
      "0.7066666666666667\n",
      "Training Epoch: 395, total loss: 44.678199\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.65625\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6805555555555556\n",
      "batch_idx: 6\n",
      "0.6904761904761905\n",
      "batch_idx: 7\n",
      "0.703125\n",
      "batch_idx: 8\n",
      "0.6851851851851852\n",
      "batch_idx: 9\n",
      "0.6833333333333333\n",
      "batch_idx: 10\n",
      "0.6893939393939394\n",
      "batch_idx: 11\n",
      "0.6909722222222222\n",
      "batch_idx: 12\n",
      "0.6987179487179487\n",
      "batch_idx: 13\n",
      "0.7023809523809523\n",
      "batch_idx: 14\n",
      "0.7138888888888889\n",
      "batch_idx: 15\n",
      "0.7161458333333334\n",
      "batch_idx: 16\n",
      "0.7132352941176471\n",
      "batch_idx: 17\n",
      "0.7152777777777778\n",
      "batch_idx: 18\n",
      "0.706140350877193\n",
      "batch_idx: 19\n",
      "0.7020833333333333\n",
      "batch_idx: 20\n",
      "0.7083333333333334\n",
      "batch_idx: 21\n",
      "0.7121212121212122\n",
      "batch_idx: 22\n",
      "0.7101449275362319\n",
      "batch_idx: 23\n",
      "0.7118055555555556\n",
      "batch_idx: 24\n",
      "0.71\n",
      "Training Epoch: 396, total loss: 44.507867\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6805555555555556\n",
      "batch_idx: 6\n",
      "0.7023809523809523\n",
      "batch_idx: 7\n",
      "0.6979166666666666\n",
      "batch_idx: 8\n",
      "0.7037037037037037\n",
      "batch_idx: 9\n",
      "0.7125\n",
      "batch_idx: 10\n",
      "0.7007575757575758\n",
      "batch_idx: 11\n",
      "0.7118055555555556\n",
      "batch_idx: 12\n",
      "0.717948717948718\n",
      "batch_idx: 13\n",
      "0.7232142857142857\n",
      "batch_idx: 14\n",
      "0.7222222222222222\n",
      "batch_idx: 15\n",
      "0.7161458333333334\n",
      "batch_idx: 16\n",
      "0.7205882352941176\n",
      "batch_idx: 17\n",
      "0.7083333333333334\n",
      "batch_idx: 18\n",
      "0.7039473684210527\n",
      "batch_idx: 19\n",
      "0.7\n",
      "batch_idx: 20\n",
      "0.7063492063492064\n",
      "batch_idx: 21\n",
      "0.7026515151515151\n",
      "batch_idx: 22\n",
      "0.7047101449275363\n",
      "batch_idx: 23\n",
      "0.7100694444444444\n",
      "batch_idx: 24\n",
      "0.715\n",
      "Training Epoch: 397, total loss: 44.228916\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7222222222222222\n",
      "batch_idx: 9\n",
      "0.7208333333333333\n",
      "batch_idx: 10\n",
      "0.7272727272727273\n",
      "batch_idx: 11\n",
      "0.7256944444444444\n",
      "batch_idx: 12\n",
      "0.7275641025641025\n",
      "batch_idx: 13\n",
      "0.7440476190476191\n",
      "batch_idx: 14\n",
      "0.7388888888888889\n",
      "batch_idx: 15\n",
      "0.7421875\n",
      "batch_idx: 16\n",
      "0.7401960784313726\n",
      "batch_idx: 17\n",
      "0.7384259259259259\n",
      "batch_idx: 18\n",
      "0.7302631578947368\n",
      "batch_idx: 19\n",
      "0.73125\n",
      "batch_idx: 20\n",
      "0.7321428571428571\n",
      "batch_idx: 21\n",
      "0.7291666666666666\n",
      "batch_idx: 22\n",
      "0.7318840579710145\n",
      "batch_idx: 23\n",
      "0.7326388888888888\n",
      "batch_idx: 24\n",
      "0.73\n",
      "Training Epoch: 398, total loss: 44.304494\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.6666666666666666\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6666666666666666\n",
      "batch_idx: 6\n",
      "0.6666666666666666\n",
      "batch_idx: 7\n",
      "0.6979166666666666\n",
      "batch_idx: 8\n",
      "0.7129629629629629\n",
      "batch_idx: 9\n",
      "0.7083333333333334\n",
      "batch_idx: 10\n",
      "0.7121212121212122\n",
      "batch_idx: 11\n",
      "0.7291666666666666\n",
      "batch_idx: 12\n",
      "0.7371794871794872\n",
      "batch_idx: 13\n",
      "0.7351190476190477\n",
      "batch_idx: 14\n",
      "0.7361111111111112\n",
      "batch_idx: 15\n",
      "0.7447916666666666\n",
      "batch_idx: 16\n",
      "0.7377450980392157\n",
      "batch_idx: 17\n",
      "0.7361111111111112\n",
      "batch_idx: 18\n",
      "0.7368421052631579\n",
      "batch_idx: 19\n",
      "0.7416666666666667\n",
      "batch_idx: 20\n",
      "0.7420634920634921\n",
      "batch_idx: 21\n",
      "0.740530303030303\n",
      "batch_idx: 22\n",
      "0.7391304347826086\n",
      "batch_idx: 23\n",
      "0.7413194444444444\n",
      "batch_idx: 24\n",
      "0.74\n",
      "Training Epoch: 399, total loss: 43.900336\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7361111111111112\n",
      "batch_idx: 6\n",
      "0.7440476190476191\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.75\n",
      "batch_idx: 9\n",
      "0.7541666666666667\n",
      "batch_idx: 10\n",
      "0.75\n",
      "batch_idx: 11\n",
      "0.7395833333333334\n",
      "batch_idx: 12\n",
      "0.7435897435897436\n",
      "batch_idx: 13\n",
      "0.7380952380952381\n",
      "batch_idx: 14\n",
      "0.7361111111111112\n",
      "batch_idx: 15\n",
      "0.7395833333333334\n",
      "batch_idx: 16\n",
      "0.7401960784313726\n",
      "batch_idx: 17\n",
      "0.75\n",
      "batch_idx: 18\n",
      "0.7543859649122807\n",
      "batch_idx: 19\n",
      "0.74375\n",
      "batch_idx: 20\n",
      "0.751984126984127\n",
      "batch_idx: 21\n",
      "0.7518939393939394\n",
      "batch_idx: 22\n",
      "0.7427536231884058\n",
      "batch_idx: 23\n",
      "0.7447916666666666\n",
      "batch_idx: 24\n",
      "0.7433333333333333\n",
      "Training Epoch: 400, total loss: 43.889769\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7013888888888888\n",
      "batch_idx: 6\n",
      "0.7023809523809523\n",
      "batch_idx: 7\n",
      "0.7135416666666666\n",
      "batch_idx: 8\n",
      "0.7175925925925926\n",
      "batch_idx: 9\n",
      "0.7125\n",
      "batch_idx: 10\n",
      "0.7045454545454546\n",
      "batch_idx: 11\n",
      "0.7013888888888888\n",
      "batch_idx: 12\n",
      "0.6891025641025641\n",
      "batch_idx: 13\n",
      "0.6785714285714286\n",
      "batch_idx: 14\n",
      "0.6861111111111111\n",
      "batch_idx: 15\n",
      "0.6848958333333334\n",
      "batch_idx: 16\n",
      "0.6838235294117647\n",
      "batch_idx: 17\n",
      "0.6828703703703703\n",
      "batch_idx: 18\n",
      "0.6864035087719298\n",
      "batch_idx: 19\n",
      "0.6916666666666667\n",
      "batch_idx: 20\n",
      "0.6944444444444444\n",
      "batch_idx: 21\n",
      "0.6988636363636364\n",
      "batch_idx: 22\n",
      "0.6956521739130435\n",
      "batch_idx: 23\n",
      "0.6961805555555556\n",
      "batch_idx: 24\n",
      "0.6983333333333334\n",
      "Training Epoch: 401, total loss: 44.797388\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.7152777777777778\n",
      "batch_idx: 6\n",
      "0.7023809523809523\n",
      "batch_idx: 7\n",
      "0.7083333333333334\n",
      "batch_idx: 8\n",
      "0.7037037037037037\n",
      "batch_idx: 9\n",
      "0.7041666666666667\n",
      "batch_idx: 10\n",
      "0.7007575757575758\n",
      "batch_idx: 11\n",
      "0.7048611111111112\n",
      "batch_idx: 12\n",
      "0.7115384615384616\n",
      "batch_idx: 13\n",
      "0.7172619047619048\n",
      "batch_idx: 14\n",
      "0.7138888888888889\n",
      "batch_idx: 15\n",
      "0.71875\n",
      "batch_idx: 16\n",
      "0.7279411764705882\n",
      "batch_idx: 17\n",
      "0.7245370370370371\n",
      "batch_idx: 18\n",
      "0.7302631578947368\n",
      "batch_idx: 19\n",
      "0.725\n",
      "batch_idx: 20\n",
      "0.7321428571428571\n",
      "batch_idx: 21\n",
      "0.740530303030303\n",
      "batch_idx: 22\n",
      "0.7463768115942029\n",
      "batch_idx: 23\n",
      "0.7465277777777778\n",
      "batch_idx: 24\n",
      "0.7466666666666667\n",
      "Training Epoch: 402, total loss: 43.887095\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.5833333333333334\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.6666666666666666\n",
      "batch_idx: 4\n",
      "0.6583333333333333\n",
      "batch_idx: 5\n",
      "0.6805555555555556\n",
      "batch_idx: 6\n",
      "0.6607142857142857\n",
      "batch_idx: 7\n",
      "0.671875\n",
      "batch_idx: 8\n",
      "0.6851851851851852\n",
      "batch_idx: 9\n",
      "0.7041666666666667\n",
      "batch_idx: 10\n",
      "0.7007575757575758\n",
      "batch_idx: 11\n",
      "0.6909722222222222\n",
      "batch_idx: 12\n",
      "0.6891025641025641\n",
      "batch_idx: 13\n",
      "0.6904761904761905\n",
      "batch_idx: 14\n",
      "0.6888888888888889\n",
      "batch_idx: 15\n",
      "0.7005208333333334\n",
      "batch_idx: 16\n",
      "0.7034313725490197\n",
      "batch_idx: 17\n",
      "0.7013888888888888\n",
      "batch_idx: 18\n",
      "0.7017543859649122\n",
      "batch_idx: 19\n",
      "0.7083333333333334\n",
      "batch_idx: 20\n",
      "0.7083333333333334\n",
      "batch_idx: 21\n",
      "0.7045454545454546\n",
      "batch_idx: 22\n",
      "0.7119565217391305\n",
      "batch_idx: 23\n",
      "0.7065972222222222\n",
      "batch_idx: 24\n",
      "0.7083333333333334\n",
      "Training Epoch: 403, total loss: 44.584068\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.7083333333333334\n",
      "batch_idx: 6\n",
      "0.7083333333333334\n",
      "batch_idx: 7\n",
      "0.7083333333333334\n",
      "batch_idx: 8\n",
      "0.7222222222222222\n",
      "batch_idx: 9\n",
      "0.725\n",
      "batch_idx: 10\n",
      "0.7196969696969697\n",
      "batch_idx: 11\n",
      "0.7152777777777778\n",
      "batch_idx: 12\n",
      "0.7083333333333334\n",
      "batch_idx: 13\n",
      "0.7142857142857143\n",
      "batch_idx: 14\n",
      "0.7111111111111111\n",
      "batch_idx: 15\n",
      "0.7161458333333334\n",
      "batch_idx: 16\n",
      "0.7058823529411765\n",
      "batch_idx: 17\n",
      "0.7013888888888888\n",
      "batch_idx: 18\n",
      "0.7039473684210527\n",
      "batch_idx: 19\n",
      "0.7125\n",
      "batch_idx: 20\n",
      "0.7103174603174603\n",
      "batch_idx: 21\n",
      "0.6988636363636364\n",
      "batch_idx: 22\n",
      "0.697463768115942\n",
      "batch_idx: 23\n",
      "0.7013888888888888\n",
      "batch_idx: 24\n",
      "0.705\n",
      "Training Epoch: 404, total loss: 44.492522\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.7239583333333334\n",
      "batch_idx: 8\n",
      "0.7129629629629629\n",
      "batch_idx: 9\n",
      "0.7208333333333333\n",
      "batch_idx: 10\n",
      "0.7272727272727273\n",
      "batch_idx: 11\n",
      "0.7291666666666666\n",
      "batch_idx: 12\n",
      "0.7339743589743589\n",
      "batch_idx: 13\n",
      "0.7232142857142857\n",
      "batch_idx: 14\n",
      "0.7222222222222222\n",
      "batch_idx: 15\n",
      "0.7161458333333334\n",
      "batch_idx: 16\n",
      "0.7156862745098039\n",
      "batch_idx: 17\n",
      "0.7175925925925926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 18\n",
      "0.7127192982456141\n",
      "batch_idx: 19\n",
      "0.7166666666666667\n",
      "batch_idx: 20\n",
      "0.7142857142857143\n",
      "batch_idx: 21\n",
      "0.7140151515151515\n",
      "batch_idx: 22\n",
      "0.7119565217391305\n",
      "batch_idx: 23\n",
      "0.7100694444444444\n",
      "batch_idx: 24\n",
      "0.7033333333333334\n",
      "Training Epoch: 405, total loss: 44.604828\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7559523809523809\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.7638888888888888\n",
      "batch_idx: 9\n",
      "0.7583333333333333\n",
      "batch_idx: 10\n",
      "0.7575757575757576\n",
      "batch_idx: 11\n",
      "0.7465277777777778\n",
      "batch_idx: 12\n",
      "0.7403846153846154\n",
      "batch_idx: 13\n",
      "0.7440476190476191\n",
      "batch_idx: 14\n",
      "0.7388888888888889\n",
      "batch_idx: 15\n",
      "0.7317708333333334\n",
      "batch_idx: 16\n",
      "0.7303921568627451\n",
      "batch_idx: 17\n",
      "0.7175925925925926\n",
      "batch_idx: 18\n",
      "0.7105263157894737\n",
      "batch_idx: 19\n",
      "0.7166666666666667\n",
      "batch_idx: 20\n",
      "0.7123015873015873\n",
      "batch_idx: 21\n",
      "0.7121212121212122\n",
      "batch_idx: 22\n",
      "0.7119565217391305\n",
      "batch_idx: 23\n",
      "0.7135416666666666\n",
      "batch_idx: 24\n",
      "0.71\n",
      "Training Epoch: 406, total loss: 44.522104\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.6904761904761905\n",
      "batch_idx: 7\n",
      "0.6927083333333334\n",
      "batch_idx: 8\n",
      "0.6990740740740741\n",
      "batch_idx: 9\n",
      "0.7\n",
      "batch_idx: 10\n",
      "0.6893939393939394\n",
      "batch_idx: 11\n",
      "0.6979166666666666\n",
      "batch_idx: 12\n",
      "0.6858974358974359\n",
      "batch_idx: 13\n",
      "0.6875\n",
      "batch_idx: 14\n",
      "0.6861111111111111\n",
      "batch_idx: 15\n",
      "0.6953125\n",
      "batch_idx: 16\n",
      "0.7009803921568627\n",
      "batch_idx: 17\n",
      "0.7060185185185185\n",
      "batch_idx: 18\n",
      "0.706140350877193\n",
      "batch_idx: 19\n",
      "0.7104166666666667\n",
      "batch_idx: 20\n",
      "0.7123015873015873\n",
      "batch_idx: 21\n",
      "0.7140151515151515\n",
      "batch_idx: 22\n",
      "0.7155797101449275\n",
      "batch_idx: 23\n",
      "0.7083333333333334\n",
      "batch_idx: 24\n",
      "0.705\n",
      "Training Epoch: 407, total loss: 44.646481\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.725\n",
      "batch_idx: 5\n",
      "0.7152777777777778\n",
      "batch_idx: 6\n",
      "0.7202380952380952\n",
      "batch_idx: 7\n",
      "0.7239583333333334\n",
      "batch_idx: 8\n",
      "0.7175925925925926\n",
      "batch_idx: 9\n",
      "0.7208333333333333\n",
      "batch_idx: 10\n",
      "0.7272727272727273\n",
      "batch_idx: 11\n",
      "0.7465277777777778\n",
      "batch_idx: 12\n",
      "0.7435897435897436\n",
      "batch_idx: 13\n",
      "0.7559523809523809\n",
      "batch_idx: 14\n",
      "0.75\n",
      "batch_idx: 15\n",
      "0.7473958333333334\n",
      "batch_idx: 16\n",
      "0.7524509803921569\n",
      "batch_idx: 17\n",
      "0.7523148148148148\n",
      "batch_idx: 18\n",
      "0.7609649122807017\n",
      "batch_idx: 19\n",
      "0.7520833333333333\n",
      "batch_idx: 20\n",
      "0.7440476190476191\n",
      "batch_idx: 21\n",
      "0.7462121212121212\n",
      "batch_idx: 22\n",
      "0.7391304347826086\n",
      "batch_idx: 23\n",
      "0.7378472222222222\n",
      "batch_idx: 24\n",
      "0.7333333333333333\n",
      "Training Epoch: 408, total loss: 44.140947\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.65625\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.6875\n",
      "batch_idx: 6\n",
      "0.6964285714285714\n",
      "batch_idx: 7\n",
      "0.6927083333333334\n",
      "batch_idx: 8\n",
      "0.6944444444444444\n",
      "batch_idx: 9\n",
      "0.6916666666666667\n",
      "batch_idx: 10\n",
      "0.6931818181818182\n",
      "batch_idx: 11\n",
      "0.6840277777777778\n",
      "batch_idx: 12\n",
      "0.6826923076923077\n",
      "batch_idx: 13\n",
      "0.6755952380952381\n",
      "batch_idx: 14\n",
      "0.6805555555555556\n",
      "batch_idx: 15\n",
      "0.671875\n",
      "batch_idx: 16\n",
      "0.6666666666666666\n",
      "batch_idx: 17\n",
      "0.6712962962962963\n",
      "batch_idx: 18\n",
      "0.6798245614035088\n",
      "batch_idx: 19\n",
      "0.6854166666666667\n",
      "batch_idx: 20\n",
      "0.6884920634920635\n",
      "batch_idx: 21\n",
      "0.6912878787878788\n",
      "batch_idx: 22\n",
      "0.6920289855072463\n",
      "batch_idx: 23\n",
      "0.6927083333333334\n",
      "batch_idx: 24\n",
      "0.695\n",
      "Training Epoch: 409, total loss: 44.601833\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7361111111111112\n",
      "batch_idx: 6\n",
      "0.7559523809523809\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.7592592592592593\n",
      "batch_idx: 9\n",
      "0.7458333333333333\n",
      "batch_idx: 10\n",
      "0.7272727272727273\n",
      "batch_idx: 11\n",
      "0.7395833333333334\n",
      "batch_idx: 12\n",
      "0.7435897435897436\n",
      "batch_idx: 13\n",
      "0.7321428571428571\n",
      "batch_idx: 14\n",
      "0.7277777777777777\n",
      "batch_idx: 15\n",
      "0.7317708333333334\n",
      "batch_idx: 16\n",
      "0.7279411764705882\n",
      "batch_idx: 17\n",
      "0.7314814814814815\n",
      "batch_idx: 18\n",
      "0.7368421052631579\n",
      "batch_idx: 19\n",
      "0.7395833333333334\n",
      "batch_idx: 20\n",
      "0.7440476190476191\n",
      "batch_idx: 21\n",
      "0.7462121212121212\n",
      "batch_idx: 22\n",
      "0.75\n",
      "batch_idx: 23\n",
      "0.7465277777777778\n",
      "batch_idx: 24\n",
      "0.745\n",
      "Training Epoch: 410, total loss: 43.975572\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.7152777777777778\n",
      "batch_idx: 6\n",
      "0.7023809523809523\n",
      "batch_idx: 7\n",
      "0.7135416666666666\n",
      "batch_idx: 8\n",
      "0.7175925925925926\n",
      "batch_idx: 9\n",
      "0.7166666666666667\n",
      "batch_idx: 10\n",
      "0.7272727272727273\n",
      "batch_idx: 11\n",
      "0.7361111111111112\n",
      "batch_idx: 12\n",
      "0.7307692307692307\n",
      "batch_idx: 13\n",
      "0.7351190476190477\n",
      "batch_idx: 14\n",
      "0.7472222222222222\n",
      "batch_idx: 15\n",
      "0.7473958333333334\n",
      "batch_idx: 16\n",
      "0.7352941176470589\n",
      "batch_idx: 17\n",
      "0.7407407407407407\n",
      "batch_idx: 18\n",
      "0.743421052631579\n",
      "batch_idx: 19\n",
      "0.7395833333333334\n",
      "batch_idx: 20\n",
      "0.7400793650793651\n",
      "batch_idx: 21\n",
      "0.7443181818181818\n",
      "batch_idx: 22\n",
      "0.7391304347826086\n",
      "batch_idx: 23\n",
      "0.7378472222222222\n",
      "batch_idx: 24\n",
      "0.735\n",
      "Training Epoch: 411, total loss: 43.860705\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.6666666666666666\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.6944444444444444\n",
      "batch_idx: 6\n",
      "0.7261904761904762\n",
      "batch_idx: 7\n",
      "0.71875\n",
      "batch_idx: 8\n",
      "0.7222222222222222\n",
      "batch_idx: 9\n",
      "0.7291666666666666\n",
      "batch_idx: 10\n",
      "0.7159090909090909\n",
      "batch_idx: 11\n",
      "0.7083333333333334\n",
      "batch_idx: 12\n",
      "0.7083333333333334\n",
      "batch_idx: 13\n",
      "0.7053571428571429\n",
      "batch_idx: 14\n",
      "0.7027777777777777\n",
      "batch_idx: 15\n",
      "0.6979166666666666\n",
      "batch_idx: 16\n",
      "0.6985294117647058\n",
      "batch_idx: 17\n",
      "0.7037037037037037\n",
      "batch_idx: 18\n",
      "0.7017543859649122\n",
      "batch_idx: 19\n",
      "0.7\n",
      "batch_idx: 20\n",
      "0.7043650793650794\n",
      "batch_idx: 21\n",
      "0.7064393939393939\n",
      "batch_idx: 22\n",
      "0.717391304347826\n",
      "batch_idx: 23\n",
      "0.7170138888888888\n",
      "batch_idx: 24\n",
      "0.7183333333333334\n",
      "Training Epoch: 412, total loss: 44.374714\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.7083333333333334\n",
      "batch_idx: 6\n",
      "0.7202380952380952\n",
      "batch_idx: 7\n",
      "0.71875\n",
      "batch_idx: 8\n",
      "0.7222222222222222\n",
      "batch_idx: 9\n",
      "0.7\n",
      "batch_idx: 10\n",
      "0.7007575757575758\n",
      "batch_idx: 11\n",
      "0.7152777777777778\n",
      "batch_idx: 12\n",
      "0.7243589743589743\n",
      "batch_idx: 13\n",
      "0.7291666666666666\n",
      "batch_idx: 14\n",
      "0.7277777777777777\n",
      "batch_idx: 15\n",
      "0.7213541666666666\n",
      "batch_idx: 16\n",
      "0.7205882352941176\n",
      "batch_idx: 17\n",
      "0.7268518518518519\n",
      "batch_idx: 18\n",
      "0.7324561403508771\n",
      "batch_idx: 19\n",
      "0.7333333333333333\n",
      "batch_idx: 20\n",
      "0.7321428571428571\n",
      "batch_idx: 21\n",
      "0.7272727272727273\n",
      "batch_idx: 22\n",
      "0.730072463768116\n",
      "batch_idx: 23\n",
      "0.7291666666666666\n",
      "batch_idx: 24\n",
      "0.7366666666666667\n",
      "Training Epoch: 413, total loss: 43.979189\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7083333333333334\n",
      "batch_idx: 7\n",
      "0.7083333333333334\n",
      "batch_idx: 8\n",
      "0.6944444444444444\n",
      "batch_idx: 9\n",
      "0.6958333333333333\n",
      "batch_idx: 10\n",
      "0.6818181818181818\n",
      "batch_idx: 11\n",
      "0.6909722222222222\n",
      "batch_idx: 12\n",
      "0.6858974358974359\n",
      "batch_idx: 13\n",
      "0.6845238095238095\n",
      "batch_idx: 14\n",
      "0.6833333333333333\n",
      "batch_idx: 15\n",
      "0.6822916666666666\n",
      "batch_idx: 16\n",
      "0.6911764705882353\n",
      "batch_idx: 17\n",
      "0.6967592592592593\n",
      "batch_idx: 18\n",
      "0.7039473684210527\n",
      "batch_idx: 19\n",
      "0.7104166666666667\n",
      "batch_idx: 20\n",
      "0.7123015873015873\n",
      "batch_idx: 21\n",
      "0.7215909090909091\n",
      "batch_idx: 22\n",
      "0.7119565217391305\n",
      "batch_idx: 23\n",
      "0.7170138888888888\n",
      "batch_idx: 24\n",
      "0.7183333333333334\n",
      "Training Epoch: 414, total loss: 44.247262\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7546296296296297\n",
      "batch_idx: 9\n",
      "0.7541666666666667\n",
      "batch_idx: 10\n",
      "0.7651515151515151\n",
      "batch_idx: 11\n",
      "0.75\n",
      "batch_idx: 12\n",
      "0.7467948717948718\n",
      "batch_idx: 13\n",
      "0.7559523809523809\n",
      "batch_idx: 14\n",
      "0.7555555555555555\n",
      "batch_idx: 15\n",
      "0.7526041666666666\n",
      "batch_idx: 16\n",
      "0.7524509803921569\n",
      "batch_idx: 17\n",
      "0.7546296296296297\n",
      "batch_idx: 18\n",
      "0.7521929824561403\n",
      "batch_idx: 19\n",
      "0.7479166666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 20\n",
      "0.746031746031746\n",
      "batch_idx: 21\n",
      "0.7424242424242424\n",
      "batch_idx: 22\n",
      "0.7409420289855072\n",
      "batch_idx: 23\n",
      "0.7430555555555556\n",
      "batch_idx: 24\n",
      "0.7433333333333333\n",
      "Training Epoch: 415, total loss: 43.778913\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7559523809523809\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.7731481481481481\n",
      "batch_idx: 9\n",
      "0.7666666666666667\n",
      "batch_idx: 10\n",
      "0.75\n",
      "batch_idx: 11\n",
      "0.7638888888888888\n",
      "batch_idx: 12\n",
      "0.7564102564102564\n",
      "batch_idx: 13\n",
      "0.7470238095238095\n",
      "batch_idx: 14\n",
      "0.7444444444444445\n",
      "batch_idx: 15\n",
      "0.7526041666666666\n",
      "batch_idx: 16\n",
      "0.7549019607843137\n",
      "batch_idx: 17\n",
      "0.75\n",
      "batch_idx: 18\n",
      "0.743421052631579\n",
      "batch_idx: 19\n",
      "0.7375\n",
      "batch_idx: 20\n",
      "0.7321428571428571\n",
      "batch_idx: 21\n",
      "0.7310606060606061\n",
      "batch_idx: 22\n",
      "0.7282608695652174\n",
      "batch_idx: 23\n",
      "0.7239583333333334\n",
      "batch_idx: 24\n",
      "0.7216666666666667\n",
      "Training Epoch: 416, total loss: 44.162662\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7559523809523809\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.75\n",
      "batch_idx: 9\n",
      "0.7458333333333333\n",
      "batch_idx: 10\n",
      "0.7462121212121212\n",
      "batch_idx: 11\n",
      "0.7465277777777778\n",
      "batch_idx: 12\n",
      "0.7435897435897436\n",
      "batch_idx: 13\n",
      "0.7470238095238095\n",
      "batch_idx: 14\n",
      "0.7388888888888889\n",
      "batch_idx: 15\n",
      "0.7421875\n",
      "batch_idx: 16\n",
      "0.7450980392156863\n",
      "batch_idx: 17\n",
      "0.7453703703703703\n",
      "batch_idx: 18\n",
      "0.7543859649122807\n",
      "batch_idx: 19\n",
      "0.7541666666666667\n",
      "batch_idx: 20\n",
      "0.7579365079365079\n",
      "batch_idx: 21\n",
      "0.7556818181818182\n",
      "batch_idx: 22\n",
      "0.7572463768115942\n",
      "batch_idx: 23\n",
      "0.7517361111111112\n",
      "batch_idx: 24\n",
      "0.75\n",
      "Training Epoch: 417, total loss: 43.839687\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7321428571428571\n",
      "batch_idx: 7\n",
      "0.7395833333333334\n",
      "batch_idx: 8\n",
      "0.75\n",
      "batch_idx: 9\n",
      "0.7416666666666667\n",
      "batch_idx: 10\n",
      "0.75\n",
      "batch_idx: 11\n",
      "0.75\n",
      "batch_idx: 12\n",
      "0.7467948717948718\n",
      "batch_idx: 13\n",
      "0.75\n",
      "batch_idx: 14\n",
      "0.7444444444444445\n",
      "batch_idx: 15\n",
      "0.7369791666666666\n",
      "batch_idx: 16\n",
      "0.7450980392156863\n",
      "batch_idx: 17\n",
      "0.7453703703703703\n",
      "batch_idx: 18\n",
      "0.743421052631579\n",
      "batch_idx: 19\n",
      "0.7375\n",
      "batch_idx: 20\n",
      "0.7321428571428571\n",
      "batch_idx: 21\n",
      "0.7348484848484849\n",
      "batch_idx: 22\n",
      "0.7391304347826086\n",
      "batch_idx: 23\n",
      "0.7413194444444444\n",
      "batch_idx: 24\n",
      "0.7416666666666667\n",
      "Training Epoch: 418, total loss: 43.879249\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.7202380952380952\n",
      "batch_idx: 7\n",
      "0.7083333333333334\n",
      "batch_idx: 8\n",
      "0.7083333333333334\n",
      "batch_idx: 9\n",
      "0.6958333333333333\n",
      "batch_idx: 10\n",
      "0.6931818181818182\n",
      "batch_idx: 11\n",
      "0.6944444444444444\n",
      "batch_idx: 12\n",
      "0.7051282051282052\n",
      "batch_idx: 13\n",
      "0.7083333333333334\n",
      "batch_idx: 14\n",
      "0.7111111111111111\n",
      "batch_idx: 15\n",
      "0.7135416666666666\n",
      "batch_idx: 16\n",
      "0.7107843137254902\n",
      "batch_idx: 17\n",
      "0.7083333333333334\n",
      "batch_idx: 18\n",
      "0.7083333333333334\n",
      "batch_idx: 19\n",
      "0.7125\n",
      "batch_idx: 20\n",
      "0.7142857142857143\n",
      "batch_idx: 21\n",
      "0.7121212121212122\n",
      "batch_idx: 22\n",
      "0.7137681159420289\n",
      "batch_idx: 23\n",
      "0.7135416666666666\n",
      "batch_idx: 24\n",
      "0.715\n",
      "Training Epoch: 419, total loss: 44.351165\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.65625\n",
      "batch_idx: 4\n",
      "0.7\n",
      "batch_idx: 5\n",
      "0.6805555555555556\n",
      "batch_idx: 6\n",
      "0.7083333333333334\n",
      "batch_idx: 7\n",
      "0.7135416666666666\n",
      "batch_idx: 8\n",
      "0.7083333333333334\n",
      "batch_idx: 9\n",
      "0.725\n",
      "batch_idx: 10\n",
      "0.7310606060606061\n",
      "batch_idx: 11\n",
      "0.7222222222222222\n",
      "batch_idx: 12\n",
      "0.7243589743589743\n",
      "batch_idx: 13\n",
      "0.7291666666666666\n",
      "batch_idx: 14\n",
      "0.7305555555555555\n",
      "batch_idx: 15\n",
      "0.7317708333333334\n",
      "batch_idx: 16\n",
      "0.7352941176470589\n",
      "batch_idx: 17\n",
      "0.7291666666666666\n",
      "batch_idx: 18\n",
      "0.7324561403508771\n",
      "batch_idx: 19\n",
      "0.7291666666666666\n",
      "batch_idx: 20\n",
      "0.7261904761904762\n",
      "batch_idx: 21\n",
      "0.7272727272727273\n",
      "batch_idx: 22\n",
      "0.7246376811594203\n",
      "batch_idx: 23\n",
      "0.7274305555555556\n",
      "batch_idx: 24\n",
      "0.73\n",
      "Training Epoch: 420, total loss: 44.215155\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.7152777777777778\n",
      "batch_idx: 6\n",
      "0.7261904761904762\n",
      "batch_idx: 7\n",
      "0.7135416666666666\n",
      "batch_idx: 8\n",
      "0.7175925925925926\n",
      "batch_idx: 9\n",
      "0.7208333333333333\n",
      "batch_idx: 10\n",
      "0.7272727272727273\n",
      "batch_idx: 11\n",
      "0.7256944444444444\n",
      "batch_idx: 12\n",
      "0.7115384615384616\n",
      "batch_idx: 13\n",
      "0.7083333333333334\n",
      "batch_idx: 14\n",
      "0.7166666666666667\n",
      "batch_idx: 15\n",
      "0.7161458333333334\n",
      "batch_idx: 16\n",
      "0.7181372549019608\n",
      "batch_idx: 17\n",
      "0.7199074074074074\n",
      "batch_idx: 18\n",
      "0.7236842105263158\n",
      "batch_idx: 19\n",
      "0.7291666666666666\n",
      "batch_idx: 20\n",
      "0.7261904761904762\n",
      "batch_idx: 21\n",
      "0.7178030303030303\n",
      "batch_idx: 22\n",
      "0.7192028985507246\n",
      "batch_idx: 23\n",
      "0.7100694444444444\n",
      "batch_idx: 24\n",
      "0.7133333333333334\n",
      "Training Epoch: 421, total loss: 44.440481\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7638888888888888\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7689393939393939\n",
      "batch_idx: 11\n",
      "0.7673611111111112\n",
      "batch_idx: 12\n",
      "0.7788461538461539\n",
      "batch_idx: 13\n",
      "0.7738095238095238\n",
      "batch_idx: 14\n",
      "0.7694444444444445\n",
      "batch_idx: 15\n",
      "0.7734375\n",
      "batch_idx: 16\n",
      "0.7671568627450981\n",
      "batch_idx: 17\n",
      "0.7754629629629629\n",
      "batch_idx: 18\n",
      "0.7697368421052632\n",
      "batch_idx: 19\n",
      "0.7708333333333334\n",
      "batch_idx: 20\n",
      "0.7678571428571429\n",
      "batch_idx: 21\n",
      "0.7727272727272727\n",
      "batch_idx: 22\n",
      "0.7717391304347826\n",
      "batch_idx: 23\n",
      "0.7690972222222222\n",
      "batch_idx: 24\n",
      "0.7616666666666667\n",
      "Training Epoch: 422, total loss: 43.638118\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.7673611111111112\n",
      "batch_idx: 12\n",
      "0.7628205128205128\n",
      "batch_idx: 13\n",
      "0.7619047619047619\n",
      "batch_idx: 14\n",
      "0.7555555555555555\n",
      "batch_idx: 15\n",
      "0.7526041666666666\n",
      "batch_idx: 16\n",
      "0.7401960784313726\n",
      "batch_idx: 17\n",
      "0.7337962962962963\n",
      "batch_idx: 18\n",
      "0.7302631578947368\n",
      "batch_idx: 19\n",
      "0.7229166666666667\n",
      "batch_idx: 20\n",
      "0.7242063492063492\n",
      "batch_idx: 21\n",
      "0.7253787878787878\n",
      "batch_idx: 22\n",
      "0.7264492753623188\n",
      "batch_idx: 23\n",
      "0.7274305555555556\n",
      "batch_idx: 24\n",
      "0.72\n",
      "Training Epoch: 423, total loss: 44.315307\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7440476190476191\n",
      "batch_idx: 7\n",
      "0.7447916666666666\n",
      "batch_idx: 8\n",
      "0.7453703703703703\n",
      "batch_idx: 9\n",
      "0.7416666666666667\n",
      "batch_idx: 10\n",
      "0.7310606060606061\n",
      "batch_idx: 11\n",
      "0.7222222222222222\n",
      "batch_idx: 12\n",
      "0.7211538461538461\n",
      "batch_idx: 13\n",
      "0.7232142857142857\n",
      "batch_idx: 14\n",
      "0.7222222222222222\n",
      "batch_idx: 15\n",
      "0.7239583333333334\n",
      "batch_idx: 16\n",
      "0.7156862745098039\n",
      "batch_idx: 17\n",
      "0.7175925925925926\n",
      "batch_idx: 18\n",
      "0.7149122807017544\n",
      "batch_idx: 19\n",
      "0.7125\n",
      "batch_idx: 20\n",
      "0.7083333333333334\n",
      "batch_idx: 21\n",
      "0.7064393939393939\n",
      "batch_idx: 22\n",
      "0.7065217391304348\n",
      "batch_idx: 23\n",
      "0.7048611111111112\n",
      "batch_idx: 24\n",
      "0.7116666666666667\n",
      "Training Epoch: 424, total loss: 44.491086\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6041666666666666\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.675\n",
      "batch_idx: 5\n",
      "0.6875\n",
      "batch_idx: 6\n",
      "0.7083333333333334\n",
      "batch_idx: 7\n",
      "0.6770833333333334\n",
      "batch_idx: 8\n",
      "0.6851851851851852\n",
      "batch_idx: 9\n",
      "0.7\n",
      "batch_idx: 10\n",
      "0.7121212121212122\n",
      "batch_idx: 11\n",
      "0.71875\n",
      "batch_idx: 12\n",
      "0.7147435897435898\n",
      "batch_idx: 13\n",
      "0.7142857142857143\n",
      "batch_idx: 14\n",
      "0.7166666666666667\n",
      "batch_idx: 15\n",
      "0.703125\n",
      "batch_idx: 16\n",
      "0.7034313725490197\n",
      "batch_idx: 17\n",
      "0.7083333333333334\n",
      "batch_idx: 18\n",
      "0.7083333333333334\n",
      "batch_idx: 19\n",
      "0.7041666666666667\n",
      "batch_idx: 20\n",
      "0.7123015873015873\n",
      "batch_idx: 21\n",
      "0.7064393939393939\n",
      "batch_idx: 22\n",
      "0.7137681159420289\n",
      "batch_idx: 23\n",
      "0.71875\n",
      "batch_idx: 24\n",
      "0.71\n",
      "Training Epoch: 425, total loss: 44.423937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.7083333333333334\n",
      "batch_idx: 6\n",
      "0.6964285714285714\n",
      "batch_idx: 7\n",
      "0.7083333333333334\n",
      "batch_idx: 8\n",
      "0.6944444444444444\n",
      "batch_idx: 9\n",
      "0.6916666666666667\n",
      "batch_idx: 10\n",
      "0.6856060606060606\n",
      "batch_idx: 11\n",
      "0.6909722222222222\n",
      "batch_idx: 12\n",
      "0.6923076923076923\n",
      "batch_idx: 13\n",
      "0.7023809523809523\n",
      "batch_idx: 14\n",
      "0.7\n",
      "batch_idx: 15\n",
      "0.6979166666666666\n",
      "batch_idx: 16\n",
      "0.6936274509803921\n",
      "batch_idx: 17\n",
      "0.6921296296296297\n",
      "batch_idx: 18\n",
      "0.6973684210526315\n",
      "batch_idx: 19\n",
      "0.7020833333333333\n",
      "batch_idx: 20\n",
      "0.7063492063492064\n",
      "batch_idx: 21\n",
      "0.7083333333333334\n",
      "batch_idx: 22\n",
      "0.7065217391304348\n",
      "batch_idx: 23\n",
      "0.7065972222222222\n",
      "batch_idx: 24\n",
      "0.7016666666666667\n",
      "Training Epoch: 426, total loss: 44.664761\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7440476190476191\n",
      "batch_idx: 7\n",
      "0.7291666666666666\n",
      "batch_idx: 8\n",
      "0.7453703703703703\n",
      "batch_idx: 9\n",
      "0.7416666666666667\n",
      "batch_idx: 10\n",
      "0.7310606060606061\n",
      "batch_idx: 11\n",
      "0.71875\n",
      "batch_idx: 12\n",
      "0.7147435897435898\n",
      "batch_idx: 13\n",
      "0.7083333333333334\n",
      "batch_idx: 14\n",
      "0.7138888888888889\n",
      "batch_idx: 15\n",
      "0.7213541666666666\n",
      "batch_idx: 16\n",
      "0.7230392156862745\n",
      "batch_idx: 17\n",
      "0.7129629629629629\n",
      "batch_idx: 18\n",
      "0.7149122807017544\n",
      "batch_idx: 19\n",
      "0.7208333333333333\n",
      "batch_idx: 20\n",
      "0.7222222222222222\n",
      "batch_idx: 21\n",
      "0.7234848484848485\n",
      "batch_idx: 22\n",
      "0.730072463768116\n",
      "batch_idx: 23\n",
      "0.7326388888888888\n",
      "batch_idx: 24\n",
      "0.735\n",
      "Training Epoch: 427, total loss: 44.070958\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.625\n",
      "batch_idx: 3\n",
      "0.6145833333333334\n",
      "batch_idx: 4\n",
      "0.6583333333333333\n",
      "batch_idx: 5\n",
      "0.7083333333333334\n",
      "batch_idx: 6\n",
      "0.7083333333333334\n",
      "batch_idx: 7\n",
      "0.7239583333333334\n",
      "batch_idx: 8\n",
      "0.7268518518518519\n",
      "batch_idx: 9\n",
      "0.7208333333333333\n",
      "batch_idx: 10\n",
      "0.7272727272727273\n",
      "batch_idx: 11\n",
      "0.7222222222222222\n",
      "batch_idx: 12\n",
      "0.7147435897435898\n",
      "batch_idx: 13\n",
      "0.7172619047619048\n",
      "batch_idx: 14\n",
      "0.7194444444444444\n",
      "batch_idx: 15\n",
      "0.7213541666666666\n",
      "batch_idx: 16\n",
      "0.7181372549019608\n",
      "batch_idx: 17\n",
      "0.7175925925925926\n",
      "batch_idx: 18\n",
      "0.7171052631578947\n",
      "batch_idx: 19\n",
      "0.7208333333333333\n",
      "batch_idx: 20\n",
      "0.7142857142857143\n",
      "batch_idx: 21\n",
      "0.7083333333333334\n",
      "batch_idx: 22\n",
      "0.7101449275362319\n",
      "batch_idx: 23\n",
      "0.7065972222222222\n",
      "batch_idx: 24\n",
      "0.7116666666666667\n",
      "Training Epoch: 428, total loss: 44.450673\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7731481481481481\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.7692307692307693\n",
      "batch_idx: 13\n",
      "0.7529761904761905\n",
      "batch_idx: 14\n",
      "0.7555555555555555\n",
      "batch_idx: 15\n",
      "0.7526041666666666\n",
      "batch_idx: 16\n",
      "0.7426470588235294\n",
      "batch_idx: 17\n",
      "0.7407407407407407\n",
      "batch_idx: 18\n",
      "0.7324561403508771\n",
      "batch_idx: 19\n",
      "0.7395833333333334\n",
      "batch_idx: 20\n",
      "0.7440476190476191\n",
      "batch_idx: 21\n",
      "0.7443181818181818\n",
      "batch_idx: 22\n",
      "0.7445652173913043\n",
      "batch_idx: 23\n",
      "0.7465277777777778\n",
      "batch_idx: 24\n",
      "0.75\n",
      "Training Epoch: 429, total loss: 43.676944\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7689393939393939\n",
      "batch_idx: 11\n",
      "0.7604166666666666\n",
      "batch_idx: 12\n",
      "0.7628205128205128\n",
      "batch_idx: 13\n",
      "0.7708333333333334\n",
      "batch_idx: 14\n",
      "0.7722222222222223\n",
      "batch_idx: 15\n",
      "0.7682291666666666\n",
      "batch_idx: 16\n",
      "0.7622549019607843\n",
      "batch_idx: 17\n",
      "0.7662037037037037\n",
      "batch_idx: 18\n",
      "0.7609649122807017\n",
      "batch_idx: 19\n",
      "0.7625\n",
      "batch_idx: 20\n",
      "0.7698412698412699\n",
      "batch_idx: 21\n",
      "0.7670454545454546\n",
      "batch_idx: 22\n",
      "0.7681159420289855\n",
      "batch_idx: 23\n",
      "0.7673611111111112\n",
      "batch_idx: 24\n",
      "0.7616666666666667\n",
      "Training Epoch: 430, total loss: 43.722603\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.7083333333333334\n",
      "batch_idx: 6\n",
      "0.7202380952380952\n",
      "batch_idx: 7\n",
      "0.7239583333333334\n",
      "batch_idx: 8\n",
      "0.7268518518518519\n",
      "batch_idx: 9\n",
      "0.7208333333333333\n",
      "batch_idx: 10\n",
      "0.7272727272727273\n",
      "batch_idx: 11\n",
      "0.7291666666666666\n",
      "batch_idx: 12\n",
      "0.7307692307692307\n",
      "batch_idx: 13\n",
      "0.7291666666666666\n",
      "batch_idx: 14\n",
      "0.7333333333333333\n",
      "batch_idx: 15\n",
      "0.734375\n",
      "batch_idx: 16\n",
      "0.7377450980392157\n",
      "batch_idx: 17\n",
      "0.7268518518518519\n",
      "batch_idx: 18\n",
      "0.7149122807017544\n",
      "batch_idx: 19\n",
      "0.7125\n",
      "batch_idx: 20\n",
      "0.7063492063492064\n",
      "batch_idx: 21\n",
      "0.7102272727272727\n",
      "batch_idx: 22\n",
      "0.7047101449275363\n",
      "batch_idx: 23\n",
      "0.7100694444444444\n",
      "batch_idx: 24\n",
      "0.7183333333333334\n",
      "Training Epoch: 431, total loss: 44.501451\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.7440476190476191\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.75\n",
      "batch_idx: 9\n",
      "0.7375\n",
      "batch_idx: 10\n",
      "0.7386363636363636\n",
      "batch_idx: 11\n",
      "0.7361111111111112\n",
      "batch_idx: 12\n",
      "0.7435897435897436\n",
      "batch_idx: 13\n",
      "0.7321428571428571\n",
      "batch_idx: 14\n",
      "0.7277777777777777\n",
      "batch_idx: 15\n",
      "0.7265625\n",
      "batch_idx: 16\n",
      "0.7328431372549019\n",
      "batch_idx: 17\n",
      "0.7384259259259259\n",
      "batch_idx: 18\n",
      "0.7324561403508771\n",
      "batch_idx: 19\n",
      "0.725\n",
      "batch_idx: 20\n",
      "0.7281746031746031\n",
      "batch_idx: 21\n",
      "0.7329545454545454\n",
      "batch_idx: 22\n",
      "0.7427536231884058\n",
      "batch_idx: 23\n",
      "0.7447916666666666\n",
      "batch_idx: 24\n",
      "0.7466666666666667\n",
      "Training Epoch: 432, total loss: 43.757723\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.7\n",
      "batch_idx: 5\n",
      "0.6666666666666666\n",
      "batch_idx: 6\n",
      "0.6726190476190477\n",
      "batch_idx: 7\n",
      "0.6875\n",
      "batch_idx: 8\n",
      "0.6990740740740741\n",
      "batch_idx: 9\n",
      "0.7\n",
      "batch_idx: 10\n",
      "0.7045454545454546\n",
      "batch_idx: 11\n",
      "0.7118055555555556\n",
      "batch_idx: 12\n",
      "0.7147435897435898\n",
      "batch_idx: 13\n",
      "0.7291666666666666\n",
      "batch_idx: 14\n",
      "0.7222222222222222\n",
      "batch_idx: 15\n",
      "0.7239583333333334\n",
      "batch_idx: 16\n",
      "0.7279411764705882\n",
      "batch_idx: 17\n",
      "0.7361111111111112\n",
      "batch_idx: 18\n",
      "0.7412280701754386\n",
      "batch_idx: 19\n",
      "0.7416666666666667\n",
      "batch_idx: 20\n",
      "0.7440476190476191\n",
      "batch_idx: 21\n",
      "0.7443181818181818\n",
      "batch_idx: 22\n",
      "0.7445652173913043\n",
      "batch_idx: 23\n",
      "0.7395833333333334\n",
      "batch_idx: 24\n",
      "0.74\n",
      "Training Epoch: 433, total loss: 43.918205\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8257575757575758\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8365384615384616\n",
      "batch_idx: 13\n",
      "0.8154761904761905\n",
      "batch_idx: 14\n",
      "0.8\n",
      "batch_idx: 15\n",
      "0.8020833333333334\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.7916666666666666\n",
      "batch_idx: 18\n",
      "0.793859649122807\n",
      "batch_idx: 19\n",
      "0.7895833333333333\n",
      "batch_idx: 20\n",
      "0.7837301587301587\n",
      "batch_idx: 21\n",
      "0.7821969696969697\n",
      "batch_idx: 22\n",
      "0.7807971014492754\n",
      "batch_idx: 23\n",
      "0.7760416666666666\n",
      "batch_idx: 24\n",
      "0.7733333333333333\n",
      "Training Epoch: 434, total loss: 43.340301\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7559523809523809\n",
      "batch_idx: 7\n",
      "0.7552083333333334\n",
      "batch_idx: 8\n",
      "0.75\n",
      "batch_idx: 9\n",
      "0.7333333333333333\n",
      "batch_idx: 10\n",
      "0.7348484848484849\n",
      "batch_idx: 11\n",
      "0.7430555555555556\n",
      "batch_idx: 12\n",
      "0.7403846153846154\n",
      "batch_idx: 13\n",
      "0.7410714285714286\n",
      "batch_idx: 14\n",
      "0.7472222222222222\n",
      "batch_idx: 15\n",
      "0.7369791666666666\n",
      "batch_idx: 16\n",
      "0.7328431372549019\n",
      "batch_idx: 17\n",
      "0.7268518518518519\n",
      "batch_idx: 18\n",
      "0.7258771929824561\n",
      "batch_idx: 19\n",
      "0.71875\n",
      "batch_idx: 20\n",
      "0.7182539682539683\n",
      "batch_idx: 21\n",
      "0.7121212121212122\n",
      "batch_idx: 22\n",
      "0.7155797101449275\n",
      "batch_idx: 23\n",
      "0.71875\n",
      "batch_idx: 24\n",
      "0.7183333333333334\n",
      "Training Epoch: 435, total loss: 44.478928\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7559523809523809\n",
      "batch_idx: 7\n",
      "0.7552083333333334\n",
      "batch_idx: 8\n",
      "0.75\n",
      "batch_idx: 9\n",
      "0.7625\n",
      "batch_idx: 10\n",
      "0.7765151515151515\n",
      "batch_idx: 11\n",
      "0.7743055555555556\n",
      "batch_idx: 12\n",
      "0.7660256410256411\n",
      "batch_idx: 13\n",
      "0.7559523809523809\n",
      "batch_idx: 14\n",
      "0.75\n",
      "batch_idx: 15\n",
      "0.7447916666666666\n",
      "batch_idx: 16\n",
      "0.7426470588235294\n",
      "batch_idx: 17\n",
      "0.7430555555555556\n",
      "batch_idx: 18\n",
      "0.7478070175438597\n",
      "batch_idx: 19\n",
      "0.75\n",
      "batch_idx: 20\n",
      "0.748015873015873\n",
      "batch_idx: 21\n",
      "0.740530303030303\n",
      "batch_idx: 22\n",
      "0.7427536231884058\n",
      "batch_idx: 23\n",
      "0.7482638888888888\n",
      "batch_idx: 24\n",
      "0.7433333333333333\n",
      "Training Epoch: 436, total loss: 43.953469\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7395833333333334\n",
      "batch_idx: 8\n",
      "0.7592592592592593\n",
      "batch_idx: 9\n",
      "0.75\n",
      "batch_idx: 10\n",
      "0.75\n",
      "batch_idx: 11\n",
      "0.75\n",
      "batch_idx: 12\n",
      "0.7435897435897436\n",
      "batch_idx: 13\n",
      "0.7410714285714286\n",
      "batch_idx: 14\n",
      "0.7388888888888889\n",
      "batch_idx: 15\n",
      "0.7291666666666666\n",
      "batch_idx: 16\n",
      "0.7328431372549019\n",
      "batch_idx: 17\n",
      "0.7384259259259259\n",
      "batch_idx: 18\n",
      "0.7390350877192983\n",
      "batch_idx: 19\n",
      "0.74375\n",
      "batch_idx: 20\n",
      "0.7440476190476191\n",
      "batch_idx: 21\n",
      "0.7424242424242424\n",
      "batch_idx: 22\n",
      "0.7391304347826086\n",
      "batch_idx: 23\n",
      "0.734375\n",
      "batch_idx: 24\n",
      "0.735\n",
      "Training Epoch: 437, total loss: 44.040343\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7552083333333334\n",
      "batch_idx: 8\n",
      "0.7546296296296297\n",
      "batch_idx: 9\n",
      "0.7541666666666667\n",
      "batch_idx: 10\n",
      "0.7651515151515151\n",
      "batch_idx: 11\n",
      "0.7743055555555556\n",
      "batch_idx: 12\n",
      "0.7660256410256411\n",
      "batch_idx: 13\n",
      "0.7708333333333334\n",
      "batch_idx: 14\n",
      "0.7722222222222223\n",
      "batch_idx: 15\n",
      "0.7734375\n",
      "batch_idx: 16\n",
      "0.7573529411764706\n",
      "batch_idx: 17\n",
      "0.7615740740740741\n",
      "batch_idx: 18\n",
      "0.756578947368421\n",
      "batch_idx: 19\n",
      "0.7604166666666666\n",
      "batch_idx: 20\n",
      "0.7579365079365079\n",
      "batch_idx: 21\n",
      "0.7632575757575758\n",
      "batch_idx: 22\n",
      "0.7663043478260869\n",
      "batch_idx: 23\n",
      "0.7690972222222222\n",
      "batch_idx: 24\n",
      "0.77\n",
      "Training Epoch: 438, total loss: 43.510843\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7453703703703703\n",
      "batch_idx: 9\n",
      "0.7458333333333333\n",
      "batch_idx: 10\n",
      "0.7537878787878788\n",
      "batch_idx: 11\n",
      "0.7569444444444444\n",
      "batch_idx: 12\n",
      "0.7532051282051282\n",
      "batch_idx: 13\n",
      "0.75\n",
      "batch_idx: 14\n",
      "0.7472222222222222\n",
      "batch_idx: 15\n",
      "0.7447916666666666\n",
      "batch_idx: 16\n",
      "0.7426470588235294\n",
      "batch_idx: 17\n",
      "0.7407407407407407\n",
      "batch_idx: 18\n",
      "0.7324561403508771\n",
      "batch_idx: 19\n",
      "0.73125\n",
      "batch_idx: 20\n",
      "0.7341269841269841\n",
      "batch_idx: 21\n",
      "0.7329545454545454\n",
      "batch_idx: 22\n",
      "0.7282608695652174\n",
      "batch_idx: 23\n",
      "0.7222222222222222\n",
      "batch_idx: 24\n",
      "0.7283333333333334\n",
      "Training Epoch: 439, total loss: 43.986896\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7291666666666666\n",
      "batch_idx: 8\n",
      "0.7175925925925926\n",
      "batch_idx: 9\n",
      "0.7291666666666666\n",
      "batch_idx: 10\n",
      "0.7386363636363636\n",
      "batch_idx: 11\n",
      "0.7395833333333334\n",
      "batch_idx: 12\n",
      "0.75\n",
      "batch_idx: 13\n",
      "0.75\n",
      "batch_idx: 14\n",
      "0.7472222222222222\n",
      "batch_idx: 15\n",
      "0.7473958333333334\n",
      "batch_idx: 16\n",
      "0.7475490196078431\n",
      "batch_idx: 17\n",
      "0.75\n",
      "batch_idx: 18\n",
      "0.75\n",
      "batch_idx: 19\n",
      "0.7520833333333333\n",
      "batch_idx: 20\n",
      "0.748015873015873\n",
      "batch_idx: 21\n",
      "0.75\n",
      "batch_idx: 22\n",
      "0.7518115942028986\n",
      "batch_idx: 23\n",
      "0.7552083333333334\n",
      "batch_idx: 24\n",
      "0.755\n",
      "Training Epoch: 440, total loss: 43.648236\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.75\n",
      "batch_idx: 9\n",
      "0.7583333333333333\n",
      "batch_idx: 10\n",
      "0.7689393939393939\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.7660256410256411\n",
      "batch_idx: 13\n",
      "0.7589285714285714\n",
      "batch_idx: 14\n",
      "0.7611111111111111\n",
      "batch_idx: 15\n",
      "0.75\n",
      "batch_idx: 16\n",
      "0.7549019607843137\n",
      "batch_idx: 17\n",
      "0.75\n",
      "batch_idx: 18\n",
      "0.743421052631579\n",
      "batch_idx: 19\n",
      "0.7520833333333333\n",
      "batch_idx: 20\n",
      "0.7400793650793651\n",
      "batch_idx: 21\n",
      "0.7462121212121212\n",
      "batch_idx: 22\n",
      "0.75\n",
      "batch_idx: 23\n",
      "0.7517361111111112\n",
      "batch_idx: 24\n",
      "0.7533333333333333\n",
      "Training Epoch: 441, total loss: 43.602763\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.7546296296296297\n",
      "batch_idx: 9\n",
      "0.75\n",
      "batch_idx: 10\n",
      "0.7310606060606061\n",
      "batch_idx: 11\n",
      "0.7291666666666666\n",
      "batch_idx: 12\n",
      "0.7211538461538461\n",
      "batch_idx: 13\n",
      "0.7261904761904762\n",
      "batch_idx: 14\n",
      "0.7333333333333333\n",
      "batch_idx: 15\n",
      "0.7395833333333334\n",
      "batch_idx: 16\n",
      "0.7401960784313726\n",
      "batch_idx: 17\n",
      "0.7476851851851852\n",
      "batch_idx: 18\n",
      "0.75\n",
      "batch_idx: 19\n",
      "0.7479166666666667\n",
      "batch_idx: 20\n",
      "0.75\n",
      "batch_idx: 21\n",
      "0.75\n",
      "batch_idx: 22\n",
      "0.75\n",
      "batch_idx: 23\n",
      "0.7447916666666666\n",
      "batch_idx: 24\n",
      "0.7416666666666667\n",
      "Training Epoch: 442, total loss: 43.860821\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.7361111111111112\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.7361111111111112\n",
      "batch_idx: 9\n",
      "0.7375\n",
      "batch_idx: 10\n",
      "0.75\n",
      "batch_idx: 11\n",
      "0.7430555555555556\n",
      "batch_idx: 12\n",
      "0.7371794871794872\n",
      "batch_idx: 13\n",
      "0.7380952380952381\n",
      "batch_idx: 14\n",
      "0.7444444444444445\n",
      "batch_idx: 15\n",
      "0.7395833333333334\n",
      "batch_idx: 16\n",
      "0.7401960784313726\n",
      "batch_idx: 17\n",
      "0.7384259259259259\n",
      "batch_idx: 18\n",
      "0.7368421052631579\n",
      "batch_idx: 19\n",
      "0.7375\n",
      "batch_idx: 20\n",
      "0.7380952380952381\n",
      "batch_idx: 21\n",
      "0.7443181818181818\n",
      "batch_idx: 22\n",
      "0.75\n",
      "batch_idx: 23\n",
      "0.7465277777777778\n",
      "batch_idx: 24\n",
      "0.7516666666666667\n",
      "Training Epoch: 443, total loss: 43.768595\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7559523809523809\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.7407407407407407\n",
      "batch_idx: 9\n",
      "0.7458333333333333\n",
      "batch_idx: 10\n",
      "0.7575757575757576\n",
      "batch_idx: 11\n",
      "0.7604166666666666\n",
      "batch_idx: 12\n",
      "0.7596153846153846\n",
      "batch_idx: 13\n",
      "0.7619047619047619\n",
      "batch_idx: 14\n",
      "0.7638888888888888\n",
      "batch_idx: 15\n",
      "0.7682291666666666\n",
      "batch_idx: 16\n",
      "0.7598039215686274\n",
      "batch_idx: 17\n",
      "0.7569444444444444\n",
      "batch_idx: 18\n",
      "0.75\n",
      "batch_idx: 19\n",
      "0.75\n",
      "batch_idx: 20\n",
      "0.7440476190476191\n",
      "batch_idx: 21\n",
      "0.7348484848484849\n",
      "batch_idx: 22\n",
      "0.7282608695652174\n",
      "batch_idx: 23\n",
      "0.7256944444444444\n",
      "batch_idx: 24\n",
      "0.7166666666666667\n",
      "Training Epoch: 444, total loss: 44.458747\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7731481481481481\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7765151515151515\n",
      "batch_idx: 11\n",
      "0.7777777777777778\n",
      "batch_idx: 12\n",
      "0.7788461538461539\n",
      "batch_idx: 13\n",
      "0.7827380952380952\n",
      "batch_idx: 14\n",
      "0.7888888888888889\n",
      "batch_idx: 15\n",
      "0.7916666666666666\n",
      "batch_idx: 16\n",
      "0.7916666666666666\n",
      "batch_idx: 17\n",
      "0.7847222222222222\n",
      "batch_idx: 18\n",
      "0.7828947368421053\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7757936507936508\n",
      "batch_idx: 21\n",
      "0.7632575757575758\n",
      "batch_idx: 22\n",
      "0.7554347826086957\n",
      "batch_idx: 23\n",
      "0.7586805555555556\n",
      "batch_idx: 24\n",
      "0.7566666666666667\n",
      "Training Epoch: 445, total loss: 43.605542\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7559523809523809\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.7625\n",
      "batch_idx: 10\n",
      "0.7575757575757576\n",
      "batch_idx: 11\n",
      "0.7638888888888888\n",
      "batch_idx: 12\n",
      "0.7756410256410257\n",
      "batch_idx: 13\n",
      "0.7797619047619048\n",
      "batch_idx: 14\n",
      "0.7888888888888889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 15\n",
      "0.78125\n",
      "batch_idx: 16\n",
      "0.7769607843137255\n",
      "batch_idx: 17\n",
      "0.7777777777777778\n",
      "batch_idx: 18\n",
      "0.7785087719298246\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7757936507936508\n",
      "batch_idx: 21\n",
      "0.7765151515151515\n",
      "batch_idx: 22\n",
      "0.7663043478260869\n",
      "batch_idx: 23\n",
      "0.7673611111111112\n",
      "batch_idx: 24\n",
      "0.7666666666666667\n",
      "Training Epoch: 446, total loss: 43.479323\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.7\n",
      "batch_idx: 5\n",
      "0.7083333333333334\n",
      "batch_idx: 6\n",
      "0.7023809523809523\n",
      "batch_idx: 7\n",
      "0.6927083333333334\n",
      "batch_idx: 8\n",
      "0.6990740740740741\n",
      "batch_idx: 9\n",
      "0.7041666666666667\n",
      "batch_idx: 10\n",
      "0.7121212121212122\n",
      "batch_idx: 11\n",
      "0.7222222222222222\n",
      "batch_idx: 12\n",
      "0.7243589743589743\n",
      "batch_idx: 13\n",
      "0.7261904761904762\n",
      "batch_idx: 14\n",
      "0.7222222222222222\n",
      "batch_idx: 15\n",
      "0.7265625\n",
      "batch_idx: 16\n",
      "0.7156862745098039\n",
      "batch_idx: 17\n",
      "0.7175925925925926\n",
      "batch_idx: 18\n",
      "0.7214912280701754\n",
      "batch_idx: 19\n",
      "0.725\n",
      "batch_idx: 20\n",
      "0.7281746031746031\n",
      "batch_idx: 21\n",
      "0.7310606060606061\n",
      "batch_idx: 22\n",
      "0.730072463768116\n",
      "batch_idx: 23\n",
      "0.7291666666666666\n",
      "batch_idx: 24\n",
      "0.725\n",
      "Training Epoch: 447, total loss: 44.310454\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.7666666666666667\n",
      "batch_idx: 10\n",
      "0.7651515151515151\n",
      "batch_idx: 11\n",
      "0.7638888888888888\n",
      "batch_idx: 12\n",
      "0.7628205128205128\n",
      "batch_idx: 13\n",
      "0.7529761904761905\n",
      "batch_idx: 14\n",
      "0.7444444444444445\n",
      "batch_idx: 15\n",
      "0.7421875\n",
      "batch_idx: 16\n",
      "0.7450980392156863\n",
      "batch_idx: 17\n",
      "0.7476851851851852\n",
      "batch_idx: 18\n",
      "0.743421052631579\n",
      "batch_idx: 19\n",
      "0.7479166666666667\n",
      "batch_idx: 20\n",
      "0.746031746031746\n",
      "batch_idx: 21\n",
      "0.75\n",
      "batch_idx: 22\n",
      "0.7554347826086957\n",
      "batch_idx: 23\n",
      "0.7534722222222222\n",
      "batch_idx: 24\n",
      "0.7533333333333333\n",
      "Training Epoch: 448, total loss: 43.766466\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7175925925925926\n",
      "batch_idx: 9\n",
      "0.7416666666666667\n",
      "batch_idx: 10\n",
      "0.7310606060606061\n",
      "batch_idx: 11\n",
      "0.7291666666666666\n",
      "batch_idx: 12\n",
      "0.7339743589743589\n",
      "batch_idx: 13\n",
      "0.7410714285714286\n",
      "batch_idx: 14\n",
      "0.7416666666666667\n",
      "batch_idx: 15\n",
      "0.75\n",
      "batch_idx: 16\n",
      "0.7524509803921569\n",
      "batch_idx: 17\n",
      "0.7638888888888888\n",
      "batch_idx: 18\n",
      "0.7609649122807017\n",
      "batch_idx: 19\n",
      "0.7604166666666666\n",
      "batch_idx: 20\n",
      "0.7658730158730159\n",
      "batch_idx: 21\n",
      "0.7708333333333334\n",
      "batch_idx: 22\n",
      "0.769927536231884\n",
      "batch_idx: 23\n",
      "0.7673611111111112\n",
      "batch_idx: 24\n",
      "0.7583333333333333\n",
      "Training Epoch: 449, total loss: 43.749786\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.7546296296296297\n",
      "batch_idx: 9\n",
      "0.7625\n",
      "batch_idx: 10\n",
      "0.7727272727272727\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.782051282051282\n",
      "batch_idx: 13\n",
      "0.7678571428571429\n",
      "batch_idx: 14\n",
      "0.775\n",
      "batch_idx: 15\n",
      "0.7734375\n",
      "batch_idx: 16\n",
      "0.7647058823529411\n",
      "batch_idx: 17\n",
      "0.7685185185185185\n",
      "batch_idx: 18\n",
      "0.7631578947368421\n",
      "batch_idx: 19\n",
      "0.7541666666666667\n",
      "batch_idx: 20\n",
      "0.75\n",
      "batch_idx: 21\n",
      "0.7481060606060606\n",
      "batch_idx: 22\n",
      "0.7554347826086957\n",
      "batch_idx: 23\n",
      "0.7482638888888888\n",
      "batch_idx: 24\n",
      "0.75\n",
      "Training Epoch: 450, total loss: 43.828969\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.7731481481481481\n",
      "batch_idx: 9\n",
      "0.7666666666666667\n",
      "batch_idx: 10\n",
      "0.7689393939393939\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.7467948717948718\n",
      "batch_idx: 13\n",
      "0.7559523809523809\n",
      "batch_idx: 14\n",
      "0.7472222222222222\n",
      "batch_idx: 15\n",
      "0.7369791666666666\n",
      "batch_idx: 16\n",
      "0.7401960784313726\n",
      "batch_idx: 17\n",
      "0.7407407407407407\n",
      "batch_idx: 18\n",
      "0.743421052631579\n",
      "batch_idx: 19\n",
      "0.7458333333333333\n",
      "batch_idx: 20\n",
      "0.748015873015873\n",
      "batch_idx: 21\n",
      "0.75\n",
      "batch_idx: 22\n",
      "0.75\n",
      "batch_idx: 23\n",
      "0.7534722222222222\n",
      "batch_idx: 24\n",
      "0.7566666666666667\n",
      "Training Epoch: 451, total loss: 43.536762\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.6805555555555556\n",
      "batch_idx: 6\n",
      "0.7023809523809523\n",
      "batch_idx: 7\n",
      "0.7135416666666666\n",
      "batch_idx: 8\n",
      "0.7175925925925926\n",
      "batch_idx: 9\n",
      "0.725\n",
      "batch_idx: 10\n",
      "0.7310606060606061\n",
      "batch_idx: 11\n",
      "0.7430555555555556\n",
      "batch_idx: 12\n",
      "0.7467948717948718\n",
      "batch_idx: 13\n",
      "0.75\n",
      "batch_idx: 14\n",
      "0.7527777777777778\n",
      "batch_idx: 15\n",
      "0.7447916666666666\n",
      "batch_idx: 16\n",
      "0.7450980392156863\n",
      "batch_idx: 17\n",
      "0.75\n",
      "batch_idx: 18\n",
      "0.7456140350877193\n",
      "batch_idx: 19\n",
      "0.7458333333333333\n",
      "batch_idx: 20\n",
      "0.751984126984127\n",
      "batch_idx: 21\n",
      "0.7462121212121212\n",
      "batch_idx: 22\n",
      "0.7409420289855072\n",
      "batch_idx: 23\n",
      "0.7430555555555556\n",
      "batch_idx: 24\n",
      "0.7483333333333333\n",
      "Training Epoch: 452, total loss: 43.842625\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.7546296296296297\n",
      "batch_idx: 9\n",
      "0.7541666666666667\n",
      "batch_idx: 10\n",
      "0.7424242424242424\n",
      "batch_idx: 11\n",
      "0.7395833333333334\n",
      "batch_idx: 12\n",
      "0.7435897435897436\n",
      "batch_idx: 13\n",
      "0.7440476190476191\n",
      "batch_idx: 14\n",
      "0.7444444444444445\n",
      "batch_idx: 15\n",
      "0.7447916666666666\n",
      "batch_idx: 16\n",
      "0.7401960784313726\n",
      "batch_idx: 17\n",
      "0.7546296296296297\n",
      "batch_idx: 18\n",
      "0.7543859649122807\n",
      "batch_idx: 19\n",
      "0.75625\n",
      "batch_idx: 20\n",
      "0.7559523809523809\n",
      "batch_idx: 21\n",
      "0.7556818181818182\n",
      "batch_idx: 22\n",
      "0.7572463768115942\n",
      "batch_idx: 23\n",
      "0.7621527777777778\n",
      "batch_idx: 24\n",
      "0.7583333333333333\n",
      "Training Epoch: 453, total loss: 43.582847\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7361111111111112\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.7395833333333334\n",
      "batch_idx: 8\n",
      "0.75\n",
      "batch_idx: 9\n",
      "0.75\n",
      "batch_idx: 10\n",
      "0.7575757575757576\n",
      "batch_idx: 11\n",
      "0.7604166666666666\n",
      "batch_idx: 12\n",
      "0.7596153846153846\n",
      "batch_idx: 13\n",
      "0.7559523809523809\n",
      "batch_idx: 14\n",
      "0.7555555555555555\n",
      "batch_idx: 15\n",
      "0.7552083333333334\n",
      "batch_idx: 16\n",
      "0.7524509803921569\n",
      "batch_idx: 17\n",
      "0.75\n",
      "batch_idx: 18\n",
      "0.7478070175438597\n",
      "batch_idx: 19\n",
      "0.7479166666666667\n",
      "batch_idx: 20\n",
      "0.753968253968254\n",
      "batch_idx: 21\n",
      "0.7575757575757576\n",
      "batch_idx: 22\n",
      "0.7554347826086957\n",
      "batch_idx: 23\n",
      "0.7517361111111112\n",
      "batch_idx: 24\n",
      "0.7516666666666667\n",
      "Training Epoch: 454, total loss: 43.621099\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.7083333333333334\n",
      "batch_idx: 6\n",
      "0.7202380952380952\n",
      "batch_idx: 7\n",
      "0.7447916666666666\n",
      "batch_idx: 8\n",
      "0.7453703703703703\n",
      "batch_idx: 9\n",
      "0.7458333333333333\n",
      "batch_idx: 10\n",
      "0.7613636363636364\n",
      "batch_idx: 11\n",
      "0.7604166666666666\n",
      "batch_idx: 12\n",
      "0.7628205128205128\n",
      "batch_idx: 13\n",
      "0.7619047619047619\n",
      "batch_idx: 14\n",
      "0.7666666666666667\n",
      "batch_idx: 15\n",
      "0.7708333333333334\n",
      "batch_idx: 16\n",
      "0.7622549019607843\n",
      "batch_idx: 17\n",
      "0.7546296296296297\n",
      "batch_idx: 18\n",
      "0.7543859649122807\n",
      "batch_idx: 19\n",
      "0.7541666666666667\n",
      "batch_idx: 20\n",
      "0.751984126984127\n",
      "batch_idx: 21\n",
      "0.7462121212121212\n",
      "batch_idx: 22\n",
      "0.7536231884057971\n",
      "batch_idx: 23\n",
      "0.7447916666666666\n",
      "batch_idx: 24\n",
      "0.7433333333333333\n",
      "Training Epoch: 455, total loss: 43.837023\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.725\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7559523809523809\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.7592592592592593\n",
      "batch_idx: 9\n",
      "0.75\n",
      "batch_idx: 10\n",
      "0.7537878787878788\n",
      "batch_idx: 11\n",
      "0.75\n",
      "batch_idx: 12\n",
      "0.7532051282051282\n",
      "batch_idx: 13\n",
      "0.7619047619047619\n",
      "batch_idx: 14\n",
      "0.7638888888888888\n",
      "batch_idx: 15\n",
      "0.7682291666666666\n",
      "batch_idx: 16\n",
      "0.7696078431372549\n",
      "batch_idx: 17\n",
      "0.7708333333333334\n",
      "batch_idx: 18\n",
      "0.7653508771929824\n",
      "batch_idx: 19\n",
      "0.7666666666666667\n",
      "batch_idx: 20\n",
      "0.7658730158730159\n",
      "batch_idx: 21\n",
      "0.7727272727272727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 22\n",
      "0.7735507246376812\n",
      "batch_idx: 23\n",
      "0.7743055555555556\n",
      "batch_idx: 24\n",
      "0.77\n",
      "Training Epoch: 456, total loss: 43.431475\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7407407407407407\n",
      "batch_idx: 9\n",
      "0.725\n",
      "batch_idx: 10\n",
      "0.7310606060606061\n",
      "batch_idx: 11\n",
      "0.7222222222222222\n",
      "batch_idx: 12\n",
      "0.7243589743589743\n",
      "batch_idx: 13\n",
      "0.7232142857142857\n",
      "batch_idx: 14\n",
      "0.7222222222222222\n",
      "batch_idx: 15\n",
      "0.7213541666666666\n",
      "batch_idx: 16\n",
      "0.7279411764705882\n",
      "batch_idx: 17\n",
      "0.7314814814814815\n",
      "batch_idx: 18\n",
      "0.7368421052631579\n",
      "batch_idx: 19\n",
      "0.7416666666666667\n",
      "batch_idx: 20\n",
      "0.7400793650793651\n",
      "batch_idx: 21\n",
      "0.7386363636363636\n",
      "batch_idx: 22\n",
      "0.7391304347826086\n",
      "batch_idx: 23\n",
      "0.7395833333333334\n",
      "batch_idx: 24\n",
      "0.7416666666666667\n",
      "Training Epoch: 457, total loss: 43.943135\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7546296296296297\n",
      "batch_idx: 9\n",
      "0.7416666666666667\n",
      "batch_idx: 10\n",
      "0.7424242424242424\n",
      "batch_idx: 11\n",
      "0.75\n",
      "batch_idx: 12\n",
      "0.7660256410256411\n",
      "batch_idx: 13\n",
      "0.7767857142857143\n",
      "batch_idx: 14\n",
      "0.7722222222222223\n",
      "batch_idx: 15\n",
      "0.7734375\n",
      "batch_idx: 16\n",
      "0.7794117647058824\n",
      "batch_idx: 17\n",
      "0.7847222222222222\n",
      "batch_idx: 18\n",
      "0.7785087719298246\n",
      "batch_idx: 19\n",
      "0.7791666666666667\n",
      "batch_idx: 20\n",
      "0.7718253968253969\n",
      "batch_idx: 21\n",
      "0.7670454545454546\n",
      "batch_idx: 22\n",
      "0.7590579710144928\n",
      "batch_idx: 23\n",
      "0.7517361111111112\n",
      "batch_idx: 24\n",
      "0.7483333333333333\n",
      "Training Epoch: 458, total loss: 43.787680\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7261904761904762\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7361111111111112\n",
      "batch_idx: 9\n",
      "0.7375\n",
      "batch_idx: 10\n",
      "0.7462121212121212\n",
      "batch_idx: 11\n",
      "0.75\n",
      "batch_idx: 12\n",
      "0.75\n",
      "batch_idx: 13\n",
      "0.7440476190476191\n",
      "batch_idx: 14\n",
      "0.7472222222222222\n",
      "batch_idx: 15\n",
      "0.734375\n",
      "batch_idx: 16\n",
      "0.7401960784313726\n",
      "batch_idx: 17\n",
      "0.7384259259259259\n",
      "batch_idx: 18\n",
      "0.7346491228070176\n",
      "batch_idx: 19\n",
      "0.7270833333333333\n",
      "batch_idx: 20\n",
      "0.7301587301587301\n",
      "batch_idx: 21\n",
      "0.7329545454545454\n",
      "batch_idx: 22\n",
      "0.7355072463768116\n",
      "batch_idx: 23\n",
      "0.7447916666666666\n",
      "batch_idx: 24\n",
      "0.745\n",
      "Training Epoch: 459, total loss: 43.878459\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.7559523809523809\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.7592592592592593\n",
      "batch_idx: 9\n",
      "0.7541666666666667\n",
      "batch_idx: 10\n",
      "0.7310606060606061\n",
      "batch_idx: 11\n",
      "0.7361111111111112\n",
      "batch_idx: 12\n",
      "0.7403846153846154\n",
      "batch_idx: 13\n",
      "0.7410714285714286\n",
      "batch_idx: 14\n",
      "0.75\n",
      "batch_idx: 15\n",
      "0.7604166666666666\n",
      "batch_idx: 16\n",
      "0.7647058823529411\n",
      "batch_idx: 17\n",
      "0.7638888888888888\n",
      "batch_idx: 18\n",
      "0.7587719298245614\n",
      "batch_idx: 19\n",
      "0.76875\n",
      "batch_idx: 20\n",
      "0.7619047619047619\n",
      "batch_idx: 21\n",
      "0.759469696969697\n",
      "batch_idx: 22\n",
      "0.7644927536231884\n",
      "batch_idx: 23\n",
      "0.7604166666666666\n",
      "batch_idx: 24\n",
      "0.755\n",
      "Training Epoch: 460, total loss: 43.501672\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.7916666666666666\n",
      "batch_idx: 13\n",
      "0.7827380952380952\n",
      "batch_idx: 14\n",
      "0.7805555555555556\n",
      "batch_idx: 15\n",
      "0.7708333333333334\n",
      "batch_idx: 16\n",
      "0.7549019607843137\n",
      "batch_idx: 17\n",
      "0.7592592592592593\n",
      "batch_idx: 18\n",
      "0.7587719298245614\n",
      "batch_idx: 19\n",
      "0.7604166666666666\n",
      "batch_idx: 20\n",
      "0.7638888888888888\n",
      "batch_idx: 21\n",
      "0.759469696969697\n",
      "batch_idx: 22\n",
      "0.7608695652173914\n",
      "batch_idx: 23\n",
      "0.7621527777777778\n",
      "batch_idx: 24\n",
      "0.7633333333333333\n",
      "Training Epoch: 461, total loss: 43.552860\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.8063725490196079\n",
      "batch_idx: 17\n",
      "0.8055555555555556\n",
      "batch_idx: 18\n",
      "0.8004385964912281\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.7936507936507936\n",
      "batch_idx: 21\n",
      "0.7973484848484849\n",
      "batch_idx: 22\n",
      "0.7952898550724637\n",
      "batch_idx: 23\n",
      "0.7916666666666666\n",
      "batch_idx: 24\n",
      "0.7916666666666666\n",
      "Training Epoch: 462, total loss: 43.161586\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.6666666666666666\n",
      "batch_idx: 5\n",
      "0.7083333333333334\n",
      "batch_idx: 6\n",
      "0.7202380952380952\n",
      "batch_idx: 7\n",
      "0.7083333333333334\n",
      "batch_idx: 8\n",
      "0.7175925925925926\n",
      "batch_idx: 9\n",
      "0.7333333333333333\n",
      "batch_idx: 10\n",
      "0.7348484848484849\n",
      "batch_idx: 11\n",
      "0.7291666666666666\n",
      "batch_idx: 12\n",
      "0.7339743589743589\n",
      "batch_idx: 13\n",
      "0.7380952380952381\n",
      "batch_idx: 14\n",
      "0.7444444444444445\n",
      "batch_idx: 15\n",
      "0.7473958333333334\n",
      "batch_idx: 16\n",
      "0.7549019607843137\n",
      "batch_idx: 17\n",
      "0.7592592592592593\n",
      "batch_idx: 18\n",
      "0.7543859649122807\n",
      "batch_idx: 19\n",
      "0.75625\n",
      "batch_idx: 20\n",
      "0.7599206349206349\n",
      "batch_idx: 21\n",
      "0.7613636363636364\n",
      "batch_idx: 22\n",
      "0.7590579710144928\n",
      "batch_idx: 23\n",
      "0.7586805555555556\n",
      "batch_idx: 24\n",
      "0.7633333333333333\n",
      "Training Epoch: 463, total loss: 43.452507\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7559523809523809\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.7453703703703703\n",
      "batch_idx: 9\n",
      "0.7458333333333333\n",
      "batch_idx: 10\n",
      "0.7424242424242424\n",
      "batch_idx: 11\n",
      "0.7430555555555556\n",
      "batch_idx: 12\n",
      "0.7403846153846154\n",
      "batch_idx: 13\n",
      "0.7440476190476191\n",
      "batch_idx: 14\n",
      "0.7527777777777778\n",
      "batch_idx: 15\n",
      "0.7552083333333334\n",
      "batch_idx: 16\n",
      "0.7401960784313726\n",
      "batch_idx: 17\n",
      "0.7407407407407407\n",
      "batch_idx: 18\n",
      "0.743421052631579\n",
      "batch_idx: 19\n",
      "0.7416666666666667\n",
      "batch_idx: 20\n",
      "0.748015873015873\n",
      "batch_idx: 21\n",
      "0.7537878787878788\n",
      "batch_idx: 22\n",
      "0.7518115942028986\n",
      "batch_idx: 23\n",
      "0.7517361111111112\n",
      "batch_idx: 24\n",
      "0.7483333333333333\n",
      "Training Epoch: 464, total loss: 43.628985\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.7395833333333334\n",
      "batch_idx: 8\n",
      "0.7407407407407407\n",
      "batch_idx: 9\n",
      "0.75\n",
      "batch_idx: 10\n",
      "0.7310606060606061\n",
      "batch_idx: 11\n",
      "0.7395833333333334\n",
      "batch_idx: 12\n",
      "0.7467948717948718\n",
      "batch_idx: 13\n",
      "0.7619047619047619\n",
      "batch_idx: 14\n",
      "0.7638888888888888\n",
      "batch_idx: 15\n",
      "0.75\n",
      "batch_idx: 16\n",
      "0.7401960784313726\n",
      "batch_idx: 17\n",
      "0.7384259259259259\n",
      "batch_idx: 18\n",
      "0.7390350877192983\n",
      "batch_idx: 19\n",
      "0.7395833333333334\n",
      "batch_idx: 20\n",
      "0.7380952380952381\n",
      "batch_idx: 21\n",
      "0.7424242424242424\n",
      "batch_idx: 22\n",
      "0.7427536231884058\n",
      "batch_idx: 23\n",
      "0.7447916666666666\n",
      "batch_idx: 24\n",
      "0.7483333333333333\n",
      "Training Epoch: 465, total loss: 43.780804\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.7152777777777778\n",
      "batch_idx: 6\n",
      "0.6845238095238095\n",
      "batch_idx: 7\n",
      "0.6979166666666666\n",
      "batch_idx: 8\n",
      "0.7175925925925926\n",
      "batch_idx: 9\n",
      "0.7333333333333333\n",
      "batch_idx: 10\n",
      "0.7348484848484849\n",
      "batch_idx: 11\n",
      "0.7361111111111112\n",
      "batch_idx: 12\n",
      "0.7307692307692307\n",
      "batch_idx: 13\n",
      "0.7291666666666666\n",
      "batch_idx: 14\n",
      "0.7361111111111112\n",
      "batch_idx: 15\n",
      "0.7473958333333334\n",
      "batch_idx: 16\n",
      "0.7549019607843137\n",
      "batch_idx: 17\n",
      "0.7638888888888888\n",
      "batch_idx: 18\n",
      "0.7587719298245614\n",
      "batch_idx: 19\n",
      "0.7541666666666667\n",
      "batch_idx: 20\n",
      "0.7599206349206349\n",
      "batch_idx: 21\n",
      "0.759469696969697\n",
      "batch_idx: 22\n",
      "0.7608695652173914\n",
      "batch_idx: 23\n",
      "0.7569444444444444\n",
      "batch_idx: 24\n",
      "0.7583333333333333\n",
      "Training Epoch: 466, total loss: 43.644972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n",
      "batch_idx: 12\n",
      "0.7724358974358975\n",
      "batch_idx: 13\n",
      "0.7708333333333334\n",
      "batch_idx: 14\n",
      "0.775\n",
      "batch_idx: 15\n",
      "0.7786458333333334\n",
      "batch_idx: 16\n",
      "0.7720588235294118\n",
      "batch_idx: 17\n",
      "0.7708333333333334\n",
      "batch_idx: 18\n",
      "0.7741228070175439\n",
      "batch_idx: 19\n",
      "0.775\n",
      "batch_idx: 20\n",
      "0.7797619047619048\n",
      "batch_idx: 21\n",
      "0.7803030303030303\n",
      "batch_idx: 22\n",
      "0.7807971014492754\n",
      "batch_idx: 23\n",
      "0.78125\n",
      "batch_idx: 24\n",
      "0.7716666666666666\n",
      "Training Epoch: 467, total loss: 43.451892\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.7916666666666666\n",
      "batch_idx: 13\n",
      "0.7708333333333334\n",
      "batch_idx: 14\n",
      "0.775\n",
      "batch_idx: 15\n",
      "0.7786458333333334\n",
      "batch_idx: 16\n",
      "0.7794117647058824\n",
      "batch_idx: 17\n",
      "0.7800925925925926\n",
      "batch_idx: 18\n",
      "0.7807017543859649\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7797619047619048\n",
      "batch_idx: 21\n",
      "0.7746212121212122\n",
      "batch_idx: 22\n",
      "0.7789855072463768\n",
      "batch_idx: 23\n",
      "0.7777777777777778\n",
      "batch_idx: 24\n",
      "0.7733333333333333\n",
      "Training Epoch: 468, total loss: 43.474208\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.725\n",
      "batch_idx: 5\n",
      "0.7361111111111112\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7592592592592593\n",
      "batch_idx: 9\n",
      "0.7666666666666667\n",
      "batch_idx: 10\n",
      "0.7575757575757576\n",
      "batch_idx: 11\n",
      "0.7534722222222222\n",
      "batch_idx: 12\n",
      "0.75\n",
      "batch_idx: 13\n",
      "0.75\n",
      "batch_idx: 14\n",
      "0.7583333333333333\n",
      "batch_idx: 15\n",
      "0.7578125\n",
      "batch_idx: 16\n",
      "0.7549019607843137\n",
      "batch_idx: 17\n",
      "0.7523148148148148\n",
      "batch_idx: 18\n",
      "0.7631578947368421\n",
      "batch_idx: 19\n",
      "0.7604166666666666\n",
      "batch_idx: 20\n",
      "0.7619047619047619\n",
      "batch_idx: 21\n",
      "0.7575757575757576\n",
      "batch_idx: 22\n",
      "0.7554347826086957\n",
      "batch_idx: 23\n",
      "0.7569444444444444\n",
      "batch_idx: 24\n",
      "0.7533333333333333\n",
      "Training Epoch: 469, total loss: 43.754965\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7202380952380952\n",
      "batch_idx: 7\n",
      "0.71875\n",
      "batch_idx: 8\n",
      "0.7268518518518519\n",
      "batch_idx: 9\n",
      "0.7375\n",
      "batch_idx: 10\n",
      "0.75\n",
      "batch_idx: 11\n",
      "0.75\n",
      "batch_idx: 12\n",
      "0.7596153846153846\n",
      "batch_idx: 13\n",
      "0.7529761904761905\n",
      "batch_idx: 14\n",
      "0.7583333333333333\n",
      "batch_idx: 15\n",
      "0.7552083333333334\n",
      "batch_idx: 16\n",
      "0.7598039215686274\n",
      "batch_idx: 17\n",
      "0.7662037037037037\n",
      "batch_idx: 18\n",
      "0.7697368421052632\n",
      "batch_idx: 19\n",
      "0.7708333333333334\n",
      "batch_idx: 20\n",
      "0.7738095238095238\n",
      "batch_idx: 21\n",
      "0.7821969696969697\n",
      "batch_idx: 22\n",
      "0.7735507246376812\n",
      "batch_idx: 23\n",
      "0.7795138888888888\n",
      "batch_idx: 24\n",
      "0.7833333333333333\n",
      "Training Epoch: 470, total loss: 43.077240\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.6770833333333334\n",
      "batch_idx: 4\n",
      "0.7083333333333334\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7440476190476191\n",
      "batch_idx: 7\n",
      "0.7395833333333334\n",
      "batch_idx: 8\n",
      "0.7453703703703703\n",
      "batch_idx: 9\n",
      "0.7583333333333333\n",
      "batch_idx: 10\n",
      "0.7575757575757576\n",
      "batch_idx: 11\n",
      "0.7569444444444444\n",
      "batch_idx: 12\n",
      "0.7628205128205128\n",
      "batch_idx: 13\n",
      "0.7619047619047619\n",
      "batch_idx: 14\n",
      "0.7638888888888888\n",
      "batch_idx: 15\n",
      "0.7578125\n",
      "batch_idx: 16\n",
      "0.7622549019607843\n",
      "batch_idx: 17\n",
      "0.7592592592592593\n",
      "batch_idx: 18\n",
      "0.7587719298245614\n",
      "batch_idx: 19\n",
      "0.7645833333333333\n",
      "batch_idx: 20\n",
      "0.7678571428571429\n",
      "batch_idx: 21\n",
      "0.7632575757575758\n",
      "batch_idx: 22\n",
      "0.7608695652173914\n",
      "batch_idx: 23\n",
      "0.765625\n",
      "batch_idx: 24\n",
      "0.77\n",
      "Training Epoch: 471, total loss: 43.425440\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.7916666666666666\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.7942708333333334\n",
      "batch_idx: 16\n",
      "0.7916666666666666\n",
      "batch_idx: 17\n",
      "0.7893518518518519\n",
      "batch_idx: 18\n",
      "0.7894736842105263\n",
      "batch_idx: 19\n",
      "0.7895833333333333\n",
      "batch_idx: 20\n",
      "0.7876984126984127\n",
      "batch_idx: 21\n",
      "0.7878787878787878\n",
      "batch_idx: 22\n",
      "0.7880434782608695\n",
      "batch_idx: 23\n",
      "0.7899305555555556\n",
      "batch_idx: 24\n",
      "0.7833333333333333\n",
      "Training Epoch: 472, total loss: 43.068379\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7791666666666667\n",
      "batch_idx: 10\n",
      "0.7689393939393939\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.7628205128205128\n",
      "batch_idx: 13\n",
      "0.7708333333333334\n",
      "batch_idx: 14\n",
      "0.7777777777777778\n",
      "batch_idx: 15\n",
      "0.7786458333333334\n",
      "batch_idx: 16\n",
      "0.7794117647058824\n",
      "batch_idx: 17\n",
      "0.7847222222222222\n",
      "batch_idx: 18\n",
      "0.7850877192982456\n",
      "batch_idx: 19\n",
      "0.7791666666666667\n",
      "batch_idx: 20\n",
      "0.7817460317460317\n",
      "batch_idx: 21\n",
      "0.7840909090909091\n",
      "batch_idx: 22\n",
      "0.7844202898550725\n",
      "batch_idx: 23\n",
      "0.7795138888888888\n",
      "batch_idx: 24\n",
      "0.7733333333333333\n",
      "Training Epoch: 473, total loss: 43.237054\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7202380952380952\n",
      "batch_idx: 7\n",
      "0.7135416666666666\n",
      "batch_idx: 8\n",
      "0.7222222222222222\n",
      "batch_idx: 9\n",
      "0.7458333333333333\n",
      "batch_idx: 10\n",
      "0.7462121212121212\n",
      "batch_idx: 11\n",
      "0.7465277777777778\n",
      "batch_idx: 12\n",
      "0.7467948717948718\n",
      "batch_idx: 13\n",
      "0.75\n",
      "batch_idx: 14\n",
      "0.7527777777777778\n",
      "batch_idx: 15\n",
      "0.75\n",
      "batch_idx: 16\n",
      "0.7426470588235294\n",
      "batch_idx: 17\n",
      "0.75\n",
      "batch_idx: 18\n",
      "0.7521929824561403\n",
      "batch_idx: 19\n",
      "0.7604166666666666\n",
      "batch_idx: 20\n",
      "0.7638888888888888\n",
      "batch_idx: 21\n",
      "0.759469696969697\n",
      "batch_idx: 22\n",
      "0.7554347826086957\n",
      "batch_idx: 23\n",
      "0.7534722222222222\n",
      "batch_idx: 24\n",
      "0.755\n",
      "Training Epoch: 474, total loss: 43.626596\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7731481481481481\n",
      "batch_idx: 9\n",
      "0.7666666666666667\n",
      "batch_idx: 10\n",
      "0.7651515151515151\n",
      "batch_idx: 11\n",
      "0.7673611111111112\n",
      "batch_idx: 12\n",
      "0.7628205128205128\n",
      "batch_idx: 13\n",
      "0.7678571428571429\n",
      "batch_idx: 14\n",
      "0.7611111111111111\n",
      "batch_idx: 15\n",
      "0.7552083333333334\n",
      "batch_idx: 16\n",
      "0.7549019607843137\n",
      "batch_idx: 17\n",
      "0.7592592592592593\n",
      "batch_idx: 18\n",
      "0.7675438596491229\n",
      "batch_idx: 19\n",
      "0.75625\n",
      "batch_idx: 20\n",
      "0.7619047619047619\n",
      "batch_idx: 21\n",
      "0.7670454545454546\n",
      "batch_idx: 22\n",
      "0.7572463768115942\n",
      "batch_idx: 23\n",
      "0.7604166666666666\n",
      "batch_idx: 24\n",
      "0.7633333333333333\n",
      "Training Epoch: 475, total loss: 43.443219\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.7942708333333334\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.8004385964912281\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.8015873015873016\n",
      "batch_idx: 21\n",
      "0.7992424242424242\n",
      "batch_idx: 22\n",
      "0.7898550724637681\n",
      "batch_idx: 23\n",
      "0.78125\n",
      "batch_idx: 24\n",
      "0.7716666666666666\n",
      "Training Epoch: 476, total loss: 43.371355\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.75\n",
      "batch_idx: 9\n",
      "0.7416666666666667\n",
      "batch_idx: 10\n",
      "0.7462121212121212\n",
      "batch_idx: 11\n",
      "0.7395833333333334\n",
      "batch_idx: 12\n",
      "0.7371794871794872\n",
      "batch_idx: 13\n",
      "0.7351190476190477\n",
      "batch_idx: 14\n",
      "0.7333333333333333\n",
      "batch_idx: 15\n",
      "0.7395833333333334\n",
      "batch_idx: 16\n",
      "0.7450980392156863\n",
      "batch_idx: 17\n",
      "0.75\n",
      "batch_idx: 18\n",
      "0.7587719298245614\n",
      "batch_idx: 19\n",
      "0.7625\n",
      "batch_idx: 20\n",
      "0.751984126984127\n",
      "batch_idx: 21\n",
      "0.75\n",
      "batch_idx: 22\n",
      "0.7518115942028986\n",
      "batch_idx: 23\n",
      "0.7517361111111112\n",
      "batch_idx: 24\n",
      "0.7533333333333333\n",
      "Training Epoch: 477, total loss: 43.632999\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7447916666666666\n",
      "batch_idx: 8\n",
      "0.75\n",
      "batch_idx: 9\n",
      "0.7625\n",
      "batch_idx: 10\n",
      "0.7651515151515151\n",
      "batch_idx: 11\n",
      "0.7638888888888888\n",
      "batch_idx: 12\n",
      "0.7724358974358975\n",
      "batch_idx: 13\n",
      "0.7559523809523809\n",
      "batch_idx: 14\n",
      "0.7527777777777778\n",
      "batch_idx: 15\n",
      "0.75\n",
      "batch_idx: 16\n",
      "0.7573529411764706\n",
      "batch_idx: 17\n",
      "0.7615740740740741\n",
      "batch_idx: 18\n",
      "0.7609649122807017\n",
      "batch_idx: 19\n",
      "0.7666666666666667\n",
      "batch_idx: 20\n",
      "0.7599206349206349\n",
      "batch_idx: 21\n",
      "0.7575757575757576\n",
      "batch_idx: 22\n",
      "0.7663043478260869\n",
      "batch_idx: 23\n",
      "0.7690972222222222\n",
      "batch_idx: 24\n",
      "0.77\n",
      "Training Epoch: 478, total loss: 43.465671\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.8044871794871795\n",
      "batch_idx: 13\n",
      "0.7976190476190477\n",
      "batch_idx: 14\n",
      "0.8055555555555556\n",
      "batch_idx: 15\n",
      "0.7916666666666666\n",
      "batch_idx: 16\n",
      "0.7892156862745098\n",
      "batch_idx: 17\n",
      "0.7893518518518519\n",
      "batch_idx: 18\n",
      "0.793859649122807\n",
      "batch_idx: 19\n",
      "0.7854166666666667\n",
      "batch_idx: 20\n",
      "0.7857142857142857\n",
      "batch_idx: 21\n",
      "0.7803030303030303\n",
      "batch_idx: 22\n",
      "0.7789855072463768\n",
      "batch_idx: 23\n",
      "0.7760416666666666\n",
      "batch_idx: 24\n",
      "0.7766666666666666\n",
      "Training Epoch: 479, total loss: 43.338463\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.75\n",
      "batch_idx: 9\n",
      "0.7583333333333333\n",
      "batch_idx: 10\n",
      "0.75\n",
      "batch_idx: 11\n",
      "0.7395833333333334\n",
      "batch_idx: 12\n",
      "0.7307692307692307\n",
      "batch_idx: 13\n",
      "0.7470238095238095\n",
      "batch_idx: 14\n",
      "0.7472222222222222\n",
      "batch_idx: 15\n",
      "0.7552083333333334\n",
      "batch_idx: 16\n",
      "0.7549019607843137\n",
      "batch_idx: 17\n",
      "0.7569444444444444\n",
      "batch_idx: 18\n",
      "0.756578947368421\n",
      "batch_idx: 19\n",
      "0.7625\n",
      "batch_idx: 20\n",
      "0.7698412698412699\n",
      "batch_idx: 21\n",
      "0.7670454545454546\n",
      "batch_idx: 22\n",
      "0.7644927536231884\n",
      "batch_idx: 23\n",
      "0.7638888888888888\n",
      "batch_idx: 24\n",
      "0.7633333333333333\n",
      "Training Epoch: 480, total loss: 43.570121\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.7724358974358975\n",
      "batch_idx: 13\n",
      "0.7648809523809523\n",
      "batch_idx: 14\n",
      "0.7666666666666667\n",
      "batch_idx: 15\n",
      "0.7786458333333334\n",
      "batch_idx: 16\n",
      "0.7745098039215687\n",
      "batch_idx: 17\n",
      "0.7800925925925926\n",
      "batch_idx: 18\n",
      "0.7697368421052632\n",
      "batch_idx: 19\n",
      "0.7708333333333334\n",
      "batch_idx: 20\n",
      "0.7599206349206349\n",
      "batch_idx: 21\n",
      "0.759469696969697\n",
      "batch_idx: 22\n",
      "0.7626811594202898\n",
      "batch_idx: 23\n",
      "0.7621527777777778\n",
      "batch_idx: 24\n",
      "0.765\n",
      "Training Epoch: 481, total loss: 43.391832\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.7666666666666667\n",
      "batch_idx: 10\n",
      "0.7575757575757576\n",
      "batch_idx: 11\n",
      "0.7569444444444444\n",
      "batch_idx: 12\n",
      "0.7628205128205128\n",
      "batch_idx: 13\n",
      "0.75\n",
      "batch_idx: 14\n",
      "0.7583333333333333\n",
      "batch_idx: 15\n",
      "0.7604166666666666\n",
      "batch_idx: 16\n",
      "0.7647058823529411\n",
      "batch_idx: 17\n",
      "0.7662037037037037\n",
      "batch_idx: 18\n",
      "0.7675438596491229\n",
      "batch_idx: 19\n",
      "0.7708333333333334\n",
      "batch_idx: 20\n",
      "0.7797619047619048\n",
      "batch_idx: 21\n",
      "0.7803030303030303\n",
      "batch_idx: 22\n",
      "0.7807971014492754\n",
      "batch_idx: 23\n",
      "0.7760416666666666\n",
      "batch_idx: 24\n",
      "0.785\n",
      "Training Epoch: 482, total loss: 43.104521\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.7\n",
      "batch_idx: 5\n",
      "0.7152777777777778\n",
      "batch_idx: 6\n",
      "0.7202380952380952\n",
      "batch_idx: 7\n",
      "0.71875\n",
      "batch_idx: 8\n",
      "0.7361111111111112\n",
      "batch_idx: 9\n",
      "0.7375\n",
      "batch_idx: 10\n",
      "0.7537878787878788\n",
      "batch_idx: 11\n",
      "0.7569444444444444\n",
      "batch_idx: 12\n",
      "0.7596153846153846\n",
      "batch_idx: 13\n",
      "0.7559523809523809\n",
      "batch_idx: 14\n",
      "0.7555555555555555\n",
      "batch_idx: 15\n",
      "0.7526041666666666\n",
      "batch_idx: 16\n",
      "0.7524509803921569\n",
      "batch_idx: 17\n",
      "0.7546296296296297\n",
      "batch_idx: 18\n",
      "0.756578947368421\n",
      "batch_idx: 19\n",
      "0.7604166666666666\n",
      "batch_idx: 20\n",
      "0.7559523809523809\n",
      "batch_idx: 21\n",
      "0.7462121212121212\n",
      "batch_idx: 22\n",
      "0.7427536231884058\n",
      "batch_idx: 23\n",
      "0.7378472222222222\n",
      "batch_idx: 24\n",
      "0.7466666666666667\n",
      "Training Epoch: 483, total loss: 43.824274\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.7777777777777778\n",
      "batch_idx: 12\n",
      "0.7756410256410257\n",
      "batch_idx: 13\n",
      "0.7708333333333334\n",
      "batch_idx: 14\n",
      "0.7666666666666667\n",
      "batch_idx: 15\n",
      "0.7630208333333334\n",
      "batch_idx: 16\n",
      "0.7622549019607843\n",
      "batch_idx: 17\n",
      "0.7662037037037037\n",
      "batch_idx: 18\n",
      "0.7609649122807017\n",
      "batch_idx: 19\n",
      "0.7604166666666666\n",
      "batch_idx: 20\n",
      "0.7599206349206349\n",
      "batch_idx: 21\n",
      "0.7632575757575758\n",
      "batch_idx: 22\n",
      "0.7644927536231884\n",
      "batch_idx: 23\n",
      "0.7604166666666666\n",
      "batch_idx: 24\n",
      "0.7616666666666667\n",
      "Training Epoch: 484, total loss: 43.550317\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8020833333333334\n",
      "batch_idx: 16\n",
      "0.7941176470588235\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.8070175438596491\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.7992424242424242\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.7986111111111112\n",
      "batch_idx: 24\n",
      "0.795\n",
      "Training Epoch: 485, total loss: 43.006501\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7261904761904762\n",
      "batch_idx: 7\n",
      "0.7135416666666666\n",
      "batch_idx: 8\n",
      "0.7361111111111112\n",
      "batch_idx: 9\n",
      "0.7291666666666666\n",
      "batch_idx: 10\n",
      "0.7424242424242424\n",
      "batch_idx: 11\n",
      "0.7326388888888888\n",
      "batch_idx: 12\n",
      "0.7371794871794872\n",
      "batch_idx: 13\n",
      "0.7380952380952381\n",
      "batch_idx: 14\n",
      "0.7388888888888889\n",
      "batch_idx: 15\n",
      "0.7395833333333334\n",
      "batch_idx: 16\n",
      "0.7426470588235294\n",
      "batch_idx: 17\n",
      "0.7384259259259259\n",
      "batch_idx: 18\n",
      "0.7412280701754386\n",
      "batch_idx: 19\n",
      "0.7354166666666667\n",
      "batch_idx: 20\n",
      "0.7341269841269841\n",
      "batch_idx: 21\n",
      "0.7253787878787878\n",
      "batch_idx: 22\n",
      "0.730072463768116\n",
      "batch_idx: 23\n",
      "0.7309027777777778\n",
      "batch_idx: 24\n",
      "0.735\n",
      "Training Epoch: 486, total loss: 43.880141\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.765625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.7791666666666667\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.7564102564102564\n",
      "batch_idx: 13\n",
      "0.7559523809523809\n",
      "batch_idx: 14\n",
      "0.7638888888888888\n",
      "batch_idx: 15\n",
      "0.7682291666666666\n",
      "batch_idx: 16\n",
      "0.7696078431372549\n",
      "batch_idx: 17\n",
      "0.7708333333333334\n",
      "batch_idx: 18\n",
      "0.7763157894736842\n",
      "batch_idx: 19\n",
      "0.7833333333333333\n",
      "batch_idx: 20\n",
      "0.7777777777777778\n",
      "batch_idx: 21\n",
      "0.7840909090909091\n",
      "batch_idx: 22\n",
      "0.782608695652174\n",
      "batch_idx: 23\n",
      "0.7864583333333334\n",
      "batch_idx: 24\n",
      "0.79\n",
      "Training Epoch: 487, total loss: 43.150091\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7592592592592593\n",
      "batch_idx: 9\n",
      "0.7541666666666667\n",
      "batch_idx: 10\n",
      "0.7424242424242424\n",
      "batch_idx: 11\n",
      "0.7534722222222222\n",
      "batch_idx: 12\n",
      "0.7532051282051282\n",
      "batch_idx: 13\n",
      "0.75\n",
      "batch_idx: 14\n",
      "0.7444444444444445\n",
      "batch_idx: 15\n",
      "0.7473958333333334\n",
      "batch_idx: 16\n",
      "0.7549019607843137\n",
      "batch_idx: 17\n",
      "0.7615740740740741\n",
      "batch_idx: 18\n",
      "0.7675438596491229\n",
      "batch_idx: 19\n",
      "0.7708333333333334\n",
      "batch_idx: 20\n",
      "0.7698412698412699\n",
      "batch_idx: 21\n",
      "0.7708333333333334\n",
      "batch_idx: 22\n",
      "0.7717391304347826\n",
      "batch_idx: 23\n",
      "0.7690972222222222\n",
      "batch_idx: 24\n",
      "0.7683333333333333\n",
      "Training Epoch: 488, total loss: 43.392730\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7638888888888888\n",
      "batch_idx: 9\n",
      "0.7583333333333333\n",
      "batch_idx: 10\n",
      "0.7537878787878788\n",
      "batch_idx: 11\n",
      "0.7534722222222222\n",
      "batch_idx: 12\n",
      "0.7564102564102564\n",
      "batch_idx: 13\n",
      "0.7529761904761905\n",
      "batch_idx: 14\n",
      "0.75\n",
      "batch_idx: 15\n",
      "0.7421875\n",
      "batch_idx: 16\n",
      "0.7401960784313726\n",
      "batch_idx: 17\n",
      "0.7384259259259259\n",
      "batch_idx: 18\n",
      "0.7368421052631579\n",
      "batch_idx: 19\n",
      "0.7375\n",
      "batch_idx: 20\n",
      "0.7420634920634921\n",
      "batch_idx: 21\n",
      "0.7462121212121212\n",
      "batch_idx: 22\n",
      "0.7409420289855072\n",
      "batch_idx: 23\n",
      "0.7482638888888888\n",
      "batch_idx: 24\n",
      "0.7516666666666667\n",
      "Training Epoch: 489, total loss: 43.759848\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7592592592592593\n",
      "batch_idx: 9\n",
      "0.7666666666666667\n",
      "batch_idx: 10\n",
      "0.7689393939393939\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.7692307692307693\n",
      "batch_idx: 13\n",
      "0.7678571428571429\n",
      "batch_idx: 14\n",
      "0.7666666666666667\n",
      "batch_idx: 15\n",
      "0.7578125\n",
      "batch_idx: 16\n",
      "0.7573529411764706\n",
      "batch_idx: 17\n",
      "0.7523148148148148\n",
      "batch_idx: 18\n",
      "0.756578947368421\n",
      "batch_idx: 19\n",
      "0.7604166666666666\n",
      "batch_idx: 20\n",
      "0.7638888888888888\n",
      "batch_idx: 21\n",
      "0.7651515151515151\n",
      "batch_idx: 22\n",
      "0.7663043478260869\n",
      "batch_idx: 23\n",
      "0.7621527777777778\n",
      "batch_idx: 24\n",
      "0.7616666666666667\n",
      "Training Epoch: 490, total loss: 43.600399\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7827380952380952\n",
      "batch_idx: 14\n",
      "0.775\n",
      "batch_idx: 15\n",
      "0.765625\n",
      "batch_idx: 16\n",
      "0.7647058823529411\n",
      "batch_idx: 17\n",
      "0.7662037037037037\n",
      "batch_idx: 18\n",
      "0.7741228070175439\n",
      "batch_idx: 19\n",
      "0.7708333333333334\n",
      "batch_idx: 20\n",
      "0.7718253968253969\n",
      "batch_idx: 21\n",
      "0.7670454545454546\n",
      "batch_idx: 22\n",
      "0.769927536231884\n",
      "batch_idx: 23\n",
      "0.7708333333333334\n",
      "batch_idx: 24\n",
      "0.775\n",
      "Training Epoch: 491, total loss: 43.431443\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7361111111111112\n",
      "batch_idx: 9\n",
      "0.7333333333333333\n",
      "batch_idx: 10\n",
      "0.7386363636363636\n",
      "batch_idx: 11\n",
      "0.7361111111111112\n",
      "batch_idx: 12\n",
      "0.7435897435897436\n",
      "batch_idx: 13\n",
      "0.7291666666666666\n",
      "batch_idx: 14\n",
      "0.7388888888888889\n",
      "batch_idx: 15\n",
      "0.7395833333333334\n",
      "batch_idx: 16\n",
      "0.7401960784313726\n",
      "batch_idx: 17\n",
      "0.7384259259259259\n",
      "batch_idx: 18\n",
      "0.7346491228070176\n",
      "batch_idx: 19\n",
      "0.7395833333333334\n",
      "batch_idx: 20\n",
      "0.7301587301587301\n",
      "batch_idx: 21\n",
      "0.7329545454545454\n",
      "batch_idx: 22\n",
      "0.7355072463768116\n",
      "batch_idx: 23\n",
      "0.734375\n",
      "batch_idx: 24\n",
      "0.7366666666666667\n",
      "Training Epoch: 492, total loss: 44.088626\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7321428571428571\n",
      "batch_idx: 7\n",
      "0.7239583333333334\n",
      "batch_idx: 8\n",
      "0.7407407407407407\n",
      "batch_idx: 9\n",
      "0.7416666666666667\n",
      "batch_idx: 10\n",
      "0.7462121212121212\n",
      "batch_idx: 11\n",
      "0.7534722222222222\n",
      "batch_idx: 12\n",
      "0.7596153846153846\n",
      "batch_idx: 13\n",
      "0.7619047619047619\n",
      "batch_idx: 14\n",
      "0.7666666666666667\n",
      "batch_idx: 15\n",
      "0.7734375\n",
      "batch_idx: 16\n",
      "0.7696078431372549\n",
      "batch_idx: 17\n",
      "0.7685185185185185\n",
      "batch_idx: 18\n",
      "0.7719298245614035\n",
      "batch_idx: 19\n",
      "0.775\n",
      "batch_idx: 20\n",
      "0.7698412698412699\n",
      "batch_idx: 21\n",
      "0.7689393939393939\n",
      "batch_idx: 22\n",
      "0.7717391304347826\n",
      "batch_idx: 23\n",
      "0.7760416666666666\n",
      "batch_idx: 24\n",
      "0.775\n",
      "Training Epoch: 493, total loss: 43.309700\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7743055555555556\n",
      "batch_idx: 12\n",
      "0.7724358974358975\n",
      "batch_idx: 13\n",
      "0.7767857142857143\n",
      "batch_idx: 14\n",
      "0.7722222222222223\n",
      "batch_idx: 15\n",
      "0.765625\n",
      "batch_idx: 16\n",
      "0.7549019607843137\n",
      "batch_idx: 17\n",
      "0.7615740740740741\n",
      "batch_idx: 18\n",
      "0.7543859649122807\n",
      "batch_idx: 19\n",
      "0.7583333333333333\n",
      "batch_idx: 20\n",
      "0.7559523809523809\n",
      "batch_idx: 21\n",
      "0.7518939393939394\n",
      "batch_idx: 22\n",
      "0.7536231884057971\n",
      "batch_idx: 23\n",
      "0.7552083333333334\n",
      "batch_idx: 24\n",
      "0.755\n",
      "Training Epoch: 494, total loss: 43.659442\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.7777777777777778\n",
      "batch_idx: 12\n",
      "0.7628205128205128\n",
      "batch_idx: 13\n",
      "0.7589285714285714\n",
      "batch_idx: 14\n",
      "0.7638888888888888\n",
      "batch_idx: 15\n",
      "0.765625\n",
      "batch_idx: 16\n",
      "0.7671568627450981\n",
      "batch_idx: 17\n",
      "0.7615740740740741\n",
      "batch_idx: 18\n",
      "0.7609649122807017\n",
      "batch_idx: 19\n",
      "0.7666666666666667\n",
      "batch_idx: 20\n",
      "0.7619047619047619\n",
      "batch_idx: 21\n",
      "0.7575757575757576\n",
      "batch_idx: 22\n",
      "0.7572463768115942\n",
      "batch_idx: 23\n",
      "0.7569444444444444\n",
      "batch_idx: 24\n",
      "0.7633333333333333\n",
      "Training Epoch: 495, total loss: 43.442751\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7765151515151515\n",
      "batch_idx: 11\n",
      "0.7777777777777778\n",
      "batch_idx: 12\n",
      "0.7756410256410257\n",
      "batch_idx: 13\n",
      "0.7738095238095238\n",
      "batch_idx: 14\n",
      "0.7777777777777778\n",
      "batch_idx: 15\n",
      "0.7708333333333334\n",
      "batch_idx: 16\n",
      "0.7696078431372549\n",
      "batch_idx: 17\n",
      "0.7708333333333334\n",
      "batch_idx: 18\n",
      "0.7719298245614035\n",
      "batch_idx: 19\n",
      "0.7770833333333333\n",
      "batch_idx: 20\n",
      "0.7797619047619048\n",
      "batch_idx: 21\n",
      "0.7859848484848485\n",
      "batch_idx: 22\n",
      "0.7862318840579711\n",
      "batch_idx: 23\n",
      "0.7760416666666666\n",
      "batch_idx: 24\n",
      "0.7783333333333333\n",
      "Training Epoch: 496, total loss: 43.168549\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7731481481481481\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.7946428571428571\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.8014705882352942\n",
      "batch_idx: 17\n",
      "0.7893518518518519\n",
      "batch_idx: 18\n",
      "0.7894736842105263\n",
      "batch_idx: 19\n",
      "0.7833333333333333\n",
      "batch_idx: 20\n",
      "0.7797619047619048\n",
      "batch_idx: 21\n",
      "0.7765151515151515\n",
      "batch_idx: 22\n",
      "0.7807971014492754\n",
      "batch_idx: 23\n",
      "0.7777777777777778\n",
      "batch_idx: 24\n",
      "0.7766666666666666\n",
      "Training Epoch: 497, total loss: 43.340288\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7878787878787878\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n",
      "batch_idx: 12\n",
      "0.7756410256410257\n",
      "batch_idx: 13\n",
      "0.7767857142857143\n",
      "batch_idx: 14\n",
      "0.7805555555555556\n",
      "batch_idx: 15\n",
      "0.7708333333333334\n",
      "batch_idx: 16\n",
      "0.7696078431372549\n",
      "batch_idx: 17\n",
      "0.7685185185185185\n",
      "batch_idx: 18\n",
      "0.7675438596491229\n",
      "batch_idx: 19\n",
      "0.7666666666666667\n",
      "batch_idx: 20\n",
      "0.7738095238095238\n",
      "batch_idx: 21\n",
      "0.7746212121212122\n",
      "batch_idx: 22\n",
      "0.7735507246376812\n",
      "batch_idx: 23\n",
      "0.7743055555555556\n",
      "batch_idx: 24\n",
      "0.7783333333333333\n",
      "Training Epoch: 498, total loss: 43.140480\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.6666666666666666\n",
      "batch_idx: 5\n",
      "0.6875\n",
      "batch_idx: 6\n",
      "0.7023809523809523\n",
      "batch_idx: 7\n",
      "0.7239583333333334\n",
      "batch_idx: 8\n",
      "0.7314814814814815\n",
      "batch_idx: 9\n",
      "0.7291666666666666\n",
      "batch_idx: 10\n",
      "0.7348484848484849\n",
      "batch_idx: 11\n",
      "0.7395833333333334\n",
      "batch_idx: 12\n",
      "0.7467948717948718\n",
      "batch_idx: 13\n",
      "0.75\n",
      "batch_idx: 14\n",
      "0.7416666666666667\n",
      "batch_idx: 15\n",
      "0.7473958333333334\n",
      "batch_idx: 16\n",
      "0.7475490196078431\n",
      "batch_idx: 17\n",
      "0.7546296296296297\n",
      "batch_idx: 18\n",
      "0.7543859649122807\n",
      "batch_idx: 19\n",
      "0.7583333333333333\n",
      "batch_idx: 20\n",
      "0.7559523809523809\n",
      "batch_idx: 21\n",
      "0.759469696969697\n",
      "batch_idx: 22\n",
      "0.7590579710144928\n",
      "batch_idx: 23\n",
      "0.7621527777777778\n",
      "batch_idx: 24\n",
      "0.765\n",
      "Training Epoch: 499, total loss: 43.495581\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.7791666666666667\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.7777777777777778\n",
      "batch_idx: 12\n",
      "0.7756410256410257\n",
      "batch_idx: 13\n",
      "0.7678571428571429\n",
      "batch_idx: 14\n",
      "0.7666666666666667\n",
      "batch_idx: 15\n",
      "0.7604166666666666\n",
      "batch_idx: 16\n",
      "0.7696078431372549\n",
      "batch_idx: 17\n",
      "0.7731481481481481\n",
      "batch_idx: 18\n",
      "0.7741228070175439\n",
      "batch_idx: 19\n",
      "0.7791666666666667\n",
      "batch_idx: 20\n",
      "0.7718253968253969\n",
      "batch_idx: 21\n",
      "0.7708333333333334\n",
      "batch_idx: 22\n",
      "0.7735507246376812\n",
      "batch_idx: 23\n",
      "0.7760416666666666\n",
      "batch_idx: 24\n",
      "0.775\n",
      "Training Epoch: 500, total loss: 43.353149\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7559523809523809\n",
      "batch_idx: 7\n",
      "0.7447916666666666\n",
      "batch_idx: 8\n",
      "0.7361111111111112\n",
      "batch_idx: 9\n",
      "0.7375\n",
      "batch_idx: 10\n",
      "0.7272727272727273\n",
      "batch_idx: 11\n",
      "0.7361111111111112\n",
      "batch_idx: 12\n",
      "0.7435897435897436\n",
      "batch_idx: 13\n",
      "0.7440476190476191\n",
      "batch_idx: 14\n",
      "0.75\n",
      "batch_idx: 15\n",
      "0.75\n",
      "batch_idx: 16\n",
      "0.7573529411764706\n",
      "batch_idx: 17\n",
      "0.7638888888888888\n",
      "batch_idx: 18\n",
      "0.7675438596491229\n",
      "batch_idx: 19\n",
      "0.7645833333333333\n",
      "batch_idx: 20\n",
      "0.7619047619047619\n",
      "batch_idx: 21\n",
      "0.759469696969697\n",
      "batch_idx: 22\n",
      "0.7536231884057971\n",
      "batch_idx: 23\n",
      "0.7552083333333334\n",
      "batch_idx: 24\n",
      "0.7566666666666667\n",
      "Training Epoch: 501, total loss: 43.413294\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.7946428571428571\n",
      "batch_idx: 14\n",
      "0.8\n",
      "batch_idx: 15\n",
      "0.796875\n",
      "batch_idx: 16\n",
      "0.7892156862745098\n",
      "batch_idx: 17\n",
      "0.7870370370370371\n",
      "batch_idx: 18\n",
      "0.7850877192982456\n",
      "batch_idx: 19\n",
      "0.7770833333333333\n",
      "batch_idx: 20\n",
      "0.7777777777777778\n",
      "batch_idx: 21\n",
      "0.7746212121212122\n",
      "batch_idx: 22\n",
      "0.7789855072463768\n",
      "batch_idx: 23\n",
      "0.7829861111111112\n",
      "batch_idx: 24\n",
      "0.78\n",
      "Training Epoch: 502, total loss: 43.163690\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.9375\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7878787878787878\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7916666666666666\n",
      "batch_idx: 13\n",
      "0.7886904761904762\n",
      "batch_idx: 14\n",
      "0.775\n",
      "batch_idx: 15\n",
      "0.7760416666666666\n",
      "batch_idx: 16\n",
      "0.7794117647058824\n",
      "batch_idx: 17\n",
      "0.7893518518518519\n",
      "batch_idx: 18\n",
      "0.7828947368421053\n",
      "batch_idx: 19\n",
      "0.7791666666666667\n",
      "batch_idx: 20\n",
      "0.7777777777777778\n",
      "batch_idx: 21\n",
      "0.7784090909090909\n",
      "batch_idx: 22\n",
      "0.7771739130434783\n",
      "batch_idx: 23\n",
      "0.7743055555555556\n",
      "batch_idx: 24\n",
      "0.7733333333333333\n",
      "Training Epoch: 503, total loss: 43.354009\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8055555555555556\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.8055555555555556\n",
      "batch_idx: 18\n",
      "0.8026315789473685\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.8049242424242424\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8038194444444444\n",
      "batch_idx: 24\n",
      "0.7966666666666666\n",
      "Training Epoch: 504, total loss: 42.935426\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.725\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7559523809523809\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.7546296296296297\n",
      "batch_idx: 9\n",
      "0.7541666666666667\n",
      "batch_idx: 10\n",
      "0.7462121212121212\n",
      "batch_idx: 11\n",
      "0.7395833333333334\n",
      "batch_idx: 12\n",
      "0.7596153846153846\n",
      "batch_idx: 13\n",
      "0.7619047619047619\n",
      "batch_idx: 14\n",
      "0.7611111111111111\n",
      "batch_idx: 15\n",
      "0.7734375\n",
      "batch_idx: 16\n",
      "0.7696078431372549\n",
      "batch_idx: 17\n",
      "0.7615740740740741\n",
      "batch_idx: 18\n",
      "0.7631578947368421\n",
      "batch_idx: 19\n",
      "0.7645833333333333\n",
      "batch_idx: 20\n",
      "0.7698412698412699\n",
      "batch_idx: 21\n",
      "0.7727272727272727\n",
      "batch_idx: 22\n",
      "0.7753623188405797\n",
      "batch_idx: 23\n",
      "0.7690972222222222\n",
      "batch_idx: 24\n",
      "0.7733333333333333\n",
      "Training Epoch: 505, total loss: 43.189763\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7767857142857143\n",
      "batch_idx: 14\n",
      "0.7722222222222223\n",
      "batch_idx: 15\n",
      "0.7708333333333334\n",
      "batch_idx: 16\n",
      "0.7671568627450981\n",
      "batch_idx: 17\n",
      "0.7685185185185185\n",
      "batch_idx: 18\n",
      "0.7697368421052632\n",
      "batch_idx: 19\n",
      "0.7708333333333334\n",
      "batch_idx: 20\n",
      "0.7738095238095238\n",
      "batch_idx: 21\n",
      "0.7765151515151515\n",
      "batch_idx: 22\n",
      "0.7771739130434783\n",
      "batch_idx: 23\n",
      "0.7743055555555556\n",
      "batch_idx: 24\n",
      "0.775\n",
      "Training Epoch: 506, total loss: 43.143882\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.7731481481481481\n",
      "batch_idx: 9\n",
      "0.7666666666666667\n",
      "batch_idx: 10\n",
      "0.7689393939393939\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 12\n",
      "0.7564102564102564\n",
      "batch_idx: 13\n",
      "0.7648809523809523\n",
      "batch_idx: 14\n",
      "0.7638888888888888\n",
      "batch_idx: 15\n",
      "0.7682291666666666\n",
      "batch_idx: 16\n",
      "0.7696078431372549\n",
      "batch_idx: 17\n",
      "0.7685185185185185\n",
      "batch_idx: 18\n",
      "0.7631578947368421\n",
      "batch_idx: 19\n",
      "0.7604166666666666\n",
      "batch_idx: 20\n",
      "0.7638888888888888\n",
      "batch_idx: 21\n",
      "0.7670454545454546\n",
      "batch_idx: 22\n",
      "0.7663043478260869\n",
      "batch_idx: 23\n",
      "0.765625\n",
      "batch_idx: 24\n",
      "0.7666666666666667\n",
      "Training Epoch: 507, total loss: 43.427867\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.7552083333333334\n",
      "batch_idx: 8\n",
      "0.7638888888888888\n",
      "batch_idx: 9\n",
      "0.7708333333333334\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7797619047619048\n",
      "batch_idx: 14\n",
      "0.7833333333333333\n",
      "batch_idx: 15\n",
      "0.7786458333333334\n",
      "batch_idx: 16\n",
      "0.7794117647058824\n",
      "batch_idx: 17\n",
      "0.7870370370370371\n",
      "batch_idx: 18\n",
      "0.7828947368421053\n",
      "batch_idx: 19\n",
      "0.7875\n",
      "batch_idx: 20\n",
      "0.7896825396825397\n",
      "batch_idx: 21\n",
      "0.7897727272727273\n",
      "batch_idx: 22\n",
      "0.7934782608695652\n",
      "batch_idx: 23\n",
      "0.7864583333333334\n",
      "batch_idx: 24\n",
      "0.7783333333333333\n",
      "Training Epoch: 508, total loss: 43.243164\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.7870370370370371\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.8005952380952381\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.8020833333333334\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8114035087719298\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.7935606060606061\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.796875\n",
      "batch_idx: 24\n",
      "0.7966666666666666\n",
      "Training Epoch: 509, total loss: 42.904204\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9583333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.7395833333333334\n",
      "batch_idx: 8\n",
      "0.7361111111111112\n",
      "batch_idx: 9\n",
      "0.7458333333333333\n",
      "batch_idx: 10\n",
      "0.7537878787878788\n",
      "batch_idx: 11\n",
      "0.7569444444444444\n",
      "batch_idx: 12\n",
      "0.7532051282051282\n",
      "batch_idx: 13\n",
      "0.7619047619047619\n",
      "batch_idx: 14\n",
      "0.7694444444444445\n",
      "batch_idx: 15\n",
      "0.765625\n",
      "batch_idx: 16\n",
      "0.7647058823529411\n",
      "batch_idx: 17\n",
      "0.7662037037037037\n",
      "batch_idx: 18\n",
      "0.7697368421052632\n",
      "batch_idx: 19\n",
      "0.76875\n",
      "batch_idx: 20\n",
      "0.7638888888888888\n",
      "batch_idx: 21\n",
      "0.7670454545454546\n",
      "batch_idx: 22\n",
      "0.7626811594202898\n",
      "batch_idx: 23\n",
      "0.7638888888888888\n",
      "batch_idx: 24\n",
      "0.7666666666666667\n",
      "Training Epoch: 510, total loss: 43.320935\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7638888888888888\n",
      "batch_idx: 9\n",
      "0.7708333333333334\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7738095238095238\n",
      "batch_idx: 14\n",
      "0.775\n",
      "batch_idx: 15\n",
      "0.7838541666666666\n",
      "batch_idx: 16\n",
      "0.7720588235294118\n",
      "batch_idx: 17\n",
      "0.7754629629629629\n",
      "batch_idx: 18\n",
      "0.7653508771929824\n",
      "batch_idx: 19\n",
      "0.7604166666666666\n",
      "batch_idx: 20\n",
      "0.7619047619047619\n",
      "batch_idx: 21\n",
      "0.7613636363636364\n",
      "batch_idx: 22\n",
      "0.7644927536231884\n",
      "batch_idx: 23\n",
      "0.765625\n",
      "batch_idx: 24\n",
      "0.7583333333333333\n",
      "Training Epoch: 511, total loss: 43.505607\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7592592592592593\n",
      "batch_idx: 9\n",
      "0.7666666666666667\n",
      "batch_idx: 10\n",
      "0.7575757575757576\n",
      "batch_idx: 11\n",
      "0.7638888888888888\n",
      "batch_idx: 12\n",
      "0.7724358974358975\n",
      "batch_idx: 13\n",
      "0.7619047619047619\n",
      "batch_idx: 14\n",
      "0.7555555555555555\n",
      "batch_idx: 15\n",
      "0.7526041666666666\n",
      "batch_idx: 16\n",
      "0.7549019607843137\n",
      "batch_idx: 17\n",
      "0.7569444444444444\n",
      "batch_idx: 18\n",
      "0.756578947368421\n",
      "batch_idx: 19\n",
      "0.7625\n",
      "batch_idx: 20\n",
      "0.7599206349206349\n",
      "batch_idx: 21\n",
      "0.7537878787878788\n",
      "batch_idx: 22\n",
      "0.7518115942028986\n",
      "batch_idx: 23\n",
      "0.7552083333333334\n",
      "batch_idx: 24\n",
      "0.7583333333333333\n",
      "Training Epoch: 512, total loss: 43.603389\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.7361111111111112\n",
      "batch_idx: 9\n",
      "0.7458333333333333\n",
      "batch_idx: 10\n",
      "0.7537878787878788\n",
      "batch_idx: 11\n",
      "0.7604166666666666\n",
      "batch_idx: 12\n",
      "0.7660256410256411\n",
      "batch_idx: 13\n",
      "0.7678571428571429\n",
      "batch_idx: 14\n",
      "0.7666666666666667\n",
      "batch_idx: 15\n",
      "0.7708333333333334\n",
      "batch_idx: 16\n",
      "0.7720588235294118\n",
      "batch_idx: 17\n",
      "0.7731481481481481\n",
      "batch_idx: 18\n",
      "0.7741228070175439\n",
      "batch_idx: 19\n",
      "0.775\n",
      "batch_idx: 20\n",
      "0.7777777777777778\n",
      "batch_idx: 21\n",
      "0.7765151515151515\n",
      "batch_idx: 22\n",
      "0.7771739130434783\n",
      "batch_idx: 23\n",
      "0.7760416666666666\n",
      "batch_idx: 24\n",
      "0.775\n",
      "Training Epoch: 513, total loss: 43.322384\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7546296296296297\n",
      "batch_idx: 9\n",
      "0.7583333333333333\n",
      "batch_idx: 10\n",
      "0.7651515151515151\n",
      "batch_idx: 11\n",
      "0.7569444444444444\n",
      "batch_idx: 12\n",
      "0.7596153846153846\n",
      "batch_idx: 13\n",
      "0.7619047619047619\n",
      "batch_idx: 14\n",
      "0.7638888888888888\n",
      "batch_idx: 15\n",
      "0.7526041666666666\n",
      "batch_idx: 16\n",
      "0.7450980392156863\n",
      "batch_idx: 17\n",
      "0.75\n",
      "batch_idx: 18\n",
      "0.7521929824561403\n",
      "batch_idx: 19\n",
      "0.7625\n",
      "batch_idx: 20\n",
      "0.7638888888888888\n",
      "batch_idx: 21\n",
      "0.7575757575757576\n",
      "batch_idx: 22\n",
      "0.7608695652173914\n",
      "batch_idx: 23\n",
      "0.7586805555555556\n",
      "batch_idx: 24\n",
      "0.7616666666666667\n",
      "Training Epoch: 514, total loss: 43.456914\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.782051282051282\n",
      "batch_idx: 13\n",
      "0.7886904761904762\n",
      "batch_idx: 14\n",
      "0.7916666666666666\n",
      "batch_idx: 15\n",
      "0.7890625\n",
      "batch_idx: 16\n",
      "0.7769607843137255\n",
      "batch_idx: 17\n",
      "0.7754629629629629\n",
      "batch_idx: 18\n",
      "0.7763157894736842\n",
      "batch_idx: 19\n",
      "0.775\n",
      "batch_idx: 20\n",
      "0.7797619047619048\n",
      "batch_idx: 21\n",
      "0.7765151515151515\n",
      "batch_idx: 22\n",
      "0.7753623188405797\n",
      "batch_idx: 23\n",
      "0.7777777777777778\n",
      "batch_idx: 24\n",
      "0.7733333333333333\n",
      "Training Epoch: 515, total loss: 43.212325\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.7546296296296297\n",
      "batch_idx: 9\n",
      "0.7541666666666667\n",
      "batch_idx: 10\n",
      "0.7765151515151515\n",
      "batch_idx: 11\n",
      "0.7777777777777778\n",
      "batch_idx: 12\n",
      "0.7756410256410257\n",
      "batch_idx: 13\n",
      "0.7767857142857143\n",
      "batch_idx: 14\n",
      "0.7833333333333333\n",
      "batch_idx: 15\n",
      "0.7786458333333334\n",
      "batch_idx: 16\n",
      "0.7720588235294118\n",
      "batch_idx: 17\n",
      "0.7685185185185185\n",
      "batch_idx: 18\n",
      "0.7697368421052632\n",
      "batch_idx: 19\n",
      "0.7708333333333334\n",
      "batch_idx: 20\n",
      "0.7797619047619048\n",
      "batch_idx: 21\n",
      "0.7821969696969697\n",
      "batch_idx: 22\n",
      "0.782608695652174\n",
      "batch_idx: 23\n",
      "0.7847222222222222\n",
      "batch_idx: 24\n",
      "0.79\n",
      "Training Epoch: 516, total loss: 42.955555\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.7791666666666667\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.7951388888888888\n",
      "batch_idx: 12\n",
      "0.7948717948717948\n",
      "batch_idx: 13\n",
      "0.7916666666666666\n",
      "batch_idx: 14\n",
      "0.7944444444444444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 15\n",
      "0.7942708333333334\n",
      "batch_idx: 16\n",
      "0.8014705882352942\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.7916666666666666\n",
      "batch_idx: 19\n",
      "0.7854166666666667\n",
      "batch_idx: 20\n",
      "0.7757936507936508\n",
      "batch_idx: 21\n",
      "0.7708333333333334\n",
      "batch_idx: 22\n",
      "0.7753623188405797\n",
      "batch_idx: 23\n",
      "0.7743055555555556\n",
      "batch_idx: 24\n",
      "0.7666666666666667\n",
      "Training Epoch: 517, total loss: 43.485795\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.7559523809523809\n",
      "batch_idx: 7\n",
      "0.7552083333333334\n",
      "batch_idx: 8\n",
      "0.7453703703703703\n",
      "batch_idx: 9\n",
      "0.7333333333333333\n",
      "batch_idx: 10\n",
      "0.7348484848484849\n",
      "batch_idx: 11\n",
      "0.7361111111111112\n",
      "batch_idx: 12\n",
      "0.75\n",
      "batch_idx: 13\n",
      "0.7559523809523809\n",
      "batch_idx: 14\n",
      "0.75\n",
      "batch_idx: 15\n",
      "0.75\n",
      "batch_idx: 16\n",
      "0.7475490196078431\n",
      "batch_idx: 17\n",
      "0.7430555555555556\n",
      "batch_idx: 18\n",
      "0.7368421052631579\n",
      "batch_idx: 19\n",
      "0.7416666666666667\n",
      "batch_idx: 20\n",
      "0.746031746031746\n",
      "batch_idx: 21\n",
      "0.7462121212121212\n",
      "batch_idx: 22\n",
      "0.75\n",
      "batch_idx: 23\n",
      "0.7534722222222222\n",
      "batch_idx: 24\n",
      "0.75\n",
      "Training Epoch: 518, total loss: 43.618553\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7731481481481481\n",
      "batch_idx: 9\n",
      "0.7791666666666667\n",
      "batch_idx: 10\n",
      "0.7613636363636364\n",
      "batch_idx: 11\n",
      "0.7673611111111112\n",
      "batch_idx: 12\n",
      "0.7596153846153846\n",
      "batch_idx: 13\n",
      "0.7648809523809523\n",
      "batch_idx: 14\n",
      "0.7722222222222223\n",
      "batch_idx: 15\n",
      "0.7760416666666666\n",
      "batch_idx: 16\n",
      "0.7745098039215687\n",
      "batch_idx: 17\n",
      "0.7777777777777778\n",
      "batch_idx: 18\n",
      "0.7828947368421053\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7757936507936508\n",
      "batch_idx: 21\n",
      "0.7803030303030303\n",
      "batch_idx: 22\n",
      "0.7753623188405797\n",
      "batch_idx: 23\n",
      "0.7760416666666666\n",
      "batch_idx: 24\n",
      "0.7816666666666666\n",
      "Training Epoch: 519, total loss: 43.095398\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.7083333333333334\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7268518518518519\n",
      "batch_idx: 9\n",
      "0.7333333333333333\n",
      "batch_idx: 10\n",
      "0.7348484848484849\n",
      "batch_idx: 11\n",
      "0.7395833333333334\n",
      "batch_idx: 12\n",
      "0.7371794871794872\n",
      "batch_idx: 13\n",
      "0.7440476190476191\n",
      "batch_idx: 14\n",
      "0.7555555555555555\n",
      "batch_idx: 15\n",
      "0.7526041666666666\n",
      "batch_idx: 16\n",
      "0.7549019607843137\n",
      "batch_idx: 17\n",
      "0.7546296296296297\n",
      "batch_idx: 18\n",
      "0.756578947368421\n",
      "batch_idx: 19\n",
      "0.75625\n",
      "batch_idx: 20\n",
      "0.7579365079365079\n",
      "batch_idx: 21\n",
      "0.7518939393939394\n",
      "batch_idx: 22\n",
      "0.7572463768115942\n",
      "batch_idx: 23\n",
      "0.75\n",
      "batch_idx: 24\n",
      "0.755\n",
      "Training Epoch: 520, total loss: 43.552665\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.8014705882352942\n",
      "batch_idx: 17\n",
      "0.7962962962962963\n",
      "batch_idx: 18\n",
      "0.7960526315789473\n",
      "batch_idx: 19\n",
      "0.7958333333333333\n",
      "batch_idx: 20\n",
      "0.7916666666666666\n",
      "batch_idx: 21\n",
      "0.7916666666666666\n",
      "batch_idx: 22\n",
      "0.7916666666666666\n",
      "batch_idx: 23\n",
      "0.7899305555555556\n",
      "batch_idx: 24\n",
      "0.7883333333333333\n",
      "Training Epoch: 521, total loss: 42.976341\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.8298611111111112\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8154761904761905\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8070175438596491\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.8075396825396826\n",
      "batch_idx: 21\n",
      "0.803030303030303\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.8003472222222222\n",
      "batch_idx: 24\n",
      "0.8033333333333333\n",
      "Training Epoch: 522, total loss: 42.729464\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7361111111111112\n",
      "batch_idx: 6\n",
      "0.7559523809523809\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7870370370370371\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.7976190476190477\n",
      "batch_idx: 14\n",
      "0.7916666666666666\n",
      "batch_idx: 15\n",
      "0.7838541666666666\n",
      "batch_idx: 16\n",
      "0.7867647058823529\n",
      "batch_idx: 17\n",
      "0.7824074074074074\n",
      "batch_idx: 18\n",
      "0.7872807017543859\n",
      "batch_idx: 19\n",
      "0.7875\n",
      "batch_idx: 20\n",
      "0.7857142857142857\n",
      "batch_idx: 21\n",
      "0.7878787878787878\n",
      "batch_idx: 22\n",
      "0.7934782608695652\n",
      "batch_idx: 23\n",
      "0.7899305555555556\n",
      "batch_idx: 24\n",
      "0.7916666666666666\n",
      "Training Epoch: 523, total loss: 42.953005\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.75\n",
      "batch_idx: 9\n",
      "0.7541666666666667\n",
      "batch_idx: 10\n",
      "0.7537878787878788\n",
      "batch_idx: 11\n",
      "0.7534722222222222\n",
      "batch_idx: 12\n",
      "0.7596153846153846\n",
      "batch_idx: 13\n",
      "0.7619047619047619\n",
      "batch_idx: 14\n",
      "0.7666666666666667\n",
      "batch_idx: 15\n",
      "0.765625\n",
      "batch_idx: 16\n",
      "0.7671568627450981\n",
      "batch_idx: 17\n",
      "0.7662037037037037\n",
      "batch_idx: 18\n",
      "0.7675438596491229\n",
      "batch_idx: 19\n",
      "0.7729166666666667\n",
      "batch_idx: 20\n",
      "0.7698412698412699\n",
      "batch_idx: 21\n",
      "0.7765151515151515\n",
      "batch_idx: 22\n",
      "0.7644927536231884\n",
      "batch_idx: 23\n",
      "0.7638888888888888\n",
      "batch_idx: 24\n",
      "0.765\n",
      "Training Epoch: 524, total loss: 43.377359\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.7777777777777778\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7738095238095238\n",
      "batch_idx: 14\n",
      "0.775\n",
      "batch_idx: 15\n",
      "0.7630208333333334\n",
      "batch_idx: 16\n",
      "0.7573529411764706\n",
      "batch_idx: 17\n",
      "0.7546296296296297\n",
      "batch_idx: 18\n",
      "0.75\n",
      "batch_idx: 19\n",
      "0.7520833333333333\n",
      "batch_idx: 20\n",
      "0.753968253968254\n",
      "batch_idx: 21\n",
      "0.7575757575757576\n",
      "batch_idx: 22\n",
      "0.7518115942028986\n",
      "batch_idx: 23\n",
      "0.75\n",
      "batch_idx: 24\n",
      "0.7533333333333333\n",
      "Training Epoch: 525, total loss: 43.621430\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.725\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7731481481481481\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.7788461538461539\n",
      "batch_idx: 13\n",
      "0.7827380952380952\n",
      "batch_idx: 14\n",
      "0.775\n",
      "batch_idx: 15\n",
      "0.7838541666666666\n",
      "batch_idx: 16\n",
      "0.7892156862745098\n",
      "batch_idx: 17\n",
      "0.7824074074074074\n",
      "batch_idx: 18\n",
      "0.7828947368421053\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7817460317460317\n",
      "batch_idx: 21\n",
      "0.7840909090909091\n",
      "batch_idx: 22\n",
      "0.7898550724637681\n",
      "batch_idx: 23\n",
      "0.7934027777777778\n",
      "batch_idx: 24\n",
      "0.7883333333333333\n",
      "Training Epoch: 526, total loss: 42.988788\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.7440476190476191\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7638888888888888\n",
      "batch_idx: 9\n",
      "0.7416666666666667\n",
      "batch_idx: 10\n",
      "0.7310606060606061\n",
      "batch_idx: 11\n",
      "0.7361111111111112\n",
      "batch_idx: 12\n",
      "0.7467948717948718\n",
      "batch_idx: 13\n",
      "0.7529761904761905\n",
      "batch_idx: 14\n",
      "0.7472222222222222\n",
      "batch_idx: 15\n",
      "0.7473958333333334\n",
      "batch_idx: 16\n",
      "0.7573529411764706\n",
      "batch_idx: 17\n",
      "0.7615740740740741\n",
      "batch_idx: 18\n",
      "0.7697368421052632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 19\n",
      "0.7729166666666667\n",
      "batch_idx: 20\n",
      "0.7757936507936508\n",
      "batch_idx: 21\n",
      "0.7765151515151515\n",
      "batch_idx: 22\n",
      "0.7735507246376812\n",
      "batch_idx: 23\n",
      "0.7725694444444444\n",
      "batch_idx: 24\n",
      "0.7733333333333333\n",
      "Training Epoch: 527, total loss: 43.375779\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.90625\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.7976190476190477\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.8063725490196079\n",
      "batch_idx: 17\n",
      "0.8101851851851852\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.8208333333333333\n",
      "batch_idx: 20\n",
      "0.8174603174603174\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.8134057971014492\n",
      "batch_idx: 23\n",
      "0.8125\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 528, total loss: 42.634320\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7976190476190477\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.7941176470588235\n",
      "batch_idx: 17\n",
      "0.7870370370370371\n",
      "batch_idx: 18\n",
      "0.7828947368421053\n",
      "batch_idx: 19\n",
      "0.7791666666666667\n",
      "batch_idx: 20\n",
      "0.7857142857142857\n",
      "batch_idx: 21\n",
      "0.7878787878787878\n",
      "batch_idx: 22\n",
      "0.7898550724637681\n",
      "batch_idx: 23\n",
      "0.78125\n",
      "batch_idx: 24\n",
      "0.7816666666666666\n",
      "Training Epoch: 529, total loss: 43.119919\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.782051282051282\n",
      "batch_idx: 13\n",
      "0.7916666666666666\n",
      "batch_idx: 14\n",
      "0.7916666666666666\n",
      "batch_idx: 15\n",
      "0.7916666666666666\n",
      "batch_idx: 16\n",
      "0.7892156862745098\n",
      "batch_idx: 17\n",
      "0.7870370370370371\n",
      "batch_idx: 18\n",
      "0.7850877192982456\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7857142857142857\n",
      "batch_idx: 21\n",
      "0.7897727272727273\n",
      "batch_idx: 22\n",
      "0.7898550724637681\n",
      "batch_idx: 23\n",
      "0.7951388888888888\n",
      "batch_idx: 24\n",
      "0.7933333333333333\n",
      "Training Epoch: 530, total loss: 42.936302\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7708333333333334\n",
      "batch_idx: 10\n",
      "0.7765151515151515\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.7724358974358975\n",
      "batch_idx: 13\n",
      "0.7559523809523809\n",
      "batch_idx: 14\n",
      "0.75\n",
      "batch_idx: 15\n",
      "0.75\n",
      "batch_idx: 16\n",
      "0.7426470588235294\n",
      "batch_idx: 17\n",
      "0.75\n",
      "batch_idx: 18\n",
      "0.75\n",
      "batch_idx: 19\n",
      "0.7520833333333333\n",
      "batch_idx: 20\n",
      "0.751984126984127\n",
      "batch_idx: 21\n",
      "0.7556818181818182\n",
      "batch_idx: 22\n",
      "0.7554347826086957\n",
      "batch_idx: 23\n",
      "0.7638888888888888\n",
      "batch_idx: 24\n",
      "0.7583333333333333\n",
      "Training Epoch: 531, total loss: 43.623052\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7767857142857143\n",
      "batch_idx: 14\n",
      "0.7833333333333333\n",
      "batch_idx: 15\n",
      "0.7786458333333334\n",
      "batch_idx: 16\n",
      "0.7745098039215687\n",
      "batch_idx: 17\n",
      "0.7685185185185185\n",
      "batch_idx: 18\n",
      "0.7763157894736842\n",
      "batch_idx: 19\n",
      "0.7791666666666667\n",
      "batch_idx: 20\n",
      "0.7738095238095238\n",
      "batch_idx: 21\n",
      "0.7708333333333334\n",
      "batch_idx: 22\n",
      "0.7735507246376812\n",
      "batch_idx: 23\n",
      "0.7725694444444444\n",
      "batch_idx: 24\n",
      "0.7733333333333333\n",
      "Training Epoch: 532, total loss: 43.293508\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7916666666666666\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.8020833333333334\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8135964912280702\n",
      "batch_idx: 19\n",
      "0.8104166666666667\n",
      "batch_idx: 20\n",
      "0.8115079365079365\n",
      "batch_idx: 21\n",
      "0.8162878787878788\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8055555555555556\n",
      "batch_idx: 24\n",
      "0.7983333333333333\n",
      "Training Epoch: 533, total loss: 42.778933\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8055555555555556\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.8083333333333333\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.8087121212121212\n",
      "batch_idx: 22\n",
      "0.8097826086956522\n",
      "batch_idx: 23\n",
      "0.8125\n",
      "batch_idx: 24\n",
      "0.81\n",
      "Training Epoch: 534, total loss: 42.718105\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7724358974358975\n",
      "batch_idx: 13\n",
      "0.7589285714285714\n",
      "batch_idx: 14\n",
      "0.7611111111111111\n",
      "batch_idx: 15\n",
      "0.7682291666666666\n",
      "batch_idx: 16\n",
      "0.7720588235294118\n",
      "batch_idx: 17\n",
      "0.7777777777777778\n",
      "batch_idx: 18\n",
      "0.7763157894736842\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7837301587301587\n",
      "batch_idx: 21\n",
      "0.7840909090909091\n",
      "batch_idx: 22\n",
      "0.7807971014492754\n",
      "batch_idx: 23\n",
      "0.7847222222222222\n",
      "batch_idx: 24\n",
      "0.7833333333333333\n",
      "Training Epoch: 535, total loss: 43.101788\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8269230769230769\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8055555555555556\n",
      "batch_idx: 15\n",
      "0.7994791666666666\n",
      "batch_idx: 16\n",
      "0.8014705882352942\n",
      "batch_idx: 17\n",
      "0.8055555555555556\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.7973484848484849\n",
      "batch_idx: 22\n",
      "0.7916666666666666\n",
      "batch_idx: 23\n",
      "0.7881944444444444\n",
      "batch_idx: 24\n",
      "0.7916666666666666\n",
      "Training Epoch: 536, total loss: 42.863766\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.75\n",
      "batch_idx: 9\n",
      "0.7708333333333334\n",
      "batch_idx: 10\n",
      "0.7878787878787878\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7916666666666666\n",
      "batch_idx: 13\n",
      "0.7857142857142857\n",
      "batch_idx: 14\n",
      "0.7916666666666666\n",
      "batch_idx: 15\n",
      "0.7916666666666666\n",
      "batch_idx: 16\n",
      "0.7892156862745098\n",
      "batch_idx: 17\n",
      "0.7916666666666666\n",
      "batch_idx: 18\n",
      "0.793859649122807\n",
      "batch_idx: 19\n",
      "0.79375\n",
      "batch_idx: 20\n",
      "0.7936507936507936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 21\n",
      "0.7954545454545454\n",
      "batch_idx: 22\n",
      "0.7971014492753623\n",
      "batch_idx: 23\n",
      "0.796875\n",
      "batch_idx: 24\n",
      "0.7883333333333333\n",
      "Training Epoch: 537, total loss: 43.049841\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.7666666666666667\n",
      "batch_idx: 10\n",
      "0.7689393939393939\n",
      "batch_idx: 11\n",
      "0.7534722222222222\n",
      "batch_idx: 12\n",
      "0.7435897435897436\n",
      "batch_idx: 13\n",
      "0.7529761904761905\n",
      "batch_idx: 14\n",
      "0.7444444444444445\n",
      "batch_idx: 15\n",
      "0.7447916666666666\n",
      "batch_idx: 16\n",
      "0.7475490196078431\n",
      "batch_idx: 17\n",
      "0.75\n",
      "batch_idx: 18\n",
      "0.7478070175438597\n",
      "batch_idx: 19\n",
      "0.7375\n",
      "batch_idx: 20\n",
      "0.7400793650793651\n",
      "batch_idx: 21\n",
      "0.7462121212121212\n",
      "batch_idx: 22\n",
      "0.7427536231884058\n",
      "batch_idx: 23\n",
      "0.7465277777777778\n",
      "batch_idx: 24\n",
      "0.7483333333333333\n",
      "Training Epoch: 538, total loss: 43.534234\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7731481481481481\n",
      "batch_idx: 9\n",
      "0.7666666666666667\n",
      "batch_idx: 10\n",
      "0.7651515151515151\n",
      "batch_idx: 11\n",
      "0.7534722222222222\n",
      "batch_idx: 12\n",
      "0.7532051282051282\n",
      "batch_idx: 13\n",
      "0.7619047619047619\n",
      "batch_idx: 14\n",
      "0.7638888888888888\n",
      "batch_idx: 15\n",
      "0.7760416666666666\n",
      "batch_idx: 16\n",
      "0.7769607843137255\n",
      "batch_idx: 17\n",
      "0.7708333333333334\n",
      "batch_idx: 18\n",
      "0.7631578947368421\n",
      "batch_idx: 19\n",
      "0.7645833333333333\n",
      "batch_idx: 20\n",
      "0.7619047619047619\n",
      "batch_idx: 21\n",
      "0.7632575757575758\n",
      "batch_idx: 22\n",
      "0.7590579710144928\n",
      "batch_idx: 23\n",
      "0.7621527777777778\n",
      "batch_idx: 24\n",
      "0.7666666666666667\n",
      "Training Epoch: 539, total loss: 43.463493\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7592592592592593\n",
      "batch_idx: 9\n",
      "0.7708333333333334\n",
      "batch_idx: 10\n",
      "0.7727272727272727\n",
      "batch_idx: 11\n",
      "0.7638888888888888\n",
      "batch_idx: 12\n",
      "0.7660256410256411\n",
      "batch_idx: 13\n",
      "0.7678571428571429\n",
      "batch_idx: 14\n",
      "0.7638888888888888\n",
      "batch_idx: 15\n",
      "0.765625\n",
      "batch_idx: 16\n",
      "0.7598039215686274\n",
      "batch_idx: 17\n",
      "0.7685185185185185\n",
      "batch_idx: 18\n",
      "0.7785087719298246\n",
      "batch_idx: 19\n",
      "0.7770833333333333\n",
      "batch_idx: 20\n",
      "0.7837301587301587\n",
      "batch_idx: 21\n",
      "0.7840909090909091\n",
      "batch_idx: 22\n",
      "0.7880434782608695\n",
      "batch_idx: 23\n",
      "0.7951388888888888\n",
      "batch_idx: 24\n",
      "0.79\n",
      "Training Epoch: 540, total loss: 43.036058\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.7788461538461539\n",
      "batch_idx: 13\n",
      "0.7827380952380952\n",
      "batch_idx: 14\n",
      "0.7722222222222223\n",
      "batch_idx: 15\n",
      "0.78125\n",
      "batch_idx: 16\n",
      "0.7843137254901961\n",
      "batch_idx: 17\n",
      "0.7777777777777778\n",
      "batch_idx: 18\n",
      "0.7719298245614035\n",
      "batch_idx: 19\n",
      "0.76875\n",
      "batch_idx: 20\n",
      "0.7718253968253969\n",
      "batch_idx: 21\n",
      "0.7727272727272727\n",
      "batch_idx: 22\n",
      "0.7753623188405797\n",
      "batch_idx: 23\n",
      "0.7760416666666666\n",
      "batch_idx: 24\n",
      "0.7783333333333333\n",
      "Training Epoch: 541, total loss: 43.300006\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8257575757575758\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8055555555555556\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.8014705882352942\n",
      "batch_idx: 17\n",
      "0.7986111111111112\n",
      "batch_idx: 18\n",
      "0.7916666666666666\n",
      "batch_idx: 19\n",
      "0.7916666666666666\n",
      "batch_idx: 20\n",
      "0.7857142857142857\n",
      "batch_idx: 21\n",
      "0.7803030303030303\n",
      "batch_idx: 22\n",
      "0.7789855072463768\n",
      "batch_idx: 23\n",
      "0.7847222222222222\n",
      "batch_idx: 24\n",
      "0.79\n",
      "Training Epoch: 542, total loss: 43.027935\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.7791666666666667\n",
      "batch_idx: 10\n",
      "0.7878787878787878\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7916666666666666\n",
      "batch_idx: 13\n",
      "0.7827380952380952\n",
      "batch_idx: 14\n",
      "0.7888888888888889\n",
      "batch_idx: 15\n",
      "0.7916666666666666\n",
      "batch_idx: 16\n",
      "0.7892156862745098\n",
      "batch_idx: 17\n",
      "0.7916666666666666\n",
      "batch_idx: 18\n",
      "0.793859649122807\n",
      "batch_idx: 19\n",
      "0.7875\n",
      "batch_idx: 20\n",
      "0.7936507936507936\n",
      "batch_idx: 21\n",
      "0.7973484848484849\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.7951388888888888\n",
      "batch_idx: 24\n",
      "0.7933333333333333\n",
      "Training Epoch: 543, total loss: 42.997797\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.7951388888888888\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.7886904761904762\n",
      "batch_idx: 14\n",
      "0.7833333333333333\n",
      "batch_idx: 15\n",
      "0.7786458333333334\n",
      "batch_idx: 16\n",
      "0.7794117647058824\n",
      "batch_idx: 17\n",
      "0.7754629629629629\n",
      "batch_idx: 18\n",
      "0.7719298245614035\n",
      "batch_idx: 19\n",
      "0.7666666666666667\n",
      "batch_idx: 20\n",
      "0.7619047619047619\n",
      "batch_idx: 21\n",
      "0.7632575757575758\n",
      "batch_idx: 22\n",
      "0.7663043478260869\n",
      "batch_idx: 23\n",
      "0.7638888888888888\n",
      "batch_idx: 24\n",
      "0.7666666666666667\n",
      "Training Epoch: 544, total loss: 43.485698\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7152777777777778\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.7592592592592593\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7613636363636364\n",
      "batch_idx: 11\n",
      "0.7361111111111112\n",
      "batch_idx: 12\n",
      "0.7275641025641025\n",
      "batch_idx: 13\n",
      "0.7291666666666666\n",
      "batch_idx: 14\n",
      "0.7361111111111112\n",
      "batch_idx: 15\n",
      "0.7369791666666666\n",
      "batch_idx: 16\n",
      "0.7401960784313726\n",
      "batch_idx: 17\n",
      "0.7361111111111112\n",
      "batch_idx: 18\n",
      "0.7456140350877193\n",
      "batch_idx: 19\n",
      "0.7416666666666667\n",
      "batch_idx: 20\n",
      "0.7420634920634921\n",
      "batch_idx: 21\n",
      "0.7443181818181818\n",
      "batch_idx: 22\n",
      "0.7445652173913043\n",
      "batch_idx: 23\n",
      "0.75\n",
      "batch_idx: 24\n",
      "0.7466666666666667\n",
      "Training Epoch: 545, total loss: 43.675231\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.6875\n",
      "batch_idx: 6\n",
      "0.7023809523809523\n",
      "batch_idx: 7\n",
      "0.7135416666666666\n",
      "batch_idx: 8\n",
      "0.7268518518518519\n",
      "batch_idx: 9\n",
      "0.7291666666666666\n",
      "batch_idx: 10\n",
      "0.7348484848484849\n",
      "batch_idx: 11\n",
      "0.7395833333333334\n",
      "batch_idx: 12\n",
      "0.75\n",
      "batch_idx: 13\n",
      "0.7410714285714286\n",
      "batch_idx: 14\n",
      "0.75\n",
      "batch_idx: 15\n",
      "0.7526041666666666\n",
      "batch_idx: 16\n",
      "0.7573529411764706\n",
      "batch_idx: 17\n",
      "0.7638888888888888\n",
      "batch_idx: 18\n",
      "0.7675438596491229\n",
      "batch_idx: 19\n",
      "0.7729166666666667\n",
      "batch_idx: 20\n",
      "0.7638888888888888\n",
      "batch_idx: 21\n",
      "0.7689393939393939\n",
      "batch_idx: 22\n",
      "0.7608695652173914\n",
      "batch_idx: 23\n",
      "0.7552083333333334\n",
      "batch_idx: 24\n",
      "0.75\n",
      "Training Epoch: 546, total loss: 43.710846\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7689393939393939\n",
      "batch_idx: 11\n",
      "0.7638888888888888\n",
      "batch_idx: 12\n",
      "0.7532051282051282\n",
      "batch_idx: 13\n",
      "0.7619047619047619\n",
      "batch_idx: 14\n",
      "0.7611111111111111\n",
      "batch_idx: 15\n",
      "0.7630208333333334\n",
      "batch_idx: 16\n",
      "0.7647058823529411\n",
      "batch_idx: 17\n",
      "0.7638888888888888\n",
      "batch_idx: 18\n",
      "0.756578947368421\n",
      "batch_idx: 19\n",
      "0.7583333333333333\n",
      "batch_idx: 20\n",
      "0.7579365079365079\n",
      "batch_idx: 21\n",
      "0.7537878787878788\n",
      "batch_idx: 22\n",
      "0.7518115942028986\n",
      "batch_idx: 23\n",
      "0.7534722222222222\n",
      "batch_idx: 24\n",
      "0.745\n",
      "Training Epoch: 547, total loss: 43.816716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.725\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.7321428571428571\n",
      "batch_idx: 7\n",
      "0.7291666666666666\n",
      "batch_idx: 8\n",
      "0.7175925925925926\n",
      "batch_idx: 9\n",
      "0.7208333333333333\n",
      "batch_idx: 10\n",
      "0.7045454545454546\n",
      "batch_idx: 11\n",
      "0.7048611111111112\n",
      "batch_idx: 12\n",
      "0.7211538461538461\n",
      "batch_idx: 13\n",
      "0.7261904761904762\n",
      "batch_idx: 14\n",
      "0.7277777777777777\n",
      "batch_idx: 15\n",
      "0.7291666666666666\n",
      "batch_idx: 16\n",
      "0.7230392156862745\n",
      "batch_idx: 17\n",
      "0.7245370370370371\n",
      "batch_idx: 18\n",
      "0.7236842105263158\n",
      "batch_idx: 19\n",
      "0.7208333333333333\n",
      "batch_idx: 20\n",
      "0.7242063492063492\n",
      "batch_idx: 21\n",
      "0.7329545454545454\n",
      "batch_idx: 22\n",
      "0.7282608695652174\n",
      "batch_idx: 23\n",
      "0.7309027777777778\n",
      "batch_idx: 24\n",
      "0.7366666666666667\n",
      "Training Epoch: 548, total loss: 43.943771\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7666666666666667\n",
      "batch_idx: 10\n",
      "0.7575757575757576\n",
      "batch_idx: 11\n",
      "0.7604166666666666\n",
      "batch_idx: 12\n",
      "0.7564102564102564\n",
      "batch_idx: 13\n",
      "0.7529761904761905\n",
      "batch_idx: 14\n",
      "0.7555555555555555\n",
      "batch_idx: 15\n",
      "0.7473958333333334\n",
      "batch_idx: 16\n",
      "0.75\n",
      "batch_idx: 17\n",
      "0.7569444444444444\n",
      "batch_idx: 18\n",
      "0.7543859649122807\n",
      "batch_idx: 19\n",
      "0.74375\n",
      "batch_idx: 20\n",
      "0.7341269841269841\n",
      "batch_idx: 21\n",
      "0.7348484848484849\n",
      "batch_idx: 22\n",
      "0.7373188405797102\n",
      "batch_idx: 23\n",
      "0.7361111111111112\n",
      "batch_idx: 24\n",
      "0.7383333333333333\n",
      "Training Epoch: 549, total loss: 44.043505\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7261904761904762\n",
      "batch_idx: 7\n",
      "0.7395833333333334\n",
      "batch_idx: 8\n",
      "0.7546296296296297\n",
      "batch_idx: 9\n",
      "0.7583333333333333\n",
      "batch_idx: 10\n",
      "0.7689393939393939\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.7692307692307693\n",
      "batch_idx: 13\n",
      "0.7648809523809523\n",
      "batch_idx: 14\n",
      "0.7527777777777778\n",
      "batch_idx: 15\n",
      "0.7552083333333334\n",
      "batch_idx: 16\n",
      "0.7549019607843137\n",
      "batch_idx: 17\n",
      "0.7523148148148148\n",
      "batch_idx: 18\n",
      "0.7543859649122807\n",
      "batch_idx: 19\n",
      "0.75625\n",
      "batch_idx: 20\n",
      "0.753968253968254\n",
      "batch_idx: 21\n",
      "0.7556818181818182\n",
      "batch_idx: 22\n",
      "0.7608695652173914\n",
      "batch_idx: 23\n",
      "0.7569444444444444\n",
      "batch_idx: 24\n",
      "0.7566666666666667\n",
      "Training Epoch: 550, total loss: 43.634584\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7886904761904762\n",
      "batch_idx: 14\n",
      "0.7861111111111111\n",
      "batch_idx: 15\n",
      "0.78125\n",
      "batch_idx: 16\n",
      "0.7696078431372549\n",
      "batch_idx: 17\n",
      "0.7731481481481481\n",
      "batch_idx: 18\n",
      "0.7785087719298246\n",
      "batch_idx: 19\n",
      "0.7729166666666667\n",
      "batch_idx: 20\n",
      "0.7718253968253969\n",
      "batch_idx: 21\n",
      "0.7651515151515151\n",
      "batch_idx: 22\n",
      "0.7644927536231884\n",
      "batch_idx: 23\n",
      "0.765625\n",
      "batch_idx: 24\n",
      "0.7633333333333333\n",
      "Training Epoch: 551, total loss: 43.529446\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7870370370370371\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.7777777777777778\n",
      "batch_idx: 12\n",
      "0.7756410256410257\n",
      "batch_idx: 13\n",
      "0.7857142857142857\n",
      "batch_idx: 14\n",
      "0.7805555555555556\n",
      "batch_idx: 15\n",
      "0.7760416666666666\n",
      "batch_idx: 16\n",
      "0.7696078431372549\n",
      "batch_idx: 17\n",
      "0.7685185185185185\n",
      "batch_idx: 18\n",
      "0.7741228070175439\n",
      "batch_idx: 19\n",
      "0.775\n",
      "batch_idx: 20\n",
      "0.7638888888888888\n",
      "batch_idx: 21\n",
      "0.7651515151515151\n",
      "batch_idx: 22\n",
      "0.7626811594202898\n",
      "batch_idx: 23\n",
      "0.7621527777777778\n",
      "batch_idx: 24\n",
      "0.765\n",
      "Training Epoch: 552, total loss: 43.468738\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7638888888888888\n",
      "batch_idx: 9\n",
      "0.7625\n",
      "batch_idx: 10\n",
      "0.7689393939393939\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.7660256410256411\n",
      "batch_idx: 13\n",
      "0.7797619047619048\n",
      "batch_idx: 14\n",
      "0.7666666666666667\n",
      "batch_idx: 15\n",
      "0.7578125\n",
      "batch_idx: 16\n",
      "0.7573529411764706\n",
      "batch_idx: 17\n",
      "0.7615740740740741\n",
      "batch_idx: 18\n",
      "0.7609649122807017\n",
      "batch_idx: 19\n",
      "0.75625\n",
      "batch_idx: 20\n",
      "0.753968253968254\n",
      "batch_idx: 21\n",
      "0.7481060606060606\n",
      "batch_idx: 22\n",
      "0.7445652173913043\n",
      "batch_idx: 23\n",
      "0.75\n",
      "batch_idx: 24\n",
      "0.7516666666666667\n",
      "Training Epoch: 553, total loss: 43.638966\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.6666666666666666\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.7\n",
      "batch_idx: 5\n",
      "0.7013888888888888\n",
      "batch_idx: 6\n",
      "0.7083333333333334\n",
      "batch_idx: 7\n",
      "0.7083333333333334\n",
      "batch_idx: 8\n",
      "0.7175925925925926\n",
      "batch_idx: 9\n",
      "0.7166666666666667\n",
      "batch_idx: 10\n",
      "0.7159090909090909\n",
      "batch_idx: 11\n",
      "0.7291666666666666\n",
      "batch_idx: 12\n",
      "0.7339743589743589\n",
      "batch_idx: 13\n",
      "0.7351190476190477\n",
      "batch_idx: 14\n",
      "0.7416666666666667\n",
      "batch_idx: 15\n",
      "0.7447916666666666\n",
      "batch_idx: 16\n",
      "0.7475490196078431\n",
      "batch_idx: 17\n",
      "0.7453703703703703\n",
      "batch_idx: 18\n",
      "0.756578947368421\n",
      "batch_idx: 19\n",
      "0.7541666666666667\n",
      "batch_idx: 20\n",
      "0.7619047619047619\n",
      "batch_idx: 21\n",
      "0.7651515151515151\n",
      "batch_idx: 22\n",
      "0.7608695652173914\n",
      "batch_idx: 23\n",
      "0.7621527777777778\n",
      "batch_idx: 24\n",
      "0.7633333333333333\n",
      "Training Epoch: 554, total loss: 43.492288\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7638888888888888\n",
      "batch_idx: 9\n",
      "0.7458333333333333\n",
      "batch_idx: 10\n",
      "0.7348484848484849\n",
      "batch_idx: 11\n",
      "0.7465277777777778\n",
      "batch_idx: 12\n",
      "0.75\n",
      "batch_idx: 13\n",
      "0.75\n",
      "batch_idx: 14\n",
      "0.7555555555555555\n",
      "batch_idx: 15\n",
      "0.7552083333333334\n",
      "batch_idx: 16\n",
      "0.7549019607843137\n",
      "batch_idx: 17\n",
      "0.7476851851851852\n",
      "batch_idx: 18\n",
      "0.7521929824561403\n",
      "batch_idx: 19\n",
      "0.75625\n",
      "batch_idx: 20\n",
      "0.7559523809523809\n",
      "batch_idx: 21\n",
      "0.7575757575757576\n",
      "batch_idx: 22\n",
      "0.7626811594202898\n",
      "batch_idx: 23\n",
      "0.7604166666666666\n",
      "batch_idx: 24\n",
      "0.7633333333333333\n",
      "Training Epoch: 555, total loss: 43.456057\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.725\n",
      "batch_idx: 5\n",
      "0.6944444444444444\n",
      "batch_idx: 6\n",
      "0.7023809523809523\n",
      "batch_idx: 7\n",
      "0.6979166666666666\n",
      "batch_idx: 8\n",
      "0.7129629629629629\n",
      "batch_idx: 9\n",
      "0.725\n",
      "batch_idx: 10\n",
      "0.7159090909090909\n",
      "batch_idx: 11\n",
      "0.7222222222222222\n",
      "batch_idx: 12\n",
      "0.7275641025641025\n",
      "batch_idx: 13\n",
      "0.7232142857142857\n",
      "batch_idx: 14\n",
      "0.7277777777777777\n",
      "batch_idx: 15\n",
      "0.734375\n",
      "batch_idx: 16\n",
      "0.7352941176470589\n",
      "batch_idx: 17\n",
      "0.7268518518518519\n",
      "batch_idx: 18\n",
      "0.7324561403508771\n",
      "batch_idx: 19\n",
      "0.7354166666666667\n",
      "batch_idx: 20\n",
      "0.7420634920634921\n",
      "batch_idx: 21\n",
      "0.7367424242424242\n",
      "batch_idx: 22\n",
      "0.7391304347826086\n",
      "batch_idx: 23\n",
      "0.734375\n",
      "batch_idx: 24\n",
      "0.7366666666666667\n",
      "Training Epoch: 556, total loss: 43.936039\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.7638888888888888\n",
      "batch_idx: 12\n",
      "0.7692307692307693\n",
      "batch_idx: 13\n",
      "0.7529761904761905\n",
      "batch_idx: 14\n",
      "0.7666666666666667\n",
      "batch_idx: 15\n",
      "0.7734375\n",
      "batch_idx: 16\n",
      "0.7745098039215687\n",
      "batch_idx: 17\n",
      "0.7754629629629629\n",
      "batch_idx: 18\n",
      "0.7741228070175439\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7738095238095238\n",
      "batch_idx: 21\n",
      "0.7708333333333334\n",
      "batch_idx: 22\n",
      "0.769927536231884\n",
      "batch_idx: 23\n",
      "0.7708333333333334\n",
      "batch_idx: 24\n",
      "0.765\n",
      "Training Epoch: 557, total loss: 43.544275\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7827380952380952\n",
      "batch_idx: 14\n",
      "0.7916666666666666\n",
      "batch_idx: 15\n",
      "0.7890625\n",
      "batch_idx: 16\n",
      "0.7843137254901961\n",
      "batch_idx: 17\n",
      "0.7824074074074074\n",
      "batch_idx: 18\n",
      "0.7785087719298246\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7797619047619048\n",
      "batch_idx: 21\n",
      "0.7821969696969697\n",
      "batch_idx: 22\n",
      "0.7844202898550725\n",
      "batch_idx: 23\n",
      "0.7864583333333334\n",
      "batch_idx: 24\n",
      "0.7883333333333333\n",
      "Training Epoch: 558, total loss: 43.061967\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7395833333333334\n",
      "batch_idx: 8\n",
      "0.7592592592592593\n",
      "batch_idx: 9\n",
      "0.7708333333333334\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7797619047619048\n",
      "batch_idx: 14\n",
      "0.7805555555555556\n",
      "batch_idx: 15\n",
      "0.7864583333333334\n",
      "batch_idx: 16\n",
      "0.7843137254901961\n",
      "batch_idx: 17\n",
      "0.7800925925925926\n",
      "batch_idx: 18\n",
      "0.7785087719298246\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7857142857142857\n",
      "batch_idx: 21\n",
      "0.7859848484848485\n",
      "batch_idx: 22\n",
      "0.7844202898550725\n",
      "batch_idx: 23\n",
      "0.78125\n",
      "batch_idx: 24\n",
      "0.7816666666666666\n",
      "Training Epoch: 559, total loss: 43.097894\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.7708333333333334\n",
      "batch_idx: 10\n",
      "0.7765151515151515\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n",
      "batch_idx: 12\n",
      "0.7756410256410257\n",
      "batch_idx: 13\n",
      "0.7738095238095238\n",
      "batch_idx: 14\n",
      "0.7777777777777778\n",
      "batch_idx: 15\n",
      "0.7708333333333334\n",
      "batch_idx: 16\n",
      "0.7671568627450981\n",
      "batch_idx: 17\n",
      "0.7777777777777778\n",
      "batch_idx: 18\n",
      "0.7828947368421053\n",
      "batch_idx: 19\n",
      "0.7875\n",
      "batch_idx: 20\n",
      "0.7857142857142857\n",
      "batch_idx: 21\n",
      "0.7878787878787878\n",
      "batch_idx: 22\n",
      "0.7898550724637681\n",
      "batch_idx: 23\n",
      "0.796875\n",
      "batch_idx: 24\n",
      "0.8\n",
      "Training Epoch: 560, total loss: 42.765137\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.8\n",
      "batch_idx: 15\n",
      "0.8020833333333334\n",
      "batch_idx: 16\n",
      "0.8063725490196079\n",
      "batch_idx: 17\n",
      "0.7962962962962963\n",
      "batch_idx: 18\n",
      "0.7960526315789473\n",
      "batch_idx: 19\n",
      "0.7979166666666667\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.803030303030303\n",
      "batch_idx: 22\n",
      "0.8097826086956522\n",
      "batch_idx: 23\n",
      "0.8020833333333334\n",
      "batch_idx: 24\n",
      "0.8066666666666666\n",
      "Training Epoch: 561, total loss: 42.711978\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.7361111111111112\n",
      "batch_idx: 9\n",
      "0.7166666666666667\n",
      "batch_idx: 10\n",
      "0.7159090909090909\n",
      "batch_idx: 11\n",
      "0.7361111111111112\n",
      "batch_idx: 12\n",
      "0.7435897435897436\n",
      "batch_idx: 13\n",
      "0.7559523809523809\n",
      "batch_idx: 14\n",
      "0.7583333333333333\n",
      "batch_idx: 15\n",
      "0.7604166666666666\n",
      "batch_idx: 16\n",
      "0.7622549019607843\n",
      "batch_idx: 17\n",
      "0.7638888888888888\n",
      "batch_idx: 18\n",
      "0.7609649122807017\n",
      "batch_idx: 19\n",
      "0.7625\n",
      "batch_idx: 20\n",
      "0.7638888888888888\n",
      "batch_idx: 21\n",
      "0.7670454545454546\n",
      "batch_idx: 22\n",
      "0.769927536231884\n",
      "batch_idx: 23\n",
      "0.7690972222222222\n",
      "batch_idx: 24\n",
      "0.7666666666666667\n",
      "Training Epoch: 562, total loss: 43.477909\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.8004385964912281\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.8015873015873016\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.7952898550724637\n",
      "batch_idx: 23\n",
      "0.7934027777777778\n",
      "batch_idx: 24\n",
      "0.7916666666666666\n",
      "Training Epoch: 563, total loss: 43.086931\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.6875\n",
      "batch_idx: 6\n",
      "0.7083333333333334\n",
      "batch_idx: 7\n",
      "0.7135416666666666\n",
      "batch_idx: 8\n",
      "0.7222222222222222\n",
      "batch_idx: 9\n",
      "0.725\n",
      "batch_idx: 10\n",
      "0.7234848484848485\n",
      "batch_idx: 11\n",
      "0.7361111111111112\n",
      "batch_idx: 12\n",
      "0.7564102564102564\n",
      "batch_idx: 13\n",
      "0.7589285714285714\n",
      "batch_idx: 14\n",
      "0.7472222222222222\n",
      "batch_idx: 15\n",
      "0.7552083333333334\n",
      "batch_idx: 16\n",
      "0.7598039215686274\n",
      "batch_idx: 17\n",
      "0.7592592592592593\n",
      "batch_idx: 18\n",
      "0.7675438596491229\n",
      "batch_idx: 19\n",
      "0.7666666666666667\n",
      "batch_idx: 20\n",
      "0.7638888888888888\n",
      "batch_idx: 21\n",
      "0.7575757575757576\n",
      "batch_idx: 22\n",
      "0.7626811594202898\n",
      "batch_idx: 23\n",
      "0.7638888888888888\n",
      "batch_idx: 24\n",
      "0.7666666666666667\n",
      "Training Epoch: 564, total loss: 43.229889\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8005952380952381\n",
      "batch_idx: 14\n",
      "0.8\n",
      "batch_idx: 15\n",
      "0.7942708333333334\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.7986111111111112\n",
      "batch_idx: 18\n",
      "0.8026315789473685\n",
      "batch_idx: 19\n",
      "0.8041666666666667\n",
      "batch_idx: 20\n",
      "0.8075396825396826\n",
      "batch_idx: 21\n",
      "0.8068181818181818\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8003472222222222\n",
      "batch_idx: 24\n",
      "0.8\n",
      "Training Epoch: 565, total loss: 42.749898\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7666666666666667\n",
      "batch_idx: 10\n",
      "0.7727272727272727\n",
      "batch_idx: 11\n",
      "0.7777777777777778\n",
      "batch_idx: 12\n",
      "0.7724358974358975\n",
      "batch_idx: 13\n",
      "0.7767857142857143\n",
      "batch_idx: 14\n",
      "0.775\n",
      "batch_idx: 15\n",
      "0.7786458333333334\n",
      "batch_idx: 16\n",
      "0.7843137254901961\n",
      "batch_idx: 17\n",
      "0.7893518518518519\n",
      "batch_idx: 18\n",
      "0.793859649122807\n",
      "batch_idx: 19\n",
      "0.79375\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8038194444444444\n",
      "batch_idx: 24\n",
      "0.8\n",
      "Training Epoch: 566, total loss: 42.721790\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8005952380952381\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.8098958333333334\n",
      "batch_idx: 16\n",
      "0.8088235294117647\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.7916666666666666\n",
      "batch_idx: 19\n",
      "0.7854166666666667\n",
      "batch_idx: 20\n",
      "0.7738095238095238\n",
      "batch_idx: 21\n",
      "0.7803030303030303\n",
      "batch_idx: 22\n",
      "0.7807971014492754\n",
      "batch_idx: 23\n",
      "0.7795138888888888\n",
      "batch_idx: 24\n",
      "0.7766666666666666\n",
      "Training Epoch: 567, total loss: 43.209144\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.7361111111111112\n",
      "batch_idx: 6\n",
      "0.7261904761904762\n",
      "batch_idx: 7\n",
      "0.7395833333333334\n",
      "batch_idx: 8\n",
      "0.7268518518518519\n",
      "batch_idx: 9\n",
      "0.7375\n",
      "batch_idx: 10\n",
      "0.7310606060606061\n",
      "batch_idx: 11\n",
      "0.7291666666666666\n",
      "batch_idx: 12\n",
      "0.7243589743589743\n",
      "batch_idx: 13\n",
      "0.7172619047619048\n",
      "batch_idx: 14\n",
      "0.7333333333333333\n",
      "batch_idx: 15\n",
      "0.7317708333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 16\n",
      "0.7377450980392157\n",
      "batch_idx: 17\n",
      "0.7430555555555556\n",
      "batch_idx: 18\n",
      "0.7412280701754386\n",
      "batch_idx: 19\n",
      "0.7354166666666667\n",
      "batch_idx: 20\n",
      "0.7361111111111112\n",
      "batch_idx: 21\n",
      "0.7424242424242424\n",
      "batch_idx: 22\n",
      "0.7427536231884058\n",
      "batch_idx: 23\n",
      "0.7482638888888888\n",
      "batch_idx: 24\n",
      "0.7483333333333333\n",
      "Training Epoch: 568, total loss: 43.699398\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7765151515151515\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.7660256410256411\n",
      "batch_idx: 13\n",
      "0.7708333333333334\n",
      "batch_idx: 14\n",
      "0.775\n",
      "batch_idx: 15\n",
      "0.7708333333333334\n",
      "batch_idx: 16\n",
      "0.7671568627450981\n",
      "batch_idx: 17\n",
      "0.7615740740740741\n",
      "batch_idx: 18\n",
      "0.7653508771929824\n",
      "batch_idx: 19\n",
      "0.7625\n",
      "batch_idx: 20\n",
      "0.7678571428571429\n",
      "batch_idx: 21\n",
      "0.7727272727272727\n",
      "batch_idx: 22\n",
      "0.7717391304347826\n",
      "batch_idx: 23\n",
      "0.7743055555555556\n",
      "batch_idx: 24\n",
      "0.7733333333333333\n",
      "Training Epoch: 569, total loss: 43.373904\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7870370370370371\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7951388888888888\n",
      "batch_idx: 12\n",
      "0.782051282051282\n",
      "batch_idx: 13\n",
      "0.7857142857142857\n",
      "batch_idx: 14\n",
      "0.7777777777777778\n",
      "batch_idx: 15\n",
      "0.7734375\n",
      "batch_idx: 16\n",
      "0.7696078431372549\n",
      "batch_idx: 17\n",
      "0.7777777777777778\n",
      "batch_idx: 18\n",
      "0.7785087719298246\n",
      "batch_idx: 19\n",
      "0.7729166666666667\n",
      "batch_idx: 20\n",
      "0.7698412698412699\n",
      "batch_idx: 21\n",
      "0.7727272727272727\n",
      "batch_idx: 22\n",
      "0.7735507246376812\n",
      "batch_idx: 23\n",
      "0.7690972222222222\n",
      "batch_idx: 24\n",
      "0.77\n",
      "Training Epoch: 570, total loss: 43.356613\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7689393939393939\n",
      "batch_idx: 11\n",
      "0.7743055555555556\n",
      "batch_idx: 12\n",
      "0.782051282051282\n",
      "batch_idx: 13\n",
      "0.7886904761904762\n",
      "batch_idx: 14\n",
      "0.7916666666666666\n",
      "batch_idx: 15\n",
      "0.7942708333333334\n",
      "batch_idx: 16\n",
      "0.8014705882352942\n",
      "batch_idx: 17\n",
      "0.8055555555555556\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.8041666666666667\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.7935606060606061\n",
      "batch_idx: 22\n",
      "0.7934782608695652\n",
      "batch_idx: 23\n",
      "0.7934027777777778\n",
      "batch_idx: 24\n",
      "0.7916666666666666\n",
      "Training Epoch: 571, total loss: 43.007780\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.8\n",
      "batch_idx: 15\n",
      "0.8098958333333334\n",
      "batch_idx: 16\n",
      "0.8063725490196079\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.7916666666666666\n",
      "batch_idx: 19\n",
      "0.7979166666666667\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.7992424242424242\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.8020833333333334\n",
      "batch_idx: 24\n",
      "0.805\n",
      "Training Epoch: 572, total loss: 42.843895\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.8589743589743589\n",
      "batch_idx: 13\n",
      "0.8601190476190477\n",
      "batch_idx: 14\n",
      "0.8527777777777777\n",
      "batch_idx: 15\n",
      "0.8515625\n",
      "batch_idx: 16\n",
      "0.8431372549019608\n",
      "batch_idx: 17\n",
      "0.8472222222222222\n",
      "batch_idx: 18\n",
      "0.8486842105263158\n",
      "batch_idx: 19\n",
      "0.8479166666666667\n",
      "batch_idx: 20\n",
      "0.8492063492063492\n",
      "batch_idx: 21\n",
      "0.8446969696969697\n",
      "batch_idx: 22\n",
      "0.842391304347826\n",
      "batch_idx: 23\n",
      "0.8385416666666666\n",
      "batch_idx: 24\n",
      "0.8316666666666667\n",
      "Training Epoch: 573, total loss: 42.217008\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.90625\n",
      "batch_idx: 4\n",
      "0.8916666666666667\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8388888888888889\n",
      "batch_idx: 15\n",
      "0.8385416666666666\n",
      "batch_idx: 16\n",
      "0.8406862745098039\n",
      "batch_idx: 17\n",
      "0.8425925925925926\n",
      "batch_idx: 18\n",
      "0.8355263157894737\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.8276515151515151\n",
      "batch_idx: 22\n",
      "0.8333333333333334\n",
      "batch_idx: 23\n",
      "0.8229166666666666\n",
      "batch_idx: 24\n",
      "0.8166666666666667\n",
      "Training Epoch: 574, total loss: 42.534462\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8070175438596491\n",
      "batch_idx: 19\n",
      "0.7979166666666667\n",
      "batch_idx: 20\n",
      "0.7996031746031746\n",
      "batch_idx: 21\n",
      "0.7973484848484849\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.8020833333333334\n",
      "batch_idx: 24\n",
      "0.805\n",
      "Training Epoch: 575, total loss: 42.770255\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7440476190476191\n",
      "batch_idx: 7\n",
      "0.7239583333333334\n",
      "batch_idx: 8\n",
      "0.7222222222222222\n",
      "batch_idx: 9\n",
      "0.7375\n",
      "batch_idx: 10\n",
      "0.7424242424242424\n",
      "batch_idx: 11\n",
      "0.7256944444444444\n",
      "batch_idx: 12\n",
      "0.7307692307692307\n",
      "batch_idx: 13\n",
      "0.7380952380952381\n",
      "batch_idx: 14\n",
      "0.7472222222222222\n",
      "batch_idx: 15\n",
      "0.7526041666666666\n",
      "batch_idx: 16\n",
      "0.7549019607843137\n",
      "batch_idx: 17\n",
      "0.7615740740740741\n",
      "batch_idx: 18\n",
      "0.7587719298245614\n",
      "batch_idx: 19\n",
      "0.7604166666666666\n",
      "batch_idx: 20\n",
      "0.7619047619047619\n",
      "batch_idx: 21\n",
      "0.7651515151515151\n",
      "batch_idx: 22\n",
      "0.7626811594202898\n",
      "batch_idx: 23\n",
      "0.7673611111111112\n",
      "batch_idx: 24\n",
      "0.77\n",
      "Training Epoch: 576, total loss: 43.336167\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7731481481481481\n",
      "batch_idx: 9\n",
      "0.7666666666666667\n",
      "batch_idx: 10\n",
      "0.7727272727272727\n",
      "batch_idx: 11\n",
      "0.7638888888888888\n",
      "batch_idx: 12\n",
      "0.7660256410256411\n",
      "batch_idx: 13\n",
      "0.7589285714285714\n",
      "batch_idx: 14\n",
      "0.7583333333333333\n",
      "batch_idx: 15\n",
      "0.7526041666666666\n",
      "batch_idx: 16\n",
      "0.7426470588235294\n",
      "batch_idx: 17\n",
      "0.7523148148148148\n",
      "batch_idx: 18\n",
      "0.7543859649122807\n",
      "batch_idx: 19\n",
      "0.7604166666666666\n",
      "batch_idx: 20\n",
      "0.7599206349206349\n",
      "batch_idx: 21\n",
      "0.7575757575757576\n",
      "batch_idx: 22\n",
      "0.7554347826086957\n",
      "batch_idx: 23\n",
      "0.7569444444444444\n",
      "batch_idx: 24\n",
      "0.7533333333333333\n",
      "Training Epoch: 577, total loss: 43.639040\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7765151515151515\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n",
      "batch_idx: 12\n",
      "0.782051282051282\n",
      "batch_idx: 13\n",
      "0.7857142857142857\n",
      "batch_idx: 14\n",
      "0.7805555555555556\n",
      "batch_idx: 15\n",
      "0.7760416666666666\n",
      "batch_idx: 16\n",
      "0.7818627450980392\n",
      "batch_idx: 17\n",
      "0.7824074074074074\n",
      "batch_idx: 18\n",
      "0.7872807017543859\n",
      "batch_idx: 19\n",
      "0.7875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 20\n",
      "0.7896825396825397\n",
      "batch_idx: 21\n",
      "0.7784090909090909\n",
      "batch_idx: 22\n",
      "0.7771739130434783\n",
      "batch_idx: 23\n",
      "0.7743055555555556\n",
      "batch_idx: 24\n",
      "0.7766666666666666\n",
      "Training Epoch: 578, total loss: 43.422735\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.7743055555555556\n",
      "batch_idx: 12\n",
      "0.7756410256410257\n",
      "batch_idx: 13\n",
      "0.7738095238095238\n",
      "batch_idx: 14\n",
      "0.7805555555555556\n",
      "batch_idx: 15\n",
      "0.78125\n",
      "batch_idx: 16\n",
      "0.7843137254901961\n",
      "batch_idx: 17\n",
      "0.7777777777777778\n",
      "batch_idx: 18\n",
      "0.7785087719298246\n",
      "batch_idx: 19\n",
      "0.775\n",
      "batch_idx: 20\n",
      "0.7777777777777778\n",
      "batch_idx: 21\n",
      "0.7670454545454546\n",
      "batch_idx: 22\n",
      "0.7681159420289855\n",
      "batch_idx: 23\n",
      "0.765625\n",
      "batch_idx: 24\n",
      "0.7683333333333333\n",
      "Training Epoch: 579, total loss: 43.260669\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.7708333333333334\n",
      "batch_idx: 10\n",
      "0.7651515151515151\n",
      "batch_idx: 11\n",
      "0.7604166666666666\n",
      "batch_idx: 12\n",
      "0.7660256410256411\n",
      "batch_idx: 13\n",
      "0.7678571428571429\n",
      "batch_idx: 14\n",
      "0.775\n",
      "batch_idx: 15\n",
      "0.7786458333333334\n",
      "batch_idx: 16\n",
      "0.7818627450980392\n",
      "batch_idx: 17\n",
      "0.7824074074074074\n",
      "batch_idx: 18\n",
      "0.7741228070175439\n",
      "batch_idx: 19\n",
      "0.7645833333333333\n",
      "batch_idx: 20\n",
      "0.7718253968253969\n",
      "batch_idx: 21\n",
      "0.7632575757575758\n",
      "batch_idx: 22\n",
      "0.7626811594202898\n",
      "batch_idx: 23\n",
      "0.7569444444444444\n",
      "batch_idx: 24\n",
      "0.7533333333333333\n",
      "Training Epoch: 580, total loss: 43.579695\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7870370370370371\n",
      "batch_idx: 9\n",
      "0.7708333333333334\n",
      "batch_idx: 10\n",
      "0.7727272727272727\n",
      "batch_idx: 11\n",
      "0.7673611111111112\n",
      "batch_idx: 12\n",
      "0.7628205128205128\n",
      "batch_idx: 13\n",
      "0.7678571428571429\n",
      "batch_idx: 14\n",
      "0.7722222222222223\n",
      "batch_idx: 15\n",
      "0.7760416666666666\n",
      "batch_idx: 16\n",
      "0.7745098039215687\n",
      "batch_idx: 17\n",
      "0.7777777777777778\n",
      "batch_idx: 18\n",
      "0.7807017543859649\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7857142857142857\n",
      "batch_idx: 21\n",
      "0.7840909090909091\n",
      "batch_idx: 22\n",
      "0.7862318840579711\n",
      "batch_idx: 23\n",
      "0.7864583333333334\n",
      "batch_idx: 24\n",
      "0.785\n",
      "Training Epoch: 581, total loss: 43.090610\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.7942708333333334\n",
      "batch_idx: 16\n",
      "0.7916666666666666\n",
      "batch_idx: 17\n",
      "0.7939814814814815\n",
      "batch_idx: 18\n",
      "0.7894736842105263\n",
      "batch_idx: 19\n",
      "0.7958333333333333\n",
      "batch_idx: 20\n",
      "0.7956349206349206\n",
      "batch_idx: 21\n",
      "0.7973484848484849\n",
      "batch_idx: 22\n",
      "0.7934782608695652\n",
      "batch_idx: 23\n",
      "0.7899305555555556\n",
      "batch_idx: 24\n",
      "0.7883333333333333\n",
      "Training Epoch: 582, total loss: 42.912117\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7575757575757576\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.7692307692307693\n",
      "batch_idx: 13\n",
      "0.7648809523809523\n",
      "batch_idx: 14\n",
      "0.7694444444444445\n",
      "batch_idx: 15\n",
      "0.765625\n",
      "batch_idx: 16\n",
      "0.7720588235294118\n",
      "batch_idx: 17\n",
      "0.7638888888888888\n",
      "batch_idx: 18\n",
      "0.7587719298245614\n",
      "batch_idx: 19\n",
      "0.7625\n",
      "batch_idx: 20\n",
      "0.7619047619047619\n",
      "batch_idx: 21\n",
      "0.7670454545454546\n",
      "batch_idx: 22\n",
      "0.7608695652173914\n",
      "batch_idx: 23\n",
      "0.7569444444444444\n",
      "batch_idx: 24\n",
      "0.76\n",
      "Training Epoch: 583, total loss: 43.443789\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7727272727272727\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.7628205128205128\n",
      "batch_idx: 13\n",
      "0.7678571428571429\n",
      "batch_idx: 14\n",
      "0.7694444444444445\n",
      "batch_idx: 15\n",
      "0.765625\n",
      "batch_idx: 16\n",
      "0.7671568627450981\n",
      "batch_idx: 17\n",
      "0.7685185185185185\n",
      "batch_idx: 18\n",
      "0.7741228070175439\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7817460317460317\n",
      "batch_idx: 21\n",
      "0.7784090909090909\n",
      "batch_idx: 22\n",
      "0.7807971014492754\n",
      "batch_idx: 23\n",
      "0.7760416666666666\n",
      "batch_idx: 24\n",
      "0.7733333333333333\n",
      "Training Epoch: 584, total loss: 43.306829\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.7864583333333334\n",
      "batch_idx: 16\n",
      "0.7794117647058824\n",
      "batch_idx: 17\n",
      "0.7800925925925926\n",
      "batch_idx: 18\n",
      "0.7697368421052632\n",
      "batch_idx: 19\n",
      "0.7708333333333334\n",
      "batch_idx: 20\n",
      "0.7718253968253969\n",
      "batch_idx: 21\n",
      "0.7765151515151515\n",
      "batch_idx: 22\n",
      "0.7753623188405797\n",
      "batch_idx: 23\n",
      "0.7743055555555556\n",
      "batch_idx: 24\n",
      "0.77\n",
      "Training Epoch: 585, total loss: 43.298959\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.7638888888888888\n",
      "batch_idx: 9\n",
      "0.7708333333333334\n",
      "batch_idx: 10\n",
      "0.7727272727272727\n",
      "batch_idx: 11\n",
      "0.7777777777777778\n",
      "batch_idx: 12\n",
      "0.7756410256410257\n",
      "batch_idx: 13\n",
      "0.7827380952380952\n",
      "batch_idx: 14\n",
      "0.7777777777777778\n",
      "batch_idx: 15\n",
      "0.7786458333333334\n",
      "batch_idx: 16\n",
      "0.7794117647058824\n",
      "batch_idx: 17\n",
      "0.7708333333333334\n",
      "batch_idx: 18\n",
      "0.7719298245614035\n",
      "batch_idx: 19\n",
      "0.7666666666666667\n",
      "batch_idx: 20\n",
      "0.7678571428571429\n",
      "batch_idx: 21\n",
      "0.7708333333333334\n",
      "batch_idx: 22\n",
      "0.769927536231884\n",
      "batch_idx: 23\n",
      "0.7673611111111112\n",
      "batch_idx: 24\n",
      "0.7633333333333333\n",
      "Training Epoch: 586, total loss: 43.567737\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7083333333333334\n",
      "batch_idx: 6\n",
      "0.7202380952380952\n",
      "batch_idx: 7\n",
      "0.7291666666666666\n",
      "batch_idx: 8\n",
      "0.7222222222222222\n",
      "batch_idx: 9\n",
      "0.7291666666666666\n",
      "batch_idx: 10\n",
      "0.7272727272727273\n",
      "batch_idx: 11\n",
      "0.7395833333333334\n",
      "batch_idx: 12\n",
      "0.7371794871794872\n",
      "batch_idx: 13\n",
      "0.7380952380952381\n",
      "batch_idx: 14\n",
      "0.7444444444444445\n",
      "batch_idx: 15\n",
      "0.75\n",
      "batch_idx: 16\n",
      "0.7573529411764706\n",
      "batch_idx: 17\n",
      "0.7546296296296297\n",
      "batch_idx: 18\n",
      "0.7609649122807017\n",
      "batch_idx: 19\n",
      "0.7604166666666666\n",
      "batch_idx: 20\n",
      "0.7619047619047619\n",
      "batch_idx: 21\n",
      "0.7670454545454546\n",
      "batch_idx: 22\n",
      "0.7771739130434783\n",
      "batch_idx: 23\n",
      "0.7795138888888888\n",
      "batch_idx: 24\n",
      "0.7883333333333333\n",
      "Training Epoch: 587, total loss: 42.937214\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.725\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7731481481481481\n",
      "batch_idx: 9\n",
      "0.7541666666666667\n",
      "batch_idx: 10\n",
      "0.7651515151515151\n",
      "batch_idx: 11\n",
      "0.7638888888888888\n",
      "batch_idx: 12\n",
      "0.7660256410256411\n",
      "batch_idx: 13\n",
      "0.7648809523809523\n",
      "batch_idx: 14\n",
      "0.7666666666666667\n",
      "batch_idx: 15\n",
      "0.7630208333333334\n",
      "batch_idx: 16\n",
      "0.7647058823529411\n",
      "batch_idx: 17\n",
      "0.7731481481481481\n",
      "batch_idx: 18\n",
      "0.7719298245614035\n",
      "batch_idx: 19\n",
      "0.7791666666666667\n",
      "batch_idx: 20\n",
      "0.7857142857142857\n",
      "batch_idx: 21\n",
      "0.7878787878787878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 22\n",
      "0.7844202898550725\n",
      "batch_idx: 23\n",
      "0.7829861111111112\n",
      "batch_idx: 24\n",
      "0.78\n",
      "Training Epoch: 588, total loss: 43.140850\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7916666666666666\n",
      "batch_idx: 13\n",
      "0.7916666666666666\n",
      "batch_idx: 14\n",
      "0.7944444444444444\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.8101851851851852\n",
      "batch_idx: 18\n",
      "0.8026315789473685\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.7916666666666666\n",
      "batch_idx: 21\n",
      "0.7878787878787878\n",
      "batch_idx: 22\n",
      "0.7880434782608695\n",
      "batch_idx: 23\n",
      "0.7916666666666666\n",
      "batch_idx: 24\n",
      "0.7983333333333333\n",
      "Training Epoch: 589, total loss: 42.714003\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.7708333333333334\n",
      "batch_idx: 10\n",
      "0.7689393939393939\n",
      "batch_idx: 11\n",
      "0.7673611111111112\n",
      "batch_idx: 12\n",
      "0.7596153846153846\n",
      "batch_idx: 13\n",
      "0.7619047619047619\n",
      "batch_idx: 14\n",
      "0.7583333333333333\n",
      "batch_idx: 15\n",
      "0.7630208333333334\n",
      "batch_idx: 16\n",
      "0.7647058823529411\n",
      "batch_idx: 17\n",
      "0.7731481481481481\n",
      "batch_idx: 18\n",
      "0.7719298245614035\n",
      "batch_idx: 19\n",
      "0.7708333333333334\n",
      "batch_idx: 20\n",
      "0.7698412698412699\n",
      "batch_idx: 21\n",
      "0.7670454545454546\n",
      "batch_idx: 22\n",
      "0.7717391304347826\n",
      "batch_idx: 23\n",
      "0.7673611111111112\n",
      "batch_idx: 24\n",
      "0.7666666666666667\n",
      "Training Epoch: 590, total loss: 43.348707\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7314814814814815\n",
      "batch_idx: 9\n",
      "0.7291666666666666\n",
      "batch_idx: 10\n",
      "0.7348484848484849\n",
      "batch_idx: 11\n",
      "0.7465277777777778\n",
      "batch_idx: 12\n",
      "0.7467948717948718\n",
      "batch_idx: 13\n",
      "0.7529761904761905\n",
      "batch_idx: 14\n",
      "0.75\n",
      "batch_idx: 15\n",
      "0.7473958333333334\n",
      "batch_idx: 16\n",
      "0.7450980392156863\n",
      "batch_idx: 17\n",
      "0.75\n",
      "batch_idx: 18\n",
      "0.7478070175438597\n",
      "batch_idx: 19\n",
      "0.7479166666666667\n",
      "batch_idx: 20\n",
      "0.7559523809523809\n",
      "batch_idx: 21\n",
      "0.7537878787878788\n",
      "batch_idx: 22\n",
      "0.7590579710144928\n",
      "batch_idx: 23\n",
      "0.7638888888888888\n",
      "batch_idx: 24\n",
      "0.7666666666666667\n",
      "Training Epoch: 591, total loss: 43.614858\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.7916666666666666\n",
      "batch_idx: 15\n",
      "0.7916666666666666\n",
      "batch_idx: 16\n",
      "0.7892156862745098\n",
      "batch_idx: 17\n",
      "0.7824074074074074\n",
      "batch_idx: 18\n",
      "0.7828947368421053\n",
      "batch_idx: 19\n",
      "0.7791666666666667\n",
      "batch_idx: 20\n",
      "0.7757936507936508\n",
      "batch_idx: 21\n",
      "0.7784090909090909\n",
      "batch_idx: 22\n",
      "0.7771739130434783\n",
      "batch_idx: 23\n",
      "0.7829861111111112\n",
      "batch_idx: 24\n",
      "0.785\n",
      "Training Epoch: 592, total loss: 42.993373\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.796875\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.7979166666666667\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.8087121212121212\n",
      "batch_idx: 22\n",
      "0.8097826086956522\n",
      "batch_idx: 23\n",
      "0.8142361111111112\n",
      "batch_idx: 24\n",
      "0.8166666666666667\n",
      "Training Epoch: 593, total loss: 42.539002\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.6979166666666666\n",
      "batch_idx: 4\n",
      "0.7\n",
      "batch_idx: 5\n",
      "0.7152777777777778\n",
      "batch_idx: 6\n",
      "0.7261904761904762\n",
      "batch_idx: 7\n",
      "0.71875\n",
      "batch_idx: 8\n",
      "0.7175925925925926\n",
      "batch_idx: 9\n",
      "0.725\n",
      "batch_idx: 10\n",
      "0.7272727272727273\n",
      "batch_idx: 11\n",
      "0.7291666666666666\n",
      "batch_idx: 12\n",
      "0.7339743589743589\n",
      "batch_idx: 13\n",
      "0.7232142857142857\n",
      "batch_idx: 14\n",
      "0.7333333333333333\n",
      "batch_idx: 15\n",
      "0.734375\n",
      "batch_idx: 16\n",
      "0.7377450980392157\n",
      "batch_idx: 17\n",
      "0.7407407407407407\n",
      "batch_idx: 18\n",
      "0.7412280701754386\n",
      "batch_idx: 19\n",
      "0.74375\n",
      "batch_idx: 20\n",
      "0.75\n",
      "batch_idx: 21\n",
      "0.7537878787878788\n",
      "batch_idx: 22\n",
      "0.7554347826086957\n",
      "batch_idx: 23\n",
      "0.7517361111111112\n",
      "batch_idx: 24\n",
      "0.7533333333333333\n",
      "Training Epoch: 594, total loss: 43.605159\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.625\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7261904761904762\n",
      "batch_idx: 7\n",
      "0.7291666666666666\n",
      "batch_idx: 8\n",
      "0.7453703703703703\n",
      "batch_idx: 9\n",
      "0.7375\n",
      "batch_idx: 10\n",
      "0.7424242424242424\n",
      "batch_idx: 11\n",
      "0.7534722222222222\n",
      "batch_idx: 12\n",
      "0.7596153846153846\n",
      "batch_idx: 13\n",
      "0.7648809523809523\n",
      "batch_idx: 14\n",
      "0.7666666666666667\n",
      "batch_idx: 15\n",
      "0.7734375\n",
      "batch_idx: 16\n",
      "0.7769607843137255\n",
      "batch_idx: 17\n",
      "0.7754629629629629\n",
      "batch_idx: 18\n",
      "0.7807017543859649\n",
      "batch_idx: 19\n",
      "0.775\n",
      "batch_idx: 20\n",
      "0.7797619047619048\n",
      "batch_idx: 21\n",
      "0.7821969696969697\n",
      "batch_idx: 22\n",
      "0.7807971014492754\n",
      "batch_idx: 23\n",
      "0.7864583333333334\n",
      "batch_idx: 24\n",
      "0.7816666666666666\n",
      "Training Epoch: 595, total loss: 43.159012\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8135964912280702\n",
      "batch_idx: 19\n",
      "0.8083333333333333\n",
      "batch_idx: 20\n",
      "0.8075396825396826\n",
      "batch_idx: 21\n",
      "0.8068181818181818\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8090277777777778\n",
      "batch_idx: 24\n",
      "0.8016666666666666\n",
      "Training Epoch: 596, total loss: 42.846711\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.7916666666666666\n",
      "batch_idx: 13\n",
      "0.7946428571428571\n",
      "batch_idx: 14\n",
      "0.7888888888888889\n",
      "batch_idx: 15\n",
      "0.7916666666666666\n",
      "batch_idx: 16\n",
      "0.7867647058823529\n",
      "batch_idx: 17\n",
      "0.7847222222222222\n",
      "batch_idx: 18\n",
      "0.7828947368421053\n",
      "batch_idx: 19\n",
      "0.7833333333333333\n",
      "batch_idx: 20\n",
      "0.7817460317460317\n",
      "batch_idx: 21\n",
      "0.7727272727272727\n",
      "batch_idx: 22\n",
      "0.7717391304347826\n",
      "batch_idx: 23\n",
      "0.7708333333333334\n",
      "batch_idx: 24\n",
      "0.7766666666666666\n",
      "Training Epoch: 597, total loss: 43.155303\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.8005952380952381\n",
      "batch_idx: 14\n",
      "0.8\n",
      "batch_idx: 15\n",
      "0.8020833333333334\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.7916666666666666\n",
      "batch_idx: 18\n",
      "0.7916666666666666\n",
      "batch_idx: 19\n",
      "0.7916666666666666\n",
      "batch_idx: 20\n",
      "0.7857142857142857\n",
      "batch_idx: 21\n",
      "0.7821969696969697\n",
      "batch_idx: 22\n",
      "0.7862318840579711\n",
      "batch_idx: 23\n",
      "0.7899305555555556\n",
      "batch_idx: 24\n",
      "0.7866666666666666\n",
      "Training Epoch: 598, total loss: 43.127946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7878787878787878\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7857142857142857\n",
      "batch_idx: 14\n",
      "0.7944444444444444\n",
      "batch_idx: 15\n",
      "0.796875\n",
      "batch_idx: 16\n",
      "0.7916666666666666\n",
      "batch_idx: 17\n",
      "0.7893518518518519\n",
      "batch_idx: 18\n",
      "0.7872807017543859\n",
      "batch_idx: 19\n",
      "0.7895833333333333\n",
      "batch_idx: 20\n",
      "0.7777777777777778\n",
      "batch_idx: 21\n",
      "0.7784090909090909\n",
      "batch_idx: 22\n",
      "0.7789855072463768\n",
      "batch_idx: 23\n",
      "0.7847222222222222\n",
      "batch_idx: 24\n",
      "0.7833333333333333\n",
      "Training Epoch: 599, total loss: 43.072509\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7708333333333334\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7857142857142857\n",
      "batch_idx: 14\n",
      "0.7833333333333333\n",
      "batch_idx: 15\n",
      "0.7942708333333334\n",
      "batch_idx: 16\n",
      "0.7941176470588235\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.7960526315789473\n",
      "batch_idx: 19\n",
      "0.7895833333333333\n",
      "batch_idx: 20\n",
      "0.7956349206349206\n",
      "batch_idx: 21\n",
      "0.7954545454545454\n",
      "batch_idx: 22\n",
      "0.7971014492753623\n",
      "batch_idx: 23\n",
      "0.7986111111111112\n",
      "batch_idx: 24\n",
      "0.79\n",
      "Training Epoch: 600, total loss: 43.122837\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7951388888888888\n",
      "batch_idx: 12\n",
      "0.7948717948717948\n",
      "batch_idx: 13\n",
      "0.7976190476190477\n",
      "batch_idx: 14\n",
      "0.7944444444444444\n",
      "batch_idx: 15\n",
      "0.8020833333333334\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.8004385964912281\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.8154761904761905\n",
      "batch_idx: 21\n",
      "0.8143939393939394\n",
      "batch_idx: 22\n",
      "0.8170289855072463\n",
      "batch_idx: 23\n",
      "0.8125\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 601, total loss: 42.650448\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.9375\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8305555555555556\n",
      "batch_idx: 15\n",
      "0.828125\n",
      "batch_idx: 16\n",
      "0.8357843137254902\n",
      "batch_idx: 17\n",
      "0.8333333333333334\n",
      "batch_idx: 18\n",
      "0.8333333333333334\n",
      "batch_idx: 19\n",
      "0.8291666666666667\n",
      "batch_idx: 20\n",
      "0.8234126984126984\n",
      "batch_idx: 21\n",
      "0.8257575757575758\n",
      "batch_idx: 22\n",
      "0.8242753623188406\n",
      "batch_idx: 23\n",
      "0.8315972222222222\n",
      "batch_idx: 24\n",
      "0.83\n",
      "Training Epoch: 602, total loss: 42.303083\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8287037037037037\n",
      "batch_idx: 18\n",
      "0.8289473684210527\n",
      "batch_idx: 19\n",
      "0.8270833333333333\n",
      "batch_idx: 20\n",
      "0.8293650793650794\n",
      "batch_idx: 21\n",
      "0.8276515151515151\n",
      "batch_idx: 22\n",
      "0.822463768115942\n",
      "batch_idx: 23\n",
      "0.8194444444444444\n",
      "batch_idx: 24\n",
      "0.82\n",
      "Training Epoch: 603, total loss: 42.402297\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.9166666666666666\n",
      "batch_idx: 4\n",
      "0.9083333333333333\n",
      "batch_idx: 5\n",
      "0.8958333333333334\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8625\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n",
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.8269230769230769\n",
      "batch_idx: 13\n",
      "0.8244047619047619\n",
      "batch_idx: 14\n",
      "0.8305555555555556\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8308823529411765\n",
      "batch_idx: 17\n",
      "0.8333333333333334\n",
      "batch_idx: 18\n",
      "0.8399122807017544\n",
      "batch_idx: 19\n",
      "0.84375\n",
      "batch_idx: 20\n",
      "0.8353174603174603\n",
      "batch_idx: 21\n",
      "0.8276515151515151\n",
      "batch_idx: 22\n",
      "0.8242753623188406\n",
      "batch_idx: 23\n",
      "0.8177083333333334\n",
      "batch_idx: 24\n",
      "0.8166666666666667\n",
      "Training Epoch: 604, total loss: 42.408966\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8088235294117647\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.8145833333333333\n",
      "batch_idx: 20\n",
      "0.8174603174603174\n",
      "batch_idx: 21\n",
      "0.8106060606060606\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8038194444444444\n",
      "batch_idx: 24\n",
      "0.8\n",
      "Training Epoch: 605, total loss: 42.559491\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7727272727272727\n",
      "batch_idx: 11\n",
      "0.7673611111111112\n",
      "batch_idx: 12\n",
      "0.7756410256410257\n",
      "batch_idx: 13\n",
      "0.7767857142857143\n",
      "batch_idx: 14\n",
      "0.7833333333333333\n",
      "batch_idx: 15\n",
      "0.7760416666666666\n",
      "batch_idx: 16\n",
      "0.7818627450980392\n",
      "batch_idx: 17\n",
      "0.7777777777777778\n",
      "batch_idx: 18\n",
      "0.7850877192982456\n",
      "batch_idx: 19\n",
      "0.7833333333333333\n",
      "batch_idx: 20\n",
      "0.7817460317460317\n",
      "batch_idx: 21\n",
      "0.7784090909090909\n",
      "batch_idx: 22\n",
      "0.782608695652174\n",
      "batch_idx: 23\n",
      "0.7864583333333334\n",
      "batch_idx: 24\n",
      "0.785\n",
      "Training Epoch: 606, total loss: 43.175851\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.65625\n",
      "batch_idx: 4\n",
      "0.6833333333333333\n",
      "batch_idx: 5\n",
      "0.7152777777777778\n",
      "batch_idx: 6\n",
      "0.7321428571428571\n",
      "batch_idx: 7\n",
      "0.7447916666666666\n",
      "batch_idx: 8\n",
      "0.7407407407407407\n",
      "batch_idx: 9\n",
      "0.7458333333333333\n",
      "batch_idx: 10\n",
      "0.7575757575757576\n",
      "batch_idx: 11\n",
      "0.7673611111111112\n",
      "batch_idx: 12\n",
      "0.7692307692307693\n",
      "batch_idx: 13\n",
      "0.7738095238095238\n",
      "batch_idx: 14\n",
      "0.7777777777777778\n",
      "batch_idx: 15\n",
      "0.78125\n",
      "batch_idx: 16\n",
      "0.7794117647058824\n",
      "batch_idx: 17\n",
      "0.7777777777777778\n",
      "batch_idx: 18\n",
      "0.7697368421052632\n",
      "batch_idx: 19\n",
      "0.7645833333333333\n",
      "batch_idx: 20\n",
      "0.7599206349206349\n",
      "batch_idx: 21\n",
      "0.7518939393939394\n",
      "batch_idx: 22\n",
      "0.7518115942028986\n",
      "batch_idx: 23\n",
      "0.7482638888888888\n",
      "batch_idx: 24\n",
      "0.745\n",
      "Training Epoch: 607, total loss: 43.789224\n",
      "batch_idx: 0\n",
      "1.0\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8257575757575758\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.8259803921568627\n",
      "batch_idx: 17\n",
      "0.8263888888888888\n",
      "batch_idx: 18\n",
      "0.8245614035087719\n",
      "batch_idx: 19\n",
      "0.8270833333333333\n",
      "batch_idx: 20\n",
      "0.8234126984126984\n",
      "batch_idx: 21\n",
      "0.8125\n",
      "batch_idx: 22\n",
      "0.8097826086956522\n",
      "batch_idx: 23\n",
      "0.8107638888888888\n",
      "batch_idx: 24\n",
      "0.8133333333333334\n",
      "Training Epoch: 608, total loss: 42.551880\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.7625\n",
      "batch_idx: 10\n",
      "0.7651515151515151\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.7532051282051282\n",
      "batch_idx: 13\n",
      "0.7470238095238095\n",
      "batch_idx: 14\n",
      "0.7444444444444445\n",
      "batch_idx: 15\n",
      "0.75\n",
      "batch_idx: 16\n",
      "0.75\n",
      "batch_idx: 17\n",
      "0.7476851851851852\n",
      "batch_idx: 18\n",
      "0.7478070175438597\n",
      "batch_idx: 19\n",
      "0.74375\n",
      "batch_idx: 20\n",
      "0.748015873015873\n",
      "batch_idx: 21\n",
      "0.75\n",
      "batch_idx: 22\n",
      "0.7481884057971014\n",
      "batch_idx: 23\n",
      "0.7482638888888888\n",
      "batch_idx: 24\n",
      "0.755\n",
      "Training Epoch: 609, total loss: 43.573475\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7592592592592593\n",
      "batch_idx: 9\n",
      "0.7583333333333333\n",
      "batch_idx: 10\n",
      "0.7575757575757576\n",
      "batch_idx: 11\n",
      "0.7604166666666666\n",
      "batch_idx: 12\n",
      "0.7596153846153846\n",
      "batch_idx: 13\n",
      "0.7559523809523809\n",
      "batch_idx: 14\n",
      "0.7638888888888888\n",
      "batch_idx: 15\n",
      "0.7604166666666666\n",
      "batch_idx: 16\n",
      "0.7720588235294118\n",
      "batch_idx: 17\n",
      "0.7777777777777778\n",
      "batch_idx: 18\n",
      "0.7719298245614035\n",
      "batch_idx: 19\n",
      "0.7729166666666667\n",
      "batch_idx: 20\n",
      "0.7777777777777778\n",
      "batch_idx: 21\n",
      "0.7821969696969697\n",
      "batch_idx: 22\n",
      "0.7771739130434783\n",
      "batch_idx: 23\n",
      "0.7743055555555556\n",
      "batch_idx: 24\n",
      "0.7733333333333333\n",
      "Training Epoch: 610, total loss: 43.156111\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8154761904761905\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.8014705882352942\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.8083333333333333\n",
      "batch_idx: 20\n",
      "0.8075396825396826\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8003472222222222\n",
      "batch_idx: 24\n",
      "0.8016666666666666\n",
      "Training Epoch: 611, total loss: 42.746766\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7878787878787878\n",
      "batch_idx: 11\n",
      "0.7951388888888888\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7797619047619048\n",
      "batch_idx: 14\n",
      "0.7805555555555556\n",
      "batch_idx: 15\n",
      "0.7786458333333334\n",
      "batch_idx: 16\n",
      "0.7867647058823529\n",
      "batch_idx: 17\n",
      "0.7916666666666666\n",
      "batch_idx: 18\n",
      "0.7894736842105263\n",
      "batch_idx: 19\n",
      "0.7875\n",
      "batch_idx: 20\n",
      "0.7777777777777778\n",
      "batch_idx: 21\n",
      "0.7746212121212122\n",
      "batch_idx: 22\n",
      "0.7753623188405797\n",
      "batch_idx: 23\n",
      "0.7777777777777778\n",
      "batch_idx: 24\n",
      "0.7733333333333333\n",
      "Training Epoch: 612, total loss: 43.303778\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.6770833333333334\n",
      "batch_idx: 4\n",
      "0.6916666666666667\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.7321428571428571\n",
      "batch_idx: 7\n",
      "0.7552083333333334\n",
      "batch_idx: 8\n",
      "0.7638888888888888\n",
      "batch_idx: 9\n",
      "0.75\n",
      "batch_idx: 10\n",
      "0.75\n",
      "batch_idx: 11\n",
      "0.7465277777777778\n",
      "batch_idx: 12\n",
      "0.75\n",
      "batch_idx: 13\n",
      "0.75\n",
      "batch_idx: 14\n",
      "0.7583333333333333\n",
      "batch_idx: 15\n",
      "0.7630208333333334\n",
      "batch_idx: 16\n",
      "0.7622549019607843\n",
      "batch_idx: 17\n",
      "0.7662037037037037\n",
      "batch_idx: 18\n",
      "0.7631578947368421\n",
      "batch_idx: 19\n",
      "0.7625\n",
      "batch_idx: 20\n",
      "0.7579365079365079\n",
      "batch_idx: 21\n",
      "0.75\n",
      "batch_idx: 22\n",
      "0.7427536231884058\n",
      "batch_idx: 23\n",
      "0.7413194444444444\n",
      "batch_idx: 24\n",
      "0.7433333333333333\n",
      "Training Epoch: 613, total loss: 43.839643\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7651515151515151\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.7692307692307693\n",
      "batch_idx: 13\n",
      "0.7678571428571429\n",
      "batch_idx: 14\n",
      "0.7666666666666667\n",
      "batch_idx: 15\n",
      "0.7682291666666666\n",
      "batch_idx: 16\n",
      "0.7696078431372549\n",
      "batch_idx: 17\n",
      "0.7800925925925926\n",
      "batch_idx: 18\n",
      "0.7807017543859649\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7837301587301587\n",
      "batch_idx: 21\n",
      "0.7821969696969697\n",
      "batch_idx: 22\n",
      "0.7844202898550725\n",
      "batch_idx: 23\n",
      "0.7829861111111112\n",
      "batch_idx: 24\n",
      "0.7866666666666666\n",
      "Training Epoch: 614, total loss: 43.021618\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.75\n",
      "batch_idx: 9\n",
      "0.7458333333333333\n",
      "batch_idx: 10\n",
      "0.7424242424242424\n",
      "batch_idx: 11\n",
      "0.7395833333333334\n",
      "batch_idx: 12\n",
      "0.7371794871794872\n",
      "batch_idx: 13\n",
      "0.7440476190476191\n",
      "batch_idx: 14\n",
      "0.7416666666666667\n",
      "batch_idx: 15\n",
      "0.7473958333333334\n",
      "batch_idx: 16\n",
      "0.7426470588235294\n",
      "batch_idx: 17\n",
      "0.7453703703703703\n",
      "batch_idx: 18\n",
      "0.743421052631579\n",
      "batch_idx: 19\n",
      "0.7458333333333333\n",
      "batch_idx: 20\n",
      "0.75\n",
      "batch_idx: 21\n",
      "0.7518939393939394\n",
      "batch_idx: 22\n",
      "0.7518115942028986\n",
      "batch_idx: 23\n",
      "0.7534722222222222\n",
      "batch_idx: 24\n",
      "0.7566666666666667\n",
      "Training Epoch: 615, total loss: 43.567056\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8301282051282052\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.8333333333333334\n",
      "batch_idx: 15\n",
      "0.828125\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8114035087719298\n",
      "batch_idx: 19\n",
      "0.8041666666666667\n",
      "batch_idx: 20\n",
      "0.8015873015873016\n",
      "batch_idx: 21\n",
      "0.8049242424242424\n",
      "batch_idx: 22\n",
      "0.7971014492753623\n",
      "batch_idx: 23\n",
      "0.8003472222222222\n",
      "batch_idx: 24\n",
      "0.8016666666666666\n",
      "Training Epoch: 616, total loss: 42.904614\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.7592592592592593\n",
      "batch_idx: 9\n",
      "0.7666666666666667\n",
      "batch_idx: 10\n",
      "0.7689393939393939\n",
      "batch_idx: 11\n",
      "0.7743055555555556\n",
      "batch_idx: 12\n",
      "0.7692307692307693\n",
      "batch_idx: 13\n",
      "0.7708333333333334\n",
      "batch_idx: 14\n",
      "0.7722222222222223\n",
      "batch_idx: 15\n",
      "0.7630208333333334\n",
      "batch_idx: 16\n",
      "0.7671568627450981\n",
      "batch_idx: 17\n",
      "0.7662037037037037\n",
      "batch_idx: 18\n",
      "0.7675438596491229\n",
      "batch_idx: 19\n",
      "0.7729166666666667\n",
      "batch_idx: 20\n",
      "0.7757936507936508\n",
      "batch_idx: 21\n",
      "0.7708333333333334\n",
      "batch_idx: 22\n",
      "0.7681159420289855\n",
      "batch_idx: 23\n",
      "0.7621527777777778\n",
      "batch_idx: 24\n",
      "0.765\n",
      "Training Epoch: 617, total loss: 43.477337\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.9375\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.8472222222222222\n",
      "batch_idx: 12\n",
      "0.8301282051282052\n",
      "batch_idx: 13\n",
      "0.8244047619047619\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8063725490196079\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.8083333333333333\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.8106060606060606\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8107638888888888\n",
      "batch_idx: 24\n",
      "0.8083333333333333\n",
      "Training Epoch: 618, total loss: 42.679991\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.7888888888888889\n",
      "batch_idx: 15\n",
      "0.7890625\n",
      "batch_idx: 16\n",
      "0.7916666666666666\n",
      "batch_idx: 17\n",
      "0.7870370370370371\n",
      "batch_idx: 18\n",
      "0.7828947368421053\n",
      "batch_idx: 19\n",
      "0.7770833333333333\n",
      "batch_idx: 20\n",
      "0.7738095238095238\n",
      "batch_idx: 21\n",
      "0.7746212121212122\n",
      "batch_idx: 22\n",
      "0.7735507246376812\n",
      "batch_idx: 23\n",
      "0.7777777777777778\n",
      "batch_idx: 24\n",
      "0.77\n",
      "Training Epoch: 619, total loss: 43.495205\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7592592592592593\n",
      "batch_idx: 9\n",
      "0.7583333333333333\n",
      "batch_idx: 10\n",
      "0.7537878787878788\n",
      "batch_idx: 11\n",
      "0.75\n",
      "batch_idx: 12\n",
      "0.7467948717948718\n",
      "batch_idx: 13\n",
      "0.75\n",
      "batch_idx: 14\n",
      "0.7555555555555555\n",
      "batch_idx: 15\n",
      "0.7526041666666666\n",
      "batch_idx: 16\n",
      "0.7573529411764706\n",
      "batch_idx: 17\n",
      "0.7546296296296297\n",
      "batch_idx: 18\n",
      "0.756578947368421\n",
      "batch_idx: 19\n",
      "0.7583333333333333\n",
      "batch_idx: 20\n",
      "0.7559523809523809\n",
      "batch_idx: 21\n",
      "0.7537878787878788\n",
      "batch_idx: 22\n",
      "0.7536231884057971\n",
      "batch_idx: 23\n",
      "0.7534722222222222\n",
      "batch_idx: 24\n",
      "0.7516666666666667\n",
      "Training Epoch: 620, total loss: 43.618617\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.7994791666666666\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.8004385964912281\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.7996031746031746\n",
      "batch_idx: 21\n",
      "0.7935606060606061\n",
      "batch_idx: 22\n",
      "0.7898550724637681\n",
      "batch_idx: 23\n",
      "0.7881944444444444\n",
      "batch_idx: 24\n",
      "0.79\n",
      "Training Epoch: 621, total loss: 42.966752\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.7552083333333334\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.7791666666666667\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.7743055555555556\n",
      "batch_idx: 12\n",
      "0.7788461538461539\n",
      "batch_idx: 13\n",
      "0.7857142857142857\n",
      "batch_idx: 14\n",
      "0.7861111111111111\n",
      "batch_idx: 15\n",
      "0.7786458333333334\n",
      "batch_idx: 16\n",
      "0.7818627450980392\n",
      "batch_idx: 17\n",
      "0.7847222222222222\n",
      "batch_idx: 18\n",
      "0.7850877192982456\n",
      "batch_idx: 19\n",
      "0.7854166666666667\n",
      "batch_idx: 20\n",
      "0.7896825396825397\n",
      "batch_idx: 21\n",
      "0.7916666666666666\n",
      "batch_idx: 22\n",
      "0.7916666666666666\n",
      "batch_idx: 23\n",
      "0.7881944444444444\n",
      "batch_idx: 24\n",
      "0.7883333333333333\n",
      "Training Epoch: 622, total loss: 42.933258\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7361111111111112\n",
      "batch_idx: 6\n",
      "0.7559523809523809\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.75\n",
      "batch_idx: 9\n",
      "0.7541666666666667\n",
      "batch_idx: 10\n",
      "0.7537878787878788\n",
      "batch_idx: 11\n",
      "0.7569444444444444\n",
      "batch_idx: 12\n",
      "0.7692307692307693\n",
      "batch_idx: 13\n",
      "0.7738095238095238\n",
      "batch_idx: 14\n",
      "0.7722222222222223\n",
      "batch_idx: 15\n",
      "0.7760416666666666\n",
      "batch_idx: 16\n",
      "0.7794117647058824\n",
      "batch_idx: 17\n",
      "0.7824074074074074\n",
      "batch_idx: 18\n",
      "0.7763157894736842\n",
      "batch_idx: 19\n",
      "0.775\n",
      "batch_idx: 20\n",
      "0.7777777777777778\n",
      "batch_idx: 21\n",
      "0.7784090909090909\n",
      "batch_idx: 22\n",
      "0.7807971014492754\n",
      "batch_idx: 23\n",
      "0.7864583333333334\n",
      "batch_idx: 24\n",
      "0.7866666666666666\n",
      "Training Epoch: 623, total loss: 42.959623\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.7361111111111112\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7777777777777778\n",
      "batch_idx: 12\n",
      "0.7724358974358975\n",
      "batch_idx: 13\n",
      "0.7767857142857143\n",
      "batch_idx: 14\n",
      "0.7722222222222223\n",
      "batch_idx: 15\n",
      "0.7760416666666666\n",
      "batch_idx: 16\n",
      "0.7720588235294118\n",
      "batch_idx: 17\n",
      "0.7777777777777778\n",
      "batch_idx: 18\n",
      "0.7719298245614035\n",
      "batch_idx: 19\n",
      "0.7708333333333334\n",
      "batch_idx: 20\n",
      "0.7757936507936508\n",
      "batch_idx: 21\n",
      "0.7765151515151515\n",
      "batch_idx: 22\n",
      "0.7735507246376812\n",
      "batch_idx: 23\n",
      "0.7673611111111112\n",
      "batch_idx: 24\n",
      "0.7616666666666667\n",
      "Training Epoch: 624, total loss: 43.398990\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.725\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.7559523809523809\n",
      "batch_idx: 7\n",
      "0.7552083333333334\n",
      "batch_idx: 8\n",
      "0.7638888888888888\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7857142857142857\n",
      "batch_idx: 14\n",
      "0.775\n",
      "batch_idx: 15\n",
      "0.78125\n",
      "batch_idx: 16\n",
      "0.7867647058823529\n",
      "batch_idx: 17\n",
      "0.7916666666666666\n",
      "batch_idx: 18\n",
      "0.7916666666666666\n",
      "batch_idx: 19\n",
      "0.7854166666666667\n",
      "batch_idx: 20\n",
      "0.7896825396825397\n",
      "batch_idx: 21\n",
      "0.7859848484848485\n",
      "batch_idx: 22\n",
      "0.7880434782608695\n",
      "batch_idx: 23\n",
      "0.7881944444444444\n",
      "batch_idx: 24\n",
      "0.7916666666666666\n",
      "Training Epoch: 625, total loss: 43.077335\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7321428571428571\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.7407407407407407\n",
      "batch_idx: 9\n",
      "0.7416666666666667\n",
      "batch_idx: 10\n",
      "0.7462121212121212\n",
      "batch_idx: 11\n",
      "0.7430555555555556\n",
      "batch_idx: 12\n",
      "0.7467948717948718\n",
      "batch_idx: 13\n",
      "0.7559523809523809\n",
      "batch_idx: 14\n",
      "0.7611111111111111\n",
      "batch_idx: 15\n",
      "0.765625\n",
      "batch_idx: 16\n",
      "0.7696078431372549\n",
      "batch_idx: 17\n",
      "0.7662037037037037\n",
      "batch_idx: 18\n",
      "0.7719298245614035\n",
      "batch_idx: 19\n",
      "0.7708333333333334\n",
      "batch_idx: 20\n",
      "0.7738095238095238\n",
      "batch_idx: 21\n",
      "0.7708333333333334\n",
      "batch_idx: 22\n",
      "0.7717391304347826\n",
      "batch_idx: 23\n",
      "0.765625\n",
      "batch_idx: 24\n",
      "0.7633333333333333\n",
      "Training Epoch: 626, total loss: 43.447812\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.7552083333333334\n",
      "batch_idx: 8\n",
      "0.7592592592592593\n",
      "batch_idx: 9\n",
      "0.7541666666666667\n",
      "batch_idx: 10\n",
      "0.75\n",
      "batch_idx: 11\n",
      "0.7604166666666666\n",
      "batch_idx: 12\n",
      "0.7756410256410257\n",
      "batch_idx: 13\n",
      "0.7738095238095238\n",
      "batch_idx: 14\n",
      "0.7666666666666667\n",
      "batch_idx: 15\n",
      "0.7604166666666666\n",
      "batch_idx: 16\n",
      "0.7475490196078431\n",
      "batch_idx: 17\n",
      "0.7523148148148148\n",
      "batch_idx: 18\n",
      "0.7543859649122807\n",
      "batch_idx: 19\n",
      "0.7541666666666667\n",
      "batch_idx: 20\n",
      "0.7579365079365079\n",
      "batch_idx: 21\n",
      "0.759469696969697\n",
      "batch_idx: 22\n",
      "0.7536231884057971\n",
      "batch_idx: 23\n",
      "0.7534722222222222\n",
      "batch_idx: 24\n",
      "0.7566666666666667\n",
      "Training Epoch: 627, total loss: 43.603291\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8055555555555556\n",
      "batch_idx: 15\n",
      "0.8020833333333334\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.8070175438596491\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.8043478260869565\n",
      "batch_idx: 23\n",
      "0.8003472222222222\n",
      "batch_idx: 24\n",
      "0.7983333333333333\n",
      "Training Epoch: 628, total loss: 42.766961\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8269230769230769\n",
      "batch_idx: 13\n",
      "0.8244047619047619\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.8137254901960784\n",
      "batch_idx: 17\n",
      "0.8101851851851852\n",
      "batch_idx: 18\n",
      "0.8070175438596491\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.7992424242424242\n",
      "batch_idx: 22\n",
      "0.802536231884058\n",
      "batch_idx: 23\n",
      "0.8038194444444444\n",
      "batch_idx: 24\n",
      "0.7983333333333333\n",
      "Training Epoch: 629, total loss: 42.899530\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.7946428571428571\n",
      "batch_idx: 14\n",
      "0.7861111111111111\n",
      "batch_idx: 15\n",
      "0.7838541666666666\n",
      "batch_idx: 16\n",
      "0.7941176470588235\n",
      "batch_idx: 17\n",
      "0.7962962962962963\n",
      "batch_idx: 18\n",
      "0.7916666666666666\n",
      "batch_idx: 19\n",
      "0.7833333333333333\n",
      "batch_idx: 20\n",
      "0.7797619047619048\n",
      "batch_idx: 21\n",
      "0.7859848484848485\n",
      "batch_idx: 22\n",
      "0.7898550724637681\n",
      "batch_idx: 23\n",
      "0.7899305555555556\n",
      "batch_idx: 24\n",
      "0.7966666666666666\n",
      "Training Epoch: 630, total loss: 42.862962\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.9027777777777778\n",
      "batch_idx: 6\n",
      "0.8869047619047619\n",
      "batch_idx: 7\n",
      "0.890625\n",
      "batch_idx: 8\n",
      "0.8703703703703703\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8363095238095238\n",
      "batch_idx: 14\n",
      "0.8416666666666667\n",
      "batch_idx: 15\n",
      "0.84375\n",
      "batch_idx: 16\n",
      "0.8357843137254902\n",
      "batch_idx: 17\n",
      "0.8287037037037037\n",
      "batch_idx: 18\n",
      "0.8179824561403509\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8095238095238095\n",
      "batch_idx: 21\n",
      "0.8087121212121212\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8142361111111112\n",
      "batch_idx: 24\n",
      "0.81\n",
      "Training Epoch: 631, total loss: 42.628573\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.7791666666666667\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.7916666666666666\n",
      "batch_idx: 13\n",
      "0.7857142857142857\n",
      "batch_idx: 14\n",
      "0.7888888888888889\n",
      "batch_idx: 15\n",
      "0.7864583333333334\n",
      "batch_idx: 16\n",
      "0.7794117647058824\n",
      "batch_idx: 17\n",
      "0.7708333333333334\n",
      "batch_idx: 18\n",
      "0.7741228070175439\n",
      "batch_idx: 19\n",
      "0.76875\n",
      "batch_idx: 20\n",
      "0.7658730158730159\n",
      "batch_idx: 21\n",
      "0.7651515151515151\n",
      "batch_idx: 22\n",
      "0.7717391304347826\n",
      "batch_idx: 23\n",
      "0.7725694444444444\n",
      "batch_idx: 24\n",
      "0.7766666666666666\n",
      "Training Epoch: 632, total loss: 43.189722\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8298611111111112\n",
      "batch_idx: 12\n",
      "0.8301282051282052\n",
      "batch_idx: 13\n",
      "0.8244047619047619\n",
      "batch_idx: 14\n",
      "0.8222222222222222\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8101851851851852\n",
      "batch_idx: 18\n",
      "0.8070175438596491\n",
      "batch_idx: 19\n",
      "0.8083333333333333\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.8087121212121212\n",
      "batch_idx: 22\n",
      "0.8097826086956522\n",
      "batch_idx: 23\n",
      "0.8090277777777778\n",
      "batch_idx: 24\n",
      "0.81\n",
      "Training Epoch: 633, total loss: 42.560492\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8269230769230769\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.8101851851851852\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.8104166666666667\n",
      "batch_idx: 20\n",
      "0.8134920634920635\n",
      "batch_idx: 21\n",
      "0.8106060606060606\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8072916666666666\n",
      "batch_idx: 24\n",
      "0.81\n",
      "Training Epoch: 634, total loss: 42.466084\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.8005952380952381\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8186274509803921\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.8245614035087719\n",
      "batch_idx: 19\n",
      "0.8291666666666667\n",
      "batch_idx: 20\n",
      "0.8313492063492064\n",
      "batch_idx: 21\n",
      "0.8257575757575758\n",
      "batch_idx: 22\n",
      "0.8315217391304348\n",
      "batch_idx: 23\n",
      "0.8298611111111112\n",
      "batch_idx: 24\n",
      "0.8316666666666667\n",
      "Training Epoch: 635, total loss: 42.112433\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7638888888888888\n",
      "batch_idx: 9\n",
      "0.7541666666666667\n",
      "batch_idx: 10\n",
      "0.7575757575757576\n",
      "batch_idx: 11\n",
      "0.7673611111111112\n",
      "batch_idx: 12\n",
      "0.7724358974358975\n",
      "batch_idx: 13\n",
      "0.7648809523809523\n",
      "batch_idx: 14\n",
      "0.7666666666666667\n",
      "batch_idx: 15\n",
      "0.765625\n",
      "batch_idx: 16\n",
      "0.7696078431372549\n",
      "batch_idx: 17\n",
      "0.7708333333333334\n",
      "batch_idx: 18\n",
      "0.7741228070175439\n",
      "batch_idx: 19\n",
      "0.7770833333333333\n",
      "batch_idx: 20\n",
      "0.7837301587301587\n",
      "batch_idx: 21\n",
      "0.7840909090909091\n",
      "batch_idx: 22\n",
      "0.7862318840579711\n",
      "batch_idx: 23\n",
      "0.7847222222222222\n",
      "batch_idx: 24\n",
      "0.7883333333333333\n",
      "Training Epoch: 636, total loss: 42.913215\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.7592592592592593\n",
      "batch_idx: 9\n",
      "0.7541666666666667\n",
      "batch_idx: 10\n",
      "0.7537878787878788\n",
      "batch_idx: 11\n",
      "0.7604166666666666\n",
      "batch_idx: 12\n",
      "0.7532051282051282\n",
      "batch_idx: 13\n",
      "0.7440476190476191\n",
      "batch_idx: 14\n",
      "0.75\n",
      "batch_idx: 15\n",
      "0.7578125\n",
      "batch_idx: 16\n",
      "0.7622549019607843\n",
      "batch_idx: 17\n",
      "0.7569444444444444\n",
      "batch_idx: 18\n",
      "0.756578947368421\n",
      "batch_idx: 19\n",
      "0.75625\n",
      "batch_idx: 20\n",
      "0.7579365079365079\n",
      "batch_idx: 21\n",
      "0.7518939393939394\n",
      "batch_idx: 22\n",
      "0.75\n",
      "batch_idx: 23\n",
      "0.7465277777777778\n",
      "batch_idx: 24\n",
      "0.7483333333333333\n",
      "Training Epoch: 637, total loss: 43.732525\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8244047619047619\n",
      "batch_idx: 14\n",
      "0.825\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.8137254901960784\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8267543859649122\n",
      "batch_idx: 19\n",
      "0.83125\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.8238636363636364\n",
      "batch_idx: 22\n",
      "0.8206521739130435\n",
      "batch_idx: 23\n",
      "0.8159722222222222\n",
      "batch_idx: 24\n",
      "0.8183333333333334\n",
      "Training Epoch: 638, total loss: 42.545532\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.8044871794871795\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.8004385964912281\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.803030303030303\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.8020833333333334\n",
      "batch_idx: 24\n",
      "0.8066666666666666\n",
      "Training Epoch: 639, total loss: 42.729506\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7731481481481481\n",
      "batch_idx: 9\n",
      "0.7666666666666667\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.782051282051282\n",
      "batch_idx: 13\n",
      "0.7886904761904762\n",
      "batch_idx: 14\n",
      "0.7916666666666666\n",
      "batch_idx: 15\n",
      "0.7916666666666666\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.7986111111111112\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.79375\n",
      "batch_idx: 20\n",
      "0.7896825396825397\n",
      "batch_idx: 21\n",
      "0.7916666666666666\n",
      "batch_idx: 22\n",
      "0.7916666666666666\n",
      "batch_idx: 23\n",
      "0.796875\n",
      "batch_idx: 24\n",
      "0.8016666666666666\n",
      "Training Epoch: 640, total loss: 42.744717\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n",
      "batch_idx: 12\n",
      "0.7948717948717948\n",
      "batch_idx: 13\n",
      "0.7976190476190477\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.8020833333333334\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.7986111111111112\n",
      "batch_idx: 18\n",
      "0.7960526315789473\n",
      "batch_idx: 19\n",
      "0.7958333333333333\n",
      "batch_idx: 20\n",
      "0.7936507936507936\n",
      "batch_idx: 21\n",
      "0.7954545454545454\n",
      "batch_idx: 22\n",
      "0.7844202898550725\n",
      "batch_idx: 23\n",
      "0.7829861111111112\n",
      "batch_idx: 24\n",
      "0.7866666666666666\n",
      "Training Epoch: 641, total loss: 42.859442\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.7951388888888888\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.8014705882352942\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.8015873015873016\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.8043478260869565\n",
      "batch_idx: 23\n",
      "0.8038194444444444\n",
      "batch_idx: 24\n",
      "0.8066666666666666\n",
      "Training Epoch: 642, total loss: 42.662995\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8257575757575758\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8114035087719298\n",
      "batch_idx: 19\n",
      "0.8104166666666667\n",
      "batch_idx: 20\n",
      "0.8115079365079365\n",
      "batch_idx: 21\n",
      "0.8143939393939394\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8020833333333334\n",
      "batch_idx: 24\n",
      "0.8033333333333333\n",
      "Training Epoch: 643, total loss: 42.629406\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.7546296296296297\n",
      "batch_idx: 9\n",
      "0.7583333333333333\n",
      "batch_idx: 10\n",
      "0.7575757575757576\n",
      "batch_idx: 11\n",
      "0.7743055555555556\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7827380952380952\n",
      "batch_idx: 14\n",
      "0.775\n",
      "batch_idx: 15\n",
      "0.78125\n",
      "batch_idx: 16\n",
      "0.7818627450980392\n",
      "batch_idx: 17\n",
      "0.7824074074074074\n",
      "batch_idx: 18\n",
      "0.7850877192982456\n",
      "batch_idx: 19\n",
      "0.7833333333333333\n",
      "batch_idx: 20\n",
      "0.7837301587301587\n",
      "batch_idx: 21\n",
      "0.7840909090909091\n",
      "batch_idx: 22\n",
      "0.7844202898550725\n",
      "batch_idx: 23\n",
      "0.7881944444444444\n",
      "batch_idx: 24\n",
      "0.7783333333333333\n",
      "Training Epoch: 644, total loss: 43.178715\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.8014705882352942\n",
      "batch_idx: 17\n",
      "0.7939814814814815\n",
      "batch_idx: 18\n",
      "0.7872807017543859\n",
      "batch_idx: 19\n",
      "0.79375\n",
      "batch_idx: 20\n",
      "0.7936507936507936\n",
      "batch_idx: 21\n",
      "0.7954545454545454\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.7986111111111112\n",
      "batch_idx: 24\n",
      "0.7933333333333333\n",
      "Training Epoch: 645, total loss: 42.945418\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7731481481481481\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7765151515151515\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7886904761904762\n",
      "batch_idx: 14\n",
      "0.7916666666666666\n",
      "batch_idx: 15\n",
      "0.7994791666666666\n",
      "batch_idx: 16\n",
      "0.7941176470588235\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.8026315789473685\n",
      "batch_idx: 19\n",
      "0.7958333333333333\n",
      "batch_idx: 20\n",
      "0.8015873015873016\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.7971014492753623\n",
      "batch_idx: 23\n",
      "0.7986111111111112\n",
      "batch_idx: 24\n",
      "0.8016666666666666\n",
      "Training Epoch: 646, total loss: 42.807748\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8063725490196079\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8026315789473685\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.8038194444444444\n",
      "batch_idx: 24\n",
      "0.8033333333333333\n",
      "Training Epoch: 647, total loss: 42.743165\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7727272727272727\n",
      "batch_idx: 11\n",
      "0.7777777777777778\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7797619047619048\n",
      "batch_idx: 14\n",
      "0.7888888888888889\n",
      "batch_idx: 15\n",
      "0.8020833333333334\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.7916666666666666\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.8075396825396826\n",
      "batch_idx: 21\n",
      "0.803030303030303\n",
      "batch_idx: 22\n",
      "0.802536231884058\n",
      "batch_idx: 23\n",
      "0.8020833333333334\n",
      "batch_idx: 24\n",
      "0.805\n",
      "Training Epoch: 648, total loss: 42.725851\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 14\n",
      "0.8\n",
      "batch_idx: 15\n",
      "0.7994791666666666\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.7916666666666666\n",
      "batch_idx: 18\n",
      "0.793859649122807\n",
      "batch_idx: 19\n",
      "0.7979166666666667\n",
      "batch_idx: 20\n",
      "0.7956349206349206\n",
      "batch_idx: 21\n",
      "0.7973484848484849\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.7986111111111112\n",
      "batch_idx: 24\n",
      "0.7983333333333333\n",
      "Training Epoch: 649, total loss: 42.708149\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8044871794871795\n",
      "batch_idx: 13\n",
      "0.7976190476190477\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.7890625\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.7939814814814815\n",
      "batch_idx: 18\n",
      "0.7960526315789473\n",
      "batch_idx: 19\n",
      "0.7979166666666667\n",
      "batch_idx: 20\n",
      "0.7916666666666666\n",
      "batch_idx: 21\n",
      "0.7840909090909091\n",
      "batch_idx: 22\n",
      "0.7880434782608695\n",
      "batch_idx: 23\n",
      "0.7881944444444444\n",
      "batch_idx: 24\n",
      "0.7866666666666666\n",
      "Training Epoch: 650, total loss: 42.918057\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8244047619047619\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8186274509803921\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8041666666666667\n",
      "batch_idx: 20\n",
      "0.8095238095238095\n",
      "batch_idx: 21\n",
      "0.8087121212121212\n",
      "batch_idx: 22\n",
      "0.8097826086956522\n",
      "batch_idx: 23\n",
      "0.8107638888888888\n",
      "batch_idx: 24\n",
      "0.81\n",
      "Training Epoch: 651, total loss: 42.663497\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.7948717948717948\n",
      "batch_idx: 13\n",
      "0.7946428571428571\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.8075396825396826\n",
      "batch_idx: 21\n",
      "0.8068181818181818\n",
      "batch_idx: 22\n",
      "0.8043478260869565\n",
      "batch_idx: 23\n",
      "0.8038194444444444\n",
      "batch_idx: 24\n",
      "0.8083333333333333\n",
      "Training Epoch: 652, total loss: 42.742768\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7765151515151515\n",
      "batch_idx: 11\n",
      "0.7743055555555556\n",
      "batch_idx: 12\n",
      "0.782051282051282\n",
      "batch_idx: 13\n",
      "0.7857142857142857\n",
      "batch_idx: 14\n",
      "0.7916666666666666\n",
      "batch_idx: 15\n",
      "0.7838541666666666\n",
      "batch_idx: 16\n",
      "0.7892156862745098\n",
      "batch_idx: 17\n",
      "0.7893518518518519\n",
      "batch_idx: 18\n",
      "0.7872807017543859\n",
      "batch_idx: 19\n",
      "0.7895833333333333\n",
      "batch_idx: 20\n",
      "0.7876984126984127\n",
      "batch_idx: 21\n",
      "0.7897727272727273\n",
      "batch_idx: 22\n",
      "0.7862318840579711\n",
      "batch_idx: 23\n",
      "0.7847222222222222\n",
      "batch_idx: 24\n",
      "0.7816666666666666\n",
      "Training Epoch: 653, total loss: 43.101697\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7361111111111112\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.7447916666666666\n",
      "batch_idx: 8\n",
      "0.7361111111111112\n",
      "batch_idx: 9\n",
      "0.7416666666666667\n",
      "batch_idx: 10\n",
      "0.7613636363636364\n",
      "batch_idx: 11\n",
      "0.7638888888888888\n",
      "batch_idx: 12\n",
      "0.7692307692307693\n",
      "batch_idx: 13\n",
      "0.7797619047619048\n",
      "batch_idx: 14\n",
      "0.7722222222222223\n",
      "batch_idx: 15\n",
      "0.7734375\n",
      "batch_idx: 16\n",
      "0.7769607843137255\n",
      "batch_idx: 17\n",
      "0.7662037037037037\n",
      "batch_idx: 18\n",
      "0.7741228070175439\n",
      "batch_idx: 19\n",
      "0.775\n",
      "batch_idx: 20\n",
      "0.7757936507936508\n",
      "batch_idx: 21\n",
      "0.7708333333333334\n",
      "batch_idx: 22\n",
      "0.7717391304347826\n",
      "batch_idx: 23\n",
      "0.765625\n",
      "batch_idx: 24\n",
      "0.765\n",
      "Training Epoch: 654, total loss: 43.363545\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.7\n",
      "batch_idx: 5\n",
      "0.7152777777777778\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7361111111111112\n",
      "batch_idx: 9\n",
      "0.7458333333333333\n",
      "batch_idx: 10\n",
      "0.75\n",
      "batch_idx: 11\n",
      "0.7534722222222222\n",
      "batch_idx: 12\n",
      "0.7628205128205128\n",
      "batch_idx: 13\n",
      "0.7678571428571429\n",
      "batch_idx: 14\n",
      "0.775\n",
      "batch_idx: 15\n",
      "0.7838541666666666\n",
      "batch_idx: 16\n",
      "0.7892156862745098\n",
      "batch_idx: 17\n",
      "0.7870370370370371\n",
      "batch_idx: 18\n",
      "0.7894736842105263\n",
      "batch_idx: 19\n",
      "0.7916666666666666\n",
      "batch_idx: 20\n",
      "0.7916666666666666\n",
      "batch_idx: 21\n",
      "0.7859848484848485\n",
      "batch_idx: 22\n",
      "0.7844202898550725\n",
      "batch_idx: 23\n",
      "0.7864583333333334\n",
      "batch_idx: 24\n",
      "0.7816666666666666\n",
      "Training Epoch: 655, total loss: 43.134358\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7878787878787878\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7857142857142857\n",
      "batch_idx: 14\n",
      "0.7805555555555556\n",
      "batch_idx: 15\n",
      "0.7838541666666666\n",
      "batch_idx: 16\n",
      "0.7769607843137255\n",
      "batch_idx: 17\n",
      "0.7800925925925926\n",
      "batch_idx: 18\n",
      "0.7807017543859649\n",
      "batch_idx: 19\n",
      "0.7791666666666667\n",
      "batch_idx: 20\n",
      "0.7817460317460317\n",
      "batch_idx: 21\n",
      "0.7803030303030303\n",
      "batch_idx: 22\n",
      "0.782608695652174\n",
      "batch_idx: 23\n",
      "0.7847222222222222\n",
      "batch_idx: 24\n",
      "0.7866666666666666\n",
      "Training Epoch: 656, total loss: 42.978733\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.8044871794871795\n",
      "batch_idx: 13\n",
      "0.7946428571428571\n",
      "batch_idx: 14\n",
      "0.7916666666666666\n",
      "batch_idx: 15\n",
      "0.7864583333333334\n",
      "batch_idx: 16\n",
      "0.7941176470588235\n",
      "batch_idx: 17\n",
      "0.7893518518518519\n",
      "batch_idx: 18\n",
      "0.793859649122807\n",
      "batch_idx: 19\n",
      "0.7875\n",
      "batch_idx: 20\n",
      "0.7936507936507936\n",
      "batch_idx: 21\n",
      "0.7973484848484849\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8055555555555556\n",
      "batch_idx: 24\n",
      "0.8033333333333333\n",
      "Training Epoch: 657, total loss: 42.852700\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7878787878787878\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8217592592592593\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.8208333333333333\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.8134057971014492\n",
      "batch_idx: 23\n",
      "0.8177083333333334\n",
      "batch_idx: 24\n",
      "0.815\n",
      "Training Epoch: 658, total loss: 42.527067\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8222222222222222\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 16\n",
      "0.8186274509803921\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8135964912280702\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.8068181818181818\n",
      "batch_idx: 22\n",
      "0.8097826086956522\n",
      "batch_idx: 23\n",
      "0.8090277777777778\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 659, total loss: 42.580637\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8245614035087719\n",
      "batch_idx: 19\n",
      "0.8270833333333333\n",
      "batch_idx: 20\n",
      "0.8273809523809523\n",
      "batch_idx: 21\n",
      "0.8257575757575758\n",
      "batch_idx: 22\n",
      "0.8260869565217391\n",
      "batch_idx: 23\n",
      "0.8263888888888888\n",
      "batch_idx: 24\n",
      "0.825\n",
      "Training Epoch: 660, total loss: 42.293886\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7727272727272727\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7886904761904762\n",
      "batch_idx: 14\n",
      "0.7916666666666666\n",
      "batch_idx: 15\n",
      "0.796875\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8135964912280702\n",
      "batch_idx: 19\n",
      "0.8083333333333333\n",
      "batch_idx: 20\n",
      "0.8134920634920635\n",
      "batch_idx: 21\n",
      "0.8068181818181818\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8090277777777778\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 661, total loss: 42.533821\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.7546296296296297\n",
      "batch_idx: 9\n",
      "0.7583333333333333\n",
      "batch_idx: 10\n",
      "0.7613636363636364\n",
      "batch_idx: 11\n",
      "0.7777777777777778\n",
      "batch_idx: 12\n",
      "0.782051282051282\n",
      "batch_idx: 13\n",
      "0.7708333333333334\n",
      "batch_idx: 14\n",
      "0.7722222222222223\n",
      "batch_idx: 15\n",
      "0.7734375\n",
      "batch_idx: 16\n",
      "0.7745098039215687\n",
      "batch_idx: 17\n",
      "0.7708333333333334\n",
      "batch_idx: 18\n",
      "0.7785087719298246\n",
      "batch_idx: 19\n",
      "0.7875\n",
      "batch_idx: 20\n",
      "0.7896825396825397\n",
      "batch_idx: 21\n",
      "0.7878787878787878\n",
      "batch_idx: 22\n",
      "0.7880434782608695\n",
      "batch_idx: 23\n",
      "0.7934027777777778\n",
      "batch_idx: 24\n",
      "0.7866666666666666\n",
      "Training Epoch: 662, total loss: 42.984691\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8137254901960784\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.8166666666666667\n",
      "batch_idx: 20\n",
      "0.8115079365079365\n",
      "batch_idx: 21\n",
      "0.8087121212121212\n",
      "batch_idx: 22\n",
      "0.8079710144927537\n",
      "batch_idx: 23\n",
      "0.8055555555555556\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 663, total loss: 42.589636\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.782051282051282\n",
      "batch_idx: 13\n",
      "0.7827380952380952\n",
      "batch_idx: 14\n",
      "0.7833333333333333\n",
      "batch_idx: 15\n",
      "0.7864583333333334\n",
      "batch_idx: 16\n",
      "0.7843137254901961\n",
      "batch_idx: 17\n",
      "0.7777777777777778\n",
      "batch_idx: 18\n",
      "0.7785087719298246\n",
      "batch_idx: 19\n",
      "0.7729166666666667\n",
      "batch_idx: 20\n",
      "0.7718253968253969\n",
      "batch_idx: 21\n",
      "0.7670454545454546\n",
      "batch_idx: 22\n",
      "0.7717391304347826\n",
      "batch_idx: 23\n",
      "0.7708333333333334\n",
      "batch_idx: 24\n",
      "0.7716666666666666\n",
      "Training Epoch: 664, total loss: 43.292865\n",
      "batch_idx: 0\n",
      "1.0\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.7946428571428571\n",
      "batch_idx: 14\n",
      "0.7916666666666666\n",
      "batch_idx: 15\n",
      "0.7916666666666666\n",
      "batch_idx: 16\n",
      "0.7843137254901961\n",
      "batch_idx: 17\n",
      "0.7777777777777778\n",
      "batch_idx: 18\n",
      "0.7785087719298246\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7817460317460317\n",
      "batch_idx: 21\n",
      "0.7784090909090909\n",
      "batch_idx: 22\n",
      "0.7771739130434783\n",
      "batch_idx: 23\n",
      "0.7777777777777778\n",
      "batch_idx: 24\n",
      "0.78\n",
      "Training Epoch: 665, total loss: 43.200476\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7916666666666666\n",
      "batch_idx: 14\n",
      "0.7916666666666666\n",
      "batch_idx: 15\n",
      "0.7916666666666666\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.7960526315789473\n",
      "batch_idx: 19\n",
      "0.7979166666666667\n",
      "batch_idx: 20\n",
      "0.7956349206349206\n",
      "batch_idx: 21\n",
      "0.7859848484848485\n",
      "batch_idx: 22\n",
      "0.782608695652174\n",
      "batch_idx: 23\n",
      "0.78125\n",
      "batch_idx: 24\n",
      "0.7766666666666666\n",
      "Training Epoch: 666, total loss: 43.243201\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.7447916666666666\n",
      "batch_idx: 8\n",
      "0.7546296296296297\n",
      "batch_idx: 9\n",
      "0.7583333333333333\n",
      "batch_idx: 10\n",
      "0.7613636363636364\n",
      "batch_idx: 11\n",
      "0.7743055555555556\n",
      "batch_idx: 12\n",
      "0.7724358974358975\n",
      "batch_idx: 13\n",
      "0.7797619047619048\n",
      "batch_idx: 14\n",
      "0.7833333333333333\n",
      "batch_idx: 15\n",
      "0.78125\n",
      "batch_idx: 16\n",
      "0.7818627450980392\n",
      "batch_idx: 17\n",
      "0.7800925925925926\n",
      "batch_idx: 18\n",
      "0.7850877192982456\n",
      "batch_idx: 19\n",
      "0.7895833333333333\n",
      "batch_idx: 20\n",
      "0.7817460317460317\n",
      "batch_idx: 21\n",
      "0.7821969696969697\n",
      "batch_idx: 22\n",
      "0.7844202898550725\n",
      "batch_idx: 23\n",
      "0.7881944444444444\n",
      "batch_idx: 24\n",
      "0.7933333333333333\n",
      "Training Epoch: 667, total loss: 42.891053\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.7791666666666667\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7976190476190477\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.7942708333333334\n",
      "batch_idx: 16\n",
      "0.7916666666666666\n",
      "batch_idx: 17\n",
      "0.7893518518518519\n",
      "batch_idx: 18\n",
      "0.7872807017543859\n",
      "batch_idx: 19\n",
      "0.7916666666666666\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.7934782608695652\n",
      "batch_idx: 23\n",
      "0.7916666666666666\n",
      "batch_idx: 24\n",
      "0.7883333333333333\n",
      "Training Epoch: 668, total loss: 42.956158\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.8004385964912281\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.7973484848484849\n",
      "batch_idx: 22\n",
      "0.7971014492753623\n",
      "batch_idx: 23\n",
      "0.7951388888888888\n",
      "batch_idx: 24\n",
      "0.7966666666666666\n",
      "Training Epoch: 669, total loss: 42.885159\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7727272727272727\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.7724358974358975\n",
      "batch_idx: 13\n",
      "0.7797619047619048\n",
      "batch_idx: 14\n",
      "0.775\n",
      "batch_idx: 15\n",
      "0.7734375\n",
      "batch_idx: 16\n",
      "0.7696078431372549\n",
      "batch_idx: 17\n",
      "0.7731481481481481\n",
      "batch_idx: 18\n",
      "0.7785087719298246\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7757936507936508\n",
      "batch_idx: 21\n",
      "0.7840909090909091\n",
      "batch_idx: 22\n",
      "0.7880434782608695\n",
      "batch_idx: 23\n",
      "0.7916666666666666\n",
      "batch_idx: 24\n",
      "0.7933333333333333\n",
      "Training Epoch: 670, total loss: 42.874172\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.7788461538461539\n",
      "batch_idx: 13\n",
      "0.7886904761904762\n",
      "batch_idx: 14\n",
      "0.7861111111111111\n",
      "batch_idx: 15\n",
      "0.7916666666666666\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.8026315789473685\n",
      "batch_idx: 19\n",
      "0.7979166666666667\n",
      "batch_idx: 20\n",
      "0.7996031746031746\n",
      "batch_idx: 21\n",
      "0.7973484848484849\n",
      "batch_idx: 22\n",
      "0.7952898550724637\n",
      "batch_idx: 23\n",
      "0.7951388888888888\n",
      "batch_idx: 24\n",
      "0.8\n",
      "Training Epoch: 671, total loss: 42.851073\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.7827380952380952\n",
      "batch_idx: 14\n",
      "0.7833333333333333\n",
      "batch_idx: 15\n",
      "0.7786458333333334\n",
      "batch_idx: 16\n",
      "0.7843137254901961\n",
      "batch_idx: 17\n",
      "0.7800925925925926\n",
      "batch_idx: 18\n",
      "0.7719298245614035\n",
      "batch_idx: 19\n",
      "0.7729166666666667\n",
      "batch_idx: 20\n",
      "0.7698412698412699\n",
      "batch_idx: 21\n",
      "0.7708333333333334\n",
      "batch_idx: 22\n",
      "0.7717391304347826\n",
      "batch_idx: 23\n",
      "0.7760416666666666\n",
      "batch_idx: 24\n",
      "0.7733333333333333\n",
      "Training Epoch: 672, total loss: 43.053118\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7689393939393939\n",
      "batch_idx: 11\n",
      "0.7777777777777778\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7916666666666666\n",
      "batch_idx: 14\n",
      "0.8\n",
      "batch_idx: 15\n",
      "0.796875\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.7954545454545454\n",
      "batch_idx: 22\n",
      "0.7934782608695652\n",
      "batch_idx: 23\n",
      "0.7934027777777778\n",
      "batch_idx: 24\n",
      "0.7883333333333333\n",
      "Training Epoch: 673, total loss: 42.882318\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7261904761904762\n",
      "batch_idx: 7\n",
      "0.7239583333333334\n",
      "batch_idx: 8\n",
      "0.7453703703703703\n",
      "batch_idx: 9\n",
      "0.7333333333333333\n",
      "batch_idx: 10\n",
      "0.7310606060606061\n",
      "batch_idx: 11\n",
      "0.7361111111111112\n",
      "batch_idx: 12\n",
      "0.7403846153846154\n",
      "batch_idx: 13\n",
      "0.7470238095238095\n",
      "batch_idx: 14\n",
      "0.7416666666666667\n",
      "batch_idx: 15\n",
      "0.7473958333333334\n",
      "batch_idx: 16\n",
      "0.7549019607843137\n",
      "batch_idx: 17\n",
      "0.7546296296296297\n",
      "batch_idx: 18\n",
      "0.7609649122807017\n",
      "batch_idx: 19\n",
      "0.7604166666666666\n",
      "batch_idx: 20\n",
      "0.7619047619047619\n",
      "batch_idx: 21\n",
      "0.7632575757575758\n",
      "batch_idx: 22\n",
      "0.7626811594202898\n",
      "batch_idx: 23\n",
      "0.7708333333333334\n",
      "batch_idx: 24\n",
      "0.7716666666666666\n",
      "Training Epoch: 674, total loss: 43.330384\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.7916666666666666\n",
      "batch_idx: 13\n",
      "0.7827380952380952\n",
      "batch_idx: 14\n",
      "0.7861111111111111\n",
      "batch_idx: 15\n",
      "0.7864583333333334\n",
      "batch_idx: 16\n",
      "0.7892156862745098\n",
      "batch_idx: 17\n",
      "0.7962962962962963\n",
      "batch_idx: 18\n",
      "0.7916666666666666\n",
      "batch_idx: 19\n",
      "0.7958333333333333\n",
      "batch_idx: 20\n",
      "0.7956349206349206\n",
      "batch_idx: 21\n",
      "0.7954545454545454\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.8003472222222222\n",
      "batch_idx: 24\n",
      "0.7966666666666666\n",
      "Training Epoch: 675, total loss: 42.892175\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7440476190476191\n",
      "batch_idx: 7\n",
      "0.7291666666666666\n",
      "batch_idx: 8\n",
      "0.7453703703703703\n",
      "batch_idx: 9\n",
      "0.7416666666666667\n",
      "batch_idx: 10\n",
      "0.7537878787878788\n",
      "batch_idx: 11\n",
      "0.7465277777777778\n",
      "batch_idx: 12\n",
      "0.75\n",
      "batch_idx: 13\n",
      "0.75\n",
      "batch_idx: 14\n",
      "0.75\n",
      "batch_idx: 15\n",
      "0.7604166666666666\n",
      "batch_idx: 16\n",
      "0.7622549019607843\n",
      "batch_idx: 17\n",
      "0.7546296296296297\n",
      "batch_idx: 18\n",
      "0.75\n",
      "batch_idx: 19\n",
      "0.7520833333333333\n",
      "batch_idx: 20\n",
      "0.748015873015873\n",
      "batch_idx: 21\n",
      "0.7518939393939394\n",
      "batch_idx: 22\n",
      "0.7554347826086957\n",
      "batch_idx: 23\n",
      "0.7482638888888888\n",
      "batch_idx: 24\n",
      "0.7466666666666667\n",
      "Training Epoch: 676, total loss: 43.776292\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.7948717948717948\n",
      "batch_idx: 13\n",
      "0.7886904761904762\n",
      "batch_idx: 14\n",
      "0.7833333333333333\n",
      "batch_idx: 15\n",
      "0.78125\n",
      "batch_idx: 16\n",
      "0.7843137254901961\n",
      "batch_idx: 17\n",
      "0.7847222222222222\n",
      "batch_idx: 18\n",
      "0.7828947368421053\n",
      "batch_idx: 19\n",
      "0.7875\n",
      "batch_idx: 20\n",
      "0.7837301587301587\n",
      "batch_idx: 21\n",
      "0.7859848484848485\n",
      "batch_idx: 22\n",
      "0.7934782608695652\n",
      "batch_idx: 23\n",
      "0.7934027777777778\n",
      "batch_idx: 24\n",
      "0.795\n",
      "Training Epoch: 677, total loss: 42.868622\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8217592592592593\n",
      "batch_idx: 18\n",
      "0.8245614035087719\n",
      "batch_idx: 19\n",
      "0.8291666666666667\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.821969696969697\n",
      "batch_idx: 22\n",
      "0.8170289855072463\n",
      "batch_idx: 23\n",
      "0.8125\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 678, total loss: 42.725903\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8\n",
      "batch_idx: 15\n",
      "0.7994791666666666\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.7962962962962963\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.8003472222222222\n",
      "batch_idx: 24\n",
      "0.8016666666666666\n",
      "Training Epoch: 679, total loss: 42.696402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7321428571428571\n",
      "batch_idx: 7\n",
      "0.7291666666666666\n",
      "batch_idx: 8\n",
      "0.7175925925925926\n",
      "batch_idx: 9\n",
      "0.7166666666666667\n",
      "batch_idx: 10\n",
      "0.7272727272727273\n",
      "batch_idx: 11\n",
      "0.7326388888888888\n",
      "batch_idx: 12\n",
      "0.7403846153846154\n",
      "batch_idx: 13\n",
      "0.7559523809523809\n",
      "batch_idx: 14\n",
      "0.7583333333333333\n",
      "batch_idx: 15\n",
      "0.765625\n",
      "batch_idx: 16\n",
      "0.7598039215686274\n",
      "batch_idx: 17\n",
      "0.7731481481481481\n",
      "batch_idx: 18\n",
      "0.7763157894736842\n",
      "batch_idx: 19\n",
      "0.7791666666666667\n",
      "batch_idx: 20\n",
      "0.7857142857142857\n",
      "batch_idx: 21\n",
      "0.7784090909090909\n",
      "batch_idx: 22\n",
      "0.7789855072463768\n",
      "batch_idx: 23\n",
      "0.78125\n",
      "batch_idx: 24\n",
      "0.775\n",
      "Training Epoch: 680, total loss: 43.207200\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.7951388888888888\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.8005952380952381\n",
      "batch_idx: 14\n",
      "0.7944444444444444\n",
      "batch_idx: 15\n",
      "0.7916666666666666\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.7939814814814815\n",
      "batch_idx: 18\n",
      "0.793859649122807\n",
      "batch_idx: 19\n",
      "0.79375\n",
      "batch_idx: 20\n",
      "0.7996031746031746\n",
      "batch_idx: 21\n",
      "0.7935606060606061\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.8055555555555556\n",
      "batch_idx: 24\n",
      "0.8066666666666666\n",
      "Training Epoch: 681, total loss: 42.642995\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8365384615384616\n",
      "batch_idx: 13\n",
      "0.8422619047619048\n",
      "batch_idx: 14\n",
      "0.8472222222222222\n",
      "batch_idx: 15\n",
      "0.8411458333333334\n",
      "batch_idx: 16\n",
      "0.8406862745098039\n",
      "batch_idx: 17\n",
      "0.8402777777777778\n",
      "batch_idx: 18\n",
      "0.8355263157894737\n",
      "batch_idx: 19\n",
      "0.8291666666666667\n",
      "batch_idx: 20\n",
      "0.8333333333333334\n",
      "batch_idx: 21\n",
      "0.8295454545454546\n",
      "batch_idx: 22\n",
      "0.822463768115942\n",
      "batch_idx: 23\n",
      "0.8246527777777778\n",
      "batch_idx: 24\n",
      "0.8216666666666667\n",
      "Training Epoch: 682, total loss: 42.243129\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7552083333333334\n",
      "batch_idx: 8\n",
      "0.7638888888888888\n",
      "batch_idx: 9\n",
      "0.75\n",
      "batch_idx: 10\n",
      "0.7537878787878788\n",
      "batch_idx: 11\n",
      "0.7395833333333334\n",
      "batch_idx: 12\n",
      "0.7403846153846154\n",
      "batch_idx: 13\n",
      "0.7440476190476191\n",
      "batch_idx: 14\n",
      "0.75\n",
      "batch_idx: 15\n",
      "0.7604166666666666\n",
      "batch_idx: 16\n",
      "0.7671568627450981\n",
      "batch_idx: 17\n",
      "0.7708333333333334\n",
      "batch_idx: 18\n",
      "0.7697368421052632\n",
      "batch_idx: 19\n",
      "0.76875\n",
      "batch_idx: 20\n",
      "0.7658730158730159\n",
      "batch_idx: 21\n",
      "0.7651515151515151\n",
      "batch_idx: 22\n",
      "0.7717391304347826\n",
      "batch_idx: 23\n",
      "0.7760416666666666\n",
      "batch_idx: 24\n",
      "0.7816666666666666\n",
      "Training Epoch: 683, total loss: 43.060521\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7708333333333334\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.7743055555555556\n",
      "batch_idx: 12\n",
      "0.7756410256410257\n",
      "batch_idx: 13\n",
      "0.7678571428571429\n",
      "batch_idx: 14\n",
      "0.7666666666666667\n",
      "batch_idx: 15\n",
      "0.7760416666666666\n",
      "batch_idx: 16\n",
      "0.7745098039215687\n",
      "batch_idx: 17\n",
      "0.7731481481481481\n",
      "batch_idx: 18\n",
      "0.7675438596491229\n",
      "batch_idx: 19\n",
      "0.7708333333333334\n",
      "batch_idx: 20\n",
      "0.7678571428571429\n",
      "batch_idx: 21\n",
      "0.7689393939393939\n",
      "batch_idx: 22\n",
      "0.7771739130434783\n",
      "batch_idx: 23\n",
      "0.78125\n",
      "batch_idx: 24\n",
      "0.7766666666666666\n",
      "Training Epoch: 684, total loss: 43.153067\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8098958333333334\n",
      "batch_idx: 16\n",
      "0.8088235294117647\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8135964912280702\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8134920634920635\n",
      "batch_idx: 21\n",
      "0.8087121212121212\n",
      "batch_idx: 22\n",
      "0.7971014492753623\n",
      "batch_idx: 23\n",
      "0.7951388888888888\n",
      "batch_idx: 24\n",
      "0.7916666666666666\n",
      "Training Epoch: 685, total loss: 42.900686\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.79375\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.7897727272727273\n",
      "batch_idx: 22\n",
      "0.7971014492753623\n",
      "batch_idx: 23\n",
      "0.8038194444444444\n",
      "batch_idx: 24\n",
      "0.8066666666666666\n",
      "Training Epoch: 686, total loss: 42.650967\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.8014705882352942\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.7992424242424242\n",
      "batch_idx: 22\n",
      "0.8043478260869565\n",
      "batch_idx: 23\n",
      "0.8072916666666666\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 687, total loss: 42.663569\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8020833333333334\n",
      "batch_idx: 16\n",
      "0.8014705882352942\n",
      "batch_idx: 17\n",
      "0.7986111111111112\n",
      "batch_idx: 18\n",
      "0.8070175438596491\n",
      "batch_idx: 19\n",
      "0.8041666666666667\n",
      "batch_idx: 20\n",
      "0.8115079365079365\n",
      "batch_idx: 21\n",
      "0.8162878787878788\n",
      "batch_idx: 22\n",
      "0.8152173913043478\n",
      "batch_idx: 23\n",
      "0.8107638888888888\n",
      "batch_idx: 24\n",
      "0.8133333333333334\n",
      "Training Epoch: 688, total loss: 42.602613\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8055555555555556\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.7939814814814815\n",
      "batch_idx: 18\n",
      "0.793859649122807\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.7954545454545454\n",
      "batch_idx: 22\n",
      "0.7934782608695652\n",
      "batch_idx: 23\n",
      "0.7916666666666666\n",
      "batch_idx: 24\n",
      "0.79\n",
      "Training Epoch: 689, total loss: 42.997324\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.8044871794871795\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.7893518518518519\n",
      "batch_idx: 18\n",
      "0.7894736842105263\n",
      "batch_idx: 19\n",
      "0.7875\n",
      "batch_idx: 20\n",
      "0.7916666666666666\n",
      "batch_idx: 21\n",
      "0.7840909090909091\n",
      "batch_idx: 22\n",
      "0.7807971014492754\n",
      "batch_idx: 23\n",
      "0.7829861111111112\n",
      "batch_idx: 24\n",
      "0.7866666666666666\n",
      "Training Epoch: 690, total loss: 43.068562\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7857142857142857\n",
      "batch_idx: 14\n",
      "0.7944444444444444\n",
      "batch_idx: 15\n",
      "0.7890625\n",
      "batch_idx: 16\n",
      "0.7892156862745098\n",
      "batch_idx: 17\n",
      "0.7962962962962963\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.7958333333333333\n",
      "batch_idx: 20\n",
      "0.7896825396825397\n",
      "batch_idx: 21\n",
      "0.7954545454545454\n",
      "batch_idx: 22\n",
      "0.7952898550724637\n",
      "batch_idx: 23\n",
      "0.7986111111111112\n",
      "batch_idx: 24\n",
      "0.7966666666666666\n",
      "Training Epoch: 691, total loss: 42.695807\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8244047619047619\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8088235294117647\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.8070175438596491\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.8075396825396826\n",
      "batch_idx: 21\n",
      "0.8068181818181818\n",
      "batch_idx: 22\n",
      "0.802536231884058\n",
      "batch_idx: 23\n",
      "0.8020833333333334\n",
      "batch_idx: 24\n",
      "0.7983333333333333\n",
      "Training Epoch: 692, total loss: 42.805157\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.8259803921568627\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8245614035087719\n",
      "batch_idx: 19\n",
      "0.83125\n",
      "batch_idx: 20\n",
      "0.8373015873015873\n",
      "batch_idx: 21\n",
      "0.8333333333333334\n",
      "batch_idx: 22\n",
      "0.8351449275362319\n",
      "batch_idx: 23\n",
      "0.8333333333333334\n",
      "batch_idx: 24\n",
      "0.8316666666666667\n",
      "Training Epoch: 693, total loss: 42.225297\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.8044871794871795\n",
      "batch_idx: 13\n",
      "0.8154761904761905\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.7994791666666666\n",
      "batch_idx: 16\n",
      "0.8063725490196079\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8070175438596491\n",
      "batch_idx: 19\n",
      "0.7979166666666667\n",
      "batch_idx: 20\n",
      "0.8015873015873016\n",
      "batch_idx: 21\n",
      "0.7992424242424242\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8055555555555556\n",
      "batch_idx: 24\n",
      "0.805\n",
      "Training Epoch: 694, total loss: 42.619264\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8135964912280702\n",
      "batch_idx: 19\n",
      "0.8083333333333333\n",
      "batch_idx: 20\n",
      "0.8115079365079365\n",
      "batch_idx: 21\n",
      "0.8143939393939394\n",
      "batch_idx: 22\n",
      "0.8152173913043478\n",
      "batch_idx: 23\n",
      "0.8072916666666666\n",
      "batch_idx: 24\n",
      "0.805\n",
      "Training Epoch: 695, total loss: 42.671993\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9583333333333334\n",
      "batch_idx: 2\n",
      "0.9583333333333334\n",
      "batch_idx: 3\n",
      "0.90625\n",
      "batch_idx: 4\n",
      "0.9166666666666666\n",
      "batch_idx: 5\n",
      "0.9166666666666666\n",
      "batch_idx: 6\n",
      "0.9166666666666666\n",
      "batch_idx: 7\n",
      "0.90625\n",
      "batch_idx: 8\n",
      "0.8842592592592593\n",
      "batch_idx: 9\n",
      "0.8791666666666667\n",
      "batch_idx: 10\n",
      "0.875\n",
      "batch_idx: 11\n",
      "0.875\n",
      "batch_idx: 12\n",
      "0.8717948717948718\n",
      "batch_idx: 13\n",
      "0.8720238095238095\n",
      "batch_idx: 14\n",
      "0.8638888888888889\n",
      "batch_idx: 15\n",
      "0.8619791666666666\n",
      "batch_idx: 16\n",
      "0.8602941176470589\n",
      "batch_idx: 17\n",
      "0.8518518518518519\n",
      "batch_idx: 18\n",
      "0.8421052631578947\n",
      "batch_idx: 19\n",
      "0.8416666666666667\n",
      "batch_idx: 20\n",
      "0.8452380952380952\n",
      "batch_idx: 21\n",
      "0.8428030303030303\n",
      "batch_idx: 22\n",
      "0.842391304347826\n",
      "batch_idx: 23\n",
      "0.8420138888888888\n",
      "batch_idx: 24\n",
      "0.8416666666666667\n",
      "Training Epoch: 696, total loss: 42.021696\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8\n",
      "batch_idx: 15\n",
      "0.8098958333333334\n",
      "batch_idx: 16\n",
      "0.8063725490196079\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8145833333333333\n",
      "batch_idx: 20\n",
      "0.8154761904761905\n",
      "batch_idx: 21\n",
      "0.8200757575757576\n",
      "batch_idx: 22\n",
      "0.822463768115942\n",
      "batch_idx: 23\n",
      "0.8229166666666666\n",
      "batch_idx: 24\n",
      "0.8183333333333334\n",
      "Training Epoch: 697, total loss: 42.504255\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.7956349206349206\n",
      "batch_idx: 21\n",
      "0.7935606060606061\n",
      "batch_idx: 22\n",
      "0.7952898550724637\n",
      "batch_idx: 23\n",
      "0.7916666666666666\n",
      "batch_idx: 24\n",
      "0.785\n",
      "Training Epoch: 698, total loss: 43.066923\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7708333333333334\n",
      "batch_idx: 10\n",
      "0.7765151515151515\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.7788461538461539\n",
      "batch_idx: 13\n",
      "0.7708333333333334\n",
      "batch_idx: 14\n",
      "0.7694444444444445\n",
      "batch_idx: 15\n",
      "0.7734375\n",
      "batch_idx: 16\n",
      "0.7794117647058824\n",
      "batch_idx: 17\n",
      "0.7824074074074074\n",
      "batch_idx: 18\n",
      "0.7828947368421053\n",
      "batch_idx: 19\n",
      "0.7791666666666667\n",
      "batch_idx: 20\n",
      "0.7757936507936508\n",
      "batch_idx: 21\n",
      "0.7784090909090909\n",
      "batch_idx: 22\n",
      "0.7789855072463768\n",
      "batch_idx: 23\n",
      "0.7777777777777778\n",
      "batch_idx: 24\n",
      "0.7816666666666666\n",
      "Training Epoch: 699, total loss: 43.181947\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8298611111111112\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.825\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.8055555555555556\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.8041666666666667\n",
      "batch_idx: 20\n",
      "0.8075396825396826\n",
      "batch_idx: 21\n",
      "0.8068181818181818\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.8003472222222222\n",
      "batch_idx: 24\n",
      "0.8033333333333333\n",
      "Training Epoch: 700, total loss: 42.755653\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.7870370370370371\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7878787878787878\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.7788461538461539\n",
      "batch_idx: 13\n",
      "0.7797619047619048\n",
      "batch_idx: 14\n",
      "0.7833333333333333\n",
      "batch_idx: 15\n",
      "0.7942708333333334\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.7962962962962963\n",
      "batch_idx: 18\n",
      "0.7894736842105263\n",
      "batch_idx: 19\n",
      "0.7895833333333333\n",
      "batch_idx: 20\n",
      "0.7777777777777778\n",
      "batch_idx: 21\n",
      "0.7784090909090909\n",
      "batch_idx: 22\n",
      "0.7735507246376812\n",
      "batch_idx: 23\n",
      "0.7673611111111112\n",
      "batch_idx: 24\n",
      "0.7716666666666666\n",
      "Training Epoch: 701, total loss: 43.276165\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.9305555555555556\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8564814814814815\n",
      "batch_idx: 9\n",
      "0.8625\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.8541666666666666\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8305555555555556\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8217592592592593\n",
      "batch_idx: 18\n",
      "0.8223684210526315\n",
      "batch_idx: 19\n",
      "0.8083333333333333\n",
      "batch_idx: 20\n",
      "0.8015873015873016\n",
      "batch_idx: 21\n",
      "0.803030303030303\n",
      "batch_idx: 22\n",
      "0.8043478260869565\n",
      "batch_idx: 23\n",
      "0.8020833333333334\n",
      "batch_idx: 24\n",
      "0.805\n",
      "Training Epoch: 702, total loss: 42.616433\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8055555555555556\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8063725490196079\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8135964912280702\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8134920634920635\n",
      "batch_idx: 21\n",
      "0.803030303030303\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.8003472222222222\n",
      "batch_idx: 24\n",
      "0.7983333333333333\n",
      "Training Epoch: 703, total loss: 42.762757\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7870370370370371\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.7756410256410257\n",
      "batch_idx: 13\n",
      "0.7886904761904762\n",
      "batch_idx: 14\n",
      "0.7944444444444444\n",
      "batch_idx: 15\n",
      "0.7864583333333334\n",
      "batch_idx: 16\n",
      "0.7745098039215687\n",
      "batch_idx: 17\n",
      "0.7847222222222222\n",
      "batch_idx: 18\n",
      "0.7850877192982456\n",
      "batch_idx: 19\n",
      "0.7833333333333333\n",
      "batch_idx: 20\n",
      "0.7777777777777778\n",
      "batch_idx: 21\n",
      "0.7784090909090909\n",
      "batch_idx: 22\n",
      "0.7753623188405797\n",
      "batch_idx: 23\n",
      "0.7743055555555556\n",
      "batch_idx: 24\n",
      "0.7783333333333333\n",
      "Training Epoch: 704, total loss: 43.219350\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8154761904761905\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.8088235294117647\n",
      "batch_idx: 17\n",
      "0.8101851851851852\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8083333333333333\n",
      "batch_idx: 20\n",
      "0.8134920634920635\n",
      "batch_idx: 21\n",
      "0.8162878787878788\n",
      "batch_idx: 22\n",
      "0.8170289855072463\n",
      "batch_idx: 23\n",
      "0.8159722222222222\n",
      "batch_idx: 24\n",
      "0.8166666666666667\n",
      "Training Epoch: 705, total loss: 42.457242\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.7951388888888888\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8098958333333334\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.81875\n",
      "batch_idx: 20\n",
      "0.8174603174603174\n",
      "batch_idx: 21\n",
      "0.8162878787878788\n",
      "batch_idx: 22\n",
      "0.8152173913043478\n",
      "batch_idx: 23\n",
      "0.8125\n",
      "batch_idx: 24\n",
      "0.81\n",
      "Training Epoch: 706, total loss: 42.565214\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8179824561403509\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8214285714285714\n",
      "batch_idx: 21\n",
      "0.821969696969697\n",
      "batch_idx: 22\n",
      "0.822463768115942\n",
      "batch_idx: 23\n",
      "0.8211805555555556\n",
      "batch_idx: 24\n",
      "0.8216666666666667\n",
      "Training Epoch: 707, total loss: 42.515208\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6527777777777778\n",
      "batch_idx: 3\n",
      "0.6770833333333334\n",
      "batch_idx: 4\n",
      "0.725\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7261904761904762\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7407407407407407\n",
      "batch_idx: 9\n",
      "0.7375\n",
      "batch_idx: 10\n",
      "0.7348484848484849\n",
      "batch_idx: 11\n",
      "0.7465277777777778\n",
      "batch_idx: 12\n",
      "0.7339743589743589\n",
      "batch_idx: 13\n",
      "0.7470238095238095\n",
      "batch_idx: 14\n",
      "0.7416666666666667\n",
      "batch_idx: 15\n",
      "0.7473958333333334\n",
      "batch_idx: 16\n",
      "0.7524509803921569\n",
      "batch_idx: 17\n",
      "0.7638888888888888\n",
      "batch_idx: 18\n",
      "0.7675438596491229\n",
      "batch_idx: 19\n",
      "0.7729166666666667\n",
      "batch_idx: 20\n",
      "0.7738095238095238\n",
      "batch_idx: 21\n",
      "0.7821969696969697\n",
      "batch_idx: 22\n",
      "0.7862318840579711\n",
      "batch_idx: 23\n",
      "0.7899305555555556\n",
      "batch_idx: 24\n",
      "0.795\n",
      "Training Epoch: 708, total loss: 42.871947\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8\n",
      "batch_idx: 15\n",
      "0.796875\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.7986111111111112\n",
      "batch_idx: 18\n",
      "0.7960526315789473\n",
      "batch_idx: 19\n",
      "0.79375\n",
      "batch_idx: 20\n",
      "0.7996031746031746\n",
      "batch_idx: 21\n",
      "0.803030303030303\n",
      "batch_idx: 22\n",
      "0.8043478260869565\n",
      "batch_idx: 23\n",
      "0.8038194444444444\n",
      "batch_idx: 24\n",
      "0.8033333333333333\n",
      "Training Epoch: 709, total loss: 42.758253\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.7916666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 13\n",
      "0.7916666666666666\n",
      "batch_idx: 14\n",
      "0.7916666666666666\n",
      "batch_idx: 15\n",
      "0.7942708333333334\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.7962962962962963\n",
      "batch_idx: 18\n",
      "0.8004385964912281\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.7936507936507936\n",
      "batch_idx: 21\n",
      "0.7954545454545454\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.8020833333333334\n",
      "batch_idx: 24\n",
      "0.805\n",
      "Training Epoch: 710, total loss: 42.750728\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8257575757575758\n",
      "batch_idx: 11\n",
      "0.8298611111111112\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8482142857142857\n",
      "batch_idx: 14\n",
      "0.85\n",
      "batch_idx: 15\n",
      "0.8515625\n",
      "batch_idx: 16\n",
      "0.8480392156862745\n",
      "batch_idx: 17\n",
      "0.8472222222222222\n",
      "batch_idx: 18\n",
      "0.8486842105263158\n",
      "batch_idx: 19\n",
      "0.8458333333333333\n",
      "batch_idx: 20\n",
      "0.8392857142857143\n",
      "batch_idx: 21\n",
      "0.8352272727272727\n",
      "batch_idx: 22\n",
      "0.8369565217391305\n",
      "batch_idx: 23\n",
      "0.8385416666666666\n",
      "batch_idx: 24\n",
      "0.8366666666666667\n",
      "Training Epoch: 711, total loss: 42.064608\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.7916666666666666\n",
      "batch_idx: 14\n",
      "0.8055555555555556\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8114035087719298\n",
      "batch_idx: 19\n",
      "0.8166666666666667\n",
      "batch_idx: 20\n",
      "0.8134920634920635\n",
      "batch_idx: 21\n",
      "0.8106060606060606\n",
      "batch_idx: 22\n",
      "0.8097826086956522\n",
      "batch_idx: 23\n",
      "0.8142361111111112\n",
      "batch_idx: 24\n",
      "0.8166666666666667\n",
      "Training Epoch: 712, total loss: 42.478215\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.8088235294117647\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8083333333333333\n",
      "batch_idx: 20\n",
      "0.8075396825396826\n",
      "batch_idx: 21\n",
      "0.803030303030303\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.7934027777777778\n",
      "batch_idx: 24\n",
      "0.7966666666666666\n",
      "Training Epoch: 713, total loss: 42.727303\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.8044871794871795\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.7986111111111112\n",
      "batch_idx: 18\n",
      "0.7916666666666666\n",
      "batch_idx: 19\n",
      "0.7916666666666666\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.7951388888888888\n",
      "batch_idx: 24\n",
      "0.7966666666666666\n",
      "Training Epoch: 714, total loss: 42.708032\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8269230769230769\n",
      "batch_idx: 13\n",
      "0.8303571428571429\n",
      "batch_idx: 14\n",
      "0.8333333333333334\n",
      "batch_idx: 15\n",
      "0.8333333333333334\n",
      "batch_idx: 16\n",
      "0.8259803921568627\n",
      "batch_idx: 17\n",
      "0.8217592592592593\n",
      "batch_idx: 18\n",
      "0.8179824561403509\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8154761904761905\n",
      "batch_idx: 21\n",
      "0.8162878787878788\n",
      "batch_idx: 22\n",
      "0.8079710144927537\n",
      "batch_idx: 23\n",
      "0.8072916666666666\n",
      "batch_idx: 24\n",
      "0.8\n",
      "Training Epoch: 715, total loss: 42.821314\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8308823529411765\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8223684210526315\n",
      "batch_idx: 19\n",
      "0.8166666666666667\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.8162878787878788\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8090277777777778\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 716, total loss: 42.463520\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.725\n",
      "batch_idx: 5\n",
      "0.7222222222222222\n",
      "batch_idx: 6\n",
      "0.7321428571428571\n",
      "batch_idx: 7\n",
      "0.7447916666666666\n",
      "batch_idx: 8\n",
      "0.7361111111111112\n",
      "batch_idx: 9\n",
      "0.7458333333333333\n",
      "batch_idx: 10\n",
      "0.7613636363636364\n",
      "batch_idx: 11\n",
      "0.75\n",
      "batch_idx: 12\n",
      "0.7564102564102564\n",
      "batch_idx: 13\n",
      "0.7589285714285714\n",
      "batch_idx: 14\n",
      "0.7611111111111111\n",
      "batch_idx: 15\n",
      "0.7630208333333334\n",
      "batch_idx: 16\n",
      "0.7671568627450981\n",
      "batch_idx: 17\n",
      "0.7592592592592593\n",
      "batch_idx: 18\n",
      "0.7587719298245614\n",
      "batch_idx: 19\n",
      "0.7625\n",
      "batch_idx: 20\n",
      "0.7678571428571429\n",
      "batch_idx: 21\n",
      "0.7689393939393939\n",
      "batch_idx: 22\n",
      "0.7663043478260869\n",
      "batch_idx: 23\n",
      "0.765625\n",
      "batch_idx: 24\n",
      "0.765\n",
      "Training Epoch: 717, total loss: 43.315760\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.7893518518518519\n",
      "batch_idx: 18\n",
      "0.7916666666666666\n",
      "batch_idx: 19\n",
      "0.7833333333333333\n",
      "batch_idx: 20\n",
      "0.7876984126984127\n",
      "batch_idx: 21\n",
      "0.7878787878787878\n",
      "batch_idx: 22\n",
      "0.7916666666666666\n",
      "batch_idx: 23\n",
      "0.7916666666666666\n",
      "batch_idx: 24\n",
      "0.7933333333333333\n",
      "Training Epoch: 718, total loss: 42.923816\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8365384615384616\n",
      "batch_idx: 13\n",
      "0.8333333333333334\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.8333333333333334\n",
      "batch_idx: 16\n",
      "0.8406862745098039\n",
      "batch_idx: 17\n",
      "0.8449074074074074\n",
      "batch_idx: 18\n",
      "0.8399122807017544\n",
      "batch_idx: 19\n",
      "0.8291666666666667\n",
      "batch_idx: 20\n",
      "0.8313492063492064\n",
      "batch_idx: 21\n",
      "0.8314393939393939\n",
      "batch_idx: 22\n",
      "0.8315217391304348\n",
      "batch_idx: 23\n",
      "0.8246527777777778\n",
      "batch_idx: 24\n",
      "0.8233333333333334\n",
      "Training Epoch: 719, total loss: 42.299112\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8333333333333334\n",
      "batch_idx: 14\n",
      "0.8416666666666667\n",
      "batch_idx: 15\n",
      "0.8385416666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 16\n",
      "0.8357843137254902\n",
      "batch_idx: 17\n",
      "0.8287037037037037\n",
      "batch_idx: 18\n",
      "0.8245614035087719\n",
      "batch_idx: 19\n",
      "0.8208333333333333\n",
      "batch_idx: 20\n",
      "0.8174603174603174\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.8134057971014492\n",
      "batch_idx: 23\n",
      "0.8177083333333334\n",
      "batch_idx: 24\n",
      "0.8133333333333334\n",
      "Training Epoch: 720, total loss: 42.585999\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.782051282051282\n",
      "batch_idx: 13\n",
      "0.7827380952380952\n",
      "batch_idx: 14\n",
      "0.7833333333333333\n",
      "batch_idx: 15\n",
      "0.78125\n",
      "batch_idx: 16\n",
      "0.7843137254901961\n",
      "batch_idx: 17\n",
      "0.7893518518518519\n",
      "batch_idx: 18\n",
      "0.7872807017543859\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7817460317460317\n",
      "batch_idx: 21\n",
      "0.7784090909090909\n",
      "batch_idx: 22\n",
      "0.7807971014492754\n",
      "batch_idx: 23\n",
      "0.7829861111111112\n",
      "batch_idx: 24\n",
      "0.7866666666666666\n",
      "Training Epoch: 721, total loss: 42.979709\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.8063725490196079\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.7894736842105263\n",
      "batch_idx: 19\n",
      "0.7958333333333333\n",
      "batch_idx: 20\n",
      "0.7996031746031746\n",
      "batch_idx: 21\n",
      "0.7973484848484849\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.7951388888888888\n",
      "batch_idx: 24\n",
      "0.7916666666666666\n",
      "Training Epoch: 722, total loss: 42.935122\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.8461538461538461\n",
      "batch_idx: 13\n",
      "0.8422619047619048\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.8385416666666666\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8083333333333333\n",
      "batch_idx: 20\n",
      "0.8015873015873016\n",
      "batch_idx: 21\n",
      "0.8049242424242424\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.7934027777777778\n",
      "batch_idx: 24\n",
      "0.7966666666666666\n",
      "Training Epoch: 723, total loss: 42.764442\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8098958333333334\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.8055555555555556\n",
      "batch_idx: 18\n",
      "0.8070175438596491\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8115079365079365\n",
      "batch_idx: 21\n",
      "0.8125\n",
      "batch_idx: 22\n",
      "0.8097826086956522\n",
      "batch_idx: 23\n",
      "0.8090277777777778\n",
      "batch_idx: 24\n",
      "0.8\n",
      "Training Epoch: 724, total loss: 42.867226\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.7559523809523809\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.7743055555555556\n",
      "batch_idx: 12\n",
      "0.7756410256410257\n",
      "batch_idx: 13\n",
      "0.7827380952380952\n",
      "batch_idx: 14\n",
      "0.7833333333333333\n",
      "batch_idx: 15\n",
      "0.7864583333333334\n",
      "batch_idx: 16\n",
      "0.7794117647058824\n",
      "batch_idx: 17\n",
      "0.7824074074074074\n",
      "batch_idx: 18\n",
      "0.7807017543859649\n",
      "batch_idx: 19\n",
      "0.7833333333333333\n",
      "batch_idx: 20\n",
      "0.7936507936507936\n",
      "batch_idx: 21\n",
      "0.7897727272727273\n",
      "batch_idx: 22\n",
      "0.7880434782608695\n",
      "batch_idx: 23\n",
      "0.7847222222222222\n",
      "batch_idx: 24\n",
      "0.7833333333333333\n",
      "Training Epoch: 725, total loss: 42.910436\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8098958333333334\n",
      "batch_idx: 16\n",
      "0.8063725490196079\n",
      "batch_idx: 17\n",
      "0.8055555555555556\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.7996031746031746\n",
      "batch_idx: 21\n",
      "0.7973484848484849\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.796875\n",
      "batch_idx: 24\n",
      "0.7933333333333333\n",
      "Training Epoch: 726, total loss: 42.807910\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8114035087719298\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8134920634920635\n",
      "batch_idx: 21\n",
      "0.8162878787878788\n",
      "batch_idx: 22\n",
      "0.8079710144927537\n",
      "batch_idx: 23\n",
      "0.8020833333333334\n",
      "batch_idx: 24\n",
      "0.8\n",
      "Training Epoch: 727, total loss: 42.684272\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.8257575757575758\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.825\n",
      "batch_idx: 15\n",
      "0.828125\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8263888888888888\n",
      "batch_idx: 18\n",
      "0.8245614035087719\n",
      "batch_idx: 19\n",
      "0.8229166666666666\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.8295454545454546\n",
      "batch_idx: 22\n",
      "0.8242753623188406\n",
      "batch_idx: 23\n",
      "0.8211805555555556\n",
      "batch_idx: 24\n",
      "0.82\n",
      "Training Epoch: 728, total loss: 42.452433\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8333333333333334\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.828125\n",
      "batch_idx: 16\n",
      "0.8259803921568627\n",
      "batch_idx: 17\n",
      "0.8263888888888888\n",
      "batch_idx: 18\n",
      "0.8223684210526315\n",
      "batch_idx: 19\n",
      "0.8208333333333333\n",
      "batch_idx: 20\n",
      "0.8234126984126984\n",
      "batch_idx: 21\n",
      "0.8162878787878788\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8142361111111112\n",
      "batch_idx: 24\n",
      "0.8133333333333334\n",
      "Training Epoch: 729, total loss: 42.591269\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8257575757575758\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8154761904761905\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.8166666666666667\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.8087121212121212\n",
      "batch_idx: 22\n",
      "0.8134057971014492\n",
      "batch_idx: 23\n",
      "0.8159722222222222\n",
      "batch_idx: 24\n",
      "0.81\n",
      "Training Epoch: 730, total loss: 42.526248\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.7916666666666666\n",
      "batch_idx: 14\n",
      "0.7944444444444444\n",
      "batch_idx: 15\n",
      "0.796875\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8166666666666667\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.8206521739130435\n",
      "batch_idx: 23\n",
      "0.8107638888888888\n",
      "batch_idx: 24\n",
      "0.81\n",
      "Training Epoch: 731, total loss: 42.638850\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.8044871794871795\n",
      "batch_idx: 13\n",
      "0.8005952380952381\n",
      "batch_idx: 14\n",
      "0.8055555555555556\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.8014705882352942\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.8004385964912281\n",
      "batch_idx: 19\n",
      "0.7958333333333333\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.7973484848484849\n",
      "batch_idx: 22\n",
      "0.7952898550724637\n",
      "batch_idx: 23\n",
      "0.7934027777777778\n",
      "batch_idx: 24\n",
      "0.79\n",
      "Training Epoch: 732, total loss: 42.875792\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7951388888888888\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7827380952380952\n",
      "batch_idx: 14\n",
      "0.7861111111111111\n",
      "batch_idx: 15\n",
      "0.7864583333333334\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.7996031746031746\n",
      "batch_idx: 21\n",
      "0.8049242424242424\n",
      "batch_idx: 22\n",
      "0.7971014492753623\n",
      "batch_idx: 23\n",
      "0.796875\n",
      "batch_idx: 24\n",
      "0.7966666666666666\n",
      "Training Epoch: 733, total loss: 42.847644\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.8186274509803921\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8245614035087719\n",
      "batch_idx: 19\n",
      "0.8291666666666667\n",
      "batch_idx: 20\n",
      "0.8293650793650794\n",
      "batch_idx: 21\n",
      "0.8238636363636364\n",
      "batch_idx: 22\n",
      "0.8297101449275363\n",
      "batch_idx: 23\n",
      "0.8315972222222222\n",
      "batch_idx: 24\n",
      "0.8216666666666667\n",
      "Training Epoch: 734, total loss: 42.348008\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7261904761904762\n",
      "batch_idx: 7\n",
      "0.7291666666666666\n",
      "batch_idx: 8\n",
      "0.7361111111111112\n",
      "batch_idx: 9\n",
      "0.7375\n",
      "batch_idx: 10\n",
      "0.75\n",
      "batch_idx: 11\n",
      "0.7430555555555556\n",
      "batch_idx: 12\n",
      "0.7532051282051282\n",
      "batch_idx: 13\n",
      "0.7648809523809523\n",
      "batch_idx: 14\n",
      "0.7805555555555556\n",
      "batch_idx: 15\n",
      "0.7760416666666666\n",
      "batch_idx: 16\n",
      "0.7818627450980392\n",
      "batch_idx: 17\n",
      "0.7824074074074074\n",
      "batch_idx: 18\n",
      "0.7872807017543859\n",
      "batch_idx: 19\n",
      "0.79375\n",
      "batch_idx: 20\n",
      "0.7916666666666666\n",
      "batch_idx: 21\n",
      "0.7954545454545454\n",
      "batch_idx: 22\n",
      "0.7934782608695652\n",
      "batch_idx: 23\n",
      "0.7934027777777778\n",
      "batch_idx: 24\n",
      "0.7983333333333333\n",
      "Training Epoch: 735, total loss: 42.832611\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.8257575757575758\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8269230769230769\n",
      "batch_idx: 13\n",
      "0.8303571428571429\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.828125\n",
      "batch_idx: 16\n",
      "0.8186274509803921\n",
      "batch_idx: 17\n",
      "0.8217592592592593\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.8208333333333333\n",
      "batch_idx: 20\n",
      "0.8214285714285714\n",
      "batch_idx: 21\n",
      "0.821969696969697\n",
      "batch_idx: 22\n",
      "0.8188405797101449\n",
      "batch_idx: 23\n",
      "0.8177083333333334\n",
      "batch_idx: 24\n",
      "0.815\n",
      "Training Epoch: 736, total loss: 42.570871\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.7946428571428571\n",
      "batch_idx: 14\n",
      "0.7861111111111111\n",
      "batch_idx: 15\n",
      "0.796875\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.7962962962962963\n",
      "batch_idx: 18\n",
      "0.7916666666666666\n",
      "batch_idx: 19\n",
      "0.7979166666666667\n",
      "batch_idx: 20\n",
      "0.7936507936507936\n",
      "batch_idx: 21\n",
      "0.7916666666666666\n",
      "batch_idx: 22\n",
      "0.7916666666666666\n",
      "batch_idx: 23\n",
      "0.7847222222222222\n",
      "batch_idx: 24\n",
      "0.785\n",
      "Training Epoch: 737, total loss: 42.930553\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7166666666666667\n",
      "batch_idx: 5\n",
      "0.7291666666666666\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7689393939393939\n",
      "batch_idx: 11\n",
      "0.7777777777777778\n",
      "batch_idx: 12\n",
      "0.7788461538461539\n",
      "batch_idx: 13\n",
      "0.7857142857142857\n",
      "batch_idx: 14\n",
      "0.7833333333333333\n",
      "batch_idx: 15\n",
      "0.7838541666666666\n",
      "batch_idx: 16\n",
      "0.7892156862745098\n",
      "batch_idx: 17\n",
      "0.7939814814814815\n",
      "batch_idx: 18\n",
      "0.793859649122807\n",
      "batch_idx: 19\n",
      "0.79375\n",
      "batch_idx: 20\n",
      "0.7916666666666666\n",
      "batch_idx: 21\n",
      "0.7954545454545454\n",
      "batch_idx: 22\n",
      "0.7952898550724637\n",
      "batch_idx: 23\n",
      "0.7951388888888888\n",
      "batch_idx: 24\n",
      "0.7916666666666666\n",
      "Training Epoch: 738, total loss: 42.882586\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7951388888888888\n",
      "batch_idx: 12\n",
      "0.7948717948717948\n",
      "batch_idx: 13\n",
      "0.7976190476190477\n",
      "batch_idx: 14\n",
      "0.7944444444444444\n",
      "batch_idx: 15\n",
      "0.796875\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.8004385964912281\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.7956349206349206\n",
      "batch_idx: 21\n",
      "0.7992424242424242\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8090277777777778\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 739, total loss: 42.451190\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7727272727272727\n",
      "batch_idx: 11\n",
      "0.7743055555555556\n",
      "batch_idx: 12\n",
      "0.7660256410256411\n",
      "batch_idx: 13\n",
      "0.7619047619047619\n",
      "batch_idx: 14\n",
      "0.7638888888888888\n",
      "batch_idx: 15\n",
      "0.7682291666666666\n",
      "batch_idx: 16\n",
      "0.7720588235294118\n",
      "batch_idx: 17\n",
      "0.7731481481481481\n",
      "batch_idx: 18\n",
      "0.7741228070175439\n",
      "batch_idx: 19\n",
      "0.76875\n",
      "batch_idx: 20\n",
      "0.7757936507936508\n",
      "batch_idx: 21\n",
      "0.7803030303030303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 22\n",
      "0.7735507246376812\n",
      "batch_idx: 23\n",
      "0.7777777777777778\n",
      "batch_idx: 24\n",
      "0.7783333333333333\n",
      "Training Epoch: 740, total loss: 43.225095\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8154761904761905\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8137254901960784\n",
      "batch_idx: 17\n",
      "0.8101851851851852\n",
      "batch_idx: 18\n",
      "0.8070175438596491\n",
      "batch_idx: 19\n",
      "0.8104166666666667\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.7952898550724637\n",
      "batch_idx: 23\n",
      "0.7829861111111112\n",
      "batch_idx: 24\n",
      "0.78\n",
      "Training Epoch: 741, total loss: 43.161196\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7083333333333334\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.7314814814814815\n",
      "batch_idx: 9\n",
      "0.7291666666666666\n",
      "batch_idx: 10\n",
      "0.7196969696969697\n",
      "batch_idx: 11\n",
      "0.7395833333333334\n",
      "batch_idx: 12\n",
      "0.7435897435897436\n",
      "batch_idx: 13\n",
      "0.7261904761904762\n",
      "batch_idx: 14\n",
      "0.7333333333333333\n",
      "batch_idx: 15\n",
      "0.7265625\n",
      "batch_idx: 16\n",
      "0.7303921568627451\n",
      "batch_idx: 17\n",
      "0.7314814814814815\n",
      "batch_idx: 18\n",
      "0.7302631578947368\n",
      "batch_idx: 19\n",
      "0.7354166666666667\n",
      "batch_idx: 20\n",
      "0.7400793650793651\n",
      "batch_idx: 21\n",
      "0.740530303030303\n",
      "batch_idx: 22\n",
      "0.7409420289855072\n",
      "batch_idx: 23\n",
      "0.7447916666666666\n",
      "batch_idx: 24\n",
      "0.75\n",
      "Training Epoch: 742, total loss: 43.668126\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.7951388888888888\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.7944444444444444\n",
      "batch_idx: 15\n",
      "0.7890625\n",
      "batch_idx: 16\n",
      "0.7916666666666666\n",
      "batch_idx: 17\n",
      "0.7824074074074074\n",
      "batch_idx: 18\n",
      "0.7807017543859649\n",
      "batch_idx: 19\n",
      "0.7854166666666667\n",
      "batch_idx: 20\n",
      "0.7837301587301587\n",
      "batch_idx: 21\n",
      "0.7859848484848485\n",
      "batch_idx: 22\n",
      "0.7844202898550725\n",
      "batch_idx: 23\n",
      "0.7847222222222222\n",
      "batch_idx: 24\n",
      "0.7916666666666666\n",
      "Training Epoch: 743, total loss: 42.936232\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7666666666666667\n",
      "batch_idx: 10\n",
      "0.7727272727272727\n",
      "batch_idx: 11\n",
      "0.7743055555555556\n",
      "batch_idx: 12\n",
      "0.7788461538461539\n",
      "batch_idx: 13\n",
      "0.7827380952380952\n",
      "batch_idx: 14\n",
      "0.7916666666666666\n",
      "batch_idx: 15\n",
      "0.7916666666666666\n",
      "batch_idx: 16\n",
      "0.7892156862745098\n",
      "batch_idx: 17\n",
      "0.7847222222222222\n",
      "batch_idx: 18\n",
      "0.7763157894736842\n",
      "batch_idx: 19\n",
      "0.7833333333333333\n",
      "batch_idx: 20\n",
      "0.7896825396825397\n",
      "batch_idx: 21\n",
      "0.7954545454545454\n",
      "batch_idx: 22\n",
      "0.802536231884058\n",
      "batch_idx: 23\n",
      "0.7986111111111112\n",
      "batch_idx: 24\n",
      "0.805\n",
      "Training Epoch: 744, total loss: 42.609104\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7440476190476191\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.7592592592592593\n",
      "batch_idx: 9\n",
      "0.7791666666666667\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.8044871794871795\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.7992424242424242\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.8038194444444444\n",
      "batch_idx: 24\n",
      "0.805\n",
      "Training Epoch: 745, total loss: 42.646225\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.8063725490196079\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.8095238095238095\n",
      "batch_idx: 21\n",
      "0.8106060606060606\n",
      "batch_idx: 22\n",
      "0.8097826086956522\n",
      "batch_idx: 23\n",
      "0.8107638888888888\n",
      "batch_idx: 24\n",
      "0.8066666666666666\n",
      "Training Epoch: 746, total loss: 42.611040\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7604166666666666\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.7708333333333334\n",
      "batch_idx: 10\n",
      "0.7689393939393939\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.782051282051282\n",
      "batch_idx: 13\n",
      "0.7916666666666666\n",
      "batch_idx: 14\n",
      "0.7916666666666666\n",
      "batch_idx: 15\n",
      "0.8020833333333334\n",
      "batch_idx: 16\n",
      "0.8088235294117647\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8179824561403509\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8115079365079365\n",
      "batch_idx: 21\n",
      "0.8125\n",
      "batch_idx: 22\n",
      "0.8079710144927537\n",
      "batch_idx: 23\n",
      "0.8072916666666666\n",
      "batch_idx: 24\n",
      "0.805\n",
      "Training Epoch: 747, total loss: 42.563367\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7651515151515151\n",
      "batch_idx: 11\n",
      "0.7604166666666666\n",
      "batch_idx: 12\n",
      "0.7596153846153846\n",
      "batch_idx: 13\n",
      "0.7589285714285714\n",
      "batch_idx: 14\n",
      "0.7611111111111111\n",
      "batch_idx: 15\n",
      "0.7682291666666666\n",
      "batch_idx: 16\n",
      "0.7745098039215687\n",
      "batch_idx: 17\n",
      "0.7777777777777778\n",
      "batch_idx: 18\n",
      "0.7828947368421053\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7876984126984127\n",
      "batch_idx: 21\n",
      "0.7916666666666666\n",
      "batch_idx: 22\n",
      "0.7934782608695652\n",
      "batch_idx: 23\n",
      "0.796875\n",
      "batch_idx: 24\n",
      "0.7983333333333333\n",
      "Training Epoch: 748, total loss: 42.787970\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8055555555555556\n",
      "batch_idx: 15\n",
      "0.8020833333333334\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.7962962962962963\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.8041666666666667\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.8068181818181818\n",
      "batch_idx: 22\n",
      "0.8097826086956522\n",
      "batch_idx: 23\n",
      "0.8159722222222222\n",
      "batch_idx: 24\n",
      "0.8183333333333334\n",
      "Training Epoch: 749, total loss: 42.475189\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.734375\n",
      "batch_idx: 8\n",
      "0.7407407407407407\n",
      "batch_idx: 9\n",
      "0.7416666666666667\n",
      "batch_idx: 10\n",
      "0.7462121212121212\n",
      "batch_idx: 11\n",
      "0.7430555555555556\n",
      "batch_idx: 12\n",
      "0.7532051282051282\n",
      "batch_idx: 13\n",
      "0.7619047619047619\n",
      "batch_idx: 14\n",
      "0.7638888888888888\n",
      "batch_idx: 15\n",
      "0.7630208333333334\n",
      "batch_idx: 16\n",
      "0.7671568627450981\n",
      "batch_idx: 17\n",
      "0.7685185185185185\n",
      "batch_idx: 18\n",
      "0.7763157894736842\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7837301587301587\n",
      "batch_idx: 21\n",
      "0.7859848484848485\n",
      "batch_idx: 22\n",
      "0.7844202898550725\n",
      "batch_idx: 23\n",
      "0.7864583333333334\n",
      "batch_idx: 24\n",
      "0.7883333333333333\n",
      "Training Epoch: 750, total loss: 42.862397\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8257575757575758\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.8088235294117647\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8166666666666667\n",
      "batch_idx: 20\n",
      "0.8115079365079365\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.8206521739130435\n",
      "batch_idx: 23\n",
      "0.8211805555555556\n",
      "batch_idx: 24\n",
      "0.8166666666666667\n",
      "Training Epoch: 751, total loss: 42.578238\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.8063725490196079\n",
      "batch_idx: 17\n",
      "0.8055555555555556\n",
      "batch_idx: 18\n",
      "0.8070175438596491\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.7996031746031746\n",
      "batch_idx: 21\n",
      "0.803030303030303\n",
      "batch_idx: 22\n",
      "0.7952898550724637\n",
      "batch_idx: 23\n",
      "0.8003472222222222\n",
      "batch_idx: 24\n",
      "0.8\n",
      "Training Epoch: 752, total loss: 42.823231\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.9375\n",
      "batch_idx: 2\n",
      "0.9305555555555556\n",
      "batch_idx: 3\n",
      "0.9270833333333334\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.8589743589743589\n",
      "batch_idx: 13\n",
      "0.8452380952380952\n",
      "batch_idx: 14\n",
      "0.8388888888888889\n",
      "batch_idx: 15\n",
      "0.84375\n",
      "batch_idx: 16\n",
      "0.8333333333333334\n",
      "batch_idx: 17\n",
      "0.8287037037037037\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.8166666666666667\n",
      "batch_idx: 20\n",
      "0.8154761904761905\n",
      "batch_idx: 21\n",
      "0.8162878787878788\n",
      "batch_idx: 22\n",
      "0.8188405797101449\n",
      "batch_idx: 23\n",
      "0.8177083333333334\n",
      "batch_idx: 24\n",
      "0.8166666666666667\n",
      "Training Epoch: 753, total loss: 42.560622\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8305555555555556\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8308823529411765\n",
      "batch_idx: 17\n",
      "0.8287037037037037\n",
      "batch_idx: 18\n",
      "0.8245614035087719\n",
      "batch_idx: 19\n",
      "0.8208333333333333\n",
      "batch_idx: 20\n",
      "0.8214285714285714\n",
      "batch_idx: 21\n",
      "0.8238636363636364\n",
      "batch_idx: 22\n",
      "0.8242753623188406\n",
      "batch_idx: 23\n",
      "0.828125\n",
      "batch_idx: 24\n",
      "0.82\n",
      "Training Epoch: 754, total loss: 42.364029\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.8137254901960784\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.796875\n",
      "batch_idx: 24\n",
      "0.7933333333333333\n",
      "Training Epoch: 755, total loss: 43.128361\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.7942708333333334\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.7962962962962963\n",
      "batch_idx: 18\n",
      "0.7894736842105263\n",
      "batch_idx: 19\n",
      "0.7833333333333333\n",
      "batch_idx: 20\n",
      "0.7817460317460317\n",
      "batch_idx: 21\n",
      "0.7803030303030303\n",
      "batch_idx: 22\n",
      "0.782608695652174\n",
      "batch_idx: 23\n",
      "0.7847222222222222\n",
      "batch_idx: 24\n",
      "0.79\n",
      "Training Epoch: 756, total loss: 42.967719\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7878787878787878\n",
      "batch_idx: 11\n",
      "0.7951388888888888\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.7946428571428571\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.7994791666666666\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.8026315789473685\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.7973484848484849\n",
      "batch_idx: 22\n",
      "0.7952898550724637\n",
      "batch_idx: 23\n",
      "0.7934027777777778\n",
      "batch_idx: 24\n",
      "0.7883333333333333\n",
      "Training Epoch: 757, total loss: 42.928087\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.8020833333333334\n",
      "batch_idx: 16\n",
      "0.8088235294117647\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.7956349206349206\n",
      "batch_idx: 21\n",
      "0.7954545454545454\n",
      "batch_idx: 22\n",
      "0.7952898550724637\n",
      "batch_idx: 23\n",
      "0.7916666666666666\n",
      "batch_idx: 24\n",
      "0.7933333333333333\n",
      "Training Epoch: 758, total loss: 42.942096\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.8055555555555556\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.8068181818181818\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8072916666666666\n",
      "batch_idx: 24\n",
      "0.8066666666666666\n",
      "Training Epoch: 759, total loss: 42.585840\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8245614035087719\n",
      "batch_idx: 19\n",
      "0.8229166666666666\n",
      "batch_idx: 20\n",
      "0.8293650793650794\n",
      "batch_idx: 21\n",
      "0.8200757575757576\n",
      "batch_idx: 22\n",
      "0.8170289855072463\n",
      "batch_idx: 23\n",
      "0.8125\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 760, total loss: 42.451369\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8257575757575758\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8137254901960784\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8104166666666667\n",
      "batch_idx: 20\n",
      "0.8134920634920635\n",
      "batch_idx: 21\n",
      "0.8125\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8125\n",
      "batch_idx: 24\n",
      "0.8133333333333334\n",
      "Training Epoch: 761, total loss: 42.419901\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7552083333333334\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.7583333333333333\n",
      "batch_idx: 10\n",
      "0.7537878787878788\n",
      "batch_idx: 11\n",
      "0.75\n",
      "batch_idx: 12\n",
      "0.7532051282051282\n",
      "batch_idx: 13\n",
      "0.7619047619047619\n",
      "batch_idx: 14\n",
      "0.7694444444444445\n",
      "batch_idx: 15\n",
      "0.7630208333333334\n",
      "batch_idx: 16\n",
      "0.7745098039215687\n",
      "batch_idx: 17\n",
      "0.7754629629629629\n",
      "batch_idx: 18\n",
      "0.7785087719298246\n",
      "batch_idx: 19\n",
      "0.7833333333333333\n",
      "batch_idx: 20\n",
      "0.7797619047619048\n",
      "batch_idx: 21\n",
      "0.7840909090909091\n",
      "batch_idx: 22\n",
      "0.7844202898550725\n",
      "batch_idx: 23\n",
      "0.7881944444444444\n",
      "batch_idx: 24\n",
      "0.79\n",
      "Training Epoch: 762, total loss: 42.985311\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8055555555555556\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.8095238095238095\n",
      "batch_idx: 21\n",
      "0.8068181818181818\n",
      "batch_idx: 22\n",
      "0.7971014492753623\n",
      "batch_idx: 23\n",
      "0.7916666666666666\n",
      "batch_idx: 24\n",
      "0.7933333333333333\n",
      "Training Epoch: 763, total loss: 42.831270\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8244047619047619\n",
      "batch_idx: 14\n",
      "0.8305555555555556\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8217592592592593\n",
      "batch_idx: 18\n",
      "0.8223684210526315\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.8143939393939394\n",
      "batch_idx: 22\n",
      "0.8152173913043478\n",
      "batch_idx: 23\n",
      "0.8194444444444444\n",
      "batch_idx: 24\n",
      "0.8133333333333334\n",
      "Training Epoch: 764, total loss: 42.537048\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8259803921568627\n",
      "batch_idx: 17\n",
      "0.8217592592592593\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.8166666666666667\n",
      "batch_idx: 20\n",
      "0.8214285714285714\n",
      "batch_idx: 21\n",
      "0.8200757575757576\n",
      "batch_idx: 22\n",
      "0.8206521739130435\n",
      "batch_idx: 23\n",
      "0.8159722222222222\n",
      "batch_idx: 24\n",
      "0.8066666666666666\n",
      "Training Epoch: 765, total loss: 42.732431\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.8098958333333334\n",
      "batch_idx: 16\n",
      "0.8137254901960784\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8095238095238095\n",
      "batch_idx: 21\n",
      "0.8106060606060606\n",
      "batch_idx: 22\n",
      "0.8043478260869565\n",
      "batch_idx: 23\n",
      "0.8055555555555556\n",
      "batch_idx: 24\n",
      "0.8066666666666666\n",
      "Training Epoch: 766, total loss: 42.626480\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.7976190476190477\n",
      "batch_idx: 14\n",
      "0.7944444444444444\n",
      "batch_idx: 15\n",
      "0.796875\n",
      "batch_idx: 16\n",
      "0.7941176470588235\n",
      "batch_idx: 17\n",
      "0.7962962962962963\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.8049242424242424\n",
      "batch_idx: 22\n",
      "0.8079710144927537\n",
      "batch_idx: 23\n",
      "0.8055555555555556\n",
      "batch_idx: 24\n",
      "0.8066666666666666\n",
      "Training Epoch: 767, total loss: 42.762419\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.8005952380952381\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.7942708333333334\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.7986111111111112\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8095238095238095\n",
      "batch_idx: 21\n",
      "0.8068181818181818\n",
      "batch_idx: 22\n",
      "0.8079710144927537\n",
      "batch_idx: 23\n",
      "0.8090277777777778\n",
      "batch_idx: 24\n",
      "0.8033333333333333\n",
      "Training Epoch: 768, total loss: 42.765871\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8263888888888888\n",
      "batch_idx: 18\n",
      "0.8333333333333334\n",
      "batch_idx: 19\n",
      "0.83125\n",
      "batch_idx: 20\n",
      "0.8234126984126984\n",
      "batch_idx: 21\n",
      "0.8238636363636364\n",
      "batch_idx: 22\n",
      "0.8242753623188406\n",
      "batch_idx: 23\n",
      "0.8263888888888888\n",
      "batch_idx: 24\n",
      "0.8266666666666667\n",
      "Training Epoch: 769, total loss: 42.260333\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.8088235294117647\n",
      "batch_idx: 17\n",
      "0.8101851851851852\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8145833333333333\n",
      "batch_idx: 20\n",
      "0.8134920634920635\n",
      "batch_idx: 21\n",
      "0.8143939393939394\n",
      "batch_idx: 22\n",
      "0.8134057971014492\n",
      "batch_idx: 23\n",
      "0.8194444444444444\n",
      "batch_idx: 24\n",
      "0.82\n",
      "Training Epoch: 770, total loss: 42.400027\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8154761904761905\n",
      "batch_idx: 14\n",
      "0.825\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8310185185185185\n",
      "batch_idx: 18\n",
      "0.8333333333333334\n",
      "batch_idx: 19\n",
      "0.8354166666666667\n",
      "batch_idx: 20\n",
      "0.8313492063492064\n",
      "batch_idx: 21\n",
      "0.8295454545454546\n",
      "batch_idx: 22\n",
      "0.8333333333333334\n",
      "batch_idx: 23\n",
      "0.8298611111111112\n",
      "batch_idx: 24\n",
      "0.8316666666666667\n",
      "Training Epoch: 771, total loss: 42.262371\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8363095238095238\n",
      "batch_idx: 14\n",
      "0.8222222222222222\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8101851851851852\n",
      "batch_idx: 18\n",
      "0.8135964912280702\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8095238095238095\n",
      "batch_idx: 21\n",
      "0.8087121212121212\n",
      "batch_idx: 22\n",
      "0.8079710144927537\n",
      "batch_idx: 23\n",
      "0.8090277777777778\n",
      "batch_idx: 24\n",
      "0.815\n",
      "Training Epoch: 772, total loss: 42.425321\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.7948717948717948\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.7994791666666666\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.8026315789473685\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.8049242424242424\n",
      "batch_idx: 22\n",
      "0.8079710144927537\n",
      "batch_idx: 23\n",
      "0.8038194444444444\n",
      "batch_idx: 24\n",
      "0.8066666666666666\n",
      "Training Epoch: 773, total loss: 42.581856\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8154761904761905\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8098958333333334\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8179824561403509\n",
      "batch_idx: 19\n",
      "0.8208333333333333\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.8125\n",
      "batch_idx: 22\n",
      "0.8152173913043478\n",
      "batch_idx: 23\n",
      "0.8125\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 774, total loss: 42.565049\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7878787878787878\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8104166666666667\n",
      "batch_idx: 20\n",
      "0.8095238095238095\n",
      "batch_idx: 21\n",
      "0.8049242424242424\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8125\n",
      "batch_idx: 24\n",
      "0.8133333333333334\n",
      "Training Epoch: 775, total loss: 42.522929\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7878787878787878\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7948717948717948\n",
      "batch_idx: 13\n",
      "0.7916666666666666\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.8098958333333334\n",
      "batch_idx: 16\n",
      "0.8063725490196079\n",
      "batch_idx: 17\n",
      "0.7986111111111112\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.8049242424242424\n",
      "batch_idx: 22\n",
      "0.8097826086956522\n",
      "batch_idx: 23\n",
      "0.8090277777777778\n",
      "batch_idx: 24\n",
      "0.8066666666666666\n",
      "Training Epoch: 776, total loss: 42.701661\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7948717948717948\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.7867647058823529\n",
      "batch_idx: 17\n",
      "0.7893518518518519\n",
      "batch_idx: 18\n",
      "0.7916666666666666\n",
      "batch_idx: 19\n",
      "0.79375\n",
      "batch_idx: 20\n",
      "0.7896825396825397\n",
      "batch_idx: 21\n",
      "0.7897727272727273\n",
      "batch_idx: 22\n",
      "0.7898550724637681\n",
      "batch_idx: 23\n",
      "0.7951388888888888\n",
      "batch_idx: 24\n",
      "0.7966666666666666\n",
      "Training Epoch: 777, total loss: 42.742797\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.7951388888888888\n",
      "batch_idx: 12\n",
      "0.8044871794871795\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.7962962962962963\n",
      "batch_idx: 18\n",
      "0.8004385964912281\n",
      "batch_idx: 19\n",
      "0.7979166666666667\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.7973484848484849\n",
      "batch_idx: 22\n",
      "0.7971014492753623\n",
      "batch_idx: 23\n",
      "0.7986111111111112\n",
      "batch_idx: 24\n",
      "0.8\n",
      "Training Epoch: 778, total loss: 42.624783\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8422619047619048\n",
      "batch_idx: 14\n",
      "0.8388888888888889\n",
      "batch_idx: 15\n",
      "0.8385416666666666\n",
      "batch_idx: 16\n",
      "0.8406862745098039\n",
      "batch_idx: 17\n",
      "0.8425925925925926\n",
      "batch_idx: 18\n",
      "0.8399122807017544\n",
      "batch_idx: 19\n",
      "0.8375\n",
      "batch_idx: 20\n",
      "0.8313492063492064\n",
      "batch_idx: 21\n",
      "0.8314393939393939\n",
      "batch_idx: 22\n",
      "0.8297101449275363\n",
      "batch_idx: 23\n",
      "0.8315972222222222\n",
      "batch_idx: 24\n",
      "0.8266666666666667\n",
      "Training Epoch: 779, total loss: 42.244506\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8179824561403509\n",
      "batch_idx: 19\n",
      "0.81875\n",
      "batch_idx: 20\n",
      "0.8214285714285714\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.8188405797101449\n",
      "batch_idx: 23\n",
      "0.8246527777777778\n",
      "batch_idx: 24\n",
      "0.815\n",
      "Training Epoch: 780, total loss: 42.561088\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8098958333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8135964912280702\n",
      "batch_idx: 19\n",
      "0.8104166666666667\n",
      "batch_idx: 20\n",
      "0.8015873015873016\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.7952898550724637\n",
      "batch_idx: 23\n",
      "0.796875\n",
      "batch_idx: 24\n",
      "0.8\n",
      "Training Epoch: 781, total loss: 42.712127\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8222222222222222\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.8267543859649122\n",
      "batch_idx: 19\n",
      "0.8208333333333333\n",
      "batch_idx: 20\n",
      "0.8273809523809523\n",
      "batch_idx: 21\n",
      "0.8200757575757576\n",
      "batch_idx: 22\n",
      "0.8170289855072463\n",
      "batch_idx: 23\n",
      "0.8142361111111112\n",
      "batch_idx: 24\n",
      "0.815\n",
      "Training Epoch: 782, total loss: 42.423262\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8244047619047619\n",
      "batch_idx: 14\n",
      "0.8222222222222222\n",
      "batch_idx: 15\n",
      "0.828125\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8310185185185185\n",
      "batch_idx: 18\n",
      "0.8333333333333334\n",
      "batch_idx: 19\n",
      "0.8333333333333334\n",
      "batch_idx: 20\n",
      "0.8353174603174603\n",
      "batch_idx: 21\n",
      "0.8295454545454546\n",
      "batch_idx: 22\n",
      "0.8333333333333334\n",
      "batch_idx: 23\n",
      "0.8315972222222222\n",
      "batch_idx: 24\n",
      "0.8333333333333334\n",
      "Training Epoch: 783, total loss: 42.264520\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.7878787878787878\n",
      "batch_idx: 11\n",
      "0.7951388888888888\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.8005952380952381\n",
      "batch_idx: 14\n",
      "0.8\n",
      "batch_idx: 15\n",
      "0.8020833333333334\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8072916666666666\n",
      "batch_idx: 24\n",
      "0.8066666666666666\n",
      "Training Epoch: 784, total loss: 42.596876\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.7552083333333334\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7689393939393939\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.7692307692307693\n",
      "batch_idx: 13\n",
      "0.7767857142857143\n",
      "batch_idx: 14\n",
      "0.7833333333333333\n",
      "batch_idx: 15\n",
      "0.7864583333333334\n",
      "batch_idx: 16\n",
      "0.7916666666666666\n",
      "batch_idx: 17\n",
      "0.7893518518518519\n",
      "batch_idx: 18\n",
      "0.7894736842105263\n",
      "batch_idx: 19\n",
      "0.79375\n",
      "batch_idx: 20\n",
      "0.8015873015873016\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.802536231884058\n",
      "batch_idx: 23\n",
      "0.8107638888888888\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 785, total loss: 42.476347\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8365384615384616\n",
      "batch_idx: 13\n",
      "0.8333333333333334\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8114035087719298\n",
      "batch_idx: 19\n",
      "0.8083333333333333\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.8049242424242424\n",
      "batch_idx: 22\n",
      "0.8079710144927537\n",
      "batch_idx: 23\n",
      "0.8072916666666666\n",
      "batch_idx: 24\n",
      "0.81\n",
      "Training Epoch: 786, total loss: 42.715774\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.8101851851851852\n",
      "batch_idx: 18\n",
      "0.8179824561403509\n",
      "batch_idx: 19\n",
      "0.8229166666666666\n",
      "batch_idx: 20\n",
      "0.8293650793650794\n",
      "batch_idx: 21\n",
      "0.8333333333333334\n",
      "batch_idx: 22\n",
      "0.8315217391304348\n",
      "batch_idx: 23\n",
      "0.8298611111111112\n",
      "batch_idx: 24\n",
      "0.8333333333333334\n",
      "Training Epoch: 787, total loss: 42.277346\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7546296296296297\n",
      "batch_idx: 9\n",
      "0.7708333333333334\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7767857142857143\n",
      "batch_idx: 14\n",
      "0.7805555555555556\n",
      "batch_idx: 15\n",
      "0.7786458333333334\n",
      "batch_idx: 16\n",
      "0.7794117647058824\n",
      "batch_idx: 17\n",
      "0.7824074074074074\n",
      "batch_idx: 18\n",
      "0.7763157894736842\n",
      "batch_idx: 19\n",
      "0.7666666666666667\n",
      "batch_idx: 20\n",
      "0.7658730158730159\n",
      "batch_idx: 21\n",
      "0.7708333333333334\n",
      "batch_idx: 22\n",
      "0.7771739130434783\n",
      "batch_idx: 23\n",
      "0.7777777777777778\n",
      "batch_idx: 24\n",
      "0.78\n",
      "Training Epoch: 788, total loss: 43.040496\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6458333333333334\n",
      "batch_idx: 2\n",
      "0.6805555555555556\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.7416666666666667\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.7552083333333334\n",
      "batch_idx: 8\n",
      "0.7731481481481481\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7976190476190477\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.796875\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.8083333333333333\n",
      "batch_idx: 20\n",
      "0.8134920634920635\n",
      "batch_idx: 21\n",
      "0.8106060606060606\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8072916666666666\n",
      "batch_idx: 24\n",
      "0.81\n",
      "Training Epoch: 789, total loss: 42.685177\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.8557692307692307\n",
      "batch_idx: 13\n",
      "0.8482142857142857\n",
      "batch_idx: 14\n",
      "0.8416666666666667\n",
      "batch_idx: 15\n",
      "0.8411458333333334\n",
      "batch_idx: 16\n",
      "0.8333333333333334\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8267543859649122\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8214285714285714\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.822463768115942\n",
      "batch_idx: 23\n",
      "0.8194444444444444\n",
      "batch_idx: 24\n",
      "0.8216666666666667\n",
      "Training Epoch: 790, total loss: 42.386818\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8301282051282052\n",
      "batch_idx: 13\n",
      "0.8333333333333334\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.831140350877193\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8234126984126984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 21\n",
      "0.821969696969697\n",
      "batch_idx: 22\n",
      "0.822463768115942\n",
      "batch_idx: 23\n",
      "0.8211805555555556\n",
      "batch_idx: 24\n",
      "0.8166666666666667\n",
      "Training Epoch: 791, total loss: 42.439902\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.8365384615384616\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.8411458333333334\n",
      "batch_idx: 16\n",
      "0.8382352941176471\n",
      "batch_idx: 17\n",
      "0.8402777777777778\n",
      "batch_idx: 18\n",
      "0.8421052631578947\n",
      "batch_idx: 19\n",
      "0.84375\n",
      "batch_idx: 20\n",
      "0.8452380952380952\n",
      "batch_idx: 21\n",
      "0.8446969696969697\n",
      "batch_idx: 22\n",
      "0.8369565217391305\n",
      "batch_idx: 23\n",
      "0.8402777777777778\n",
      "batch_idx: 24\n",
      "0.8433333333333334\n",
      "Training Epoch: 792, total loss: 41.920104\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.8301282051282052\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8263888888888888\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.8166666666666667\n",
      "batch_idx: 20\n",
      "0.8134920634920635\n",
      "batch_idx: 21\n",
      "0.8162878787878788\n",
      "batch_idx: 22\n",
      "0.8188405797101449\n",
      "batch_idx: 23\n",
      "0.8211805555555556\n",
      "batch_idx: 24\n",
      "0.8133333333333334\n",
      "Training Epoch: 793, total loss: 42.544526\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7827380952380952\n",
      "batch_idx: 14\n",
      "0.7833333333333333\n",
      "batch_idx: 15\n",
      "0.7838541666666666\n",
      "batch_idx: 16\n",
      "0.7794117647058824\n",
      "batch_idx: 17\n",
      "0.7731481481481481\n",
      "batch_idx: 18\n",
      "0.7807017543859649\n",
      "batch_idx: 19\n",
      "0.7833333333333333\n",
      "batch_idx: 20\n",
      "0.7797619047619048\n",
      "batch_idx: 21\n",
      "0.7821969696969697\n",
      "batch_idx: 22\n",
      "0.7862318840579711\n",
      "batch_idx: 23\n",
      "0.7847222222222222\n",
      "batch_idx: 24\n",
      "0.7866666666666666\n",
      "Training Epoch: 794, total loss: 42.977944\n",
      "batch_idx: 0\n",
      "1.0\n",
      "batch_idx: 1\n",
      "0.9375\n",
      "batch_idx: 2\n",
      "0.9444444444444444\n",
      "batch_idx: 3\n",
      "0.90625\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.8958333333333334\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8625\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8611111111111112\n",
      "batch_idx: 12\n",
      "0.8525641025641025\n",
      "batch_idx: 13\n",
      "0.8482142857142857\n",
      "batch_idx: 14\n",
      "0.85\n",
      "batch_idx: 15\n",
      "0.8359375\n",
      "batch_idx: 16\n",
      "0.8382352941176471\n",
      "batch_idx: 17\n",
      "0.8287037037037037\n",
      "batch_idx: 18\n",
      "0.8245614035087719\n",
      "batch_idx: 19\n",
      "0.8229166666666666\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.821969696969697\n",
      "batch_idx: 22\n",
      "0.8134057971014492\n",
      "batch_idx: 23\n",
      "0.8142361111111112\n",
      "batch_idx: 24\n",
      "0.8133333333333334\n",
      "Training Epoch: 795, total loss: 42.578548\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8461538461538461\n",
      "batch_idx: 13\n",
      "0.8422619047619048\n",
      "batch_idx: 14\n",
      "0.8305555555555556\n",
      "batch_idx: 15\n",
      "0.828125\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.8087121212121212\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.7916666666666666\n",
      "batch_idx: 24\n",
      "0.7983333333333333\n",
      "Training Epoch: 796, total loss: 42.996849\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.8541666666666666\n",
      "batch_idx: 12\n",
      "0.8269230769230769\n",
      "batch_idx: 13\n",
      "0.8244047619047619\n",
      "batch_idx: 14\n",
      "0.8333333333333334\n",
      "batch_idx: 15\n",
      "0.8359375\n",
      "batch_idx: 16\n",
      "0.8333333333333334\n",
      "batch_idx: 17\n",
      "0.8402777777777778\n",
      "batch_idx: 18\n",
      "0.8399122807017544\n",
      "batch_idx: 19\n",
      "0.8375\n",
      "batch_idx: 20\n",
      "0.8373015873015873\n",
      "batch_idx: 21\n",
      "0.8238636363636364\n",
      "batch_idx: 22\n",
      "0.8206521739130435\n",
      "batch_idx: 23\n",
      "0.8211805555555556\n",
      "batch_idx: 24\n",
      "0.82\n",
      "Training Epoch: 797, total loss: 42.492123\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7731481481481481\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7727272727272727\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.7660256410256411\n",
      "batch_idx: 13\n",
      "0.7738095238095238\n",
      "batch_idx: 14\n",
      "0.7722222222222223\n",
      "batch_idx: 15\n",
      "0.7760416666666666\n",
      "batch_idx: 16\n",
      "0.7769607843137255\n",
      "batch_idx: 17\n",
      "0.7824074074074074\n",
      "batch_idx: 18\n",
      "0.7872807017543859\n",
      "batch_idx: 19\n",
      "0.7854166666666667\n",
      "batch_idx: 20\n",
      "0.7876984126984127\n",
      "batch_idx: 21\n",
      "0.7916666666666666\n",
      "batch_idx: 22\n",
      "0.7898550724637681\n",
      "batch_idx: 23\n",
      "0.7916666666666666\n",
      "batch_idx: 24\n",
      "0.795\n",
      "Training Epoch: 798, total loss: 42.892619\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.8088235294117647\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8154761904761905\n",
      "batch_idx: 21\n",
      "0.8049242424242424\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.7986111111111112\n",
      "batch_idx: 24\n",
      "0.8033333333333333\n",
      "Training Epoch: 799, total loss: 42.687535\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7559523809523809\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.7453703703703703\n",
      "batch_idx: 9\n",
      "0.7541666666666667\n",
      "batch_idx: 10\n",
      "0.7613636363636364\n",
      "batch_idx: 11\n",
      "0.7604166666666666\n",
      "batch_idx: 12\n",
      "0.7660256410256411\n",
      "batch_idx: 13\n",
      "0.7767857142857143\n",
      "batch_idx: 14\n",
      "0.7805555555555556\n",
      "batch_idx: 15\n",
      "0.7838541666666666\n",
      "batch_idx: 16\n",
      "0.7867647058823529\n",
      "batch_idx: 17\n",
      "0.7916666666666666\n",
      "batch_idx: 18\n",
      "0.793859649122807\n",
      "batch_idx: 19\n",
      "0.7958333333333333\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.7935606060606061\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.796875\n",
      "batch_idx: 24\n",
      "0.7833333333333333\n",
      "Training Epoch: 800, total loss: 42.992140\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.725\n",
      "batch_idx: 5\n",
      "0.7361111111111112\n",
      "batch_idx: 6\n",
      "0.7559523809523809\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.7946428571428571\n",
      "batch_idx: 14\n",
      "0.7944444444444444\n",
      "batch_idx: 15\n",
      "0.7994791666666666\n",
      "batch_idx: 16\n",
      "0.7941176470588235\n",
      "batch_idx: 17\n",
      "0.7962962962962963\n",
      "batch_idx: 18\n",
      "0.7960526315789473\n",
      "batch_idx: 19\n",
      "0.79375\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.7973484848484849\n",
      "batch_idx: 22\n",
      "0.802536231884058\n",
      "batch_idx: 23\n",
      "0.796875\n",
      "batch_idx: 24\n",
      "0.7966666666666666\n",
      "Training Epoch: 801, total loss: 42.770794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.890625\n",
      "batch_idx: 8\n",
      "0.8796296296296297\n",
      "batch_idx: 9\n",
      "0.875\n",
      "batch_idx: 10\n",
      "0.875\n",
      "batch_idx: 11\n",
      "0.8680555555555556\n",
      "batch_idx: 12\n",
      "0.8621794871794872\n",
      "batch_idx: 13\n",
      "0.8601190476190477\n",
      "batch_idx: 14\n",
      "0.8472222222222222\n",
      "batch_idx: 15\n",
      "0.84375\n",
      "batch_idx: 16\n",
      "0.8406862745098039\n",
      "batch_idx: 17\n",
      "0.8379629629629629\n",
      "batch_idx: 18\n",
      "0.8399122807017544\n",
      "batch_idx: 19\n",
      "0.8416666666666667\n",
      "batch_idx: 20\n",
      "0.8313492063492064\n",
      "batch_idx: 21\n",
      "0.8333333333333334\n",
      "batch_idx: 22\n",
      "0.8315217391304348\n",
      "batch_idx: 23\n",
      "0.8315972222222222\n",
      "batch_idx: 24\n",
      "0.83\n",
      "Training Epoch: 802, total loss: 42.132166\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.7083333333333334\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8005952380952381\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.8137254901960784\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.8208333333333333\n",
      "batch_idx: 20\n",
      "0.8234126984126984\n",
      "batch_idx: 21\n",
      "0.8257575757575758\n",
      "batch_idx: 22\n",
      "0.8278985507246377\n",
      "batch_idx: 23\n",
      "0.8211805555555556\n",
      "batch_idx: 24\n",
      "0.825\n",
      "Training Epoch: 803, total loss: 42.253658\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7857142857142857\n",
      "batch_idx: 14\n",
      "0.7861111111111111\n",
      "batch_idx: 15\n",
      "0.7864583333333334\n",
      "batch_idx: 16\n",
      "0.7769607843137255\n",
      "batch_idx: 17\n",
      "0.7777777777777778\n",
      "batch_idx: 18\n",
      "0.7807017543859649\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7797619047619048\n",
      "batch_idx: 21\n",
      "0.7821969696969697\n",
      "batch_idx: 22\n",
      "0.7898550724637681\n",
      "batch_idx: 23\n",
      "0.7916666666666666\n",
      "batch_idx: 24\n",
      "0.7933333333333333\n",
      "Training Epoch: 804, total loss: 42.956463\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7731481481481481\n",
      "batch_idx: 9\n",
      "0.75\n",
      "batch_idx: 10\n",
      "0.7537878787878788\n",
      "batch_idx: 11\n",
      "0.7638888888888888\n",
      "batch_idx: 12\n",
      "0.7756410256410257\n",
      "batch_idx: 13\n",
      "0.7767857142857143\n",
      "batch_idx: 14\n",
      "0.7638888888888888\n",
      "batch_idx: 15\n",
      "0.7682291666666666\n",
      "batch_idx: 16\n",
      "0.7696078431372549\n",
      "batch_idx: 17\n",
      "0.7708333333333334\n",
      "batch_idx: 18\n",
      "0.7653508771929824\n",
      "batch_idx: 19\n",
      "0.7645833333333333\n",
      "batch_idx: 20\n",
      "0.7599206349206349\n",
      "batch_idx: 21\n",
      "0.7632575757575758\n",
      "batch_idx: 22\n",
      "0.7663043478260869\n",
      "batch_idx: 23\n",
      "0.7586805555555556\n",
      "batch_idx: 24\n",
      "0.76\n",
      "Training Epoch: 805, total loss: 43.536739\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.7976190476190477\n",
      "batch_idx: 14\n",
      "0.7916666666666666\n",
      "batch_idx: 15\n",
      "0.7916666666666666\n",
      "batch_idx: 16\n",
      "0.8014705882352942\n",
      "batch_idx: 17\n",
      "0.7986111111111112\n",
      "batch_idx: 18\n",
      "0.8004385964912281\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.8015873015873016\n",
      "batch_idx: 21\n",
      "0.7954545454545454\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.7986111111111112\n",
      "batch_idx: 24\n",
      "0.7916666666666666\n",
      "Training Epoch: 806, total loss: 42.959618\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.8044871794871795\n",
      "batch_idx: 13\n",
      "0.8005952380952381\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.7942708333333334\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.8004385964912281\n",
      "batch_idx: 19\n",
      "0.7916666666666666\n",
      "batch_idx: 20\n",
      "0.7876984126984127\n",
      "batch_idx: 21\n",
      "0.7878787878787878\n",
      "batch_idx: 22\n",
      "0.7844202898550725\n",
      "batch_idx: 23\n",
      "0.7777777777777778\n",
      "batch_idx: 24\n",
      "0.78\n",
      "Training Epoch: 807, total loss: 43.141899\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.7948717948717948\n",
      "batch_idx: 13\n",
      "0.8005952380952381\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.7916666666666666\n",
      "batch_idx: 17\n",
      "0.7893518518518519\n",
      "batch_idx: 18\n",
      "0.7872807017543859\n",
      "batch_idx: 19\n",
      "0.7895833333333333\n",
      "batch_idx: 20\n",
      "0.7916666666666666\n",
      "batch_idx: 21\n",
      "0.7821969696969697\n",
      "batch_idx: 22\n",
      "0.7807971014492754\n",
      "batch_idx: 23\n",
      "0.7743055555555556\n",
      "batch_idx: 24\n",
      "0.7733333333333333\n",
      "Training Epoch: 808, total loss: 43.385441\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.7870370370370371\n",
      "batch_idx: 9\n",
      "0.7791666666666667\n",
      "batch_idx: 10\n",
      "0.7727272727272727\n",
      "batch_idx: 11\n",
      "0.7777777777777778\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7886904761904762\n",
      "batch_idx: 14\n",
      "0.7805555555555556\n",
      "batch_idx: 15\n",
      "0.78125\n",
      "batch_idx: 16\n",
      "0.7769607843137255\n",
      "batch_idx: 17\n",
      "0.7731481481481481\n",
      "batch_idx: 18\n",
      "0.7741228070175439\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7777777777777778\n",
      "batch_idx: 21\n",
      "0.7803030303030303\n",
      "batch_idx: 22\n",
      "0.782608695652174\n",
      "batch_idx: 23\n",
      "0.7795138888888888\n",
      "batch_idx: 24\n",
      "0.7816666666666666\n",
      "Training Epoch: 809, total loss: 43.024778\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7827380952380952\n",
      "batch_idx: 14\n",
      "0.7861111111111111\n",
      "batch_idx: 15\n",
      "0.7942708333333334\n",
      "batch_idx: 16\n",
      "0.7941176470588235\n",
      "batch_idx: 17\n",
      "0.7893518518518519\n",
      "batch_idx: 18\n",
      "0.7916666666666666\n",
      "batch_idx: 19\n",
      "0.7958333333333333\n",
      "batch_idx: 20\n",
      "0.7936507936507936\n",
      "batch_idx: 21\n",
      "0.7973484848484849\n",
      "batch_idx: 22\n",
      "0.7971014492753623\n",
      "batch_idx: 23\n",
      "0.8003472222222222\n",
      "batch_idx: 24\n",
      "0.805\n",
      "Training Epoch: 810, total loss: 42.771075\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8044871794871795\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.8055555555555556\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.8014705882352942\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.8075396825396826\n",
      "batch_idx: 21\n",
      "0.8049242424242424\n",
      "batch_idx: 22\n",
      "0.7952898550724637\n",
      "batch_idx: 23\n",
      "0.7951388888888888\n",
      "batch_idx: 24\n",
      "0.795\n",
      "Training Epoch: 811, total loss: 42.894871\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7870370370370371\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.7946428571428571\n",
      "batch_idx: 14\n",
      "0.7944444444444444\n",
      "batch_idx: 15\n",
      "0.7916666666666666\n",
      "batch_idx: 16\n",
      "0.7941176470588235\n",
      "batch_idx: 17\n",
      "0.7986111111111112\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.7979166666666667\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.8049242424242424\n",
      "batch_idx: 22\n",
      "0.802536231884058\n",
      "batch_idx: 23\n",
      "0.796875\n",
      "batch_idx: 24\n",
      "0.8\n",
      "Training Epoch: 812, total loss: 42.918055\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7689393939393939\n",
      "batch_idx: 11\n",
      "0.7638888888888888\n",
      "batch_idx: 12\n",
      "0.7692307692307693\n",
      "batch_idx: 13\n",
      "0.7797619047619048\n",
      "batch_idx: 14\n",
      "0.775\n",
      "batch_idx: 15\n",
      "0.7630208333333334\n",
      "batch_idx: 16\n",
      "0.7671568627450981\n",
      "batch_idx: 17\n",
      "0.7569444444444444\n",
      "batch_idx: 18\n",
      "0.7697368421052632\n",
      "batch_idx: 19\n",
      "0.7708333333333334\n",
      "batch_idx: 20\n",
      "0.7738095238095238\n",
      "batch_idx: 21\n",
      "0.7708333333333334\n",
      "batch_idx: 22\n",
      "0.7735507246376812\n",
      "batch_idx: 23\n",
      "0.78125\n",
      "batch_idx: 24\n",
      "0.7816666666666666\n",
      "Training Epoch: 813, total loss: 43.167490\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.7791666666666667\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n",
      "batch_idx: 12\n",
      "0.7916666666666666\n",
      "batch_idx: 13\n",
      "0.7916666666666666\n",
      "batch_idx: 14\n",
      "0.7888888888888889\n",
      "batch_idx: 15\n",
      "0.7786458333333334\n",
      "batch_idx: 16\n",
      "0.7843137254901961\n",
      "batch_idx: 17\n",
      "0.7870370370370371\n",
      "batch_idx: 18\n",
      "0.7916666666666666\n",
      "batch_idx: 19\n",
      "0.7916666666666666\n",
      "batch_idx: 20\n",
      "0.7956349206349206\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.8043478260869565\n",
      "batch_idx: 23\n",
      "0.8072916666666666\n",
      "batch_idx: 24\n",
      "0.805\n",
      "Training Epoch: 814, total loss: 42.603043\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7638888888888888\n",
      "batch_idx: 9\n",
      "0.7666666666666667\n",
      "batch_idx: 10\n",
      "0.7727272727272727\n",
      "batch_idx: 11\n",
      "0.7638888888888888\n",
      "batch_idx: 12\n",
      "0.7660256410256411\n",
      "batch_idx: 13\n",
      "0.7678571428571429\n",
      "batch_idx: 14\n",
      "0.7722222222222223\n",
      "batch_idx: 15\n",
      "0.7760416666666666\n",
      "batch_idx: 16\n",
      "0.7696078431372549\n",
      "batch_idx: 17\n",
      "0.7754629629629629\n",
      "batch_idx: 18\n",
      "0.7828947368421053\n",
      "batch_idx: 19\n",
      "0.7791666666666667\n",
      "batch_idx: 20\n",
      "0.7757936507936508\n",
      "batch_idx: 21\n",
      "0.7727272727272727\n",
      "batch_idx: 22\n",
      "0.7735507246376812\n",
      "batch_idx: 23\n",
      "0.7743055555555556\n",
      "batch_idx: 24\n",
      "0.7766666666666666\n",
      "Training Epoch: 815, total loss: 43.213960\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8137254901960784\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8114035087719298\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.8106060606060606\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8003472222222222\n",
      "batch_idx: 24\n",
      "0.7983333333333333\n",
      "Training Epoch: 816, total loss: 42.697284\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8055555555555556\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8135964912280702\n",
      "batch_idx: 19\n",
      "0.8208333333333333\n",
      "batch_idx: 20\n",
      "0.8174603174603174\n",
      "batch_idx: 21\n",
      "0.821969696969697\n",
      "batch_idx: 22\n",
      "0.8170289855072463\n",
      "batch_idx: 23\n",
      "0.8177083333333334\n",
      "batch_idx: 24\n",
      "0.8166666666666667\n",
      "Training Epoch: 817, total loss: 42.512301\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.7916666666666666\n",
      "batch_idx: 13\n",
      "0.7857142857142857\n",
      "batch_idx: 14\n",
      "0.7916666666666666\n",
      "batch_idx: 15\n",
      "0.7994791666666666\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.803030303030303\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.7986111111111112\n",
      "batch_idx: 24\n",
      "0.7966666666666666\n",
      "Training Epoch: 818, total loss: 42.867243\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8267543859649122\n",
      "batch_idx: 19\n",
      "0.8208333333333333\n",
      "batch_idx: 20\n",
      "0.8234126984126984\n",
      "batch_idx: 21\n",
      "0.8200757575757576\n",
      "batch_idx: 22\n",
      "0.8188405797101449\n",
      "batch_idx: 23\n",
      "0.8246527777777778\n",
      "batch_idx: 24\n",
      "0.8233333333333334\n",
      "Training Epoch: 819, total loss: 42.331212\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7583333333333333\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7765151515151515\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7946428571428571\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.8063725490196079\n",
      "batch_idx: 17\n",
      "0.8055555555555556\n",
      "batch_idx: 18\n",
      "0.8114035087719298\n",
      "batch_idx: 19\n",
      "0.8104166666666667\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.8003472222222222\n",
      "batch_idx: 24\n",
      "0.7916666666666666\n",
      "Training Epoch: 820, total loss: 42.931463\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.7777777777777778\n",
      "batch_idx: 12\n",
      "0.782051282051282\n",
      "batch_idx: 13\n",
      "0.7886904761904762\n",
      "batch_idx: 14\n",
      "0.7861111111111111\n",
      "batch_idx: 15\n",
      "0.7916666666666666\n",
      "batch_idx: 16\n",
      "0.7916666666666666\n",
      "batch_idx: 17\n",
      "0.7870370370370371\n",
      "batch_idx: 18\n",
      "0.7916666666666666\n",
      "batch_idx: 19\n",
      "0.7958333333333333\n",
      "batch_idx: 20\n",
      "0.7956349206349206\n",
      "batch_idx: 21\n",
      "0.7916666666666666\n",
      "batch_idx: 22\n",
      "0.7952898550724637\n",
      "batch_idx: 23\n",
      "0.7899305555555556\n",
      "batch_idx: 24\n",
      "0.7933333333333333\n",
      "Training Epoch: 821, total loss: 42.707078\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8044871794871795\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8098958333333334\n",
      "batch_idx: 16\n",
      "0.8088235294117647\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8114035087719298\n",
      "batch_idx: 19\n",
      "0.8104166666666667\n",
      "batch_idx: 20\n",
      "0.8075396825396826\n",
      "batch_idx: 21\n",
      "0.8087121212121212\n",
      "batch_idx: 22\n",
      "0.8134057971014492\n",
      "batch_idx: 23\n",
      "0.8090277777777778\n",
      "batch_idx: 24\n",
      "0.8083333333333333\n",
      "Training Epoch: 822, total loss: 42.693583\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.7791666666666667\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.8137254901960784\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8223684210526315\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8234126984126984\n",
      "batch_idx: 21\n",
      "0.821969696969697\n",
      "batch_idx: 22\n",
      "0.8242753623188406\n",
      "batch_idx: 23\n",
      "0.8211805555555556\n",
      "batch_idx: 24\n",
      "0.815\n",
      "Training Epoch: 823, total loss: 42.456608\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.7791666666666667\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8101851851851852\n",
      "batch_idx: 18\n",
      "0.8070175438596491\n",
      "batch_idx: 19\n",
      "0.8104166666666667\n",
      "batch_idx: 20\n",
      "0.8174603174603174\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.8170289855072463\n",
      "batch_idx: 23\n",
      "0.8194444444444444\n",
      "batch_idx: 24\n",
      "0.8233333333333334\n",
      "Training Epoch: 824, total loss: 42.391441\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8333333333333334\n",
      "batch_idx: 14\n",
      "0.8333333333333334\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.8229166666666666\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.8276515151515151\n",
      "batch_idx: 22\n",
      "0.8278985507246377\n",
      "batch_idx: 23\n",
      "0.828125\n",
      "batch_idx: 24\n",
      "0.8333333333333334\n",
      "Training Epoch: 825, total loss: 42.135565\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8135964912280702\n",
      "batch_idx: 19\n",
      "0.8166666666666667\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.8257575757575758\n",
      "batch_idx: 22\n",
      "0.8278985507246377\n",
      "batch_idx: 23\n",
      "0.8315972222222222\n",
      "batch_idx: 24\n",
      "0.835\n",
      "Training Epoch: 826, total loss: 42.136561\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.7916666666666666\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.8055555555555556\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.8115079365079365\n",
      "batch_idx: 21\n",
      "0.8068181818181818\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8159722222222222\n",
      "batch_idx: 24\n",
      "0.8166666666666667\n",
      "Training Epoch: 827, total loss: 42.426917\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8564814814814815\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.8269230769230769\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8222222222222222\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.8055555555555556\n",
      "batch_idx: 18\n",
      "0.793859649122807\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.8049242424242424\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8072916666666666\n",
      "batch_idx: 24\n",
      "0.8066666666666666\n",
      "Training Epoch: 828, total loss: 42.594104\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.8916666666666667\n",
      "batch_idx: 5\n",
      "0.8958333333333334\n",
      "batch_idx: 6\n",
      "0.8809523809523809\n",
      "batch_idx: 7\n",
      "0.875\n",
      "batch_idx: 8\n",
      "0.875\n",
      "batch_idx: 9\n",
      "0.8708333333333333\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8388888888888889\n",
      "batch_idx: 15\n",
      "0.8411458333333334\n",
      "batch_idx: 16\n",
      "0.8406862745098039\n",
      "batch_idx: 17\n",
      "0.8402777777777778\n",
      "batch_idx: 18\n",
      "0.8399122807017544\n",
      "batch_idx: 19\n",
      "0.8416666666666667\n",
      "batch_idx: 20\n",
      "0.8432539682539683\n",
      "batch_idx: 21\n",
      "0.8390151515151515\n",
      "batch_idx: 22\n",
      "0.8369565217391305\n",
      "batch_idx: 23\n",
      "0.8385416666666666\n",
      "batch_idx: 24\n",
      "0.835\n",
      "Training Epoch: 829, total loss: 42.137881\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.8916666666666667\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8186274509803921\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.821969696969697\n",
      "batch_idx: 22\n",
      "0.8242753623188406\n",
      "batch_idx: 23\n",
      "0.8194444444444444\n",
      "batch_idx: 24\n",
      "0.82\n",
      "Training Epoch: 830, total loss: 42.477746\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8298611111111112\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8245614035087719\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.8276515151515151\n",
      "batch_idx: 22\n",
      "0.8278985507246377\n",
      "batch_idx: 23\n",
      "0.8263888888888888\n",
      "batch_idx: 24\n",
      "0.8266666666666667\n",
      "Training Epoch: 831, total loss: 42.374335\n",
      "batch_idx: 0\n",
      "0.5416666666666666\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7878787878787878\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8098958333333334\n",
      "batch_idx: 16\n",
      "0.8137254901960784\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.8289473684210527\n",
      "batch_idx: 19\n",
      "0.83125\n",
      "batch_idx: 20\n",
      "0.8313492063492064\n",
      "batch_idx: 21\n",
      "0.8295454545454546\n",
      "batch_idx: 22\n",
      "0.8278985507246377\n",
      "batch_idx: 23\n",
      "0.8263888888888888\n",
      "batch_idx: 24\n",
      "0.825\n",
      "Training Epoch: 832, total loss: 42.375684\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8298611111111112\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.8305555555555556\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8308823529411765\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.8114035087719298\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.7956349206349206\n",
      "batch_idx: 21\n",
      "0.7973484848484849\n",
      "batch_idx: 22\n",
      "0.7862318840579711\n",
      "batch_idx: 23\n",
      "0.7881944444444444\n",
      "batch_idx: 24\n",
      "0.7933333333333333\n",
      "Training Epoch: 833, total loss: 42.846444\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.9375\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.8137254901960784\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.8072916666666666\n",
      "batch_idx: 24\n",
      "0.8066666666666666\n",
      "Training Epoch: 834, total loss: 42.716638\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8154761904761905\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8179824561403509\n",
      "batch_idx: 19\n",
      "0.81875\n",
      "batch_idx: 20\n",
      "0.8154761904761905\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.8206521739130435\n",
      "batch_idx: 23\n",
      "0.8211805555555556\n",
      "batch_idx: 24\n",
      "0.8266666666666667\n",
      "Training Epoch: 835, total loss: 42.417763\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8809523809523809\n",
      "batch_idx: 7\n",
      "0.8802083333333334\n",
      "batch_idx: 8\n",
      "0.8796296296296297\n",
      "batch_idx: 9\n",
      "0.8708333333333333\n",
      "batch_idx: 10\n",
      "0.8636363636363636\n",
      "batch_idx: 11\n",
      "0.8645833333333334\n",
      "batch_idx: 12\n",
      "0.8557692307692307\n",
      "batch_idx: 13\n",
      "0.8571428571428571\n",
      "batch_idx: 14\n",
      "0.8472222222222222\n",
      "batch_idx: 15\n",
      "0.84375\n",
      "batch_idx: 16\n",
      "0.8382352941176471\n",
      "batch_idx: 17\n",
      "0.8379629629629629\n",
      "batch_idx: 18\n",
      "0.8333333333333334\n",
      "batch_idx: 19\n",
      "0.83125\n",
      "batch_idx: 20\n",
      "0.8293650793650794\n",
      "batch_idx: 21\n",
      "0.8314393939393939\n",
      "batch_idx: 22\n",
      "0.8333333333333334\n",
      "batch_idx: 23\n",
      "0.8333333333333334\n",
      "batch_idx: 24\n",
      "0.8316666666666667\n",
      "Training Epoch: 836, total loss: 42.171383\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8244047619047619\n",
      "batch_idx: 14\n",
      "0.8222222222222222\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8333333333333334\n",
      "batch_idx: 18\n",
      "0.8333333333333334\n",
      "batch_idx: 19\n",
      "0.8291666666666667\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.8200757575757576\n",
      "batch_idx: 22\n",
      "0.822463768115942\n",
      "batch_idx: 23\n",
      "0.8246527777777778\n",
      "batch_idx: 24\n",
      "0.8166666666666667\n",
      "Training Epoch: 837, total loss: 42.334717\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8257575757575758\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8422619047619048\n",
      "batch_idx: 14\n",
      "0.8416666666666667\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8480392156862745\n",
      "batch_idx: 17\n",
      "0.8449074074074074\n",
      "batch_idx: 18\n",
      "0.8464912280701754\n",
      "batch_idx: 19\n",
      "0.8458333333333333\n",
      "batch_idx: 20\n",
      "0.8392857142857143\n",
      "batch_idx: 21\n",
      "0.8409090909090909\n",
      "batch_idx: 22\n",
      "0.8405797101449275\n",
      "batch_idx: 23\n",
      "0.8350694444444444\n",
      "batch_idx: 24\n",
      "0.8383333333333334\n",
      "Training Epoch: 838, total loss: 42.028348\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.825\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8179824561403509\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8273809523809523\n",
      "batch_idx: 21\n",
      "0.821969696969697\n",
      "batch_idx: 22\n",
      "0.8278985507246377\n",
      "batch_idx: 23\n",
      "0.8350694444444444\n",
      "batch_idx: 24\n",
      "0.8383333333333334\n",
      "Training Epoch: 839, total loss: 42.042250\n",
      "batch_idx: 0\n",
      "1.0\n",
      "batch_idx: 1\n",
      "0.9583333333333334\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.90625\n",
      "batch_idx: 4\n",
      "0.9083333333333333\n",
      "batch_idx: 5\n",
      "0.8888888888888888\n",
      "batch_idx: 6\n",
      "0.9047619047619048\n",
      "batch_idx: 7\n",
      "0.890625\n",
      "batch_idx: 8\n",
      "0.8703703703703703\n",
      "batch_idx: 9\n",
      "0.8791666666666667\n",
      "batch_idx: 10\n",
      "0.8825757575757576\n",
      "batch_idx: 11\n",
      "0.875\n",
      "batch_idx: 12\n",
      "0.8717948717948718\n",
      "batch_idx: 13\n",
      "0.8660714285714286\n",
      "batch_idx: 14\n",
      "0.8611111111111112\n",
      "batch_idx: 15\n",
      "0.859375\n",
      "batch_idx: 16\n",
      "0.8480392156862745\n",
      "batch_idx: 17\n",
      "0.8495370370370371\n",
      "batch_idx: 18\n",
      "0.8464912280701754\n",
      "batch_idx: 19\n",
      "0.8416666666666667\n",
      "batch_idx: 20\n",
      "0.8392857142857143\n",
      "batch_idx: 21\n",
      "0.8333333333333334\n",
      "batch_idx: 22\n",
      "0.8297101449275363\n",
      "batch_idx: 23\n",
      "0.828125\n",
      "batch_idx: 24\n",
      "0.8316666666666667\n",
      "Training Epoch: 840, total loss: 42.258263\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.8365384615384616\n",
      "batch_idx: 13\n",
      "0.8333333333333334\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.828125\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.8223684210526315\n",
      "batch_idx: 19\n",
      "0.8229166666666666\n",
      "batch_idx: 20\n",
      "0.8234126984126984\n",
      "batch_idx: 21\n",
      "0.8238636363636364\n",
      "batch_idx: 22\n",
      "0.8188405797101449\n",
      "batch_idx: 23\n",
      "0.8229166666666666\n",
      "batch_idx: 24\n",
      "0.8166666666666667\n",
      "Training Epoch: 841, total loss: 42.385823\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.8365384615384616\n",
      "batch_idx: 13\n",
      "0.8363095238095238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 14\n",
      "0.8333333333333334\n",
      "batch_idx: 15\n",
      "0.8333333333333334\n",
      "batch_idx: 16\n",
      "0.8333333333333334\n",
      "batch_idx: 17\n",
      "0.8263888888888888\n",
      "batch_idx: 18\n",
      "0.8223684210526315\n",
      "batch_idx: 19\n",
      "0.81875\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.8162878787878788\n",
      "batch_idx: 22\n",
      "0.8170289855072463\n",
      "batch_idx: 23\n",
      "0.8142361111111112\n",
      "batch_idx: 24\n",
      "0.8133333333333334\n",
      "Training Epoch: 842, total loss: 42.437678\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.8472222222222222\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.8305555555555556\n",
      "batch_idx: 15\n",
      "0.8359375\n",
      "batch_idx: 16\n",
      "0.8406862745098039\n",
      "batch_idx: 17\n",
      "0.8356481481481481\n",
      "batch_idx: 18\n",
      "0.8289473684210527\n",
      "batch_idx: 19\n",
      "0.8291666666666667\n",
      "batch_idx: 20\n",
      "0.8333333333333334\n",
      "batch_idx: 21\n",
      "0.8276515151515151\n",
      "batch_idx: 22\n",
      "0.8351449275362319\n",
      "batch_idx: 23\n",
      "0.8350694444444444\n",
      "batch_idx: 24\n",
      "0.8316666666666667\n",
      "Training Epoch: 843, total loss: 42.101393\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.8305555555555556\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.8259803921568627\n",
      "batch_idx: 17\n",
      "0.8287037037037037\n",
      "batch_idx: 18\n",
      "0.8267543859649122\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.8188405797101449\n",
      "batch_idx: 23\n",
      "0.8177083333333334\n",
      "batch_idx: 24\n",
      "0.8133333333333334\n",
      "Training Epoch: 844, total loss: 42.593402\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.8137254901960784\n",
      "batch_idx: 17\n",
      "0.8101851851851852\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.8015873015873016\n",
      "batch_idx: 21\n",
      "0.803030303030303\n",
      "batch_idx: 22\n",
      "0.7971014492753623\n",
      "batch_idx: 23\n",
      "0.796875\n",
      "batch_idx: 24\n",
      "0.795\n",
      "Training Epoch: 845, total loss: 42.804217\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8298611111111112\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8217592592592593\n",
      "batch_idx: 18\n",
      "0.8179824561403509\n",
      "batch_idx: 19\n",
      "0.8166666666666667\n",
      "batch_idx: 20\n",
      "0.8214285714285714\n",
      "batch_idx: 21\n",
      "0.8238636363636364\n",
      "batch_idx: 22\n",
      "0.822463768115942\n",
      "batch_idx: 23\n",
      "0.8246527777777778\n",
      "batch_idx: 24\n",
      "0.82\n",
      "Training Epoch: 846, total loss: 42.350758\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7870370370370371\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.8088235294117647\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8179824561403509\n",
      "batch_idx: 19\n",
      "0.8145833333333333\n",
      "batch_idx: 20\n",
      "0.8234126984126984\n",
      "batch_idx: 21\n",
      "0.8295454545454546\n",
      "batch_idx: 22\n",
      "0.8297101449275363\n",
      "batch_idx: 23\n",
      "0.8246527777777778\n",
      "batch_idx: 24\n",
      "0.82\n",
      "Training Epoch: 847, total loss: 42.346406\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.8365384615384616\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.828125\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8075396825396826\n",
      "batch_idx: 21\n",
      "0.8049242424242424\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.8020833333333334\n",
      "batch_idx: 24\n",
      "0.8033333333333333\n",
      "Training Epoch: 848, total loss: 42.806482\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.7625\n",
      "batch_idx: 10\n",
      "0.7727272727272727\n",
      "batch_idx: 11\n",
      "0.7708333333333334\n",
      "batch_idx: 12\n",
      "0.782051282051282\n",
      "batch_idx: 13\n",
      "0.7827380952380952\n",
      "batch_idx: 14\n",
      "0.7833333333333333\n",
      "batch_idx: 15\n",
      "0.7786458333333334\n",
      "batch_idx: 16\n",
      "0.7818627450980392\n",
      "batch_idx: 17\n",
      "0.7777777777777778\n",
      "batch_idx: 18\n",
      "0.7828947368421053\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7857142857142857\n",
      "batch_idx: 21\n",
      "0.7878787878787878\n",
      "batch_idx: 22\n",
      "0.7880434782608695\n",
      "batch_idx: 23\n",
      "0.7916666666666666\n",
      "batch_idx: 24\n",
      "0.79\n",
      "Training Epoch: 849, total loss: 42.848987\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8154761904761905\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8310185185185185\n",
      "batch_idx: 18\n",
      "0.8355263157894737\n",
      "batch_idx: 19\n",
      "0.8395833333333333\n",
      "batch_idx: 20\n",
      "0.8432539682539683\n",
      "batch_idx: 21\n",
      "0.8371212121212122\n",
      "batch_idx: 22\n",
      "0.8387681159420289\n",
      "batch_idx: 23\n",
      "0.8350694444444444\n",
      "batch_idx: 24\n",
      "0.8333333333333334\n",
      "Training Epoch: 850, total loss: 42.283932\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8269230769230769\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8098958333333334\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.81875\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.8238636363636364\n",
      "batch_idx: 22\n",
      "0.8242753623188406\n",
      "batch_idx: 23\n",
      "0.8263888888888888\n",
      "batch_idx: 24\n",
      "0.825\n",
      "Training Epoch: 851, total loss: 42.303154\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8298611111111112\n",
      "batch_idx: 12\n",
      "0.8365384615384616\n",
      "batch_idx: 13\n",
      "0.8363095238095238\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.8359375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 16\n",
      "0.8259803921568627\n",
      "batch_idx: 17\n",
      "0.8356481481481481\n",
      "batch_idx: 18\n",
      "0.8399122807017544\n",
      "batch_idx: 19\n",
      "0.8354166666666667\n",
      "batch_idx: 20\n",
      "0.8373015873015873\n",
      "batch_idx: 21\n",
      "0.8352272727272727\n",
      "batch_idx: 22\n",
      "0.8315217391304348\n",
      "batch_idx: 23\n",
      "0.8298611111111112\n",
      "batch_idx: 24\n",
      "0.8283333333333334\n",
      "Training Epoch: 852, total loss: 42.223201\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.8472222222222222\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8452380952380952\n",
      "batch_idx: 14\n",
      "0.8472222222222222\n",
      "batch_idx: 15\n",
      "0.84375\n",
      "batch_idx: 16\n",
      "0.8333333333333334\n",
      "batch_idx: 17\n",
      "0.8356481481481481\n",
      "batch_idx: 18\n",
      "0.8399122807017544\n",
      "batch_idx: 19\n",
      "0.8375\n",
      "batch_idx: 20\n",
      "0.8373015873015873\n",
      "batch_idx: 21\n",
      "0.8390151515151515\n",
      "batch_idx: 22\n",
      "0.8351449275362319\n",
      "batch_idx: 23\n",
      "0.8315972222222222\n",
      "batch_idx: 24\n",
      "0.835\n",
      "Training Epoch: 853, total loss: 42.010701\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.7870370370370371\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8303571428571429\n",
      "batch_idx: 14\n",
      "0.825\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8217592592592593\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.8229166666666666\n",
      "batch_idx: 20\n",
      "0.8214285714285714\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.8188405797101449\n",
      "batch_idx: 23\n",
      "0.8211805555555556\n",
      "batch_idx: 24\n",
      "0.8216666666666667\n",
      "Training Epoch: 854, total loss: 42.411286\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8055555555555556\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.8088235294117647\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.8208333333333333\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.8200757575757576\n",
      "batch_idx: 22\n",
      "0.8152173913043478\n",
      "batch_idx: 23\n",
      "0.8159722222222222\n",
      "batch_idx: 24\n",
      "0.8166666666666667\n",
      "Training Epoch: 855, total loss: 42.498916\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.8055555555555556\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8186274509803921\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8135964912280702\n",
      "batch_idx: 19\n",
      "0.8166666666666667\n",
      "batch_idx: 20\n",
      "0.8095238095238095\n",
      "batch_idx: 21\n",
      "0.8087121212121212\n",
      "batch_idx: 22\n",
      "0.8079710144927537\n",
      "batch_idx: 23\n",
      "0.8038194444444444\n",
      "batch_idx: 24\n",
      "0.8033333333333333\n",
      "Training Epoch: 856, total loss: 42.586009\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8525641025641025\n",
      "batch_idx: 13\n",
      "0.8244047619047619\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8114035087719298\n",
      "batch_idx: 19\n",
      "0.8166666666666667\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.8068181818181818\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.8003472222222222\n",
      "batch_idx: 24\n",
      "0.8033333333333333\n",
      "Training Epoch: 857, total loss: 42.612726\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7731481481481481\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7788461538461539\n",
      "batch_idx: 13\n",
      "0.7767857142857143\n",
      "batch_idx: 14\n",
      "0.7777777777777778\n",
      "batch_idx: 15\n",
      "0.7838541666666666\n",
      "batch_idx: 16\n",
      "0.7769607843137255\n",
      "batch_idx: 17\n",
      "0.7800925925925926\n",
      "batch_idx: 18\n",
      "0.7828947368421053\n",
      "batch_idx: 19\n",
      "0.7875\n",
      "batch_idx: 20\n",
      "0.7876984126984127\n",
      "batch_idx: 21\n",
      "0.7821969696969697\n",
      "batch_idx: 22\n",
      "0.7844202898550725\n",
      "batch_idx: 23\n",
      "0.78125\n",
      "batch_idx: 24\n",
      "0.7783333333333333\n",
      "Training Epoch: 858, total loss: 43.018801\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.8005952380952381\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8135964912280702\n",
      "batch_idx: 19\n",
      "0.8145833333333333\n",
      "batch_idx: 20\n",
      "0.8154761904761905\n",
      "batch_idx: 21\n",
      "0.8049242424242424\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8055555555555556\n",
      "batch_idx: 24\n",
      "0.8033333333333333\n",
      "Training Epoch: 859, total loss: 42.594186\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8070175438596491\n",
      "batch_idx: 19\n",
      "0.8145833333333333\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.8200757575757576\n",
      "batch_idx: 22\n",
      "0.8134057971014492\n",
      "batch_idx: 23\n",
      "0.8125\n",
      "batch_idx: 24\n",
      "0.8183333333333334\n",
      "Training Epoch: 860, total loss: 42.329661\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7878787878787878\n",
      "batch_idx: 11\n",
      "0.7673611111111112\n",
      "batch_idx: 12\n",
      "0.7628205128205128\n",
      "batch_idx: 13\n",
      "0.7589285714285714\n",
      "batch_idx: 14\n",
      "0.7638888888888888\n",
      "batch_idx: 15\n",
      "0.7708333333333334\n",
      "batch_idx: 16\n",
      "0.7696078431372549\n",
      "batch_idx: 17\n",
      "0.7662037037037037\n",
      "batch_idx: 18\n",
      "0.7719298245614035\n",
      "batch_idx: 19\n",
      "0.7708333333333334\n",
      "batch_idx: 20\n",
      "0.7738095238095238\n",
      "batch_idx: 21\n",
      "0.7821969696969697\n",
      "batch_idx: 22\n",
      "0.7844202898550725\n",
      "batch_idx: 23\n",
      "0.7864583333333334\n",
      "batch_idx: 24\n",
      "0.7866666666666666\n",
      "Training Epoch: 861, total loss: 42.864915\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8020833333333334\n",
      "batch_idx: 16\n",
      "0.8088235294117647\n",
      "batch_idx: 17\n",
      "0.8055555555555556\n",
      "batch_idx: 18\n",
      "0.793859649122807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 19\n",
      "0.79375\n",
      "batch_idx: 20\n",
      "0.7956349206349206\n",
      "batch_idx: 21\n",
      "0.7992424242424242\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.796875\n",
      "batch_idx: 24\n",
      "0.7966666666666666\n",
      "Training Epoch: 862, total loss: 42.881674\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8452380952380952\n",
      "batch_idx: 14\n",
      "0.8527777777777777\n",
      "batch_idx: 15\n",
      "0.8385416666666666\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8310185185185185\n",
      "batch_idx: 18\n",
      "0.8289473684210527\n",
      "batch_idx: 19\n",
      "0.8270833333333333\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.8295454545454546\n",
      "batch_idx: 22\n",
      "0.8260869565217391\n",
      "batch_idx: 23\n",
      "0.8246527777777778\n",
      "batch_idx: 24\n",
      "0.8216666666666667\n",
      "Training Epoch: 863, total loss: 42.346301\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8363095238095238\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8070175438596491\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.8095238095238095\n",
      "batch_idx: 21\n",
      "0.8143939393939394\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8142361111111112\n",
      "batch_idx: 24\n",
      "0.81\n",
      "Training Epoch: 864, total loss: 42.591334\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.8115079365079365\n",
      "batch_idx: 21\n",
      "0.803030303030303\n",
      "batch_idx: 22\n",
      "0.7971014492753623\n",
      "batch_idx: 23\n",
      "0.7934027777777778\n",
      "batch_idx: 24\n",
      "0.7933333333333333\n",
      "Training Epoch: 865, total loss: 42.887737\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8363095238095238\n",
      "batch_idx: 14\n",
      "0.8388888888888889\n",
      "batch_idx: 15\n",
      "0.8359375\n",
      "batch_idx: 16\n",
      "0.8333333333333334\n",
      "batch_idx: 17\n",
      "0.8217592592592593\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.81875\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.8143939393939394\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8142361111111112\n",
      "batch_idx: 24\n",
      "0.8083333333333333\n",
      "Training Epoch: 866, total loss: 42.583237\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7878787878787878\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7916666666666666\n",
      "batch_idx: 13\n",
      "0.7946428571428571\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.7939814814814815\n",
      "batch_idx: 18\n",
      "0.793859649122807\n",
      "batch_idx: 19\n",
      "0.79375\n",
      "batch_idx: 20\n",
      "0.7936507936507936\n",
      "batch_idx: 21\n",
      "0.7992424242424242\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.8020833333333334\n",
      "batch_idx: 24\n",
      "0.805\n",
      "Training Epoch: 867, total loss: 42.687031\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.8063725490196079\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.8083333333333333\n",
      "batch_idx: 20\n",
      "0.8115079365079365\n",
      "batch_idx: 21\n",
      "0.8087121212121212\n",
      "batch_idx: 22\n",
      "0.8079710144927537\n",
      "batch_idx: 23\n",
      "0.8072916666666666\n",
      "batch_idx: 24\n",
      "0.8066666666666666\n",
      "Training Epoch: 868, total loss: 42.668577\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8564814814814815\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.8333333333333334\n",
      "batch_idx: 16\n",
      "0.8357843137254902\n",
      "batch_idx: 17\n",
      "0.8402777777777778\n",
      "batch_idx: 18\n",
      "0.8421052631578947\n",
      "batch_idx: 19\n",
      "0.84375\n",
      "batch_idx: 20\n",
      "0.8392857142857143\n",
      "batch_idx: 21\n",
      "0.8428030303030303\n",
      "batch_idx: 22\n",
      "0.8442028985507246\n",
      "batch_idx: 23\n",
      "0.8454861111111112\n",
      "batch_idx: 24\n",
      "0.8416666666666667\n",
      "Training Epoch: 869, total loss: 42.098081\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.7777777777777778\n",
      "batch_idx: 12\n",
      "0.7788461538461539\n",
      "batch_idx: 13\n",
      "0.7827380952380952\n",
      "batch_idx: 14\n",
      "0.7861111111111111\n",
      "batch_idx: 15\n",
      "0.7838541666666666\n",
      "batch_idx: 16\n",
      "0.7941176470588235\n",
      "batch_idx: 17\n",
      "0.7893518518518519\n",
      "batch_idx: 18\n",
      "0.7916666666666666\n",
      "batch_idx: 19\n",
      "0.7854166666666667\n",
      "batch_idx: 20\n",
      "0.7896825396825397\n",
      "batch_idx: 21\n",
      "0.7878787878787878\n",
      "batch_idx: 22\n",
      "0.7880434782608695\n",
      "batch_idx: 23\n",
      "0.7795138888888888\n",
      "batch_idx: 24\n",
      "0.7816666666666666\n",
      "Training Epoch: 870, total loss: 43.142550\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.782051282051282\n",
      "batch_idx: 13\n",
      "0.7857142857142857\n",
      "batch_idx: 14\n",
      "0.7777777777777778\n",
      "batch_idx: 15\n",
      "0.7864583333333334\n",
      "batch_idx: 16\n",
      "0.7916666666666666\n",
      "batch_idx: 17\n",
      "0.7847222222222222\n",
      "batch_idx: 18\n",
      "0.7828947368421053\n",
      "batch_idx: 19\n",
      "0.7791666666666667\n",
      "batch_idx: 20\n",
      "0.7837301587301587\n",
      "batch_idx: 21\n",
      "0.7840909090909091\n",
      "batch_idx: 22\n",
      "0.7880434782608695\n",
      "batch_idx: 23\n",
      "0.7899305555555556\n",
      "batch_idx: 24\n",
      "0.795\n",
      "Training Epoch: 871, total loss: 42.785137\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.7916666666666666\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.7986111111111112\n",
      "batch_idx: 18\n",
      "0.7960526315789473\n",
      "batch_idx: 19\n",
      "0.7979166666666667\n",
      "batch_idx: 20\n",
      "0.8015873015873016\n",
      "batch_idx: 21\n",
      "0.7992424242424242\n",
      "batch_idx: 22\n",
      "0.7952898550724637\n",
      "batch_idx: 23\n",
      "0.796875\n",
      "batch_idx: 24\n",
      "0.7966666666666666\n",
      "Training Epoch: 872, total loss: 42.869663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.8005952380952381\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8098958333333334\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8174603174603174\n",
      "batch_idx: 21\n",
      "0.8106060606060606\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8107638888888888\n",
      "batch_idx: 24\n",
      "0.8083333333333333\n",
      "Training Epoch: 873, total loss: 42.641151\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.775\n",
      "batch_idx: 10\n",
      "0.7878787878787878\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7886904761904762\n",
      "batch_idx: 14\n",
      "0.7833333333333333\n",
      "batch_idx: 15\n",
      "0.7890625\n",
      "batch_idx: 16\n",
      "0.7867647058823529\n",
      "batch_idx: 17\n",
      "0.7893518518518519\n",
      "batch_idx: 18\n",
      "0.7960526315789473\n",
      "batch_idx: 19\n",
      "0.79375\n",
      "batch_idx: 20\n",
      "0.7916666666666666\n",
      "batch_idx: 21\n",
      "0.7916666666666666\n",
      "batch_idx: 22\n",
      "0.7952898550724637\n",
      "batch_idx: 23\n",
      "0.8003472222222222\n",
      "batch_idx: 24\n",
      "0.7983333333333333\n",
      "Training Epoch: 874, total loss: 42.830607\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7951388888888888\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.7946428571428571\n",
      "batch_idx: 14\n",
      "0.7944444444444444\n",
      "batch_idx: 15\n",
      "0.7890625\n",
      "batch_idx: 16\n",
      "0.7916666666666666\n",
      "batch_idx: 17\n",
      "0.7847222222222222\n",
      "batch_idx: 18\n",
      "0.7894736842105263\n",
      "batch_idx: 19\n",
      "0.7875\n",
      "batch_idx: 20\n",
      "0.7857142857142857\n",
      "batch_idx: 21\n",
      "0.7897727272727273\n",
      "batch_idx: 22\n",
      "0.7934782608695652\n",
      "batch_idx: 23\n",
      "0.7864583333333334\n",
      "batch_idx: 24\n",
      "0.785\n",
      "Training Epoch: 875, total loss: 42.998138\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.7976190476190477\n",
      "batch_idx: 14\n",
      "0.7888888888888889\n",
      "batch_idx: 15\n",
      "0.7916666666666666\n",
      "batch_idx: 16\n",
      "0.7916666666666666\n",
      "batch_idx: 17\n",
      "0.7939814814814815\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.7916666666666666\n",
      "batch_idx: 20\n",
      "0.7996031746031746\n",
      "batch_idx: 21\n",
      "0.7954545454545454\n",
      "batch_idx: 22\n",
      "0.7916666666666666\n",
      "batch_idx: 23\n",
      "0.7864583333333334\n",
      "batch_idx: 24\n",
      "0.7916666666666666\n",
      "Training Epoch: 876, total loss: 43.078971\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8154761904761905\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.7916666666666666\n",
      "batch_idx: 16\n",
      "0.7941176470588235\n",
      "batch_idx: 17\n",
      "0.7962962962962963\n",
      "batch_idx: 18\n",
      "0.7894736842105263\n",
      "batch_idx: 19\n",
      "0.7895833333333333\n",
      "batch_idx: 20\n",
      "0.7857142857142857\n",
      "batch_idx: 21\n",
      "0.7916666666666666\n",
      "batch_idx: 22\n",
      "0.7916666666666666\n",
      "batch_idx: 23\n",
      "0.7934027777777778\n",
      "batch_idx: 24\n",
      "0.795\n",
      "Training Epoch: 877, total loss: 42.856519\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8625\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n",
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8482142857142857\n",
      "batch_idx: 14\n",
      "0.8388888888888889\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8186274509803921\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8134920634920635\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.8188405797101449\n",
      "batch_idx: 23\n",
      "0.8159722222222222\n",
      "batch_idx: 24\n",
      "0.8083333333333333\n",
      "Training Epoch: 878, total loss: 42.640540\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.782051282051282\n",
      "batch_idx: 13\n",
      "0.7857142857142857\n",
      "batch_idx: 14\n",
      "0.7888888888888889\n",
      "batch_idx: 15\n",
      "0.796875\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.8055555555555556\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.79375\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.7897727272727273\n",
      "batch_idx: 22\n",
      "0.7934782608695652\n",
      "batch_idx: 23\n",
      "0.7951388888888888\n",
      "batch_idx: 24\n",
      "0.79\n",
      "Training Epoch: 879, total loss: 42.978785\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7870370370370371\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8154761904761905\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.8063725490196079\n",
      "batch_idx: 17\n",
      "0.8055555555555556\n",
      "batch_idx: 18\n",
      "0.8070175438596491\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8154761904761905\n",
      "batch_idx: 21\n",
      "0.8162878787878788\n",
      "batch_idx: 22\n",
      "0.8206521739130435\n",
      "batch_idx: 23\n",
      "0.8142361111111112\n",
      "batch_idx: 24\n",
      "0.82\n",
      "Training Epoch: 880, total loss: 42.354671\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8244047619047619\n",
      "batch_idx: 14\n",
      "0.8222222222222222\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.8308823529411765\n",
      "batch_idx: 17\n",
      "0.8333333333333334\n",
      "batch_idx: 18\n",
      "0.8377192982456141\n",
      "batch_idx: 19\n",
      "0.8416666666666667\n",
      "batch_idx: 20\n",
      "0.8452380952380952\n",
      "batch_idx: 21\n",
      "0.8409090909090909\n",
      "batch_idx: 22\n",
      "0.842391304347826\n",
      "batch_idx: 23\n",
      "0.8385416666666666\n",
      "batch_idx: 24\n",
      "0.8366666666666667\n",
      "Training Epoch: 881, total loss: 42.032785\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.825\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8101851851851852\n",
      "batch_idx: 18\n",
      "0.8070175438596491\n",
      "batch_idx: 19\n",
      "0.8104166666666667\n",
      "batch_idx: 20\n",
      "0.8134920634920635\n",
      "batch_idx: 21\n",
      "0.8162878787878788\n",
      "batch_idx: 22\n",
      "0.8170289855072463\n",
      "batch_idx: 23\n",
      "0.8194444444444444\n",
      "batch_idx: 24\n",
      "0.8166666666666667\n",
      "Training Epoch: 882, total loss: 42.418126\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.7886904761904762\n",
      "batch_idx: 14\n",
      "0.7861111111111111\n",
      "batch_idx: 15\n",
      "0.7890625\n",
      "batch_idx: 16\n",
      "0.7916666666666666\n",
      "batch_idx: 17\n",
      "0.7800925925925926\n",
      "batch_idx: 18\n",
      "0.7828947368421053\n",
      "batch_idx: 19\n",
      "0.7895833333333333\n",
      "batch_idx: 20\n",
      "0.7956349206349206\n",
      "batch_idx: 21\n",
      "0.7954545454545454\n",
      "batch_idx: 22\n",
      "0.7916666666666666\n",
      "batch_idx: 23\n",
      "0.7951388888888888\n",
      "batch_idx: 24\n",
      "0.7966666666666666\n",
      "Training Epoch: 883, total loss: 42.685420\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8257575757575758\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.7946428571428571\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.7942708333333334\n",
      "batch_idx: 16\n",
      "0.7916666666666666\n",
      "batch_idx: 17\n",
      "0.7916666666666666\n",
      "batch_idx: 18\n",
      "0.7960526315789473\n",
      "batch_idx: 19\n",
      "0.8041666666666667\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.8049242424242424\n",
      "batch_idx: 22\n",
      "0.8043478260869565\n",
      "batch_idx: 23\n",
      "0.8072916666666666\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 884, total loss: 42.538123\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8365384615384616\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8217592592592593\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.8270833333333333\n",
      "batch_idx: 20\n",
      "0.8234126984126984\n",
      "batch_idx: 21\n",
      "0.8276515151515151\n",
      "batch_idx: 22\n",
      "0.8297101449275363\n",
      "batch_idx: 23\n",
      "0.8333333333333334\n",
      "batch_idx: 24\n",
      "0.8333333333333334\n",
      "Training Epoch: 885, total loss: 42.195393\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8703703703703703\n",
      "batch_idx: 9\n",
      "0.875\n",
      "batch_idx: 10\n",
      "0.8787878787878788\n",
      "batch_idx: 11\n",
      "0.8784722222222222\n",
      "batch_idx: 12\n",
      "0.8782051282051282\n",
      "batch_idx: 13\n",
      "0.8809523809523809\n",
      "batch_idx: 14\n",
      "0.8722222222222222\n",
      "batch_idx: 15\n",
      "0.8671875\n",
      "batch_idx: 16\n",
      "0.8627450980392157\n",
      "batch_idx: 17\n",
      "0.8541666666666666\n",
      "batch_idx: 18\n",
      "0.8508771929824561\n",
      "batch_idx: 19\n",
      "0.8541666666666666\n",
      "batch_idx: 20\n",
      "0.8511904761904762\n",
      "batch_idx: 21\n",
      "0.8428030303030303\n",
      "batch_idx: 22\n",
      "0.8387681159420289\n",
      "batch_idx: 23\n",
      "0.8402777777777778\n",
      "batch_idx: 24\n",
      "0.8366666666666667\n",
      "Training Epoch: 886, total loss: 42.097379\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.8869047619047619\n",
      "batch_idx: 7\n",
      "0.890625\n",
      "batch_idx: 8\n",
      "0.8796296296296297\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8636363636363636\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.8365384615384616\n",
      "batch_idx: 13\n",
      "0.8482142857142857\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.8385416666666666\n",
      "batch_idx: 16\n",
      "0.8357843137254902\n",
      "batch_idx: 17\n",
      "0.8310185185185185\n",
      "batch_idx: 18\n",
      "0.8267543859649122\n",
      "batch_idx: 19\n",
      "0.8270833333333333\n",
      "batch_idx: 20\n",
      "0.8293650793650794\n",
      "batch_idx: 21\n",
      "0.8314393939393939\n",
      "batch_idx: 22\n",
      "0.8297101449275363\n",
      "batch_idx: 23\n",
      "0.8350694444444444\n",
      "batch_idx: 24\n",
      "0.835\n",
      "Training Epoch: 887, total loss: 42.154274\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7886904761904762\n",
      "batch_idx: 14\n",
      "0.7861111111111111\n",
      "batch_idx: 15\n",
      "0.7942708333333334\n",
      "batch_idx: 16\n",
      "0.7941176470588235\n",
      "batch_idx: 17\n",
      "0.7962962962962963\n",
      "batch_idx: 18\n",
      "0.793859649122807\n",
      "batch_idx: 19\n",
      "0.7916666666666666\n",
      "batch_idx: 20\n",
      "0.7956349206349206\n",
      "batch_idx: 21\n",
      "0.7954545454545454\n",
      "batch_idx: 22\n",
      "0.7971014492753623\n",
      "batch_idx: 23\n",
      "0.796875\n",
      "batch_idx: 24\n",
      "0.8016666666666666\n",
      "Training Epoch: 888, total loss: 42.611492\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.8005952380952381\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.796875\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.7958333333333333\n",
      "batch_idx: 20\n",
      "0.7936507936507936\n",
      "batch_idx: 21\n",
      "0.7897727272727273\n",
      "batch_idx: 22\n",
      "0.7952898550724637\n",
      "batch_idx: 23\n",
      "0.7899305555555556\n",
      "batch_idx: 24\n",
      "0.7966666666666666\n",
      "Training Epoch: 889, total loss: 42.695213\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.7777777777777778\n",
      "batch_idx: 12\n",
      "0.7916666666666666\n",
      "batch_idx: 13\n",
      "0.7857142857142857\n",
      "batch_idx: 14\n",
      "0.7888888888888889\n",
      "batch_idx: 15\n",
      "0.78125\n",
      "batch_idx: 16\n",
      "0.7867647058823529\n",
      "batch_idx: 17\n",
      "0.7939814814814815\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.7958333333333333\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.7973484848484849\n",
      "batch_idx: 22\n",
      "0.7952898550724637\n",
      "batch_idx: 23\n",
      "0.7934027777777778\n",
      "batch_idx: 24\n",
      "0.7933333333333333\n",
      "Training Epoch: 890, total loss: 42.877048\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.8145833333333333\n",
      "batch_idx: 20\n",
      "0.8154761904761905\n",
      "batch_idx: 21\n",
      "0.8162878787878788\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8090277777777778\n",
      "batch_idx: 24\n",
      "0.7966666666666666\n",
      "Training Epoch: 891, total loss: 42.945746\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8098958333333334\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.8026315789473685\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.7973484848484849\n",
      "batch_idx: 22\n",
      "0.7898550724637681\n",
      "batch_idx: 23\n",
      "0.7899305555555556\n",
      "batch_idx: 24\n",
      "0.7866666666666666\n",
      "Training Epoch: 892, total loss: 43.071869\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.875\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8244047619047619\n",
      "batch_idx: 14\n",
      "0.825\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.81875\n",
      "batch_idx: 20\n",
      "0.8174603174603174\n",
      "batch_idx: 21\n",
      "0.8143939393939394\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8107638888888888\n",
      "batch_idx: 24\n",
      "0.8133333333333334\n",
      "Training Epoch: 893, total loss: 42.591514\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7013888888888888\n",
      "batch_idx: 6\n",
      "0.7380952380952381\n",
      "batch_idx: 7\n",
      "0.7239583333333334\n",
      "batch_idx: 8\n",
      "0.7314814814814815\n",
      "batch_idx: 9\n",
      "0.75\n",
      "batch_idx: 10\n",
      "0.7537878787878788\n",
      "batch_idx: 11\n",
      "0.7604166666666666\n",
      "batch_idx: 12\n",
      "0.7628205128205128\n",
      "batch_idx: 13\n",
      "0.7559523809523809\n",
      "batch_idx: 14\n",
      "0.7611111111111111\n",
      "batch_idx: 15\n",
      "0.765625\n",
      "batch_idx: 16\n",
      "0.7696078431372549\n",
      "batch_idx: 17\n",
      "0.7754629629629629\n",
      "batch_idx: 18\n",
      "0.7719298245614035\n",
      "batch_idx: 19\n",
      "0.775\n",
      "batch_idx: 20\n",
      "0.7777777777777778\n",
      "batch_idx: 21\n",
      "0.7708333333333334\n",
      "batch_idx: 22\n",
      "0.7717391304347826\n",
      "batch_idx: 23\n",
      "0.7760416666666666\n",
      "batch_idx: 24\n",
      "0.775\n",
      "Training Epoch: 894, total loss: 43.116256\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.8137254901960784\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8083333333333333\n",
      "batch_idx: 20\n",
      "0.8115079365079365\n",
      "batch_idx: 21\n",
      "0.8143939393939394\n",
      "batch_idx: 22\n",
      "0.8152173913043478\n",
      "batch_idx: 23\n",
      "0.8177083333333334\n",
      "batch_idx: 24\n",
      "0.8183333333333334\n",
      "Training Epoch: 895, total loss: 42.447040\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.8461538461538461\n",
      "batch_idx: 13\n",
      "0.8333333333333334\n",
      "batch_idx: 14\n",
      "0.8222222222222222\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.8145833333333333\n",
      "batch_idx: 20\n",
      "0.8154761904761905\n",
      "batch_idx: 21\n",
      "0.8143939393939394\n",
      "batch_idx: 22\n",
      "0.8134057971014492\n",
      "batch_idx: 23\n",
      "0.8194444444444444\n",
      "batch_idx: 24\n",
      "0.8216666666666667\n",
      "Training Epoch: 896, total loss: 42.335768\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8005952380952381\n",
      "batch_idx: 14\n",
      "0.8\n",
      "batch_idx: 15\n",
      "0.796875\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.8055555555555556\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.8115079365079365\n",
      "batch_idx: 21\n",
      "0.8125\n",
      "batch_idx: 22\n",
      "0.8152173913043478\n",
      "batch_idx: 23\n",
      "0.8159722222222222\n",
      "batch_idx: 24\n",
      "0.8183333333333334\n",
      "Training Epoch: 897, total loss: 42.331818\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.8333333333333334\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8310185185185185\n",
      "batch_idx: 18\n",
      "0.8377192982456141\n",
      "batch_idx: 19\n",
      "0.8333333333333334\n",
      "batch_idx: 20\n",
      "0.8333333333333334\n",
      "batch_idx: 21\n",
      "0.8295454545454546\n",
      "batch_idx: 22\n",
      "0.8260869565217391\n",
      "batch_idx: 23\n",
      "0.8263888888888888\n",
      "batch_idx: 24\n",
      "0.825\n",
      "Training Epoch: 898, total loss: 42.160821\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.7946428571428571\n",
      "batch_idx: 14\n",
      "0.8055555555555556\n",
      "batch_idx: 15\n",
      "0.8098958333333334\n",
      "batch_idx: 16\n",
      "0.8137254901960784\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.8291666666666667\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.8200757575757576\n",
      "batch_idx: 22\n",
      "0.8206521739130435\n",
      "batch_idx: 23\n",
      "0.8211805555555556\n",
      "batch_idx: 24\n",
      "0.8216666666666667\n",
      "Training Epoch: 899, total loss: 42.414505\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.8245614035087719\n",
      "batch_idx: 19\n",
      "0.81875\n",
      "batch_idx: 20\n",
      "0.8234126984126984\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.8188405797101449\n",
      "batch_idx: 23\n",
      "0.8194444444444444\n",
      "batch_idx: 24\n",
      "0.82\n",
      "Training Epoch: 900, total loss: 42.552176\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8363095238095238\n",
      "batch_idx: 14\n",
      "0.8388888888888889\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8186274509803921\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8114035087719298\n",
      "batch_idx: 19\n",
      "0.8083333333333333\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.803030303030303\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8055555555555556\n",
      "batch_idx: 24\n",
      "0.8083333333333333\n",
      "Training Epoch: 901, total loss: 42.594799\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.8958333333333334\n",
      "batch_idx: 6\n",
      "0.9047619047619048\n",
      "batch_idx: 7\n",
      "0.8958333333333334\n",
      "batch_idx: 8\n",
      "0.8888888888888888\n",
      "batch_idx: 9\n",
      "0.8833333333333333\n",
      "batch_idx: 10\n",
      "0.8787878787878788\n",
      "batch_idx: 11\n",
      "0.875\n",
      "batch_idx: 12\n",
      "0.8525641025641025\n",
      "batch_idx: 13\n",
      "0.8541666666666666\n",
      "batch_idx: 14\n",
      "0.8527777777777777\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8480392156862745\n",
      "batch_idx: 17\n",
      "0.8449074074074074\n",
      "batch_idx: 18\n",
      "0.8399122807017544\n",
      "batch_idx: 19\n",
      "0.8270833333333333\n",
      "batch_idx: 20\n",
      "0.8293650793650794\n",
      "batch_idx: 21\n",
      "0.8276515151515151\n",
      "batch_idx: 22\n",
      "0.8260869565217391\n",
      "batch_idx: 23\n",
      "0.8263888888888888\n",
      "batch_idx: 24\n",
      "0.83\n",
      "Training Epoch: 902, total loss: 42.121096\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.75\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8452380952380952\n",
      "batch_idx: 14\n",
      "0.8333333333333334\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 16\n",
      "0.8186274509803921\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8135964912280702\n",
      "batch_idx: 19\n",
      "0.8083333333333333\n",
      "batch_idx: 20\n",
      "0.8095238095238095\n",
      "batch_idx: 21\n",
      "0.8106060606060606\n",
      "batch_idx: 22\n",
      "0.8043478260869565\n",
      "batch_idx: 23\n",
      "0.8020833333333334\n",
      "batch_idx: 24\n",
      "0.7983333333333333\n",
      "Training Epoch: 903, total loss: 42.685046\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.8541666666666666\n",
      "batch_idx: 12\n",
      "0.8461538461538461\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8333333333333334\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8259803921568627\n",
      "batch_idx: 17\n",
      "0.8333333333333334\n",
      "batch_idx: 18\n",
      "0.8267543859649122\n",
      "batch_idx: 19\n",
      "0.8208333333333333\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.8238636363636364\n",
      "batch_idx: 22\n",
      "0.8242753623188406\n",
      "batch_idx: 23\n",
      "0.8229166666666666\n",
      "batch_idx: 24\n",
      "0.8266666666666667\n",
      "Training Epoch: 904, total loss: 42.175748\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8452380952380952\n",
      "batch_idx: 14\n",
      "0.85\n",
      "batch_idx: 15\n",
      "0.8515625\n",
      "batch_idx: 16\n",
      "0.8529411764705882\n",
      "batch_idx: 17\n",
      "0.8541666666666666\n",
      "batch_idx: 18\n",
      "0.8486842105263158\n",
      "batch_idx: 19\n",
      "0.8458333333333333\n",
      "batch_idx: 20\n",
      "0.8392857142857143\n",
      "batch_idx: 21\n",
      "0.8333333333333334\n",
      "batch_idx: 22\n",
      "0.8333333333333334\n",
      "batch_idx: 23\n",
      "0.8229166666666666\n",
      "batch_idx: 24\n",
      "0.8266666666666667\n",
      "Training Epoch: 905, total loss: 42.381368\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8154761904761905\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8217592592592593\n",
      "batch_idx: 18\n",
      "0.8223684210526315\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8234126984126984\n",
      "batch_idx: 21\n",
      "0.8276515151515151\n",
      "batch_idx: 22\n",
      "0.8260869565217391\n",
      "batch_idx: 23\n",
      "0.8263888888888888\n",
      "batch_idx: 24\n",
      "0.83\n",
      "Training Epoch: 906, total loss: 42.105867\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7430555555555556\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.7447916666666666\n",
      "batch_idx: 8\n",
      "0.7314814814814815\n",
      "batch_idx: 9\n",
      "0.7416666666666667\n",
      "batch_idx: 10\n",
      "0.7462121212121212\n",
      "batch_idx: 11\n",
      "0.7569444444444444\n",
      "batch_idx: 12\n",
      "0.7692307692307693\n",
      "batch_idx: 13\n",
      "0.7708333333333334\n",
      "batch_idx: 14\n",
      "0.7722222222222223\n",
      "batch_idx: 15\n",
      "0.7838541666666666\n",
      "batch_idx: 16\n",
      "0.7696078431372549\n",
      "batch_idx: 17\n",
      "0.7777777777777778\n",
      "batch_idx: 18\n",
      "0.7763157894736842\n",
      "batch_idx: 19\n",
      "0.7770833333333333\n",
      "batch_idx: 20\n",
      "0.7797619047619048\n",
      "batch_idx: 21\n",
      "0.7727272727272727\n",
      "batch_idx: 22\n",
      "0.7681159420289855\n",
      "batch_idx: 23\n",
      "0.765625\n",
      "batch_idx: 24\n",
      "0.77\n",
      "Training Epoch: 907, total loss: 43.183315\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8257575757575758\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.8208333333333333\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.821969696969697\n",
      "batch_idx: 22\n",
      "0.8188405797101449\n",
      "batch_idx: 23\n",
      "0.8211805555555556\n",
      "batch_idx: 24\n",
      "0.8183333333333334\n",
      "Training Epoch: 908, total loss: 42.432998\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.8044871794871795\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.8166666666666667\n",
      "batch_idx: 20\n",
      "0.8174603174603174\n",
      "batch_idx: 21\n",
      "0.8125\n",
      "batch_idx: 22\n",
      "0.8097826086956522\n",
      "batch_idx: 23\n",
      "0.8142361111111112\n",
      "batch_idx: 24\n",
      "0.81\n",
      "Training Epoch: 909, total loss: 42.681298\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.75\n",
      "batch_idx: 7\n",
      "0.75\n",
      "batch_idx: 8\n",
      "0.7592592592592593\n",
      "batch_idx: 9\n",
      "0.7708333333333334\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.7743055555555556\n",
      "batch_idx: 12\n",
      "0.7756410256410257\n",
      "batch_idx: 13\n",
      "0.7767857142857143\n",
      "batch_idx: 14\n",
      "0.7805555555555556\n",
      "batch_idx: 15\n",
      "0.78125\n",
      "batch_idx: 16\n",
      "0.7916666666666666\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.8004385964912281\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.8068181818181818\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8055555555555556\n",
      "batch_idx: 24\n",
      "0.8083333333333333\n",
      "Training Epoch: 910, total loss: 42.621660\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.775\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.7559523809523809\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8186274509803921\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8179824561403509\n",
      "batch_idx: 19\n",
      "0.81875\n",
      "batch_idx: 20\n",
      "0.8234126984126984\n",
      "batch_idx: 21\n",
      "0.821969696969697\n",
      "batch_idx: 22\n",
      "0.822463768115942\n",
      "batch_idx: 23\n",
      "0.8177083333333334\n",
      "batch_idx: 24\n",
      "0.82\n",
      "Training Epoch: 911, total loss: 42.446963\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.8137254901960784\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8223684210526315\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8115079365079365\n",
      "batch_idx: 21\n",
      "0.8125\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.7986111111111112\n",
      "batch_idx: 24\n",
      "0.7983333333333333\n",
      "Training Epoch: 912, total loss: 42.872682\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.6875\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7678571428571429\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7685185185185185\n",
      "batch_idx: 9\n",
      "0.7708333333333334\n",
      "batch_idx: 10\n",
      "0.7803030303030303\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.8055555555555556\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8135964912280702\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8115079365079365\n",
      "batch_idx: 21\n",
      "0.8106060606060606\n",
      "batch_idx: 22\n",
      "0.8152173913043478\n",
      "batch_idx: 23\n",
      "0.8125\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 913, total loss: 42.455752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7791666666666667\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.8137254901960784\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8293650793650794\n",
      "batch_idx: 21\n",
      "0.8314393939393939\n",
      "batch_idx: 22\n",
      "0.8333333333333334\n",
      "batch_idx: 23\n",
      "0.8298611111111112\n",
      "batch_idx: 24\n",
      "0.8266666666666667\n",
      "Training Epoch: 914, total loss: 42.221258\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.7888888888888889\n",
      "batch_idx: 15\n",
      "0.7838541666666666\n",
      "batch_idx: 16\n",
      "0.7843137254901961\n",
      "batch_idx: 17\n",
      "0.7893518518518519\n",
      "batch_idx: 18\n",
      "0.7960526315789473\n",
      "batch_idx: 19\n",
      "0.79375\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.8049242424242424\n",
      "batch_idx: 22\n",
      "0.802536231884058\n",
      "batch_idx: 23\n",
      "0.8055555555555556\n",
      "batch_idx: 24\n",
      "0.805\n",
      "Training Epoch: 915, total loss: 42.681972\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7916666666666666\n",
      "batch_idx: 14\n",
      "0.7833333333333333\n",
      "batch_idx: 15\n",
      "0.7864583333333334\n",
      "batch_idx: 16\n",
      "0.7892156862745098\n",
      "batch_idx: 17\n",
      "0.7916666666666666\n",
      "batch_idx: 18\n",
      "0.7872807017543859\n",
      "batch_idx: 19\n",
      "0.7895833333333333\n",
      "batch_idx: 20\n",
      "0.7857142857142857\n",
      "batch_idx: 21\n",
      "0.7897727272727273\n",
      "batch_idx: 22\n",
      "0.7880434782608695\n",
      "batch_idx: 23\n",
      "0.7934027777777778\n",
      "batch_idx: 24\n",
      "0.7983333333333333\n",
      "Training Epoch: 916, total loss: 42.874526\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7948717948717948\n",
      "batch_idx: 13\n",
      "0.8035714285714286\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.8020833333333334\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.7939814814814815\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.8095238095238095\n",
      "batch_idx: 21\n",
      "0.8125\n",
      "batch_idx: 22\n",
      "0.8152173913043478\n",
      "batch_idx: 23\n",
      "0.8194444444444444\n",
      "batch_idx: 24\n",
      "0.8183333333333334\n",
      "Training Epoch: 917, total loss: 42.425605\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.7976190476190477\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.8088235294117647\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.8166666666666667\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.8200757575757576\n",
      "batch_idx: 22\n",
      "0.8188405797101449\n",
      "batch_idx: 23\n",
      "0.8142361111111112\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 918, total loss: 42.583301\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.7944444444444444\n",
      "batch_idx: 15\n",
      "0.7890625\n",
      "batch_idx: 16\n",
      "0.7843137254901961\n",
      "batch_idx: 17\n",
      "0.7916666666666666\n",
      "batch_idx: 18\n",
      "0.7916666666666666\n",
      "batch_idx: 19\n",
      "0.7875\n",
      "batch_idx: 20\n",
      "0.7916666666666666\n",
      "batch_idx: 21\n",
      "0.8011363636363636\n",
      "batch_idx: 22\n",
      "0.8043478260869565\n",
      "batch_idx: 23\n",
      "0.8072916666666666\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 919, total loss: 42.414431\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8409090909090909\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8482142857142857\n",
      "batch_idx: 14\n",
      "0.8416666666666667\n",
      "batch_idx: 15\n",
      "0.8515625\n",
      "batch_idx: 16\n",
      "0.8504901960784313\n",
      "batch_idx: 17\n",
      "0.8449074074074074\n",
      "batch_idx: 18\n",
      "0.8399122807017544\n",
      "batch_idx: 19\n",
      "0.84375\n",
      "batch_idx: 20\n",
      "0.8392857142857143\n",
      "batch_idx: 21\n",
      "0.8390151515151515\n",
      "batch_idx: 22\n",
      "0.8405797101449275\n",
      "batch_idx: 23\n",
      "0.8368055555555556\n",
      "batch_idx: 24\n",
      "0.8383333333333334\n",
      "Training Epoch: 920, total loss: 42.141001\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.7951388888888888\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.8137254901960784\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.81875\n",
      "batch_idx: 20\n",
      "0.8214285714285714\n",
      "batch_idx: 21\n",
      "0.8257575757575758\n",
      "batch_idx: 22\n",
      "0.8315217391304348\n",
      "batch_idx: 23\n",
      "0.8350694444444444\n",
      "batch_idx: 24\n",
      "0.8316666666666667\n",
      "Training Epoch: 921, total loss: 42.134670\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8564814814814815\n",
      "batch_idx: 9\n",
      "0.8666666666666667\n",
      "batch_idx: 10\n",
      "0.8636363636363636\n",
      "batch_idx: 11\n",
      "0.8472222222222222\n",
      "batch_idx: 12\n",
      "0.8461538461538461\n",
      "batch_idx: 13\n",
      "0.8482142857142857\n",
      "batch_idx: 14\n",
      "0.85\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8406862745098039\n",
      "batch_idx: 17\n",
      "0.8310185185185185\n",
      "batch_idx: 18\n",
      "0.8289473684210527\n",
      "batch_idx: 19\n",
      "0.83125\n",
      "batch_idx: 20\n",
      "0.8353174603174603\n",
      "batch_idx: 21\n",
      "0.8371212121212122\n",
      "batch_idx: 22\n",
      "0.8387681159420289\n",
      "batch_idx: 23\n",
      "0.8385416666666666\n",
      "batch_idx: 24\n",
      "0.8383333333333334\n",
      "Training Epoch: 922, total loss: 42.088517\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9375\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8363095238095238\n",
      "batch_idx: 14\n",
      "0.8416666666666667\n",
      "batch_idx: 15\n",
      "0.84375\n",
      "batch_idx: 16\n",
      "0.8382352941176471\n",
      "batch_idx: 17\n",
      "0.8333333333333334\n",
      "batch_idx: 18\n",
      "0.8245614035087719\n",
      "batch_idx: 19\n",
      "0.8291666666666667\n",
      "batch_idx: 20\n",
      "0.8293650793650794\n",
      "batch_idx: 21\n",
      "0.8314393939393939\n",
      "batch_idx: 22\n",
      "0.8315217391304348\n",
      "batch_idx: 23\n",
      "0.8298611111111112\n",
      "batch_idx: 24\n",
      "0.8316666666666667\n",
      "Training Epoch: 923, total loss: 42.208340\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8055555555555556\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.8186274509803921\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.81875\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.8238636363636364\n",
      "batch_idx: 22\n",
      "0.8242753623188406\n",
      "batch_idx: 23\n",
      "0.8263888888888888\n",
      "batch_idx: 24\n",
      "0.8266666666666667\n",
      "Training Epoch: 924, total loss: 42.124648\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8796296296296297\n",
      "batch_idx: 9\n",
      "0.8791666666666667\n",
      "batch_idx: 10\n",
      "0.8636363636363636\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.8525641025641025\n",
      "batch_idx: 13\n",
      "0.8482142857142857\n",
      "batch_idx: 14\n",
      "0.85\n",
      "batch_idx: 15\n",
      "0.8411458333333334\n",
      "batch_idx: 16\n",
      "0.8455882352941176\n",
      "batch_idx: 17\n",
      "0.8449074074074074\n",
      "batch_idx: 18\n",
      "0.8486842105263158\n",
      "batch_idx: 19\n",
      "0.85\n",
      "batch_idx: 20\n",
      "0.8472222222222222\n",
      "batch_idx: 21\n",
      "0.8503787878787878\n",
      "batch_idx: 22\n",
      "0.8496376811594203\n",
      "batch_idx: 23\n",
      "0.8506944444444444\n",
      "batch_idx: 24\n",
      "0.8533333333333334\n",
      "Training Epoch: 925, total loss: 41.788025\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8154761904761905\n",
      "batch_idx: 14\n",
      "0.8055555555555556\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.8063725490196079\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8135964912280702\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8095238095238095\n",
      "batch_idx: 21\n",
      "0.8087121212121212\n",
      "batch_idx: 22\n",
      "0.8097826086956522\n",
      "batch_idx: 23\n",
      "0.8107638888888888\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 926, total loss: 42.619942\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.7986111111111112\n",
      "batch_idx: 12\n",
      "0.7980769230769231\n",
      "batch_idx: 13\n",
      "0.7946428571428571\n",
      "batch_idx: 14\n",
      "0.7916666666666666\n",
      "batch_idx: 15\n",
      "0.7890625\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.7893518518518519\n",
      "batch_idx: 18\n",
      "0.7960526315789473\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.8049242424242424\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8055555555555556\n",
      "batch_idx: 24\n",
      "0.8033333333333333\n",
      "Training Epoch: 927, total loss: 42.726089\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8333333333333334\n",
      "batch_idx: 14\n",
      "0.8333333333333334\n",
      "batch_idx: 15\n",
      "0.8385416666666666\n",
      "batch_idx: 16\n",
      "0.8406862745098039\n",
      "batch_idx: 17\n",
      "0.8379629629629629\n",
      "batch_idx: 18\n",
      "0.8355263157894737\n",
      "batch_idx: 19\n",
      "0.8333333333333334\n",
      "batch_idx: 20\n",
      "0.8333333333333334\n",
      "batch_idx: 21\n",
      "0.8333333333333334\n",
      "batch_idx: 22\n",
      "0.8333333333333334\n",
      "batch_idx: 23\n",
      "0.828125\n",
      "batch_idx: 24\n",
      "0.8316666666666667\n",
      "Training Epoch: 928, total loss: 42.177934\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.8012820512820513\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8259803921568627\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.8267543859649122\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8234126984126984\n",
      "batch_idx: 21\n",
      "0.8257575757575758\n",
      "batch_idx: 22\n",
      "0.8242753623188406\n",
      "batch_idx: 23\n",
      "0.8246527777777778\n",
      "batch_idx: 24\n",
      "0.825\n",
      "Training Epoch: 929, total loss: 42.209523\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.8055555555555556\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8137254901960784\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.81875\n",
      "batch_idx: 20\n",
      "0.8234126984126984\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.822463768115942\n",
      "batch_idx: 23\n",
      "0.8211805555555556\n",
      "batch_idx: 24\n",
      "0.815\n",
      "Training Epoch: 930, total loss: 42.533655\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.6875\n",
      "batch_idx: 2\n",
      "0.7222222222222222\n",
      "batch_idx: 3\n",
      "0.71875\n",
      "batch_idx: 4\n",
      "0.75\n",
      "batch_idx: 5\n",
      "0.75\n",
      "batch_idx: 6\n",
      "0.7559523809523809\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.7791666666666667\n",
      "batch_idx: 10\n",
      "0.7765151515151515\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.782051282051282\n",
      "batch_idx: 13\n",
      "0.7886904761904762\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.796875\n",
      "batch_idx: 16\n",
      "0.7990196078431373\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.8229166666666666\n",
      "batch_idx: 20\n",
      "0.8154761904761905\n",
      "batch_idx: 21\n",
      "0.8106060606060606\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8142361111111112\n",
      "batch_idx: 24\n",
      "0.81\n",
      "Training Epoch: 931, total loss: 42.604455\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.7731481481481481\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8092105263157895\n",
      "batch_idx: 19\n",
      "0.81875\n",
      "batch_idx: 20\n",
      "0.8134920634920635\n",
      "batch_idx: 21\n",
      "0.8162878787878788\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8159722222222222\n",
      "batch_idx: 24\n",
      "0.8133333333333334\n",
      "Training Epoch: 932, total loss: 42.522321\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8244047619047619\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8287037037037037\n",
      "batch_idx: 18\n",
      "0.8223684210526315\n",
      "batch_idx: 19\n",
      "0.81875\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.8257575757575758\n",
      "batch_idx: 22\n",
      "0.8297101449275363\n",
      "batch_idx: 23\n",
      "0.828125\n",
      "batch_idx: 24\n",
      "0.83\n",
      "Training Epoch: 933, total loss: 42.286822\n",
      "batch_idx: 0\n",
      "1.0\n",
      "batch_idx: 1\n",
      "0.9375\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.875\n",
      "batch_idx: 7\n",
      "0.875\n",
      "batch_idx: 8\n",
      "0.8703703703703703\n",
      "batch_idx: 9\n",
      "0.8708333333333333\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 11\n",
      "0.8472222222222222\n",
      "batch_idx: 12\n",
      "0.8461538461538461\n",
      "batch_idx: 13\n",
      "0.8511904761904762\n",
      "batch_idx: 14\n",
      "0.8472222222222222\n",
      "batch_idx: 15\n",
      "0.8359375\n",
      "batch_idx: 16\n",
      "0.8406862745098039\n",
      "batch_idx: 17\n",
      "0.8379629629629629\n",
      "batch_idx: 18\n",
      "0.8399122807017544\n",
      "batch_idx: 19\n",
      "0.8416666666666667\n",
      "batch_idx: 20\n",
      "0.8373015873015873\n",
      "batch_idx: 21\n",
      "0.8314393939393939\n",
      "batch_idx: 22\n",
      "0.8278985507246377\n",
      "batch_idx: 23\n",
      "0.8315972222222222\n",
      "batch_idx: 24\n",
      "0.835\n",
      "Training Epoch: 934, total loss: 42.067459\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8154761904761905\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.8055555555555556\n",
      "batch_idx: 18\n",
      "0.8026315789473685\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.803030303030303\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8038194444444444\n",
      "batch_idx: 24\n",
      "0.805\n",
      "Training Epoch: 935, total loss: 42.807636\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.7708333333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7765151515151515\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7916666666666666\n",
      "batch_idx: 14\n",
      "0.7972222222222223\n",
      "batch_idx: 15\n",
      "0.8020833333333334\n",
      "batch_idx: 16\n",
      "0.8014705882352942\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.8095238095238095\n",
      "batch_idx: 21\n",
      "0.8106060606060606\n",
      "batch_idx: 22\n",
      "0.7971014492753623\n",
      "batch_idx: 23\n",
      "0.8003472222222222\n",
      "batch_idx: 24\n",
      "0.8016666666666666\n",
      "Training Epoch: 936, total loss: 42.791799\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8020833333333334\n",
      "batch_idx: 12\n",
      "0.7948717948717948\n",
      "batch_idx: 13\n",
      "0.8005952380952381\n",
      "batch_idx: 14\n",
      "0.8027777777777778\n",
      "batch_idx: 15\n",
      "0.8098958333333334\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8048245614035088\n",
      "batch_idx: 19\n",
      "0.7979166666666667\n",
      "batch_idx: 20\n",
      "0.8015873015873016\n",
      "batch_idx: 21\n",
      "0.803030303030303\n",
      "batch_idx: 22\n",
      "0.802536231884058\n",
      "batch_idx: 23\n",
      "0.8038194444444444\n",
      "batch_idx: 24\n",
      "0.8083333333333333\n",
      "Training Epoch: 937, total loss: 42.575798\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.8200757575757576\n",
      "batch_idx: 22\n",
      "0.8170289855072463\n",
      "batch_idx: 23\n",
      "0.8142361111111112\n",
      "batch_idx: 24\n",
      "0.815\n",
      "Training Epoch: 938, total loss: 42.439909\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.825\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8259803921568627\n",
      "batch_idx: 17\n",
      "0.8287037037037037\n",
      "batch_idx: 18\n",
      "0.8267543859649122\n",
      "batch_idx: 19\n",
      "0.8270833333333333\n",
      "batch_idx: 20\n",
      "0.8273809523809523\n",
      "batch_idx: 21\n",
      "0.8238636363636364\n",
      "batch_idx: 22\n",
      "0.822463768115942\n",
      "batch_idx: 23\n",
      "0.8246527777777778\n",
      "batch_idx: 24\n",
      "0.82\n",
      "Training Epoch: 939, total loss: 42.413710\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8425925925925926\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8365384615384616\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.8333333333333334\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8310185185185185\n",
      "batch_idx: 18\n",
      "0.8289473684210527\n",
      "batch_idx: 19\n",
      "0.8333333333333334\n",
      "batch_idx: 20\n",
      "0.8214285714285714\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.8134057971014492\n",
      "batch_idx: 23\n",
      "0.8125\n",
      "batch_idx: 24\n",
      "0.815\n",
      "Training Epoch: 940, total loss: 42.587048\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8298611111111112\n",
      "batch_idx: 12\n",
      "0.8301282051282052\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.8305555555555556\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8137254901960784\n",
      "batch_idx: 17\n",
      "0.8101851851851852\n",
      "batch_idx: 18\n",
      "0.8179824561403509\n",
      "batch_idx: 19\n",
      "0.8229166666666666\n",
      "batch_idx: 20\n",
      "0.8134920634920635\n",
      "batch_idx: 21\n",
      "0.8143939393939394\n",
      "batch_idx: 22\n",
      "0.8206521739130435\n",
      "batch_idx: 23\n",
      "0.8211805555555556\n",
      "batch_idx: 24\n",
      "0.8183333333333334\n",
      "Training Epoch: 941, total loss: 42.526515\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7760416666666666\n",
      "batch_idx: 8\n",
      "0.7870370370370371\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8072916666666666\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8114035087719298\n",
      "batch_idx: 19\n",
      "0.8041666666666667\n",
      "batch_idx: 20\n",
      "0.8075396825396826\n",
      "batch_idx: 21\n",
      "0.8049242424242424\n",
      "batch_idx: 22\n",
      "0.8097826086956522\n",
      "batch_idx: 23\n",
      "0.8142361111111112\n",
      "batch_idx: 24\n",
      "0.8166666666666667\n",
      "Training Epoch: 942, total loss: 42.468726\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8257575757575758\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.8301282051282052\n",
      "batch_idx: 13\n",
      "0.8244047619047619\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.828125\n",
      "batch_idx: 16\n",
      "0.8308823529411765\n",
      "batch_idx: 17\n",
      "0.8402777777777778\n",
      "batch_idx: 18\n",
      "0.8289473684210527\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8313492063492064\n",
      "batch_idx: 21\n",
      "0.8371212121212122\n",
      "batch_idx: 22\n",
      "0.8442028985507246\n",
      "batch_idx: 23\n",
      "0.8420138888888888\n",
      "batch_idx: 24\n",
      "0.8416666666666667\n",
      "Training Epoch: 943, total loss: 42.060589\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7638888888888888\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8083333333333333\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.8137254901960784\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 18\n",
      "0.8179824561403509\n",
      "batch_idx: 19\n",
      "0.8166666666666667\n",
      "batch_idx: 20\n",
      "0.8154761904761905\n",
      "batch_idx: 21\n",
      "0.8200757575757576\n",
      "batch_idx: 22\n",
      "0.8170289855072463\n",
      "batch_idx: 23\n",
      "0.8194444444444444\n",
      "batch_idx: 24\n",
      "0.8216666666666667\n",
      "Training Epoch: 944, total loss: 42.310137\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7708333333333334\n",
      "batch_idx: 10\n",
      "0.7765151515151515\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7916666666666666\n",
      "batch_idx: 14\n",
      "0.7944444444444444\n",
      "batch_idx: 15\n",
      "0.796875\n",
      "batch_idx: 16\n",
      "0.803921568627451\n",
      "batch_idx: 17\n",
      "0.8055555555555556\n",
      "batch_idx: 18\n",
      "0.8070175438596491\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.8075396825396826\n",
      "batch_idx: 21\n",
      "0.8087121212121212\n",
      "batch_idx: 22\n",
      "0.8152173913043478\n",
      "batch_idx: 23\n",
      "0.8107638888888888\n",
      "batch_idx: 24\n",
      "0.805\n",
      "Training Epoch: 945, total loss: 42.540069\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.75\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.8072916666666666\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7886904761904762\n",
      "batch_idx: 14\n",
      "0.7944444444444444\n",
      "batch_idx: 15\n",
      "0.7942708333333334\n",
      "batch_idx: 16\n",
      "0.7941176470588235\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.7982456140350878\n",
      "batch_idx: 19\n",
      "0.8041666666666667\n",
      "batch_idx: 20\n",
      "0.8075396825396826\n",
      "batch_idx: 21\n",
      "0.8143939393939394\n",
      "batch_idx: 22\n",
      "0.8152173913043478\n",
      "batch_idx: 23\n",
      "0.8159722222222222\n",
      "batch_idx: 24\n",
      "0.8166666666666667\n",
      "Training Epoch: 946, total loss: 42.555511\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8363095238095238\n",
      "batch_idx: 14\n",
      "0.8388888888888889\n",
      "batch_idx: 15\n",
      "0.8411458333333334\n",
      "batch_idx: 16\n",
      "0.8382352941176471\n",
      "batch_idx: 17\n",
      "0.8356481481481481\n",
      "batch_idx: 18\n",
      "0.8377192982456141\n",
      "batch_idx: 19\n",
      "0.8416666666666667\n",
      "batch_idx: 20\n",
      "0.8373015873015873\n",
      "batch_idx: 21\n",
      "0.8409090909090909\n",
      "batch_idx: 22\n",
      "0.8369565217391305\n",
      "batch_idx: 23\n",
      "0.8368055555555556\n",
      "batch_idx: 24\n",
      "0.8366666666666667\n",
      "Training Epoch: 947, total loss: 42.209513\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.8365384615384616\n",
      "batch_idx: 13\n",
      "0.8303571428571429\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8137254901960784\n",
      "batch_idx: 17\n",
      "0.8148148148148148\n",
      "batch_idx: 18\n",
      "0.8223684210526315\n",
      "batch_idx: 19\n",
      "0.81875\n",
      "batch_idx: 20\n",
      "0.8174603174603174\n",
      "batch_idx: 21\n",
      "0.8106060606060606\n",
      "batch_idx: 22\n",
      "0.8134057971014492\n",
      "batch_idx: 23\n",
      "0.8125\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 948, total loss: 42.554838\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.825\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8263888888888888\n",
      "batch_idx: 18\n",
      "0.8179824561403509\n",
      "batch_idx: 19\n",
      "0.81875\n",
      "batch_idx: 20\n",
      "0.8214285714285714\n",
      "batch_idx: 21\n",
      "0.8200757575757576\n",
      "batch_idx: 22\n",
      "0.8242753623188406\n",
      "batch_idx: 23\n",
      "0.8298611111111112\n",
      "batch_idx: 24\n",
      "0.8266666666666667\n",
      "Training Epoch: 949, total loss: 42.358013\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8298611111111112\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8333333333333334\n",
      "batch_idx: 14\n",
      "0.8305555555555556\n",
      "batch_idx: 15\n",
      "0.8333333333333334\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8217592592592593\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.8145833333333333\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.8125\n",
      "batch_idx: 22\n",
      "0.8097826086956522\n",
      "batch_idx: 23\n",
      "0.8125\n",
      "batch_idx: 24\n",
      "0.8133333333333334\n",
      "Training Epoch: 950, total loss: 42.585506\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8154761904761905\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.8166666666666667\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.8200757575757576\n",
      "batch_idx: 22\n",
      "0.8206521739130435\n",
      "batch_idx: 23\n",
      "0.8177083333333334\n",
      "batch_idx: 24\n",
      "0.815\n",
      "Training Epoch: 951, total loss: 42.446065\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8065476190476191\n",
      "batch_idx: 14\n",
      "0.8111111111111111\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.8088235294117647\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.825\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.821969696969697\n",
      "batch_idx: 22\n",
      "0.8242753623188406\n",
      "batch_idx: 23\n",
      "0.8263888888888888\n",
      "batch_idx: 24\n",
      "0.8266666666666667\n",
      "Training Epoch: 952, total loss: 42.356230\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8298611111111112\n",
      "batch_idx: 12\n",
      "0.8269230769230769\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8235294117647058\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8114035087719298\n",
      "batch_idx: 19\n",
      "0.8083333333333333\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.8125\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8177083333333334\n",
      "batch_idx: 24\n",
      "0.815\n",
      "Training Epoch: 953, total loss: 42.367913\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.821969696969697\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.8359375\n",
      "batch_idx: 16\n",
      "0.8382352941176471\n",
      "batch_idx: 17\n",
      "0.8402777777777778\n",
      "batch_idx: 18\n",
      "0.8421052631578947\n",
      "batch_idx: 19\n",
      "0.8375\n",
      "batch_idx: 20\n",
      "0.8392857142857143\n",
      "batch_idx: 21\n",
      "0.8314393939393939\n",
      "batch_idx: 22\n",
      "0.8351449275362319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 23\n",
      "0.8385416666666666\n",
      "batch_idx: 24\n",
      "0.8333333333333334\n",
      "Training Epoch: 954, total loss: 42.125725\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8269230769230769\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.8333333333333334\n",
      "batch_idx: 19\n",
      "0.8395833333333333\n",
      "batch_idx: 20\n",
      "0.8432539682539683\n",
      "batch_idx: 21\n",
      "0.8484848484848485\n",
      "batch_idx: 22\n",
      "0.8514492753623188\n",
      "batch_idx: 23\n",
      "0.8506944444444444\n",
      "batch_idx: 24\n",
      "0.8483333333333334\n",
      "Training Epoch: 955, total loss: 41.839216\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.8472222222222222\n",
      "batch_idx: 12\n",
      "0.8525641025641025\n",
      "batch_idx: 13\n",
      "0.8422619047619048\n",
      "batch_idx: 14\n",
      "0.8416666666666667\n",
      "batch_idx: 15\n",
      "0.8411458333333334\n",
      "batch_idx: 16\n",
      "0.8382352941176471\n",
      "batch_idx: 17\n",
      "0.8379629629629629\n",
      "batch_idx: 18\n",
      "0.8333333333333334\n",
      "batch_idx: 19\n",
      "0.8229166666666666\n",
      "batch_idx: 20\n",
      "0.8115079365079365\n",
      "batch_idx: 21\n",
      "0.8087121212121212\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8125\n",
      "batch_idx: 24\n",
      "0.8166666666666667\n",
      "Training Epoch: 956, total loss: 42.445231\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7083333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7731481481481481\n",
      "batch_idx: 9\n",
      "0.7625\n",
      "batch_idx: 10\n",
      "0.7575757575757576\n",
      "batch_idx: 11\n",
      "0.7638888888888888\n",
      "batch_idx: 12\n",
      "0.7532051282051282\n",
      "batch_idx: 13\n",
      "0.7648809523809523\n",
      "batch_idx: 14\n",
      "0.7666666666666667\n",
      "batch_idx: 15\n",
      "0.765625\n",
      "batch_idx: 16\n",
      "0.7647058823529411\n",
      "batch_idx: 17\n",
      "0.7685185185185185\n",
      "batch_idx: 18\n",
      "0.7741228070175439\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7916666666666666\n",
      "batch_idx: 21\n",
      "0.7935606060606061\n",
      "batch_idx: 22\n",
      "0.7844202898550725\n",
      "batch_idx: 23\n",
      "0.7829861111111112\n",
      "batch_idx: 24\n",
      "0.7866666666666666\n",
      "Training Epoch: 957, total loss: 42.837624\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.9\n",
      "batch_idx: 5\n",
      "0.875\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8303571428571429\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8308823529411765\n",
      "batch_idx: 17\n",
      "0.8379629629629629\n",
      "batch_idx: 18\n",
      "0.8333333333333334\n",
      "batch_idx: 19\n",
      "0.83125\n",
      "batch_idx: 20\n",
      "0.8313492063492064\n",
      "batch_idx: 21\n",
      "0.8276515151515151\n",
      "batch_idx: 22\n",
      "0.8260869565217391\n",
      "batch_idx: 23\n",
      "0.828125\n",
      "batch_idx: 24\n",
      "0.83\n",
      "Training Epoch: 958, total loss: 42.311278\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.8223684210526315\n",
      "batch_idx: 19\n",
      "0.81875\n",
      "batch_idx: 20\n",
      "0.8214285714285714\n",
      "batch_idx: 21\n",
      "0.821969696969697\n",
      "batch_idx: 22\n",
      "0.822463768115942\n",
      "batch_idx: 23\n",
      "0.8211805555555556\n",
      "batch_idx: 24\n",
      "0.82\n",
      "Training Epoch: 959, total loss: 42.363214\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8452380952380952\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8402777777777778\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8541666666666666\n",
      "batch_idx: 14\n",
      "0.8444444444444444\n",
      "batch_idx: 15\n",
      "0.8411458333333334\n",
      "batch_idx: 16\n",
      "0.8406862745098039\n",
      "batch_idx: 17\n",
      "0.8379629629629629\n",
      "batch_idx: 18\n",
      "0.8333333333333334\n",
      "batch_idx: 19\n",
      "0.8270833333333333\n",
      "batch_idx: 20\n",
      "0.8253968253968254\n",
      "batch_idx: 21\n",
      "0.821969696969697\n",
      "batch_idx: 22\n",
      "0.8170289855072463\n",
      "batch_idx: 23\n",
      "0.8159722222222222\n",
      "batch_idx: 24\n",
      "0.8116666666666666\n",
      "Training Epoch: 960, total loss: 42.510835\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8141025641025641\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.8308823529411765\n",
      "batch_idx: 17\n",
      "0.8310185185185185\n",
      "batch_idx: 18\n",
      "0.8267543859649122\n",
      "batch_idx: 19\n",
      "0.8229166666666666\n",
      "batch_idx: 20\n",
      "0.8234126984126984\n",
      "batch_idx: 21\n",
      "0.8257575757575758\n",
      "batch_idx: 22\n",
      "0.8242753623188406\n",
      "batch_idx: 23\n",
      "0.8229166666666666\n",
      "batch_idx: 24\n",
      "0.82\n",
      "Training Epoch: 961, total loss: 42.409551\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7916666666666666\n",
      "batch_idx: 11\n",
      "0.7951388888888888\n",
      "batch_idx: 12\n",
      "0.7948717948717948\n",
      "batch_idx: 13\n",
      "0.7916666666666666\n",
      "batch_idx: 14\n",
      "0.775\n",
      "batch_idx: 15\n",
      "0.7786458333333334\n",
      "batch_idx: 16\n",
      "0.7794117647058824\n",
      "batch_idx: 17\n",
      "0.7824074074074074\n",
      "batch_idx: 18\n",
      "0.7828947368421053\n",
      "batch_idx: 19\n",
      "0.78125\n",
      "batch_idx: 20\n",
      "0.7797619047619048\n",
      "batch_idx: 21\n",
      "0.7840909090909091\n",
      "batch_idx: 22\n",
      "0.7862318840579711\n",
      "batch_idx: 23\n",
      "0.7829861111111112\n",
      "batch_idx: 24\n",
      "0.7783333333333333\n",
      "Training Epoch: 962, total loss: 43.197382\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.8\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.7916666666666666\n",
      "batch_idx: 12\n",
      "0.7916666666666666\n",
      "batch_idx: 13\n",
      "0.7916666666666666\n",
      "batch_idx: 14\n",
      "0.7861111111111111\n",
      "batch_idx: 15\n",
      "0.7916666666666666\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.7962962962962963\n",
      "batch_idx: 18\n",
      "0.7960526315789473\n",
      "batch_idx: 19\n",
      "0.7979166666666667\n",
      "batch_idx: 20\n",
      "0.7857142857142857\n",
      "batch_idx: 21\n",
      "0.7859848484848485\n",
      "batch_idx: 22\n",
      "0.7844202898550725\n",
      "batch_idx: 23\n",
      "0.7899305555555556\n",
      "batch_idx: 24\n",
      "0.795\n",
      "Training Epoch: 963, total loss: 43.001214\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8154761904761905\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8161764705882353\n",
      "batch_idx: 17\n",
      "0.8078703703703703\n",
      "batch_idx: 18\n",
      "0.8026315789473685\n",
      "batch_idx: 19\n",
      "0.8\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.7992424242424242\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.8020833333333334\n",
      "batch_idx: 24\n",
      "0.8033333333333333\n",
      "Training Epoch: 964, total loss: 42.795744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7916666666666666\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.7833333333333333\n",
      "batch_idx: 10\n",
      "0.7765151515151515\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7738095238095238\n",
      "batch_idx: 14\n",
      "0.7777777777777778\n",
      "batch_idx: 15\n",
      "0.765625\n",
      "batch_idx: 16\n",
      "0.7598039215686274\n",
      "batch_idx: 17\n",
      "0.7523148148148148\n",
      "batch_idx: 18\n",
      "0.7609649122807017\n",
      "batch_idx: 19\n",
      "0.7645833333333333\n",
      "batch_idx: 20\n",
      "0.7638888888888888\n",
      "batch_idx: 21\n",
      "0.759469696969697\n",
      "batch_idx: 22\n",
      "0.7644927536231884\n",
      "batch_idx: 23\n",
      "0.7552083333333334\n",
      "batch_idx: 24\n",
      "0.7583333333333333\n",
      "Training Epoch: 965, total loss: 43.543993\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7291666666666666\n",
      "batch_idx: 4\n",
      "0.7333333333333333\n",
      "batch_idx: 5\n",
      "0.7569444444444444\n",
      "batch_idx: 6\n",
      "0.7619047619047619\n",
      "batch_idx: 7\n",
      "0.765625\n",
      "batch_idx: 8\n",
      "0.7592592592592593\n",
      "batch_idx: 9\n",
      "0.7625\n",
      "batch_idx: 10\n",
      "0.7727272727272727\n",
      "batch_idx: 11\n",
      "0.78125\n",
      "batch_idx: 12\n",
      "0.7788461538461539\n",
      "batch_idx: 13\n",
      "0.7738095238095238\n",
      "batch_idx: 14\n",
      "0.7722222222222223\n",
      "batch_idx: 15\n",
      "0.78125\n",
      "batch_idx: 16\n",
      "0.7818627450980392\n",
      "batch_idx: 17\n",
      "0.7708333333333334\n",
      "batch_idx: 18\n",
      "0.7785087719298246\n",
      "batch_idx: 19\n",
      "0.7833333333333333\n",
      "batch_idx: 20\n",
      "0.7718253968253969\n",
      "batch_idx: 21\n",
      "0.7727272727272727\n",
      "batch_idx: 22\n",
      "0.769927536231884\n",
      "batch_idx: 23\n",
      "0.7690972222222222\n",
      "batch_idx: 24\n",
      "0.7716666666666666\n",
      "Training Epoch: 966, total loss: 43.302294\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7976190476190477\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.7954545454545454\n",
      "batch_idx: 11\n",
      "0.7847222222222222\n",
      "batch_idx: 12\n",
      "0.7884615384615384\n",
      "batch_idx: 13\n",
      "0.7886904761904762\n",
      "batch_idx: 14\n",
      "0.7916666666666666\n",
      "batch_idx: 15\n",
      "0.7838541666666666\n",
      "batch_idx: 16\n",
      "0.7892156862745098\n",
      "batch_idx: 17\n",
      "0.7986111111111112\n",
      "batch_idx: 18\n",
      "0.8070175438596491\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.7973484848484849\n",
      "batch_idx: 22\n",
      "0.7952898550724637\n",
      "batch_idx: 23\n",
      "0.7951388888888888\n",
      "batch_idx: 24\n",
      "0.79\n",
      "Training Epoch: 967, total loss: 42.919989\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.7916666666666666\n",
      "batch_idx: 2\n",
      "0.8194444444444444\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.8055555555555556\n",
      "batch_idx: 6\n",
      "0.8154761904761905\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8148148148148148\n",
      "batch_idx: 9\n",
      "0.8208333333333333\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8237179487179487\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.825\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8179824561403509\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8115079365079365\n",
      "batch_idx: 21\n",
      "0.8106060606060606\n",
      "batch_idx: 22\n",
      "0.8097826086956522\n",
      "batch_idx: 23\n",
      "0.8090277777777778\n",
      "batch_idx: 24\n",
      "0.81\n",
      "Training Epoch: 968, total loss: 42.642887\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7708333333333334\n",
      "batch_idx: 8\n",
      "0.7777777777777778\n",
      "batch_idx: 9\n",
      "0.7875\n",
      "batch_idx: 10\n",
      "0.7840909090909091\n",
      "batch_idx: 11\n",
      "0.7881944444444444\n",
      "batch_idx: 12\n",
      "0.7852564102564102\n",
      "batch_idx: 13\n",
      "0.7857142857142857\n",
      "batch_idx: 14\n",
      "0.7888888888888889\n",
      "batch_idx: 15\n",
      "0.7994791666666666\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.8070175438596491\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.8035714285714286\n",
      "batch_idx: 21\n",
      "0.7992424242424242\n",
      "batch_idx: 22\n",
      "0.7971014492753623\n",
      "batch_idx: 23\n",
      "0.796875\n",
      "batch_idx: 24\n",
      "0.7883333333333333\n",
      "Training Epoch: 969, total loss: 43.034521\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8203125\n",
      "batch_idx: 16\n",
      "0.8259803921568627\n",
      "batch_idx: 17\n",
      "0.8217592592592593\n",
      "batch_idx: 18\n",
      "0.8179824561403509\n",
      "batch_idx: 19\n",
      "0.8208333333333333\n",
      "batch_idx: 20\n",
      "0.8174603174603174\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.8242753623188406\n",
      "batch_idx: 23\n",
      "0.8263888888888888\n",
      "batch_idx: 24\n",
      "0.8266666666666667\n",
      "Training Epoch: 970, total loss: 42.241642\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8625\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.8541666666666666\n",
      "batch_idx: 12\n",
      "0.8493589743589743\n",
      "batch_idx: 13\n",
      "0.8511904761904762\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.8411458333333334\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8114035087719298\n",
      "batch_idx: 19\n",
      "0.8145833333333333\n",
      "batch_idx: 20\n",
      "0.8075396825396826\n",
      "batch_idx: 21\n",
      "0.803030303030303\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.8055555555555556\n",
      "batch_idx: 24\n",
      "0.8066666666666666\n",
      "Training Epoch: 971, total loss: 42.572306\n",
      "batch_idx: 0\n",
      "0.6666666666666666\n",
      "batch_idx: 1\n",
      "0.7708333333333334\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.7916666666666666\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.8020833333333334\n",
      "batch_idx: 8\n",
      "0.8055555555555556\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8422619047619048\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.8385416666666666\n",
      "batch_idx: 16\n",
      "0.8406862745098039\n",
      "batch_idx: 17\n",
      "0.8356481481481481\n",
      "batch_idx: 18\n",
      "0.831140350877193\n",
      "batch_idx: 19\n",
      "0.83125\n",
      "batch_idx: 20\n",
      "0.8273809523809523\n",
      "batch_idx: 21\n",
      "0.8276515151515151\n",
      "batch_idx: 22\n",
      "0.8297101449275363\n",
      "batch_idx: 23\n",
      "0.8298611111111112\n",
      "batch_idx: 24\n",
      "0.83\n",
      "Training Epoch: 972, total loss: 42.293324\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.9166666666666666\n",
      "batch_idx: 3\n",
      "0.8854166666666666\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.8809523809523809\n",
      "batch_idx: 7\n",
      "0.8802083333333334\n",
      "batch_idx: 8\n",
      "0.8842592592592593\n",
      "batch_idx: 9\n",
      "0.8791666666666667\n",
      "batch_idx: 10\n",
      "0.875\n",
      "batch_idx: 11\n",
      "0.875\n",
      "batch_idx: 12\n",
      "0.8621794871794872\n",
      "batch_idx: 13\n",
      "0.8630952380952381\n",
      "batch_idx: 14\n",
      "0.8583333333333333\n",
      "batch_idx: 15\n",
      "0.8489583333333334\n",
      "batch_idx: 16\n",
      "0.8455882352941176\n",
      "batch_idx: 17\n",
      "0.8379629629629629\n",
      "batch_idx: 18\n",
      "0.8399122807017544\n",
      "batch_idx: 19\n",
      "0.83125\n",
      "batch_idx: 20\n",
      "0.8273809523809523\n",
      "batch_idx: 21\n",
      "0.8238636363636364\n",
      "batch_idx: 22\n",
      "0.822463768115942\n",
      "batch_idx: 23\n",
      "0.828125\n",
      "batch_idx: 24\n",
      "0.8316666666666667\n",
      "Training Epoch: 973, total loss: 42.280217\n",
      "batch_idx: 0\n",
      "0.625\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.7916666666666666\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.8229166666666666\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8229166666666666\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8263888888888888\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.8229166666666666\n",
      "batch_idx: 20\n",
      "0.8214285714285714\n",
      "batch_idx: 21\n",
      "0.8162878787878788\n",
      "batch_idx: 22\n",
      "0.8152173913043478\n",
      "batch_idx: 23\n",
      "0.8159722222222222\n",
      "batch_idx: 24\n",
      "0.8133333333333334\n",
      "Training Epoch: 974, total loss: 42.438154\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.90625\n",
      "batch_idx: 4\n",
      "0.8916666666666667\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8181818181818182\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.8151041666666666\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.8101851851851852\n",
      "batch_idx: 18\n",
      "0.8135964912280702\n",
      "batch_idx: 19\n",
      "0.8208333333333333\n",
      "batch_idx: 20\n",
      "0.8194444444444444\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.8170289855072463\n",
      "batch_idx: 23\n",
      "0.8211805555555556\n",
      "batch_idx: 24\n",
      "0.8233333333333334\n",
      "Training Epoch: 975, total loss: 42.222870\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9375\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.90625\n",
      "batch_idx: 4\n",
      "0.875\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.859375\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8484848484848485\n",
      "batch_idx: 11\n",
      "0.8472222222222222\n",
      "batch_idx: 12\n",
      "0.8589743589743589\n",
      "batch_idx: 13\n",
      "0.8541666666666666\n",
      "batch_idx: 14\n",
      "0.8555555555555555\n",
      "batch_idx: 15\n",
      "0.84375\n",
      "batch_idx: 16\n",
      "0.8455882352941176\n",
      "batch_idx: 17\n",
      "0.8449074074074074\n",
      "batch_idx: 18\n",
      "0.8355263157894737\n",
      "batch_idx: 19\n",
      "0.8333333333333334\n",
      "batch_idx: 20\n",
      "0.8313492063492064\n",
      "batch_idx: 21\n",
      "0.8295454545454546\n",
      "batch_idx: 22\n",
      "0.8260869565217391\n",
      "batch_idx: 23\n",
      "0.8229166666666666\n",
      "batch_idx: 24\n",
      "0.8233333333333334\n",
      "Training Epoch: 976, total loss: 42.377122\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.8125\n",
      "batch_idx: 4\n",
      "0.8333333333333334\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8257575757575758\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8184523809523809\n",
      "batch_idx: 14\n",
      "0.8166666666666667\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8263888888888888\n",
      "batch_idx: 18\n",
      "0.8289473684210527\n",
      "batch_idx: 19\n",
      "0.8333333333333334\n",
      "batch_idx: 20\n",
      "0.8392857142857143\n",
      "batch_idx: 21\n",
      "0.8371212121212122\n",
      "batch_idx: 22\n",
      "0.8369565217391305\n",
      "batch_idx: 23\n",
      "0.8368055555555556\n",
      "batch_idx: 24\n",
      "0.8383333333333334\n",
      "Training Epoch: 977, total loss: 42.191081\n",
      "batch_idx: 0\n",
      "0.7916666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8690476190476191\n",
      "batch_idx: 7\n",
      "0.8541666666666666\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8303571428571429\n",
      "batch_idx: 14\n",
      "0.8305555555555556\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8284313725490197\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.8267543859649122\n",
      "batch_idx: 19\n",
      "0.8270833333333333\n",
      "batch_idx: 20\n",
      "0.8333333333333334\n",
      "batch_idx: 21\n",
      "0.8257575757575758\n",
      "batch_idx: 22\n",
      "0.8278985507246377\n",
      "batch_idx: 23\n",
      "0.8246527777777778\n",
      "batch_idx: 24\n",
      "0.8266666666666667\n",
      "Training Epoch: 978, total loss: 42.310671\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8287037037037037\n",
      "batch_idx: 9\n",
      "0.8458333333333333\n",
      "batch_idx: 10\n",
      "0.8446969696969697\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8461538461538461\n",
      "batch_idx: 13\n",
      "0.8482142857142857\n",
      "batch_idx: 14\n",
      "0.8416666666666667\n",
      "batch_idx: 15\n",
      "0.8333333333333334\n",
      "batch_idx: 16\n",
      "0.8308823529411765\n",
      "batch_idx: 17\n",
      "0.8333333333333334\n",
      "batch_idx: 18\n",
      "0.8377192982456141\n",
      "batch_idx: 19\n",
      "0.8291666666666667\n",
      "batch_idx: 20\n",
      "0.8293650793650794\n",
      "batch_idx: 21\n",
      "0.8314393939393939\n",
      "batch_idx: 22\n",
      "0.8315217391304348\n",
      "batch_idx: 23\n",
      "0.828125\n",
      "batch_idx: 24\n",
      "0.825\n",
      "Training Epoch: 979, total loss: 42.374915\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8020833333333334\n",
      "batch_idx: 4\n",
      "0.7833333333333333\n",
      "batch_idx: 5\n",
      "0.7777777777777778\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.78125\n",
      "batch_idx: 8\n",
      "0.7962962962962963\n",
      "batch_idx: 9\n",
      "0.8041666666666667\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8194444444444444\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8222222222222222\n",
      "batch_idx: 15\n",
      "0.828125\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8217592592592593\n",
      "batch_idx: 18\n",
      "0.8114035087719298\n",
      "batch_idx: 19\n",
      "0.8145833333333333\n",
      "batch_idx: 20\n",
      "0.8134920634920635\n",
      "batch_idx: 21\n",
      "0.8143939393939394\n",
      "batch_idx: 22\n",
      "0.8007246376811594\n",
      "batch_idx: 23\n",
      "0.8020833333333334\n",
      "batch_idx: 24\n",
      "0.8016666666666666\n",
      "Training Epoch: 980, total loss: 42.877122\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.8888888888888888\n",
      "batch_idx: 6\n",
      "0.8809523809523809\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8472222222222222\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8244047619047619\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.828125\n",
      "batch_idx: 16\n",
      "0.8259803921568627\n",
      "batch_idx: 17\n",
      "0.8125\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.8125\n",
      "batch_idx: 20\n",
      "0.8134920634920635\n",
      "batch_idx: 21\n",
      "0.8049242424242424\n",
      "batch_idx: 22\n",
      "0.8061594202898551\n",
      "batch_idx: 23\n",
      "0.8020833333333334\n",
      "batch_idx: 24\n",
      "0.8033333333333333\n",
      "Training Epoch: 981, total loss: 42.841545\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9166666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.8611111111111112\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8333333333333334\n",
      "batch_idx: 13\n",
      "0.8273809523809523\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.8308823529411765\n",
      "batch_idx: 17\n",
      "0.8240740740740741\n",
      "batch_idx: 18\n",
      "0.8245614035087719\n",
      "batch_idx: 19\n",
      "0.8208333333333333\n",
      "batch_idx: 20\n",
      "0.8214285714285714\n",
      "batch_idx: 21\n",
      "0.8238636363636364\n",
      "batch_idx: 22\n",
      "0.8242753623188406\n",
      "batch_idx: 23\n",
      "0.8229166666666666\n",
      "batch_idx: 24\n",
      "0.8183333333333334\n",
      "Training Epoch: 982, total loss: 42.353927\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.8055555555555556\n",
      "batch_idx: 3\n",
      "0.84375\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8333333333333334\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.8177083333333334\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.825\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8422619047619048\n",
      "batch_idx: 14\n",
      "0.8388888888888889\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8480392156862745\n",
      "batch_idx: 17\n",
      "0.8495370370370371\n",
      "batch_idx: 18\n",
      "0.8464912280701754\n",
      "batch_idx: 19\n",
      "0.8479166666666667\n",
      "batch_idx: 20\n",
      "0.8432539682539683\n",
      "batch_idx: 21\n",
      "0.8409090909090909\n",
      "batch_idx: 22\n",
      "0.8460144927536232\n",
      "batch_idx: 23\n",
      "0.8420138888888888\n",
      "batch_idx: 24\n",
      "0.8433333333333334\n",
      "Training Epoch: 983, total loss: 42.036844\n",
      "batch_idx: 0\n",
      "0.7083333333333334\n",
      "batch_idx: 1\n",
      "0.7291666666666666\n",
      "batch_idx: 2\n",
      "0.6944444444444444\n",
      "batch_idx: 3\n",
      "0.7395833333333334\n",
      "batch_idx: 4\n",
      "0.7666666666666667\n",
      "batch_idx: 5\n",
      "0.7847222222222222\n",
      "batch_idx: 6\n",
      "0.7738095238095238\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.7916666666666666\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.8106060606060606\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8244047619047619\n",
      "batch_idx: 14\n",
      "0.8194444444444444\n",
      "batch_idx: 15\n",
      "0.8177083333333334\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.8145833333333333\n",
      "batch_idx: 20\n",
      "0.8134920634920635\n",
      "batch_idx: 21\n",
      "0.8106060606060606\n",
      "batch_idx: 22\n",
      "0.8115942028985508\n",
      "batch_idx: 23\n",
      "0.8159722222222222\n",
      "batch_idx: 24\n",
      "0.82\n",
      "Training Epoch: 984, total loss: 42.279449\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.8229166666666666\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8819444444444444\n",
      "batch_idx: 6\n",
      "0.8809523809523809\n",
      "batch_idx: 7\n",
      "0.8802083333333334\n",
      "batch_idx: 8\n",
      "0.8935185185185185\n",
      "batch_idx: 9\n",
      "0.9\n",
      "batch_idx: 10\n",
      "0.8977272727272727\n",
      "batch_idx: 11\n",
      "0.8993055555555556\n",
      "batch_idx: 12\n",
      "0.8942307692307693\n",
      "batch_idx: 13\n",
      "0.8898809523809523\n",
      "batch_idx: 14\n",
      "0.8888888888888888\n",
      "batch_idx: 15\n",
      "0.875\n",
      "batch_idx: 16\n",
      "0.8774509803921569\n",
      "batch_idx: 17\n",
      "0.8657407407407407\n",
      "batch_idx: 18\n",
      "0.8596491228070176\n",
      "batch_idx: 19\n",
      "0.8541666666666666\n",
      "batch_idx: 20\n",
      "0.8531746031746031\n",
      "batch_idx: 21\n",
      "0.8446969696969697\n",
      "batch_idx: 22\n",
      "0.842391304347826\n",
      "batch_idx: 23\n",
      "0.84375\n",
      "batch_idx: 24\n",
      "0.84\n",
      "Training Epoch: 985, total loss: 42.085437\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8273809523809523\n",
      "batch_idx: 7\n",
      "0.8333333333333334\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8416666666666667\n",
      "batch_idx: 10\n",
      "0.8371212121212122\n",
      "batch_idx: 11\n",
      "0.84375\n",
      "batch_idx: 12\n",
      "0.8397435897435898\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.8229166666666666\n",
      "batch_idx: 16\n",
      "0.8112745098039216\n",
      "batch_idx: 17\n",
      "0.8055555555555556\n",
      "batch_idx: 18\n",
      "0.8004385964912281\n",
      "batch_idx: 19\n",
      "0.8041666666666667\n",
      "batch_idx: 20\n",
      "0.8075396825396826\n",
      "batch_idx: 21\n",
      "0.8106060606060606\n",
      "batch_idx: 22\n",
      "0.8079710144927537\n",
      "batch_idx: 23\n",
      "0.8142361111111112\n",
      "batch_idx: 24\n",
      "0.8166666666666667\n",
      "Training Epoch: 986, total loss: 42.401831\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.8958333333333334\n",
      "batch_idx: 4\n",
      "0.8916666666666667\n",
      "batch_idx: 5\n",
      "0.8541666666666666\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.8645833333333334\n",
      "batch_idx: 8\n",
      "0.8657407407407407\n",
      "batch_idx: 9\n",
      "0.85\n",
      "batch_idx: 10\n",
      "0.8560606060606061\n",
      "batch_idx: 11\n",
      "0.8506944444444444\n",
      "batch_idx: 12\n",
      "0.8557692307692307\n",
      "batch_idx: 13\n",
      "0.8511904761904762\n",
      "batch_idx: 14\n",
      "0.8527777777777777\n",
      "batch_idx: 15\n",
      "0.8541666666666666\n",
      "batch_idx: 16\n",
      "0.8504901960784313\n",
      "batch_idx: 17\n",
      "0.8379629629629629\n",
      "batch_idx: 18\n",
      "0.8355263157894737\n",
      "batch_idx: 19\n",
      "0.83125\n",
      "batch_idx: 20\n",
      "0.8313492063492064\n",
      "batch_idx: 21\n",
      "0.8352272727272727\n",
      "batch_idx: 22\n",
      "0.8333333333333334\n",
      "batch_idx: 23\n",
      "0.8315972222222222\n",
      "batch_idx: 24\n",
      "0.8283333333333334\n",
      "Training Epoch: 987, total loss: 42.140597\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8833333333333333\n",
      "batch_idx: 5\n",
      "0.8888888888888888\n",
      "batch_idx: 6\n",
      "0.8928571428571429\n",
      "batch_idx: 7\n",
      "0.890625\n",
      "batch_idx: 8\n",
      "0.8842592592592593\n",
      "batch_idx: 9\n",
      "0.8791666666666667\n",
      "batch_idx: 10\n",
      "0.8825757575757576\n",
      "batch_idx: 11\n",
      "0.8784722222222222\n",
      "batch_idx: 12\n",
      "0.8589743589743589\n",
      "batch_idx: 13\n",
      "0.8571428571428571\n",
      "batch_idx: 14\n",
      "0.8555555555555555\n",
      "batch_idx: 15\n",
      "0.8541666666666666\n",
      "batch_idx: 16\n",
      "0.8480392156862745\n",
      "batch_idx: 17\n",
      "0.8472222222222222\n",
      "batch_idx: 18\n",
      "0.8508771929824561\n",
      "batch_idx: 19\n",
      "0.84375\n",
      "batch_idx: 20\n",
      "0.8452380952380952\n",
      "batch_idx: 21\n",
      "0.8428030303030303\n",
      "batch_idx: 22\n",
      "0.8460144927536232\n",
      "batch_idx: 23\n",
      "0.8420138888888888\n",
      "batch_idx: 24\n",
      "0.8366666666666667\n",
      "Training Epoch: 988, total loss: 41.892159\n",
      "batch_idx: 0\n",
      "0.8333333333333334\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8472222222222222\n",
      "batch_idx: 6\n",
      "0.8511904761904762\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8194444444444444\n",
      "batch_idx: 9\n",
      "0.8125\n",
      "batch_idx: 10\n",
      "0.8068181818181818\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8044871794871795\n",
      "batch_idx: 13\n",
      "0.7976190476190477\n",
      "batch_idx: 14\n",
      "0.8\n",
      "batch_idx: 15\n",
      "0.796875\n",
      "batch_idx: 16\n",
      "0.7965686274509803\n",
      "batch_idx: 17\n",
      "0.8032407407407407\n",
      "batch_idx: 18\n",
      "0.8004385964912281\n",
      "batch_idx: 19\n",
      "0.8020833333333334\n",
      "batch_idx: 20\n",
      "0.8015873015873016\n",
      "batch_idx: 21\n",
      "0.8087121212121212\n",
      "batch_idx: 22\n",
      "0.8134057971014492\n",
      "batch_idx: 23\n",
      "0.8142361111111112\n",
      "batch_idx: 24\n",
      "0.81\n",
      "Training Epoch: 989, total loss: 42.536421\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.8333333333333334\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.8125\n",
      "batch_idx: 6\n",
      "0.8035714285714286\n",
      "batch_idx: 7\n",
      "0.796875\n",
      "batch_idx: 8\n",
      "0.8101851851851852\n",
      "batch_idx: 9\n",
      "0.8083333333333333\n",
      "batch_idx: 10\n",
      "0.7992424242424242\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8108974358974359\n",
      "batch_idx: 13\n",
      "0.8095238095238095\n",
      "batch_idx: 14\n",
      "0.8138888888888889\n",
      "batch_idx: 15\n",
      "0.7942708333333334\n",
      "batch_idx: 16\n",
      "0.7867647058823529\n",
      "batch_idx: 17\n",
      "0.7893518518518519\n",
      "batch_idx: 18\n",
      "0.7872807017543859\n",
      "batch_idx: 19\n",
      "0.79375\n",
      "batch_idx: 20\n",
      "0.7976190476190477\n",
      "batch_idx: 21\n",
      "0.7916666666666666\n",
      "batch_idx: 22\n",
      "0.7934782608695652\n",
      "batch_idx: 23\n",
      "0.7986111111111112\n",
      "batch_idx: 24\n",
      "0.8\n",
      "Training Epoch: 990, total loss: 42.700423\n",
      "batch_idx: 0\n",
      "0.9166666666666666\n",
      "batch_idx: 1\n",
      "0.875\n",
      "batch_idx: 2\n",
      "0.9027777777777778\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8666666666666667\n",
      "batch_idx: 5\n",
      "0.8611111111111112\n",
      "batch_idx: 6\n",
      "0.8630952380952381\n",
      "batch_idx: 7\n",
      "0.8697916666666666\n",
      "batch_idx: 8\n",
      "0.875\n",
      "batch_idx: 9\n",
      "0.8583333333333333\n",
      "batch_idx: 10\n",
      "0.8598484848484849\n",
      "batch_idx: 11\n",
      "0.8576388888888888\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8422619047619048\n",
      "batch_idx: 14\n",
      "0.8444444444444444\n",
      "batch_idx: 15\n",
      "0.8307291666666666\n",
      "batch_idx: 16\n",
      "0.8308823529411765\n",
      "batch_idx: 17\n",
      "0.8379629629629629\n",
      "batch_idx: 18\n",
      "0.8399122807017544\n",
      "batch_idx: 19\n",
      "0.8458333333333333\n",
      "batch_idx: 20\n",
      "0.8353174603174603\n",
      "batch_idx: 21\n",
      "0.8295454545454546\n",
      "batch_idx: 22\n",
      "0.8242753623188406\n",
      "batch_idx: 23\n",
      "0.8263888888888888\n",
      "batch_idx: 24\n",
      "0.8283333333333334\n",
      "Training Epoch: 991, total loss: 42.262893\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.8472222222222222\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8680555555555556\n",
      "batch_idx: 6\n",
      "0.8571428571428571\n",
      "batch_idx: 7\n",
      "0.8489583333333334\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8333333333333334\n",
      "batch_idx: 12\n",
      "0.8301282051282052\n",
      "batch_idx: 13\n",
      "0.8303571428571429\n",
      "batch_idx: 14\n",
      "0.8388888888888889\n",
      "batch_idx: 15\n",
      "0.8463541666666666\n",
      "batch_idx: 16\n",
      "0.8504901960784313\n",
      "batch_idx: 17\n",
      "0.8495370370370371\n",
      "batch_idx: 18\n",
      "0.8442982456140351\n",
      "batch_idx: 19\n",
      "0.8479166666666667\n",
      "batch_idx: 20\n",
      "0.8531746031746031\n",
      "batch_idx: 21\n",
      "0.8503787878787878\n",
      "batch_idx: 22\n",
      "0.8514492753623188\n",
      "batch_idx: 23\n",
      "0.8541666666666666\n",
      "batch_idx: 24\n",
      "0.8533333333333334\n",
      "Training Epoch: 992, total loss: 41.952668\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8958333333333334\n",
      "batch_idx: 2\n",
      "0.8611111111111112\n",
      "batch_idx: 3\n",
      "0.8645833333333334\n",
      "batch_idx: 4\n",
      "0.8416666666666667\n",
      "batch_idx: 5\n",
      "0.8263888888888888\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.84375\n",
      "batch_idx: 8\n",
      "0.8518518518518519\n",
      "batch_idx: 9\n",
      "0.8541666666666666\n",
      "batch_idx: 10\n",
      "0.8522727272727273\n",
      "batch_idx: 11\n",
      "0.8541666666666666\n",
      "batch_idx: 12\n",
      "0.842948717948718\n",
      "batch_idx: 13\n",
      "0.8392857142857143\n",
      "batch_idx: 14\n",
      "0.8388888888888889\n",
      "batch_idx: 15\n",
      "0.8385416666666666\n",
      "batch_idx: 16\n",
      "0.8382352941176471\n",
      "batch_idx: 17\n",
      "0.8333333333333334\n",
      "batch_idx: 18\n",
      "0.8289473684210527\n",
      "batch_idx: 19\n",
      "0.83125\n",
      "batch_idx: 20\n",
      "0.8293650793650794\n",
      "batch_idx: 21\n",
      "0.8295454545454546\n",
      "batch_idx: 22\n",
      "0.8278985507246377\n",
      "batch_idx: 23\n",
      "0.8263888888888888\n",
      "batch_idx: 24\n",
      "0.8316666666666667\n",
      "Training Epoch: 993, total loss: 42.114536\n",
      "batch_idx: 0\n",
      "0.9583333333333334\n",
      "batch_idx: 1\n",
      "0.9583333333333334\n",
      "batch_idx: 2\n",
      "0.8888888888888888\n",
      "batch_idx: 3\n",
      "0.875\n",
      "batch_idx: 4\n",
      "0.8583333333333333\n",
      "batch_idx: 5\n",
      "0.8402777777777778\n",
      "batch_idx: 6\n",
      "0.8392857142857143\n",
      "batch_idx: 7\n",
      "0.8385416666666666\n",
      "batch_idx: 8\n",
      "0.8379629629629629\n",
      "batch_idx: 9\n",
      "0.8375\n",
      "batch_idx: 10\n",
      "0.8295454545454546\n",
      "batch_idx: 11\n",
      "0.8263888888888888\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8214285714285714\n",
      "batch_idx: 14\n",
      "0.8277777777777777\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8171296296296297\n",
      "batch_idx: 18\n",
      "0.8157894736842105\n",
      "batch_idx: 19\n",
      "0.8166666666666667\n",
      "batch_idx: 20\n",
      "0.8174603174603174\n",
      "batch_idx: 21\n",
      "0.821969696969697\n",
      "batch_idx: 22\n",
      "0.8188405797101449\n",
      "batch_idx: 23\n",
      "0.8211805555555556\n",
      "batch_idx: 24\n",
      "0.8233333333333334\n",
      "Training Epoch: 994, total loss: 42.295200\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.7777777777777778\n",
      "batch_idx: 3\n",
      "0.7916666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 4\n",
      "0.8166666666666667\n",
      "batch_idx: 5\n",
      "0.7708333333333334\n",
      "batch_idx: 6\n",
      "0.7857142857142857\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.7916666666666666\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.8090277777777778\n",
      "batch_idx: 12\n",
      "0.8076923076923077\n",
      "batch_idx: 13\n",
      "0.8005952380952381\n",
      "batch_idx: 14\n",
      "0.8\n",
      "batch_idx: 15\n",
      "0.8046875\n",
      "batch_idx: 16\n",
      "0.8063725490196079\n",
      "batch_idx: 17\n",
      "0.8009259259259259\n",
      "batch_idx: 18\n",
      "0.8026315789473685\n",
      "batch_idx: 19\n",
      "0.80625\n",
      "batch_idx: 20\n",
      "0.8055555555555556\n",
      "batch_idx: 21\n",
      "0.803030303030303\n",
      "batch_idx: 22\n",
      "0.7989130434782609\n",
      "batch_idx: 23\n",
      "0.7916666666666666\n",
      "batch_idx: 24\n",
      "0.7933333333333333\n",
      "Training Epoch: 995, total loss: 42.774713\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.8125\n",
      "batch_idx: 2\n",
      "0.8333333333333334\n",
      "batch_idx: 3\n",
      "0.8333333333333334\n",
      "batch_idx: 4\n",
      "0.85\n",
      "batch_idx: 5\n",
      "0.8194444444444444\n",
      "batch_idx: 6\n",
      "0.8333333333333334\n",
      "batch_idx: 7\n",
      "0.828125\n",
      "batch_idx: 8\n",
      "0.8333333333333334\n",
      "batch_idx: 9\n",
      "0.8333333333333334\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8298611111111112\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8303571428571429\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.8385416666666666\n",
      "batch_idx: 16\n",
      "0.8308823529411765\n",
      "batch_idx: 17\n",
      "0.8287037037037037\n",
      "batch_idx: 18\n",
      "0.831140350877193\n",
      "batch_idx: 19\n",
      "0.8354166666666667\n",
      "batch_idx: 20\n",
      "0.8373015873015873\n",
      "batch_idx: 21\n",
      "0.8352272727272727\n",
      "batch_idx: 22\n",
      "0.8387681159420289\n",
      "batch_idx: 23\n",
      "0.8333333333333334\n",
      "batch_idx: 24\n",
      "0.8266666666666667\n",
      "Training Epoch: 996, total loss: 42.114308\n",
      "batch_idx: 0\n",
      "0.75\n",
      "batch_idx: 1\n",
      "0.75\n",
      "batch_idx: 2\n",
      "0.7638888888888888\n",
      "batch_idx: 3\n",
      "0.78125\n",
      "batch_idx: 4\n",
      "0.8083333333333333\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.8095238095238095\n",
      "batch_idx: 7\n",
      "0.8125\n",
      "batch_idx: 8\n",
      "0.8240740740740741\n",
      "batch_idx: 9\n",
      "0.8291666666666667\n",
      "batch_idx: 10\n",
      "0.8333333333333334\n",
      "batch_idx: 11\n",
      "0.8368055555555556\n",
      "batch_idx: 12\n",
      "0.8301282051282052\n",
      "batch_idx: 13\n",
      "0.8303571428571429\n",
      "batch_idx: 14\n",
      "0.8361111111111111\n",
      "batch_idx: 15\n",
      "0.828125\n",
      "batch_idx: 16\n",
      "0.8333333333333334\n",
      "batch_idx: 17\n",
      "0.8333333333333334\n",
      "batch_idx: 18\n",
      "0.831140350877193\n",
      "batch_idx: 19\n",
      "0.8270833333333333\n",
      "batch_idx: 20\n",
      "0.8273809523809523\n",
      "batch_idx: 21\n",
      "0.8238636363636364\n",
      "batch_idx: 22\n",
      "0.8260869565217391\n",
      "batch_idx: 23\n",
      "0.8159722222222222\n",
      "batch_idx: 24\n",
      "0.8183333333333334\n",
      "Training Epoch: 997, total loss: 42.391002\n",
      "batch_idx: 0\n",
      "0.875\n",
      "batch_idx: 1\n",
      "0.8541666666666666\n",
      "batch_idx: 2\n",
      "0.875\n",
      "batch_idx: 3\n",
      "0.8541666666666666\n",
      "batch_idx: 4\n",
      "0.825\n",
      "batch_idx: 5\n",
      "0.7916666666666666\n",
      "batch_idx: 6\n",
      "0.7797619047619048\n",
      "batch_idx: 7\n",
      "0.7864583333333334\n",
      "batch_idx: 8\n",
      "0.8009259259259259\n",
      "batch_idx: 9\n",
      "0.8166666666666667\n",
      "batch_idx: 10\n",
      "0.8143939393939394\n",
      "batch_idx: 11\n",
      "0.8125\n",
      "batch_idx: 12\n",
      "0.8173076923076923\n",
      "batch_idx: 13\n",
      "0.8244047619047619\n",
      "batch_idx: 14\n",
      "0.825\n",
      "batch_idx: 15\n",
      "0.8255208333333334\n",
      "batch_idx: 16\n",
      "0.821078431372549\n",
      "batch_idx: 17\n",
      "0.8194444444444444\n",
      "batch_idx: 18\n",
      "0.8201754385964912\n",
      "batch_idx: 19\n",
      "0.8104166666666667\n",
      "batch_idx: 20\n",
      "0.8134920634920635\n",
      "batch_idx: 21\n",
      "0.8181818181818182\n",
      "batch_idx: 22\n",
      "0.8242753623188406\n",
      "batch_idx: 23\n",
      "0.8263888888888888\n",
      "batch_idx: 24\n",
      "0.8283333333333334\n",
      "Training Epoch: 998, total loss: 42.179733\n",
      "batch_idx: 0\n",
      "0.5833333333333334\n",
      "batch_idx: 1\n",
      "0.6666666666666666\n",
      "batch_idx: 2\n",
      "0.7361111111111112\n",
      "batch_idx: 3\n",
      "0.7604166666666666\n",
      "batch_idx: 4\n",
      "0.8\n",
      "batch_idx: 5\n",
      "0.7986111111111112\n",
      "batch_idx: 6\n",
      "0.8214285714285714\n",
      "batch_idx: 7\n",
      "0.7916666666666666\n",
      "batch_idx: 8\n",
      "0.7824074074074074\n",
      "batch_idx: 9\n",
      "0.7958333333333333\n",
      "batch_idx: 10\n",
      "0.803030303030303\n",
      "batch_idx: 11\n",
      "0.8159722222222222\n",
      "batch_idx: 12\n",
      "0.8205128205128205\n",
      "batch_idx: 13\n",
      "0.8125\n",
      "batch_idx: 14\n",
      "0.8222222222222222\n",
      "batch_idx: 15\n",
      "0.8125\n",
      "batch_idx: 16\n",
      "0.8186274509803921\n",
      "batch_idx: 17\n",
      "0.8263888888888888\n",
      "batch_idx: 18\n",
      "0.8245614035087719\n",
      "batch_idx: 19\n",
      "0.8270833333333333\n",
      "batch_idx: 20\n",
      "0.8313492063492064\n",
      "batch_idx: 21\n",
      "0.8295454545454546\n",
      "batch_idx: 22\n",
      "0.8315217391304348\n",
      "batch_idx: 23\n",
      "0.8298611111111112\n",
      "batch_idx: 24\n",
      "0.8283333333333334\n",
      "Training Epoch: 999, total loss: 42.186688\n"
     ]
    }
   ],
   "source": [
    "train_dataset,train_loader=prepare_dataset(nfm_config['train_data'],nfm_config['train_label'],nfm_config['batch_size'],nfm_config['n_class'])\n",
    "nfm = NFM(nfm_config).cuda()#加了device防止出现GPU CPU两种设备的错误提示\n",
    "print(\"nfm:\",nfm)\n",
    "train_data(nfm,train_loader,nfm_config['batch_size'],'dataset/xiaoguan/RF/RF_for_train/train_class_9/model/2_gene_4000_NFM.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77277ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280961eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77949c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f31e68a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NFM(\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      "  (linear_model1): Linear(in_features=4224, out_features=1000, bias=True)\n",
      "  (BN_linear1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear_model2): Linear(in_features=1000, out_features=100, bias=True)\n",
      "  (BN_linear2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (embedding_layers): Embedding(1001, 100)\n",
      "  (bi_pooling): BiInteractionPooling()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (BN_bi): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dnn_layers): ModuleList(\n",
      "    (0): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (1): Linear(in_features=100, out_features=9, bias=True)\n",
      "  )\n",
      "  (dnn_softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "##测试集\n",
    "from new_nfm_network import NFM\n",
    "#加载模型\n",
    "#from MLP import MLP\n",
    "#model=MLP(4224,1000,100,9)\n",
    "#model.cuda()\n",
    "path='dataset/xiaoguan/RF/RF_for_train/train_class_9/model/2_gene_4000_NFM.pkl'\n",
    "nfm = NFM(nfm_config).cuda()\n",
    "#net=model.cuda()\n",
    "net=nfm\n",
    "print(net)\n",
    "#net = nn.DataParallel(net)\n",
    "#net = net.to(device)\n",
    "net.load_state_dict(torch.load(path),strict=False)\n",
    "net.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=nfm_config['lr'], weight_decay=nfm_config['l2_regularization'])\n",
    "total = 0\n",
    "#loss_func = torch.nn.BCELoss()\n",
    "loss_func=torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16fe4181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row: 0 label: [0]\n",
      "row: 1 label: [0]\n",
      "row: 2 label: [0]\n",
      "row: 3 label: [0]\n",
      "row: 4 label: [0]\n",
      "row: 5 label: [0]\n",
      "row: 6 label: [0]\n",
      "row: 7 label: [0]\n",
      "row: 8 label: [0]\n",
      "row: 9 label: [0]\n",
      "row: 10 label: [0]\n",
      "row: 11 label: [0]\n",
      "row: 12 label: [1]\n",
      "row: 13 label: [1]\n",
      "row: 14 label: [1]\n",
      "row: 15 label: [1]\n",
      "row: 16 label: [1]\n",
      "row: 17 label: [1]\n",
      "row: 18 label: [1]\n",
      "row: 19 label: [1]\n",
      "row: 20 label: [1]\n",
      "row: 21 label: [1]\n",
      "row: 22 label: [1]\n",
      "row: 23 label: [1]\n",
      "row: 24 label: [1]\n",
      "row: 25 label: [2]\n",
      "row: 26 label: [2]\n",
      "row: 27 label: [2]\n",
      "row: 28 label: [3]\n",
      "row: 29 label: [3]\n",
      "row: 30 label: [3]\n",
      "row: 31 label: [4]\n",
      "row: 32 label: [4]\n",
      "row: 33 label: [5]\n",
      "row: 34 label: [5]\n",
      "row: 35 label: [6]\n",
      "row: 36 label: [6]\n",
      "row: 37 label: [6]\n",
      "row: 38 label: [6]\n",
      "row: 39 label: [6]\n",
      "row: 40 label: [6]\n",
      "row: 41 label: [6]\n",
      "row: 42 label: [6]\n",
      "row: 43 label: [6]\n",
      "row: 44 label: [6]\n",
      "row: 45 label: [6]\n",
      "row: 46 label: [6]\n",
      "row: 47 label: [6]\n",
      "row: 48 label: [6]\n",
      "row: 49 label: [6]\n",
      "row: 50 label: [6]\n",
      "row: 51 label: [6]\n",
      "row: 52 label: [6]\n",
      "row: 53 label: [6]\n",
      "row: 54 label: [6]\n",
      "row: 55 label: [6]\n",
      "row: 56 label: [6]\n",
      "row: 57 label: [6]\n",
      "row: 58 label: [6]\n",
      "row: 59 label: [6]\n",
      "row: 60 label: [7]\n",
      "row: 61 label: [7]\n",
      "row: 62 label: [7]\n",
      "row: 63 label: [7]\n",
      "row: 64 label: [8]\n",
      "row: 65 label: [8]\n",
      "row: 66 label: [8]\n",
      "row: 67 label: [8]\n",
      "row: 68 label: [8]\n",
      "row: 69 label: [8]\n",
      "row: 70 label: [8]\n",
      "row: 71 label: [8]\n",
      "row: 72 label: [8]\n",
      "row: 73 label: [8]\n",
      "row: 74 label: [8]\n",
      "row: 75 label: [8]\n",
      "row: 76 label: [8]\n",
      "label: [[0.82222223 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.82222223 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.82222223 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.82222223 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.82222223 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.82222223 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.82222223 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.82222223 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.82222223 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.82222223 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.82222223 0.02222222\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.82222223\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.82222223\n",
      "  0.02222222 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.82222223 0.02222222 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.82222223 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.82222223 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.82222223 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.82222223 0.02222222]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]\n",
      " [0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      "  0.02222222 0.02222222 0.82222223]]\n",
      "features: [[34 42 25 ... 26 25 27]\n",
      " [34 42 28 ... 30 25 26]\n",
      " [35 41 25 ... 26 24 28]\n",
      " ...\n",
      " [49 43 20 ...  8 29 30]\n",
      " [45 39 13 ... 13 28 34]\n",
      " [45 47  9 ...  9 29 37]]\n"
     ]
    }
   ],
   "source": [
    "#数据集准备测试\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "#3from tensorboardX import SummaryWriter\n",
    "\n",
    "#import network\n",
    "import config\n",
    "import evaluate\n",
    "import data_utils\n",
    "import Trainer\n",
    "from dataset_process import FMData\n",
    "\n",
    "'''  \n",
    "test_dataset = FMData(nfm_config['test_data'],nfm_config['test_label'],nfm_config['n_class'])\n",
    "test_loader = data.DataLoader(test_dataset, drop_last=True,batch_size=100,shuffle=True, num_workers=4)\n",
    "\n",
    "'''\n",
    "test_dataset,test_loader=prepare_dataset(nfm_config['test_data'],nfm_config['test_label'],nfm_config['batch_size'],nfm_config['n_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2c7995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#版权声明：本文为CSDN博主「山阴少年」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。  \n",
    "#原文链接：https://blog.csdn.net/jclian91/article/details/121708431#   \n",
    "from torch.autograd import Variable   \n",
    "from torch.utils.data import DataLoader   \n",
    "from sklearn.metrics import roc_auc_score   \n",
    "from sklearn.metrics import accuracy_score   \n",
    "        \n",
    "        \n",
    "def evaluate_model(test_dl , model):   \n",
    "    predictions,  actuals = [] , []   \n",
    "    for i , (inputs , targets) in enumerate(test_dl):   \n",
    "        # evaluate the model on the test set   \n",
    "        #print(\\ inputs:\\  inputs)   \n",
    "        #print(\\ targets:\\  targets)   \n",
    "        inputs = Variable(inputs)   \n",
    "        targets = Variable(targets)   \n",
    "                    \n",
    "                    \n",
    "        #x = torch.tensor(x  dtype=torch.float)   \n",
    "        #x=x.clone().detach().requires_grad_(True)   \n",
    "        inputs=torch.tensor(inputs ,dtype=torch.float)   \n",
    "        targets=torch.tensor(targets ,dtype=torch.float)   \n",
    "        inputs , targets = inputs.cuda(),  targets.cuda()   \n",
    "        yhat = model(inputs)   \n",
    "        # retrieve numpy array   \n",
    "        #yhat = yhat.detach().numpy()   \n",
    "        yhat = yhat.detach().cpu().numpy()#转换到cpu   \n",
    "        # yhat=yhat.argmax(axis=1)   \n",
    "        #print(' yhat:',  yhat)   \n",
    "        #print('yhat.shape:' ,yhat.shape)   \n",
    "        actual = targets.detach().cpu().numpy()   \n",
    "        actual=actual.round()   \n",
    "        #print('actual:',  actual)   \n",
    "        #print('actual.shape:', actual.shape)   \n",
    "        #actual = actual.reshape(-1  1)   \n",
    "        # round to class values   \n",
    "        yhat = yhat.round()   \n",
    "        # store   \n",
    "        predictions.append(yhat)   \n",
    "        actuals.append(actual)   \n",
    "    #print('prediction:',  predictions)   \n",
    "    #print('actuals:',  actuals)   \n",
    "    predictions , actuals = np.vstack(predictions) , np.vstack(actuals)   \n",
    "    #print('prediction:',  predictions)   \n",
    "    #print('actuals:',  actuals)   \n",
    "    # calculate accuracy   \n",
    "    acc_test = accuracy_score(actuals , predictions)   \n",
    "    return  actuals , predictions ,acc_test \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62fa1229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/zhengfang/.local/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_test 0.4861111111111111\n"
     ]
    }
   ],
   "source": [
    "actuals,predictions,acc_test=evaluate_model(test_loader,net)\n",
    "print(\"acc_test\",acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d741ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
