{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bc2902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load model.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# %load model1.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NFM(nn.Module):\n",
    "    def __init__(self, num_features, num_factors, \n",
    "        act_function, layers, batch_norm, drop_prob, pretrain_FM):\n",
    "        super(NFM, self).__init__()\n",
    "        \"\"\"\n",
    "        num_features: number of features,\n",
    "        num_factors: number of hidden factors,\n",
    "        act_function: activation function for MLP layer,\n",
    "        layers: list of dimension of deep layers,\n",
    "        batch_norm: bool type, whether to use batch norm or not,\n",
    "        drop_prob: list of the dropout rate for FM and MLP,\n",
    "        pretrain_FM: the pre-trained FM weights.\n",
    "        \"\"\"\n",
    "        self.num_features = num_features\n",
    "        self.num_factors = num_factors\n",
    "        self.act_function = act_function\n",
    "        self.layers = layers\n",
    "        self.batch_norm = batch_norm\n",
    "        self.drop_prob = drop_prob\n",
    "        self.pretrain_FM = pretrain_FM\n",
    "\n",
    "        self.embeddings = nn.Embedding(num_features, num_factors)\n",
    "        self.biases = nn.Embedding(num_features, 1)\n",
    "        self.bias_ = nn.Parameter(torch.tensor([0.0]))\n",
    "\n",
    "        FM_modules = []\n",
    "        if self.batch_norm:\n",
    "            FM_modules.append(nn.BatchNorm1d(num_factors))\t\t\n",
    "        FM_modules.append(nn.Dropout(drop_prob[0]))\n",
    "        self.FM_layers = nn.Sequential(*FM_modules)\n",
    "\n",
    "        MLP_module = []\n",
    "        in_dim = num_factors\n",
    "        for dim in self.layers:\n",
    "            out_dim = dim\n",
    "            MLP_module.append(nn.Linear(in_dim, out_dim))\n",
    "            in_dim = out_dim\n",
    "\n",
    "            if self.batch_norm:\n",
    "                MLP_module.append(nn.BatchNorm1d(out_dim))\n",
    "            if self.act_function == 'relu':\n",
    "                MLP_module.append(nn.ReLU())\n",
    "            elif self.act_function == 'sigmoid':\n",
    "                MLP_module.append(nn.Sigmoid())\n",
    "            elif self.act_function == 'tanh':\n",
    "                MLP_module.append(nn.Tanh())\n",
    "            elif self.act_function == 'softmax':\n",
    "                MLP_module.append(F.softmax())#修改\n",
    "\n",
    "            MLP_module.append(nn.Dropout(drop_prob[-1]))\n",
    "        self.deep_layers = nn.Sequential(*MLP_module)\n",
    "\n",
    "        predict_size = layers[-1] if layers else num_factors\n",
    "        self.prediction = nn.Linear(predict_size, 1, bias=False)\n",
    "\n",
    "        self._init_weight_()\n",
    "\n",
    "    def _init_weight_(self):\n",
    "        \"\"\" Try to mimic the original weight initialization. \"\"\"\n",
    "        if self.pretrain_FM:\n",
    "            self.embeddings.weight.data.copy_(\n",
    "                            self.pretrain_FM.embeddings.weight)\n",
    "            self.biases.weight.data.copy_(\n",
    "                            self.pretrain_FM.biases.weight)\n",
    "            self.bias_.data.copy_(self.pretrain_FM.bias_)\n",
    "        else:\n",
    "            nn.init.normal_(self.embeddings.weight, std=0.01)\n",
    "            nn.init.constant_(self.biases.weight, 0.0)\n",
    "\n",
    "        # for deep layers\n",
    "        if len(self.layers) > 0:\n",
    "            for m in self.deep_layers:\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_normal_(m.weight)\n",
    "            nn.init.xavier_normal_(self.prediction.weight)\n",
    "        else:\n",
    "            nn.init.constant_(self.prediction.weight, 1.0)\n",
    "\n",
    "    def forward(self, features, feature_values):\n",
    "        nonzero_embed = self.embeddings(features)\n",
    "        feature_values = feature_values.unsqueeze(dim=-1)\n",
    "        nonzero_embed = nonzero_embed * feature_values\n",
    "\n",
    "        # Bi-Interaction layer\n",
    "        sum_square_embed = nonzero_embed.sum(dim=1).pow(2)\n",
    "        square_sum_embed = (nonzero_embed.pow(2)).sum(dim=1)\n",
    "\n",
    "        # FM model\n",
    "        FM = 0.5 * (sum_square_embed - square_sum_embed)\n",
    "        FM = self.FM_layers(FM)\n",
    "        if self.layers: # have deep layers\n",
    "            FM = self.deep_layers(FM)\n",
    "        FM = self.prediction(FM)\n",
    "\n",
    "        # bias addition\n",
    "        feature_bias = self.biases(features)\n",
    "        feature_bias = (feature_bias * feature_values).sum(dim=1)\n",
    "        FM = FM + feature_bias + self.bias_\n",
    "        return FM.view(-1)\n",
    "\n",
    "\n",
    "class FM(nn.Module):\n",
    "    def __init__(self, num_features, num_factors, batch_norm, drop_prob):\n",
    "        super(FM, self).__init__()\n",
    "        \"\"\"\n",
    "        num_features: number of features,\n",
    "        num_factors: number of hidden factors,\n",
    "        batch_norm: bool type, whether to use batch norm or not,\n",
    "        drop_prob: list of the dropout rate for FM and MLP,\n",
    "        \"\"\"\n",
    "        self.num_features = num_features\n",
    "        self.num_factors = num_factors\n",
    "        self.batch_norm = batch_norm\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        self.embeddings = nn.Embedding(num_features, num_factors)\n",
    "        self.biases = nn.Embedding(num_features, 1)\n",
    "        self.bias_ = nn.Parameter(torch.tensor([0.0]))\n",
    "\n",
    "        FM_modules = []\n",
    "        if self.batch_norm:\n",
    "            FM_modules.append(nn.BatchNorm1d(num_factors))\t\t\n",
    "        FM_modules.append(nn.Dropout(drop_prob[0]))\n",
    "        self.FM_layers = nn.Sequential(*FM_modules)\n",
    "\n",
    "        nn.init.normal_(self.embeddings.weight, std=0.01)\n",
    "        nn.init.constant_(self.biases.weight, 0.0)\n",
    "\n",
    "\n",
    "    def forward(self, features, feature_values):\n",
    "        nonzero_embed = self.embeddings(features)\n",
    "        feature_values = feature_values.unsqueeze(dim=-1)\n",
    "        nonzero_embed = nonzero_embed * feature_values\n",
    "\n",
    "        # Bi-Interaction layer\n",
    "        sum_square_embed = nonzero_embed.sum(dim=1).pow(2)\n",
    "        square_sum_embed = (nonzero_embed.pow(2)).sum(dim=1)\n",
    "\n",
    "        # FM model\n",
    "        FM = 0.5 * (sum_square_embed - square_sum_embed)\n",
    "        FM = self.FM_layers(FM).sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # bias addition\n",
    "        feature_bias = self.biases(features)\n",
    "        feature_bias = (feature_bias * feature_values).sum(dim=1)\n",
    "        FM = FM + feature_bias + self.bias_\n",
    "        return FM.view(-1)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56447e29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
